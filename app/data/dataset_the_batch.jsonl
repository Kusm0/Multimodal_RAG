{"id": 66157642001, "type": "news_chunk", "title": "A Solid Foundation for a Rewarding Career", "subtitle": "A Solid Foundation for a Rewarding Career", "content": "Dear friends, Years ago, I had to choose between a neural network and a decision tree learning algorithm. It was necessary to pick an efficient one, because we planned to apply the algorithm to a very large set of users on a limited compute budget. I went with a neural network. I hadn’t used boosted decision trees in a while, and I thought they required more computation than they actually do — so I made a bad call. Fortunately, my team quickly revised my decision, and the project was successful. This experience was a lesson in the importance of learning, and continually refreshing, foundational knowledge. If I had refreshed my familiarity with boosted trees, I would have made a better decision. Machine learning, like many technical fields, evolves as the community of researchers builds on top of one another's work. Some contributions have staying power and become the basis of further developments. Consequently, everything from a housing-price predictor to a text-to-image generator is built on core ideas that include algorithms (linear and logistic regression, decision trees, and so on) and concepts (regularization, optimizing a loss function, bias/variance, and the like). A solid, up-to-date foundation is one key to being a productive machine learning engineer. Many teams draw on these ideas in their day-to-day work, and blog posts and research papers often assume that you’re familiar with them. This shared base of knowledge is essential to the rapid progress we've seen in machine learning in recent years. That's why I’m updating my original machine learning class as the new Machine Learning Specialization, which will be available in a few weeks. My team spent many hours debating the most important concepts to teach. We developed extensive syllabi for various topics and prototyped course units in them. Sometimes this process helped us realize that a different topic was more important, so we cut material we had developed to focus on something else. The result, I hope, is an accessible set of courses that will help anyone master the most important algorithms and concepts in machine learning today — including deep learning but also a lot of other things — and to build effective learning systems. In that spirit, this week’s issue of The Batch explores some of our field’s most important algorithms, explaining how they work and describing some of their surprising origins. If you’re just starting out, I hope it will demystify some of the approaches at the heart of machine learning. For those who are more advanced, you’ll find lesser-known perspectives on familiar territory. Either way, I hope this special issue will help you build your intuition and give you fun facts about machine learning’s foundations that you can share with friends. Keep learning!", "image_caption": "An illustration of Andrew Ng thinking looking at Neural Networks", "metadata": {"article_id": "a_solid_foundation_for_a_rewarding_career", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F07%2FANDREW-atWhiteBoard-QuestionMARK_1200px-1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/a-solid-foundation-for-a-rewarding-career/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/a_solid_foundation_for_a_rewarding_career.html"}}
{"id": 3740276001, "type": "news_chunk", "title": "Agentic Workflow Generates Novel Scientific Research Papers", "subtitle": "Agentic Workflow Generates Novel Scientific Research Papers", "content": "While some observers argue that large language models can’t produce truly original output, new work prompted them to generate novel scientific research. What’s new: Researchers proposed AI Scientist, an agentic workflow that directs large language models to generate ideas for AI research, produce code to test them, and document the enquiry. You can see examples of its output and download the code to generate your own papers here. The team included Chris Lu, Cong Lu, Robert Tjarko Lange, and colleagues at Tokyo-based startup Sakana AI, University of Oxford, University of British Columbia, Vector Institute, and the Canadian Institute for Advanced Research. How it works: The authors used Claude Sonnet 3.5, GPT-4o, DeepSeek Coder, and LLama 3.1 405B to generate papers in three categories: diffusion image modeling, transformer-based language modeling, and “grokking,” which the authors define as generalization and speed of learning in deep neural networks. The authors prompted a given large language model (LLM) to generate “the next creative and impactful idea for research” in one of the three categories. Then they provided an API to search papers and asked the LLM to either determine whether its idea was novel (in which case it moved to the next step) or, if it couldn’t determine an answer, generate a search query to find related works. Then the authors asked again in light of the search results. They repeated this process until the LLM made a decision.Once they had a novel idea, they prompted the LLM to generate a list of experiments and run them using the Aider Python library. Then they prompted it to generate notes about the results and generate figures by altering an existing Python script.They prompted the LLM to generate a paper, one section at a time, given the notes, figures, sections generated so far, and tips on how to write a paper based on an existing guide. Then they prompted it to search for related works and add relevant citations. Finally, they asked it to remove redundancy, reduce verbosity, and finalize the document’s format. Results: The team used GPT-4o to evaluate the generated papers according to the guidelines for papers presented at the Neural Information Processing Systems (NeurIPS) conference. The guidelines include an overall score between 1 (very strongly reject) and 10 (award-quality: flawless and groundbreaking) and a decision to reject or accept the paper. Of the four LLMs, Claude Sonnet 3.5 performed best. Its highest-scoring papers achieved 6 (weak accept). With respect to one of Claude’s works, the authors wrote, “The AI Scientist correctly identifies an interesting and well-motivated direction in diffusion modeling research . . . It proposes a comprehensive experimental plan to investigate its idea, and successfully implements it all, achieving good results.\" The authors provide an archive of Claude’s output here. GPT-4o ranked second. Its highest-scoring paper achieved 5 (borderline accept).The generated papers achieved an average score of 4.05 or less (4 is borderline reject) across all models and categories of experiment. The experiments generally involved small networks that were trained and tested on generated data. The authors note that the system often failed to implement its ideas, sometimes fabricated results, and sometimes failed to cite the most relevant papers, among other issues. Why it matters: Agentic workflows are a rising theme in AI research from simpler design patterns like reflection to complex workflows for translating literature. These workflows make it possible to break down complex problems into more manageable subtasks. By breaking the task of conducting AI research into various stages of generating ideas, testing them, and writing a paper, an LLM that has access to the right tools can generate novel research papers with actual experimental results. We’re thinking: Rather than merely synthesizing existing knowledge, this work points a fascinating direction for using AI to generate new knowledge! Right now, an LLM can suggest starting points for human researchers along with experiments that back up its suggestions.", "image_caption": "Conceptual illustration of The A I Scientist, an end-to-end LLM-driven scientific discovery process.", "metadata": {"article_id": "agentic_workflow_generates_novel_scientific_research_papers", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2Funnamed---2024-08-21T140739.984-1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/agentic-workflow-generates-novel-scientific-research-papers/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/agentic_workflow_generates_novel_scientific_research_papers.html"}}
{"id": 33077828001, "type": "news_chunk", "title": "AI Agents and Infrastructure Dominate CB Insights’ Top 100 AI Startups List", "subtitle": "AI Agents and Infrastructure Dominate CB Insights’ Top 100 AI Startups List", "content": "AI agents and infrastructure made a strong showing on CB Insights’s latest list of the top 100 AI startups. What’s new: CB Insights, which tracks tech startups and venture capital, selected companies in the AI 100 based on their market traction, talent, finances, and partnerships. The list purports to highlight the next wave of winners, shedding light on the key executives, investors, fundraising, and valuations behind up-and-coming AI ventures. How it works: The analysts evaluated 17,000 early-stage, private AI companies that had raised funds within the last year and continue to seek further investment. CB Insights evaluated the startups according to its own Mosaic Score, a proprietary system designed to assess the health and growth potential of private companies. The score takes into account a startup’s market momentum (traction and growth rate), market size, financial health, and management team.The analysts divided their choices into three broad categories: (i) horizontal (providing business products or services common to multiple industries), (ii) vertical (serving a single industry or business function), or (iii) providers of AI hardware or software infrastructure.They further divided the horizontal companies by business function (customer service, cybersecurity, software development, and so on), the vertical companies into industries (healthcare, automotive, aerospace, manufacturing, finance, energy, and the like), and the infrastructure providers into segments (hardware, monitoring, data, and development and training). Where the action is: This year’s AI 100 companies are based in 14 countries, around two-thirds of them in the United States. 10 are based in the United Kingdom, five in France, and four in Germany, with one each in Norway (Braintrust), Singapore (Bria), Spain (Cartwheel), Sweden (Chainguard), and Switzerland (Clarium). More than 20 percent of this year’s AI 100 build AI agents or support them, including Texas-based Apptronik (valued at $423 million) and Canada’s 1X ($134 million, the second-most highly valued agent specialist).The report also notes the rapid growth of companies that monitor AI performance and reliability, such as California-based Arize (valued at $131 million) and the French startup Bioptimus ($76 million).Opportunity may be rising for AI companies that cater to specific industries. This year, the vertical companies pulled in the most total funding, just over $1 billion. These included the Texas aerospace specialist Saronic (valued at $4 billion) and the California software development and training provider Together.AI ($3.3 billion).The AI infrastructure category raised the second-highest total funding, a leading indicator of need for infrastructure as businesses take advantage of the technology. Infrastructure companies on the list were led by Munich’s defense startup Helsing (valued at $5.37 billion), California robot maker Figure ($2.77 billion) and Washington-state cybersecurity provider Chainguard ($1.12 billion). Why it matters: This year’s AI 100 offers a snapshot of AI becoming more central to businesses of all kinds. Most of the startups listed here offer practical products and services that are poised to deliver a timely return, rather than moonshots with long development cycles and risky payoffs. In addition, they mostly target corporate customers rather than consumers. We’re thinking: The falling cost of access to AI models and increasingly capable open-weights models make this the perfect time to build applications. What kind? The report singles out health care (8 companies) and life sciences (6 companies) as growing areas, but it also documents opportunities in defense, gaming, and finance.", "image_caption": "CB Insights AI 100 2025 infographic showing top AI startups across sectors like healthcare, robotics, and infrastructure.", "metadata": {"article_id": "ai_agents_and_infrastructure_dominate_cb_insights_top_100_ai_startups_list", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--82-.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/ai-agents-and-infrastructure-dominate-cb-insights-top-100-ai-startups-list/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/ai_agents_and_infrastructure_dominate_cb_insights_top_100_ai_startups_list.html"}}
{"id": 70807762001, "type": "news_chunk", "title": "AI and Data Center Boom Challenges Big Tech's Emissions Targets", "subtitle": "AI and Data Center Boom Challenges Big Tech's Emissions Targets", "content": "The boom in AI is jeopardizing big tech’s efforts to reach its targets for emissions of greenhouse gasses. What’s new: Google’s annual environmental report shows that the company’s total carbon dioxide emissions rose nearly 50 percent between 2019 and 2023 to 14.3 million tons. Google attributes the rise to its efforts to satisfy rising demand for AI. How it works: Google’s carbon emissions increased 16.7 percent from 2021 to 2022 and another 13.5 percent from 2022 to 2023 for a total 48 percent rise over those periods. “As we further integrate AI into our products, reducing emissions may be challenging due to increasing energy demands from the greater intensity of AI compute, and the emissions associated with the expected increases in our technical infrastructure investment,” the report states. Three-quarters of total emissions, or 10.8 million tons, are associated with purchases that include the data-center hardware and construction. These emissions increased 23 percent from 2019 to 2023 and 8 percent year-over-year.Powering, heating, and cooling data centers and other facilities accounted for around a quarter of Google’s 2023 emissions. Emissions from these activities have increased more than four-fold since 2019.Low-emissions energy has reduced Google’s total data-center emissions substantially, but some regions don’t have enough of it to meet demand. Solar, wind, hydro, geothermal, and nuclear energy account for most of the energy consumed by Google’s data centers in Europe, Canada, and South America. However, these sources account for less than 5 percent in Singapore, Qatar, and Saudi Arabia. Countering the trend: Google is working to reduce its greenhouse gas emissions on several fronts. Its effort to purchase electricity from low-emissions sources cut its net carbon footprint by around 30 percent in 2023. It claims that its owned-and-operated data centers are 1.8 times more energy-efficient than a typical enterprise data center, and its sixth-generation tensor processing units (TPUs) are 67 percent more efficient than the prior generation. Google has asked its largest hardware partners to match 100 percent of their energy consumption with renewable energy 2029. The company is pursuing several AI-based initiatives to mitigate climate change from weather prediction to fuel-efficient vehicle routing. It says that AI has the potential to mitigate 5 to 10 percent of global greenhouse gas emissions by 2030. Behind the news: In 2020, after five years of successfully reducing its carbon footprint, Google set an ambitious target to reach net-zero greenhouse gas emissions by 2030. But its total emissions since then have risen each year. Google’s experience mirrors that of Amazon and Microsoft, which aim to reach net-zero carbon emissions by 2030 and 2040 respectively. Amazon’s emissions increased 39 percent from 2019 to 2022, while Microsoft’s emissions rose 29 percent between 2020 and 2023. (Amazon’s and Microsoft’s cloud computing revenues were roughly triple Google’s in 2023 and thus their AI-related greenhouse case emissions presumably were larger.) Why it matters: Growing use of AI means greater consumption of energy. The tech giants’ ambitious emissions goals predate the rapid growth of generative AI, and their latest reports show that it’s time to rethink them. This adds urgency to already critical efforts to develop renewable and other low-emissions energy sources. We’re thinking: We applaud Google’s efforts to cut its carbon emissions and its transparency in issuing annual environmental reports. We’re somewhat relieved to note that, for now, data centers and cloud computing are responsible for 1 percent of the world’s energy-related greenhouse gas emissions; a drop in the bucket compared to transportation, construction, or agriculture. Moreover, we believe that AI stands to create huge benefits relative to the climate impact of its emissions, and AI is one of the most powerful tools we have to develop low-carbon energy sources and boost energy efficiency throughout society. Continuing to improve the technology will help us develop lower-carbon energy sources and efficient ways to harness them.", "image_caption": "AI’s Path to Zero Emissions Is Cloudy: AI and data center boom challenges big tech's emissions targets", "metadata": {"article_id": "ai_and_data_center_boom_challenges_big_techs_emissions_targets", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F07%2Funnamed--70--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/ai-and-data-center-boom-challenges-big-techs-emissions-targets/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/ai_and_data_center_boom_challenges_big_techs_emissions_targets.html"}}
{"id": 19346344001, "type": "news_chunk", "title": "AI Co-Scientist, An Agent That Generates Research Hypotheses, Aiding Drug Discovery", "subtitle": "AI Co-Scientist, An Agent That Generates Research Hypotheses, Aiding Drug Discovery", "content": "An AI agent synthesizes novel scientific research hypotheses. It's already making an impact in biomedicine. What’s new: Google introduced AI co-scientist, a general multi-agent system designed to generate in-depth research proposals within constraints specified by the user. The team generated and evaluated proposals for repurposing drugs, identifying drug targets, and explaining antimicrobial resistance in real-world laboratories. It’s available to research organizations on a limited basis. How it works: AI co-scientist accepts a text description of a research goal, including relevant constraints or ideas. In response, it generates research proposals and reviews, ranks, and improves them using seven agents based on Google’s Gemini 2.0 family of large language models. The completed proposals include sections that explain background, unmet needs, a proposed solution, goals, hypotheses, reasoning, study steps, and relevant articles. The agents take feedback and outputs from other agents to perform their prompted task simultaneously. The supervisor agent periodically determines how often to run the other six agents, how important their output is, and whether the system is finished. To accomplish this, it computes statistics that represent the number of proposals generated so far, how many have been reviewed, and so on.The generation agent generates a list of proposals. It searches the web for relevant research articles, identifies testable assumptions, and debates with itself to improve ambiguous statements and adhere to constraints.The reflection agent filters the generated proposals according to correctness, quality, safety, and novelty. First, it reviews a proposal without web search and discards obviously bad proposals. Then it reviews each proposal against literature it finds online. It breaks down and checks the proposal’s assumptions, checks whether the proposal might explain some observations in previous work, and simulates the proposed experiment (via text generation, similar to how a person performs a thought experiment).The proximity agents compute similarity between proposals to avoid redundancy.The ranking agent determines the best proposals according to a tournament. It examines one pair of proposals at a time (including reviews from the reflection agent) and debates itself to pick the better one. To save computation, it prioritizes comparing similar proposals, new proposals, and highest-ranking proposals.The evolution agent generates new proposals by improving existing ones. It does this in several different ways, including simplifying current ideas, combining top-ranking ideas, and generating proposals that are very different from current ones.The meta-review agent identifies common patterns in the reflection agent’s reviews and the ranking agent’s debates. Its feedback goes to the reflection and generation agents, which use it to address common factors in future reviews and avoid generating similar proposals, respectively. Results: AI co-scientist achieved a number of impressive biomedical results in tests. Google researchers generated proposals for experiments that would repurpose drugs to treat acute myeloid leukemia. They shared the 30 highest-ranked proposals with human experts, who chose five for lab tests. Of the five drugs tested, three killed acute myeloid leukemia cells.Experts selected three among 15 top-ranked generated proposals that proposed repurposing existing drugs to treat liver fibrosis. Two significantly inhibited liver fibrosis without being toxic to general cells. (Prior to this research, one of the drugs was approved by the United States Food and Drug Administration for a different illness, which may lead to a new treatment for liver fibrosis.)AI co-scientist invented a hypothesis to explain how microbes become resistant to antibiotics. Human researchers had proposed and experimentally validated the same hypothesis, but their work had not yet been published at the time, and AI co-scientist did not have access to it. Behind the news: A few AI systems have begun to produce original scientific work. For instance, a model generated research proposals that human judges deemed more novel than proposals written by flesh-and-blood scientists, and an agentic workflow produced research papers that met standards for acceptance by top conferences. Why it matters: While previous work used agentic workflows to propose research ideas on a general topic, this work generates proposals for specific ideas according to a researcher’s constraints (for example, a researcher could specify that a novel medical treatment for a specific disease only consider drugs already approved for human trials for other uses) and further instructions. AI co-scientist can take feedback at any point, allowing humans to collaborate with the machine: People provide ideas, feedback, and guidance for the model, and the model researches and proposes ideas in return. We’re thinking: I asked my AI system to propose a new chemical experiment. But there was no reaction!", "image_caption": "AI co-scientist workflow diagram showing a research goal assigned to specialized AI agents for hypothesis testing and ranking", "metadata": {"article_id": "ai_co_scientist_an_agent_that_generates_research_hypotheses_aiding_drug_discovery", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F03%2Funnamed--65--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/ai-co-scientist-an-agent-that-generates-research-hypotheses-aiding-drug-discovery/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/ai_co_scientist_an_agent_that_generates_research_hypotheses_aiding_drug_discovery.html"}}
{"id": 9250963001, "type": "news_chunk", "title": "AI Creates an Interactive Minecraft-Like World in Real Time", "subtitle": "AI Creates an Interactive Minecraft-Like World in Real Time", "content": "A real-time video generator lets you explore an open-ended, interactive virtual world — a video game without a game engine. What’s new: Decart, a startup that’s building a platform for AI applications, and Etched, which designs specialized AI chips, introduced Oasis, which generates a Minecraft-like game in real time. The weights are open and available here. You can play with a demo here. How it works: The system generates one frame at a time based on a user’s keystrokes, mouse movements, and previously generated frames. The training dataset is undisclosed, but it’s almost certainly based on videos of Minecraft gameplay, given the output’s striking semblance to that game. Some recent video generators produce an initial frame, then the nth frame, and then the frames in between. This approach isn’t practical for real-time gameplay. Instead, Oasis learned to generate the next frame. A ViT encoder embeds previously generated frames. Given those embeddings, an embedding of a frame to which noise had been added, and a user’s input, a diffusion transformer learned to remove the noise using a variation on diffusion called diffusion forcing.Generated frames may contain glitches, and such errors can snowball if the model incorporates glitches from previous frames into subsequent frames. To avoid this, during training, the system added noise to embeddings of previous frames before feeding them to the transformer to generate the next frame. This way, the transformer learned to ignore glitches while producing new frames.At inference, the ViT encoder embeds previously generated frames, and the system adds noise to the frame embeddings. Given the user’s input, the noisy frame embeddings, and a pure-noise embedding that represents the frame to be generated, the transformer iteratively removes the noise from the previous and current frame embeddings. The ViT’s decoder takes the denoised current frame embedding and produces an image.The system currently runs on Nvidia H100 GPUs using Decart’s inference technology, which is tuned to run transformers on that hardware. The developers aim to change the hardware to Etched’s Sohu chips, which are specialized for transformers and process Llama 70B at a jaw-dropping 500,000 tokens per second. Results: The Oasis web demo enables users to interact with 360-by-360-pixel frames at 20 frames per second. Users can place blocks, place fences, and move through a Minecraft-like world. The demo starts with an image of a location, but users can upload an image (turning, say, a photo of your cat into a blocky Minecraft-style level, as reported by Wired). Yes, but: The game has its fair share of issues. For instance, objects disappear and menus items change unaccountably. The world’s physics are similarly inconsistent. For instance, players don’t fall into holes dug directly beneath them and, after jumping into water, players are likely to find themselves standing on a blue floor. Behind the news: In February, Google announced Genie, a model that generates two-dimensional platformer games from input images. We weren’t able to find a publicly available demo or model. Why it matters: Oasis is more a proof of concept than a product. Nonetheless, as an open-world video game entirely generated by AI — albeit based on data produced by a traditional implementation — it sets a bar for future game generators. We’re thinking: Real-time video generation suggests a wealth of potential applications — say, a virtual workspace for interior decorating that can see and generate your home, or an interactive car repair manual that can create custom clips based on your own vehicle. Oasis is an early step in this direction.", "image_caption": "Comparison of Minecraft terrain with and without player modifications.", "metadata": {"article_id": "ai_creates_an_interactive_minecraft_like_world_in_real_time", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2Funnamed--32--1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/ai-creates-an-interactive-minecraft-like-world-in-real-time/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/ai_creates_an_interactive_minecraft_like_world_in_real_time.html"}}
{"id": 68263523001, "type": "news_chunk", "title": "Drug Companies are Hiring More Machine Learning Engineers Than Ever", "subtitle": "Drug Companies are Hiring More Machine Learning Engineers Than Ever", "content": "New data suggests the drug industry is hooked on AI. What’s new: Pharmaceutical companies in several countries are hiring machine learning engineers at increasing rates, industry news publication Pharmaceutical Technology reported. Most job openings are posted in the United States, though some countries in Europe and Asia are gaining ground.How it works: The publication analyzed data from GlobalData’s paywalled database, which tracks job listings in a variety of industries and analyzes the text to group them into categories. 26.4 percent of pharmaceutical companies in the database posted at least one machine learning opening in June 2022, an increase of 2.3 percent over the previous year. Of all the pharma industry jobs posted in June, 1.2 percent were related to machine learning.61 percent of machine learning jobs advertised by pharma companies globally in the three months ending in May were located in the U.S. The Boston, Massachusetts, metropolitan area saw the largest cluster of such jobs followed by the San Francisco Bay Area and San Diego, California.The top three European countries — Belgium, France, and the United Kingdom — each represented less than 6 percent of machine learning jobs advertised during the three months ending in May.The Asia-Pacific region’s total share decreased 1.9 points in the same time period. Job losses were not consistent across the region, however, China’s share declined from 5 percent to 2 percent, while India’s rose from 5 to 6 percent. Behind the news: In a recent report, GlobalData estimated that the pharmaceutical industry will spend over $3 billion on AI by 2025, driven largely by applications in drug discovery. The trend has also prompted major pharma companies including Astra-Zeneca, Pfizer, and Sanofi to acquire, invest in, or partner with startups. GlobalData counted 67 such partnerships in 2021, up from 23 in 2018.Why it matters: Bringing a new drug to market can take decades and cost billions of dollars. AI can cut time and costs in myriad ways, for instance by recognizing viable molecules without lab experimentation, identifying patients who might benefit from a drug, and predicting how patients might respond to them. We’re thinking: Given the economic value of online advertising and product recommendations, many machine learning engineers — and an entire genre of machine learning approaches — are devoted to optimizing their results. Given the value of pharmaceuticals, we have no doubt that machine learning has immense potential in that domain as well. Similarly, a large body of specialized machine learning techniques is waiting to be developed for many industries.", "image_caption": "Animated graphs and maps show global hiring trends for AI jobs in the pharma industry.", "metadata": {"article_id": "ai_pharma_jobs", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F08%2FPHARMA-2.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/ai-pharma-jobs/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/ai_pharma_jobs.html"}}
{"id": 47817758001, "type": "news_chunk", "title": "AI Predicts Scientific Breakthroughs Using Social Graphs", "subtitle": "AI Predicts Scientific Breakthroughs Using Social Graphs", "content": "A new AI method directs scientists toward promising avenues of inquiry. What's new: Jamshid Sourati and James A. Evans at University of Chicago proposed a method to predict new scientific discoveries by building a graph that connects researchers, their objects of study, and the scientific properties thereof. They evaluated their approach using data from materials science. Key insight: Overlapping interests among researchers may indicate areas where further research would be fruitful. For example, if one group of researchers studies a material A and its property P, a second group studies materials A and B, and another group studies materials B and C, it may turn out that material C exhibits property P. How it works: The authors tried to predict whether certain inorganic materials have certain electrical properties based on scientific literature through the year 2000. From 1.5 million articles that described 100,000 inorganic compounds, they extracted the author names, materials mentioned (for example, sodium nitrite), and their properties (for example, thermoelectricity, the ability to convert heat into electricity and vice versa). They used this data to construct a graph whose nodes were authors, materials, and properties. Edges connected the nodes that appeared in the same paper, for example a particular author whose paper covered specific material or property. The authors conducted random walks through the graph, stepping from node to node, to produce sequences of authors, materials, and properties. Then they removed the authors from the sequences, because they were interested mainly in establishing possible connections between materials and properties. They trained Word2Vec, which computes word embeddings, on their sequences, treating materials and properties as words and sequences as documents. This yielded an embedding for each material and property.To predict possible discoveries — that is, which material might exhibit a given property — the authors scored each material based on (i) the similarity between the material’s embedding and the given property’s embedding and (ii) the smallest number of edges in the path that connected each material and the property. Then they summed scores (i) and (ii). The 50 highest-scoring materials were predicted to have the property (that weren’t directly connected in the graph; that is, excluding materials that already were known to have the property). Results: The authors predicted which materials possessed each of three properties. They compared their results with predictions obtained in a similar way using a Word2Vec model trained exclusively on text from scientific papers. They used papers from 2001 through 2018 to evaluate the predictions. For thermoelectricity, the cumulative precision (percentage of predicted discoveries proven correct) was 76 percent, while the cumulative precision of the alternative method was 48 percent. The cumulative precision of random guesses was about 3 percent. The authors obtained similar results for the other two properties. Why it matters: Science is a social endeavor, where the connections between people and their work can be represented as a graph that reflects the collective attention of the scientific community. The collective attention acts as a signal that predicts promising avenues for further research — a signal that machine learning can help to tease out. We're thinking: The authors also predicted drug discoveries with similarly good results. Their method may be useful for identifying fruitful directions in other scientific areas, and perhaps in other domains entirely.", "image_caption": "Predicting Scientific Discoveries: AI predicts scientific breakthroughs using social graphs", "metadata": {"article_id": "ai_predicts_scientific_breakthroughs_using_social_graphs", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FSCIENCE-FIX-1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/ai-predicts-scientific-breakthroughs-using-social-graphs/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/ai_predicts_scientific_breakthroughs_using_social_graphs.html"}}
{"id": 62206695001, "type": "news_chunk", "title": "AI That Unites Us | AI News & Insights", "subtitle": "AI That Unites Us | AI News & Insights", "content": "As we approach 2025, my greatest hope for AI is that it will enable prosocial platforms that promote empathy, understanding, and collaboration rather than division. For too long, the algorithms that drive social media have functioned like strip-mining machines, extracting attention while eroding trust and social cohesion. What remains are depleted online spaces, where empathy struggles to take root and collective problem-solving finds no fertile ground. AI can — and should — help us transcend these entrenched divides. To achieve this, we must design AI systems that place prosocial values at their core. Instead of reinforcing fragmentation, recommendation algorithms can guide us toward “bridging content” that reveals common ground. They should clearly identify the communities a piece of content relates to — whether physical, religious, political, social, cultural, or professional — and illuminate the specific lines of division it seeks to mend. Realizing this vision requires a fundamental shift in what we optimize for. Instead of relying on pure engagement metrics, we should adopt values-driven indicators that prioritize constructive discourse and mutual understanding. For instance, we might spotlight “surprising validators,” or individuals and perspectives that productively challenge assumptions, thereby enriching our sense of what seemed irreconcilable. Researchers and developers should co-create new ranking and curation methods, embed them into widely used platforms, and rigorously assess their impact on democratic life. At the same time, the AI community must embrace participatory, inclusive approaches to development and governance. Research on pluralistic alignment stresses that AI systems emerge from and operate within complex social contexts, and including a wide range of voices helps guard against institutional blind spots. Tools like Polis, which can visualize stances and reveal hidden areas of consensus, already illustrate how complexity can be transformed into clarity. Such participatory methods ensure that AI reflects the priorities and values of the societies it serves, rather than amplifying the biases of the few. By embracing these inclusive, democratic principles, AI can help us co-create digital public squares that foster social cohesion rather than erode it. Embedding collective input at every stage — from how we build datasets to how we set governance policies — ensures that AI systems genuinely align with a spectrum of human values and serve as catalysts for common understanding. Audrey Tang is Taiwan’s Cyber Ambassador, former Minister of Digital Affairs, and co-author of Plurality: The Future of Collaborative Technology and Democracy.", "image_caption": "AUDREY TANG", "metadata": {"article_id": "ai_that_unites_us", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--41--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/ai-that-unites-us/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/ai_that_unites_us.html"}}
{"id": 60942697001, "type": "news_chunk", "title": "More Learning, Less Data | AI News & Insights", "subtitle": "More Learning, Less Data | AI News & Insights", "content": "Building a foundation model takes tremendous amounts of data. In the coming year, I hope we’ll enable models to learn more from less data. The AI community has achieved remarkable success by scaling up transformers and datasets. But this approach may be reaching a point of diminishing returns — an increasingly widespread belief among the pretraining community as they try to train next-generation models. In any case, the current approach poses practical problems. Training huge models on huge datasets consumes huge amounts of time and energy, and we’re running out of new sources of data for training large models. The fact is, current models consume much more data than humans require for learning. We’ve known this for a while, but we’ve ignored it due to the amazing effectiveness of scaling. It takes trillions of tokens to train a model but orders of magnitude less for a human to become a reasonably intelligent being. So there’s a difference in sample efficiency between our best models and humans. Human learning shows that there’s a learning algorithm, objective function, architecture, or a combination thereof that can learn more sample-efficiently than current models. One of the keys to solving this problem is enabling models to produce higher-level abstractions and filter out noise. I believe this concept, and thus the general problem of data efficiency, is related to several other current problems in AI: Data curation: We know that the specific data we use to train our models is extremely important. It’s an open secret that most of the work that goes into training foundation models these days is about the data, not the architecture. Why is this? I think it’s related to the fact that our models don’t learn efficiently. We have to do the work ahead of time to prepare the data for a model, which may hinder the core potential of AI as an automatic process for learning from data.Feature engineering: In deep learning, we always move toward more generalized approaches. From the beginning of the deep learning revolution, we’ve progressively removed handcrafted features such as edge detectors in computer vision and n-grams in natural language processing. But that engineering has simply moved to other parts of the pipeline. Tokenization, for instance, involves engineering implicit features. This suggests that there’s still a lot of room to make model architectures that are more data-efficient and more generally able to handle more raw modalities and data streams.Multimodality: The key to training a model to understand a variety of data types together is figuring out the core abstractions in common and relating them to each other. This should enable models to learn from less data by leveraging all the modalities jointly, which is a core goal of multimodal learning. Interpretability and robustness: To determine why a model produced the output it did, it needs to be able to produce higher-level abstractions, and we need to track the way it captures those abstractions. The better a model is at doing this, the more interpretable it should be, the more robust it should be to noise, and likely the less data it should need for learning.Reasoning: Extracting higher-level patterns and abstractions should allow models to reason better over them. Similarly, better reasoning should mean less training data.Democratization: State-of-the-art models are expensive to build, and that includes the cost of collecting and preparing enormous amounts of data. Few players can afford to do it. This makes developments in the field less applicable to domains that lack sufficient data or wealth. Thus more data-efficient models would be more accessible and useful. Considering data efficiency in light of these other problems, I believe they’re all related. It’s not clear which is the cause and which are the effects. If we solve interpretability, the mechanisms we engineer may lead to models that can extract better features and lead to more data-efficient models. Or we may find that greater data efficiency leads to more interpretable models. Either way, data efficiency is fundamentally important, and progress in that area will be an indicator of broader progress in AI. I hope to see major strides in the coming year. Albert Gu is an Assistant Professor of Machine Learning at Carnegie Mellon University and Chief Scientist of Cartesia AI. He appears on Time’s list of the most influential people in AI in 2024.", "image_caption": "ALBERT GU", "metadata": {"article_id": "albert_gu_more_learning_less_data", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--39--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/albert-gu-more-learning-less-data/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/albert_gu_more_learning_less_data.html"}}
{"id": 2574915001, "type": "news_chunk", "title": "Data Points: Alibaba outdoes itself with latest open models", "subtitle": "Data Points: Alibaba outdoes itself with latest open models", "content": "In today’s edition, you’ll learn more about: Microsoft’s Phi-4 updates add reasoning to small modelsOpenAI rolls back cloying update to GPT-4o, explains what went wrongAmazon debuts its largest multimodal teacher/agent model yetMeta partners with fast inference providers for new official API Alibaba debuts Qwen3 language models with hybrid reasoning Alibaba released Qwen3, a new family of large language models that support 119 languages and dialects. The family includes the flagship Qwen3-235B-A22B with 235 billion parameters and a smaller Qwen3-30B-A3B model, along with six dense models of various sizes. The models feature a hybrid approach that allows users to toggle between a deliberate “thinking mode” for complex reasoning and a faster “non-thinking mode” for simpler queries. Qwen3 models were trained on 36 trillion tokens — nearly double the training data of their predecessor — and boast better performance in coding, math, and other capabilities than competitors like DeepSeek-R1 and Gemini-2.5-Pro. All Qwen 3 models are open-weights and immediately available under the Apache 2.0 license on platforms including Hugging Face, ModelScope, and Kaggle. (Qwen Blog / GitHub) Study claims Chatbot Arena gave big tech companies unfair advantages Authors from Cohere, Stanford, MIT, and Ai2 accused LM Arena of allowing select AI companies to privately test multiple model variants on its Chatbot Arena benchmark while only publishing scores for their best performers. The paper alleges that companies including Meta, OpenAI, Google, and Amazon received preferential treatment that helped them achieve higher leaderboard rankings compared to competitors who weren’t offered the same opportunity. According to the study, Meta tested 27 model variants privately before its Llama 4 release but only publicly revealed the score for its top-performing model. Chatbot Arena has disputed these claims, calling the study full of “inaccuracies” and “questionable analysis,” while maintaining that its leaderboard is committed to fair evaluations and that all model providers are welcome to submit more tests. (arXiv and TechCrunch) Microsoft releases new Phi-4 reasoning models Microsoft launched three new language models: Phi-4-reasoning, Phi-4-reasoning-plus, and Phi-4-mini-reasoning. The 14 billion parameter model Phi-4-reasoning-plus outperforms larger competitors when answering mathematical problems and scientific questions, including beating DeepSeek-R1 (671 billion parameters) on the 2025 USA Math Olympiad qualifier test. The open-weights models are available on Azure AI Foundry and Hugging Face, with versions for Copilot+ PCs planned for a future release. (Microsoft) OpenAI rolls back sycophantic GPT-4o update OpenAI reverted an April 25th update to GPT-4o that made the model excessively agreeable, particularly when validating users’ negative emotions. The update combined several changes that weakened the model’s primary reward signal, including a new signal based on user feedback that likely amplified this behavior. Despite positive results in offline evaluations and limited A/B testing, the company failed to adequately weigh qualitative concerns from expert testers who noticed the model’s behavior “felt slightly off.” OpenAI says it has implemented several new safeguards, including treating model behavior issues as launch-blocking concerns, introducing an “alpha” testing phase, and committing to more proactive communication about model updates. (OpenAI) Amazon releases Nova Premier multimodal model Amazon Web Services made Nova Premier generally available in Amazon Bedrock, adding to its existing Nova model family. Nova Premier (billed as Amazon’s largest model, but total parameter count unspecified) inputs text, images, and videos with a one million token context window and outputs text. AWS benchmarked Nova Premier using 17 different metrics, where it outperformed other Nova models and also matched competing models like Claude 3.7 Sonnet and GPT-4.5 in about half the evaluations. Developers can use Nova Premier as a teacher model to distill its capabilities into smaller, faster models or use it in conjunction with these smaller models for agentic workflows. Nova Premier is now available in three AWS regions for $2.50/$12.50 per million input/output tokens. (Amazon) Meta launches Llama API with one-click key creation and model playgrounds Meta announced a limited free preview of a new Llama API. Meta’s new developer site offers API key creation, interactive playgrounds for exploring Llama models, and tools for fine-tuning and evaluating custom versions of the company’s Llama 3.3 8B model. Meta emphasized that user prompts and responses won’t be used to train their AI models, and developers can export custom models rather than being locked to Meta’s servers. The company also announced collaborations with Cerebras and Groq for faster inference speeds. Access to Llama 4 models powered by these providers is now available by request. (Meta) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng highlighted an inspiring story of a high school basketball coach who learned to code and now teaches computer science, emphasizing how AI can help scale K-12 education by empowering both students and teachers. “Starting from K-12, we should teach every student AI-enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers... Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: OpenAI launched API access to GPT Image 1, the image generator behind viral ChatGPT uploads; Google updated its AI-powered music generation tools, targeting professional musicians and creators; CB Insights’ Top 100 AI Startups list identified emerging players focused on AI agents and infrastructure; and researchers showed how large language models can improve shopping recommendations by inferring customer preferences from natural language input. Subscribe to Data Points", "image_caption": "Shocked man in library staring at laptop screen showing calm AI avatar with headphones.", "metadata": {"article_id": "alibaba_outdoes_itself_with_latest_open_models", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2FThe-Batch-ads-and-exclusive-banners---2025-05-02T115234.391.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/alibaba-outdoes-itself-with-latest-open-models/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/alibaba_outdoes_itself_with_latest_open_models.html"}}
{"id": 78680122001, "type": "news_chunk", "title": "Alibaba Releases the Qwen3 Family of Open LLMs With Optional Reasoning", "subtitle": "Alibaba Releases the Qwen3 Family of Open LLMs With Optional Reasoning", "content": "Alibaba’s new model family may unseat DeepSeek-R1’s four-month reign as the top open-weights large language model. What’s new: Alibaba released weights for eight large language models, all of which offer a reasoning mode that can be switched on or off. Two use a mixture of experts (MoE) architecture: Qwen3-235B-A22B (the name indicates 235 billion parameters, 22 billion of which are active at any given time) and Qwen3-30B-A3B). The other six are dense models in sizes between 32 billion parameters and 0.6 billion parameters — tiny by LLM standards, and with reasoning, too. Input/output: MoE models: Text in (up to 131,072 tokens), text out. Dense models: Text in (up to 32,768 tokens), text out.MoE architecture: Transformers. Qwen3-235B-A22B: 235 billion parameters, 22 billion active at any given time. Qwen3-30B-A3B: 30.5 billion parameters, 3.3 billion active at any given time.Dense architecture: Transformers with parameter counts of 32 billion, 14 billion, 8 billion, 4 billion, 1.7 billion, 0.6 billionTraining data: Pretrained on 36 trillion tokens, generated and scraped from the web, including textbooks, PDF documents, question-answer pairs, math problems, codeFeatures: Selectable reasoning mode, multilingual (119 languages and dialects) Undisclosed: Knowledge cutoff, fine-tuning data, output limits Availability: Free for noncommercial and commercial uses under Apache 2.0 license via HuggingFace and ModelScopeAPI price: Qwen3-235B-A22B: $0.22/$0.88 per million input/output tokens. Qwen3-30B-A3B: $0.15/$0.60 per million input/output tokens. Via Fireworks.ai How it works: The Qwen3 family implements chain-of-thought reasoning in both relatively large and quite small LLMs. The team pretrained Qwen3 models on roughly twice the data used to pretrain Qwen2.5. A substantial part of the additional data was devoted to training the model in several major languages plus region-specific dialects like Haitian, Luxembourgish, and Eastern Yiddish, and lesser-known Austronesian languages like Waray, Minangkabau, and Iloko. Pretraining took place over three stages that progressed to longer, more complex data. The authors fine-tuned the models on long chains of thought in domains that included coding, engineering, logical reasoning, mathematics, science, and technology. A reward model reinforced successful completions of these tasks. The in-progress models were used to generate synthetic data to train the non-reasoning mode. Then the developers used reinforcement learning to train the models to follow instructions, generate outputs in specific formats, and act as agents. Results: Qwen3-235B-A22B and Qwen3-30B-A3B performed as well as, or better than, leading open-weights models in tests performed by Alibaba. Qwen3-4B, too, achieved results that are competitive with many models several times its size. Alibaba didn’t provide results for the other dense models. On coding challenges in LiveCodeBench and Codeforces, Qwen3-235B-A22B (70.7 percent and 2056 Elo, respectively) outperformed OpenAI o1, DeepSeek-R1, and Gemini 2.5 Pro, but fell behind OpenAI o4-mini set to high effort. It outperformed the same models on the Berkeley Function-Calling Leaderboard (BFCL). Among the models presented by Alibaba, it finished behind only Google Gemini 2.5-Pro testing math skills (AIME 2024, AIME 2025) and a variety of recently updated math, language, and problem-solving questions (LiveBench).Qwen3-30B-A3B outperformed Google Gemma-3-27B-IT and DeepSeek-V3 on all benchmarks highlighted by Alibaba, and it underperformed only OpenAI GPT-4o on BFCL. On GPQA Diamond’s test of graduate-level questions in a variety of domains, Qwen3-30B-A3B (65.8 percent) outperformed next-best DeepSeek-V3.Qwen3-4B, with 4 billion parameters, was competitive across a wide range of benchmarks against DeepSeek-V3 (671 billion parameters) and Gemma-3-27B-IT (27 billion). For instance, on both Codeforces and LiveBench, Qwen3-4B (1,671 Elo and 63.6 percent, respectively) outperformed DeepSeek-V3 (1,134 Elo and 60.5 percent). Why it matters: Qwen3 continues a string of high-performance, open-weights models released by developers in China. Alibaba says it designed the models to do the thinking in agentic systems. Reasoning that can be switched on and off can help control costs in agentic and other applications. We’re thinking: Alibaba’s 235-billion parameter MoE model may perform better according to benchmarks, but Qwen3-30B-A3B does nearly as well and can run locally on a pro laptop without straining its memory. Add the easy ability to switch reasoning on or off, and Qwen3’s versatile, mid-sized MoE model may turn out to be the star of the show.", "image_caption": "LLM performance benchmark table comparing Qwen, OpenAI, Gemini, and others on coding, math, and language tasks.", "metadata": {"article_id": "alibaba_releases_the_qwen3_family_of_open_llms_with_optional_reasoning", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--85--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/alibaba-releases-the-qwen3-family-of-open-llms-with-optional-reasoning/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/alibaba_releases_the_qwen3_family_of_open_llms_with_optional_reasoning.html"}}
{"id": 61301993001, "type": "news_chunk", "title": "Data Points: All about Claude’s new Opus and Sonnet", "subtitle": "Data Points: All about Claude’s new Opus and Sonnet", "content": "In today’s edition, you’ll learn more about: The new device OpenAI is building with Jony IveMistral’s new open-weight software engineering modelFalcon-Arabic, a new 7B model that excels in multiple regional dialectsGoogle’s new multimodal model for mobile devices Anthropic introduces Claude Opus 4 and Sonnet 4 models Anthropic released its new Claude Opus 4 and Sonnet 4 models with improvements in coding and reasoning capabilities. Opus 4 reached 72.5 percent on SWE-bench and 43.2 percent on Terminal-bench coding tests, while Sonnet 4 achieved 72.7 percent on SWE-bench, outperforming earlier Claude models and rivals from OpenAI. Both new models can now use tools during their reasoning process, execute tools in parallel, and demonstrate better memory when accessing local files. The models are available through Anthropic, Amazon Bedrock, and Google Cloud’s Vertex AI, with Opus 4 priced at $15/$75 per million tokens (input/output) and Sonnet 4 at $3/$15. (Anthropic) Google rebrands subscription to AI Pro, launches new Ultra tier Google is renaming its AI Premium subscription to “Google AI Pro” while introducing a new high-end “Google AI Ultra” tier priced at $249.99 per month. Google AI Pro maintains its $19.99 monthly price with access to Gemini 2.5 Pro, 2TB storage, Deep Research, and Veo 2 video generation, plus new features like early access to Gemini in desktop Chrome and the Flow AI filmmaking tool. The Ultra tier includes all Pro features plus 30TB storage, YouTube Premium, highest usage limits for AI tools, and exclusive access to experimental features like Project Mariner, which can manage multiple tasks simultaneously. Google is offering an introductory price of $124.99 for Ultra’s first three months, with availability starting today in the U.S. and expanding to other countries soon. (9to5Google) OpenAI partners with Jony Ive on AI assistant device Sam Altman revealed to OpenAI staff that the company is developing AI “companions” with newly acquired design firm io, led by former Apple designer Jony Ive. The planned device will be aware of users’ surroundings, unobtrusive enough to fit in a pocket or on a desk, and is intended to become a third essential device alongside laptops and smartphones. Altman described the product as a “family of devices” that will integrate hardware and software similar to Apple’s approach, emphasizing that the technology will move beyond typing queries into websites. OpenAI aims to ship 100 million devices by late next year, with Altman suggesting the $6.5 billion acquisition could add $1 trillion in value to the company. (The Verge) Mistral AI releases Devstral, an open-weight coding LLM Mistral AI and All Hands AI launched Devstral, an agentic large language model specifically designed for software engineering tasks. The model achieves 46.8 percent on SWE-Bench Verified, outperforming other open-weight models by more than 6 percentage points and surpassing GPT-4.1-mini by over 20 percent. Unlike many LLMs that excel at isolated coding tasks, Mistral says Devstral can solve more complex software engineering problems by contextualizing code within large codebases and identifying relationships between components. The model is lightweight enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it suitable for local deployment. Devstral is available for free under the Apache 2.0 license on HuggingFace, Ollama, and other platforms, or through Mistral’s API at $0.10/$0.30 per million tokens of input/output. (Mistral) New Arabic language model outperforms larger competitors The Technology Innovation Institute released Falcon-Arabic, a 7B parameter language model built on the Falcon 3 architecture. The model handles Arabic, English, and several other languages with a 32,000 token context window. Testing shows Falcon-Arabic outperforms other Arabic language models of similar size and some larger models on benchmarks including Arabic MMLU, Exams, MadinahQA, and Aratrust. The developers extended the base model with 32,000 Arabic-specific tokens and used native Arabic datasets for training rather than translated content. The model supports both Modern Standard Arabic and regional dialects, addressing the relative scarcity of Arabic language AI tools. Users can test Falcon-Arabic through an online playground. (Hugging Face) Google previews Gemma 3n, a mobile-optimized multimodal model Google unveiled Gemma 3n, a new open AI model specifically engineered for on-device use with a significantly reduced memory footprint. The model leverages per-layer embeddings technology that allows 5B and 8B parameter models to operate with just 2GB and 3GB of memory, making them suitable for phones, tablets, and laptops. Gemma 3n offers multimodal capabilities including text, image, video, and audio processing, with new features like automatic speech recognition and translation. The model was designed in collaboration with mobile hardware companies like Samsung, Qualcomm, and MediaTek to enable offline use, and will be the basis for the next version of Gemini Nano. Developers can preview Gemma 3n through Google AI Studio or Google AI Edge for on-device development. (Google) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng shared how large companies can move fast in the age of AI by creating sandbox environments that allow small teams to innovate without needing constant permission. “If engineers need sign-off from 5 vice presidents before they’re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product?” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: OpenAI introduced Codex, a new multi-agent, cloud-based software engineering tool integrated into ChatGPT; xAI attributed the controversial “white genocide” responses from Grok to an unnamed, unauthorized employee, raising concerns about internal safeguards; U.S. tech giants including Nvidia, AMD, and Amazon secured deals to supply chips and infrastructure to Middle Eastern companies like Saudi Arabia’s Humain and the UAE’s G42; and Microsoft researchers showed that 4-bit quantized versions of Llama models can match the accuracy of 16-bit models, offering major efficiency gains without compromising performance. Subscribe to Data Points", "image_caption": "Woman coding on multiple monitors in modern office, colleague observing. Software development, teamwork, technology workspace.", "metadata": {"article_id": "all_about_claudes_new_opus_and_sonnet", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2FThe-Batch-ads-and-exclusive-banners---2025-05-23T114110.467.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/all-about-claudes-new-opus-and-sonnet/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/all_about_claudes_new_opus_and_sonnet.html"}}
{"id": 3790084001, "type": "news_chunk", "title": "All About the Hollywood Actors and Studios' Deal on Generative AI Usage in Films and TV", "subtitle": "All About the Hollywood Actors and Studios' Deal on Generative AI Usage in Films and TV", "content": "The longest actors’ strike in Hollywood history ended as actors and studios reached an accord on the use of generative AI in making movies. What’s new: Film studios must seek an actor’s consent before using a generated likeness or performance and compensate the actor, according to an agreement between the trade union Screen Actors Guild-American Federation of Television and Radio Artists (SAG-AFTRA) and the Alliance of Motion Picture and Television Producers (TMPTP). The pact will remain in effect for three years, once it has been ratified by SAG-AFTRA members. How it works: The agreement covers digital replicas of human actors, synthetic performers, and simulated performances created using AI and other technologies that may not be generally recognized as AI. The parties argued over terms with respect to AI until the very last day of their 118-day negotiation, according to SAG-AFTRA’s president. Among the provisions: Studios must compensate an actor if performances are used to train a model. Studios must secure an actor’s consent before using a synthetic likeness or performance, regardless of whether the replica was made by scanning the actor or extracting information from existing footage. The actor has the right to refuse. If the actor consents, studios must compensate the actor for the days they would have worked, if they had performed in person. Studios may use digital replicas of recognizable actors who have background roles and don’t speak, but they must compensate the actors. If studios alter a synthetic background actor so it appears to speak, they must pay the actor a full wage.If studios want to synthesize a deceased actor who did not consent while alive, they must seek consent from the heirs or estate.Studios can combine the likenesses of multiple actors into a “synthetic performer,” but they must seek consent and compensate the actors for “recognizable elements” they use. In addition, they must notify SAG-AFTRA and allow the union to bargain on behalf of the actors. TMPTP must meet with SAG-AFTRA semi-annually to review the state of affairs in AI, giving the actors an opportunity to adjust guidelines in response as technology and law develop. Behind the news: The agreement followed a similar three-year deal in September that ended the concurrent strike by Writers Guild of America. Yes, but: The agreement covers on-screen actors. It does not cover voice or motion actors in video games or television animation. In September, SAG-AFTRA authorized a strike against a group of video game companies if negotiations, which are ongoing, stall. Negotiations over television animation are expected as well. Why it matters: The actors’ agreement could set an international example for limits on AI in the performing arts, thanks to the U.S. film and television industry’s global reach. Entertainers’ unions in Europe and Canada are contemplating strikes inspired by SAG-AFTRA’s, and they may seek similar agreements. We’re thinking: As with the screenwriters’ contract, the agreement between actors and studios gives everyone three years to experiment with AI while respecting the consent, credit, and compensation of creative workers. We hope that shows made in this period provide ample evidence that such tools can yield wonderful productions that enlarge the market, and that the next agreement focuses more on growing the use of AI and dividing the winnings fairly among actors, studios, and technologists.", "image_caption": "SAG-AFTRA member with a picket sign that says \"A.I. is soulless\"", "metadata": {"article_id": "all_about_the_hollywood_actors_and_studios_deal_on_generative_ai_usage_in_films_and_tv", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2023%2F11%2Funnamed--73--2.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/all-about-the-hollywood-actors-and-studios-deal-on-generative-ai-usage-in-films-and-tv/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/all_about_the_hollywood_actors_and_studios_deal_on_generative_ai_usage_in_films_and_tv.html"}}
{"id": 33491164001, "type": "news_chunk", "title": "Anthropic Debuts New Claude 4 Sonnet and Claude 4 Opus Models, Featuring Top Benchmarks in Coding", "subtitle": "Anthropic Debuts New Claude 4 Sonnet and Claude 4 Opus Models, Featuring Top Benchmarks in Coding", "content": "Anthropic continued its tradition of building AI models that raise the bar in coding tasks. What’s new: Anthropic launched Claude 4 Sonnet 4 and Claude Opus 4, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit. Input/output: Text, images, PDF files in (up to 200,000 tokens); text out (Claude Sonnet 4 up to 64,000 tokens, Claude Opus 4 up to 32,000 tokens)Features: Parallel tool use including computer use, selectable reasoning mode with visible reasoning tokens, multilingual (15 languages)Performance: Ranked Number One in LMSys WebDev Arena, state-of-the-art on SWE-bench and Terminal-benchAvailability/price: Anthropic API, Amazon Bedrock, Google Cloud Vertex AI. Claude Sonnet 4 $3/$15 per million input/output tokens, Claude Opus 4 $15/$75 per million input/output tokensUndisclosed: Parameter counts, specific training methods and datasets How it works: The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to be helpful, honest, and harmless according to human and AI feedback. The models make reasoning tokens visible within limits. For especially lengthy chains of thought, an unspecified smaller model summarizes reasoning tokens.Given local file access, Claude Opus 4 can create and manipulate files to store information. For instance, prompted to maintain a knowledge base while playing a Pokémon video game, the model produced a guide to the game that offered advice such as, “If stuck, try OPPOSITE approach” and “Change Y-coordinate when horizontal movement fails.” Results: Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests. On SWE-bench Verified, which tests the model’s ability to solve software issues from GitHub, Claude Opus 4 succeeded 72.5 percent of the time, and Claude Sonnet 4 succeeded 72.7 percent of the time. The next best model, OpenAI o3, succeeded 70.3 percent of the time.Terminal-bench evaluates how well models work with the benchmark’s built-in agentic framework to perform tasks on a computer terminal. Claude Opus 4 succeeded 39.2 percent of the time and Claude Sonnet 4 succeeded 33.5 percent of the time, whereas the closest competitor, OpenAI GPT 4.1, succeeded 30.3 percent of the time. Using Claude Code as the agentic framework, Claude Opus 4 succeeded 43.2 percent of the time and Claude Sonnet 4 succeeded 35.5 percent of the time. Why it matters: The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including a Tetris clone built in one shot and a seven-hour stint refactoring Rakutan’s open-source code base. We’re thinking: Prompting expert @elder_plinius published a text file that is purported to be Claude 4’s system prompt and includes some material that does not appear in Anthropic’s own publication of the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning.", "image_caption": "AI model performance comparison chart: Claude Opus 4, Sonnet 4, Sonnet 3.7, OpenAI o3, GPT-4.1, and Gemini 2.5 Pro.", "metadata": {"article_id": "anthropic_debuts_new_claude_4_sonnet_and_claude_4_opus_models_featuring_top_benchmarks_in_coding", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--97--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/anthropic-debuts-new-claude-4-sonnet-and-claude-4-opus-models-featuring-top-benchmarks-in-coding/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/anthropic_debuts_new_claude_4_sonnet_and_claude_4_opus_models_featuring_top_benchmarks_in_coding.html"}}
{"id": 82921270001, "type": "news_chunk", "title": "Brain Implants Paired with Neural Network Reconstruct Speech for ALS Patient", "subtitle": "Brain Implants Paired with Neural Network Reconstruct Speech for ALS Patient", "content": "A man who lost the ability to speak four years ago is sounding like his earlier self, thanks to a collection of brain implants and machine learning models. What’s new: Researchers built a system that decodes speech signals from the brain of a man who lost the ability to speak clearly due to amyotrophic lateral sclerosis, also known as ALS, and enables him to speak through a synthetic version of his former voice. At the start of the study, his efforts to speak were intelligible only to his personal caregiver. Now he converses regularly with family and friends, The New York Times reported. Nicholas Card built the system with colleagues University of California-Davis, Stanford University, Washington University, Brown University, VA Providence Healthcare, and Harvard Medical School. How it works: The authors surgically implanted four electrode arrays into areas of the brain that are responsible for speech. The system learned to decode the patient’s brain signals, decide the most likely phonemes he intended to speak, determine the words those phonemes express, and display and speak the words aloud using a personalized speech synthesizer. After the patient recovered from the implantation surgery, the authors collected data for training and evaluating the system. They recorded his brain signals while he tried to speak during 84 sessions, each between 5 and 30 minutes, over 32 weeks. The sessions were split into two tasks: copying, in which the patient spoke sentences shown on a screen, and conversation, in which he spoke about whatever he wanted. Initial sessions focused on copying. Later, when the authors had accrued paired brain signals and known sentences, they focused on conversation.A gated recurrent unit (GRU) learned to translate brain signals into a sequence of phonemes. The authors trained the model after each session on all recordings made during that session. To adapt it to day-to-day changes in brain activity, they also fine-tuned it during later sessions: After they recorded a new sentence, they fine-tuned the GRU on a 60/40 mix of sentences from the current session and previous sessions.A weighted finite-state transducer (WFST), based on a pretrained 5-gram language model and described in the supplementary information here, translated sequences of phonemes into sentences. Given a sequence, it generated the 100 most likely sentences. Given the likely sentences, the authors ranked them according to the probability that the GRU, WFST, and OPT, a pretrained large language model, would generate them. A pretrained StyleTTS 2 text-to-speech model turned the highest-ranking sentence into speech. The authors fine-tuned the model on recordings of the patient’s voice from before the onset of his illness, such as podcasts. Results: After two hours of recording the patient’s brain signals and training on that data, the system achieved 90.2 percent accuracy in the copying task. By the final session, the system achieved 97.5 percent accuracy and enabled the patient to speak on average 31.6 words per minute using a vocabulary of 125,000 words. Behind the news: Previous work either had much lower accuracy or generated a limited vocabulary. The new work improved upon a 2023 study that enabled ALS patients to speak with 76.2 percent accuracy using a vocabulary of equal size. Why it matters: Relative to the 2023 study on which this one was based, the authors changed the positions of the electrodes in the brain and continued to update the GRU throughout the recording/training sessions. It’s unclear which changes contributed most to the improved outcome. As language models improve, new models potentially could act as drop-in replacements for the models in the authors’ system, further improving accuracy. Likewise, improvements in speech-to-text systems could increase the similarity between the synthetic voice and the patient’s former voice. We’re thinking: Enabling someone to speak again restores agency. Enabling someone to speak again in their own voice restores identity.", "image_caption": "A man with electrodes connected through his skull is connected to a machine.", "metadata": {"article_id": "brain_implants_paired_with_neural_network_reconstruct_speech_for_als_patient", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F08%2Funnamed-1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/brain-implants-paired-with-neural-network-reconstruct-speech-for-als-patient/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/brain_implants_paired_with_neural_network_reconstruct_speech_for_als_patient.html"}}
{"id": 98089144001, "type": "news_chunk", "title": "Brain2Qwerty, A System That Decodes Thoughts Using Brain Waves Without Surgery", "subtitle": "Brain2Qwerty, A System That Decodes Thoughts Using Brain Waves Without Surgery", "content": "To date, efforts to decode what people are thinking from their brain waves often relied on electrodes implanted in the cortex. New work used devices outside the head to pick up brain signals that enabled an AI system, as a subject typed, to accurately guess what they were typing. What’s new: Researchers presented Brain2Qwerty, a non-invasive method to translate brain waves into text. In addition, their work shed light on how the brain processes language. The team included people at Meta, Paris Sciences et Lettres University, Hospital Foundation Adolphe de Rothschild, Basque Center on Cognition, Brain and Language, Basque Foundation for Science, Aix-Marseille University, and Paris Cité University. Gathering brainwave data: The authors recorded the brain activity of 35 healthy participants who typed Spanish-language sentences. The participants were connected to either an electroencephalogram (EEG), which records the brain’s electrical activity via electrodes on the scalp, or a magnetoencephalogram (MEG), which records magnetic activity through a device that surrounds the head but isn’t attached. 15 participants used each device and five used both. Participants were asked to read and memorize short sentences of 5 to 8 words. They were shown one word at a time.After a short waiting period, participants were asked to type the sentence. They could not see what they typed.The EEG dataset comprised around 4,000 sentences and 146,000 characters, while the MEG dataset comprised around 5,100 sentences and 193,000 characters. Thoughts into text: Brain2Qwerty used a system made up of a convolutional neural network, transformer, and a 9-gram character-level language model pretrained on Spanish Wikipedia. The system classified the text a user typed from their brain activity. The authors trained separate systems on MEG and EEG data. The convolutional neural network segmented brain activity into windows of 500 milliseconds each. The transformer took these windows as input and generated possible text characters and their probabilities. The two models learned to predict characters jointly.The pretrained language model, given the most recently predicted nine characters, estimated the probability of the next character.At inference, the authors used a weighted average of probabilities from the transformer and language model. From that average, they computed the most likely sequence of characters as the final output. Results. The authors’ MEG model achieved 32 percent character error rate (CER), much higher accuracy than the EEG competitors. Their EEG system outperformed EEGNet, a model designed to process EEG data that had been trained on the authors’ EEG data. It achieved 67 percent CER, while EEGNet achieved 78 percent CER. Behind the news: For decades, researchers have used learning algorithms to interpret various aspects of brain activity with varying degrees of success. In recent years, they’ve used neural networks to generate text and speech from implanted electrodes, generate images of what people see while in an fMRI, and enable people to control robots using EEG signals. Why it matters: In research into interpreting brain signals, subjects who are outfitted with surgical implants typically have supplied the highest-quality brain signals. fMRI scans, while similarly noninvasive, are less precise temporally, which makes them less useful for monitoring or predicting language production. Effective systems based on MEG, which can tap brain signals precisely without requiring participants to undergo surgery, open the door to collecting far more data, training far more robust models, and conducting a wider variety of experiments. We’re thinking: The privacy implications of such research may be troubling, but keep in mind that Brain2Qwerty’s MEG system, which was the most effective approach tested, required patients to spend extended periods of time sitting still in a shielded room. We aren’t going to read minds in the wild anytime soon.", "image_caption": "A participant types while an MEG scan decodes brain activity into text in real-time, showing typed vs. decoded text.", "metadata": {"article_id": "brain2qwerty_a_system_that_decodes_thoughts_using_brain_waves_without_surgery", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2Funnamed--50--1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/brain2qwerty-a-system-that-decodes-thoughts-using-brain-waves-without-surgery/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/brain2qwerty_a_system_that_decodes_thoughts_using_brain_waves_without_surgery.html"}}
{"id": 19760214001, "type": "news_chunk", "title": "Andrew Ng on Finding Your First AI Job", "subtitle": "Andrew Ng on Finding Your First AI Job", "content": "Dear friends, I’ve written about how to build a career in AI and focused on tips for learning technical skills, choosing projects, and sequencing projects over a career. This time, I’d like to talk about searching for a job. A job search has a few predictable steps including selecting companies to apply to, preparing for interviews, and finally picking a job and negotiating an offer. In this letter, I’d like to focus on a framework that’s useful for many job seekers in AI, especially those who are entering AI from a different field. If you’re considering your next job, ask yourself: Are you switching roles? For example, if you’re a software engineer, university student, or physicist who’s looking to become a machine learning engineer, that’s a role switch.Are you switching industries? For example, if you work for a healthcare company, financial services company, or a government agency and want to work for a software company, that’s a switch in industries. A product manager at a tech startup who becomes a data scientist at the same company (or a different one) has switched roles. A marketer at a manufacturing firm who becomes a marketer in a tech company has switched industries. An analyst in a financial services company who becomes a machine learning engineer in a tech company has switched both roles and industries. If you’re looking for your first job in AI, you’ll probably find switching either roles or industries easier than doing both at the same time. Let’s say you’re the analyst working in financial services: If you find a data science or machine learning job in financial services, you can continue to use your domain-specific knowledge while gaining knowledge and expertise in AI. After working in this role for a while, you’ll be better positioned to switch to a tech company (if that’s still your goal).Alternatively, if you become an analyst in a tech company, you can continue to use your skills as an analyst but apply them to a different industry. Being part of a tech company also makes it much easier to learn from colleagues about practical challenges of AI, key skills to be successful in AI, and so on. If you’re considering a role switch, a startup can be an easier place to do it than a big company. While there are exceptions, startups usually don’t have enough people to do all the desired work. If you’re able to help with AI tasks — even if it’s not your official job — your work is likely to be appreciated. This lays the groundwork for a possible role switch without needing to leave the company. In contrast, in a big company, a rigid reward system is more likely to reward you for doing your job well (and your manager for supporting you in doing the job for which you were hired), but it’s not as likely to reward contributions outside your job’s scope. After working for a while in your desired role and industry (for example, a machine learning engineer in a tech company), you’ll have a good sense of the requirements for that role in that industry at a more senior level. You’ll also have a network within that industry to help you along. So future job searches — if you choose to stick with the role and industry — likely will be easier. When changing jobs, you’re taking a step into the unknown, particularly if you’re switching either roles or industries. One of the most underused tools for becoming more familiar with a new role and/or industry is the informational interview. I’ll share more about that in the next letter. Keep learning,Andrew P.S. I’m grateful to Salwa Nur Muhammad, CEO of FourthBrain (a DeepLearning.AI affiliate), for providing some of the ideas presented in this letter.", "image_caption": "An illustration shows how career changes can be a role switch, an industry switch, or both.", "metadata": {"article_id": "build_career_part_5", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F08%2FJOBSEARCH---A-1.jpeg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/build-career-part-5/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/build_career_part_5.html"}}
{"id": 54564058001, "type": "news_chunk", "title": "Andrew Ng on Fine Tuning Your AI Job Search", "subtitle": "Andrew Ng on Fine Tuning Your AI Job Search", "content": "Dear friends, I’ve devoted several recent letters to building a career in AI. In this one, I’d like to discuss some fine points of finding a job.The typical job search follows a fairly predictable path. Research roles and companies online or by talking to friends.Optionally, arrange informal informational interviews with people in companies that appeal to you.Either apply directly or, if you can, get a referral from someone on the inside.Interview with companies that give you an invitation.Receive one or more offers and pick one. Or, if you don’t receive an offer, ask for feedback from the interviewers, the human resources staff, online discussion boards, or anyone in your network who can help you plot your next move. Although the process may be familiar, every job search is different. Here are some tips to increase the odds you’ll find a position that supports your thriving and enables you to keep growing. Pay attention to the fundamentals. A compelling resume, portfolio of technical projects, and a strong interview performance will unlock doors. Even if you have a referral from someone in a company, a resume and portfolio will be your first contact with many people who don’t already know about you. Update your resume and make sure it clearly presents your education and experience relevant to the role you want. Customize your communications with each company to explain why you’re a good fit. Before an interview, ask the recruiter what to expect. Take time to review and practice answers to common interview questions, brush up key skills, and study technical materials to make sure they are fresh in your mind. Afterward, take notes to help you remember what was said. Proceed respectfully and responsibly. Approach interviews and offer negotiations with a win-win mindset. Outrage spreads faster than reasonableness on social media, so a story about how an employer underpaid someone gets amplified, whereas stories about how an employer treated someone fairly do not. The vast majority of employers are ethical and fair, so don’t let stories about the small fraction of mistreated individuals sway your approach. If you’re leaving a job, exit gracefully. Give your employer ample notice, give your full effort through your last hour on the job, transition unfinished business as best you can, and leave in a way that honors the responsibilities you were entrusted with. Choose who to work with. It’s tempting to take a position because of the projects you’ll work on. But the teammates you’ll work with are at least equally important. We’re influenced by people around us, so your colleagues will make a big difference. For example, if your friends smoke, the odds rise that you, too, will smoke. I don’t know of a study that shows this, but I’m pretty sure that if most of your colleagues work hard, learn continuously, and build AI to benefit all people, you’re likely to do the same. (By the way, some large companies won’t tell you who your teammates will be until you’ve accepted an offer. In this case, be persistent and keep pushing to identify and speak with potential teammates. Strict policies may make it impossible to accommodate you, but in my mind, that increases the risk of accepting the offer, as it increases the odds you’ll end up with a manager or teammates who aren’t a good fit.) Get help from your community. Most of us go job hunting only a small number of times in our careers, so few of us get much practice at doing it well. Collectively, though, people in your immediate community probably have a lot of experience. Don’t be shy about calling on them. Friends and associates can provide advice, share inside knowledge, and refer you to others who may help. I got a lot of help from supportive friends and mentors when I applied for my first faculty position, and many of the tips they gave me were very helpful. I know that the job search process can be intimidating. Instead of viewing it as a great leap, consider an incremental approach. Start by identifying possible roles and conducting a handful of informational interviews. If these conversations tell you that you have more learning to do before you’re ready to apply, that’s great! At least you have a clear path forward. The most important part of any journey is to take the first step, and that step can be a small one. Keep learning!", "image_caption": "Illustration shows an AI job searcher heading into the workforce.", "metadata": {"article_id": "build_career_part_6", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F08%2FJOBSEARCH_Onward_Rerevise_1200px--1--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/build-career-part-6/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/build_career_part_6.html"}}
{"id": 1990496001, "type": "news_chunk", "title": "Data Points: Building multi-agent systems in Rowboat’s IDE", "subtitle": "Data Points: Building multi-agent systems in Rowboat’s IDE", "content": "In today’s edition, you’ll learn more about: GPT-4o’s image generator now available via APIGoogle updates its Lyria model and music editing toolsGrok 3 models now available for API developersExecutive order would overhaul K-12 AI education in U.S. schools Rowboat launches open-source IDE for multi-agent AI development Rowboat, a new freely available integrated development environment, aims to simplify the creation and deployment of multi-agent AI systems. The platform features a visual interface that transforms natural language specifications into functional agent workflows, supports MCP servers for tool integration, and includes a playground for interactive testing and debugging. The Y Combinator-backed project integrates with OpenAI’s Agents SDK and is designed for developers working on applications in financial services, insurance, travel, and telecommunications. Rowboat is available now on GitHub under an Apache 2.0 license. (GitHub) ByteDance updates GUI agent, outperforms OpenAI and Anthropic ByteDance released UI-TARS-1.5, an updated multimodal agent framework that outperforms several leading models including OpenAI’s Operator and Anthropic’s Claude 3.7 Sonnet in GUI automation and game reasoning benchmarks. The model works as an end-to-end system that perceives screenshots and generates human-like control actions such as mouse movements and keyboard inputs, rather than relying on function calls or tool augmentation. The model performs well across desktop, mobile, and game environments, achieving higher success rates in complex benchmarks like ScreenSpotPro (61.6 percent) compared to earlier versions of UI-TARS and competitors. UI-TARS-1.5 is an open-weights model, available under an Apache 2.0 license through GitHub and Hugging Face. (TARS) OpenAI makes new image generation model available through API OpenAI released “gpt-image-1,” giving developers API access to the same image generation model used in ChatGPT. The company reports ChatGPT users created over 700 million images in the feature’s first week after launch. The API includes safety features and C2PA metadata in generated images. Pricing follows a token-based structure with text input tokens at $5 per million tokens, image input tokens at $10 per million tokens, and image output tokens at $40 per million tokens, which translates to approximately $0.02, $0.07, and $0.19 per generated image for low, medium, and high-quality square images, respectively. (OpenAI) Google expands Music AI Sandbox with new features and Lyria 2 model Google introduced new features and improvements to its Music AI Sandbox, including Lyria 2, their latest music generation model. The expanded toolkit offers three main capabilities: Create (generating music samples from text descriptions), Extend (continuing existing musical clips), and Edit (transforming existing audio with fine-grained control). Google developed these tools in collaboration with musicians through YouTube’s Music AI Incubator and is now giving more U.S.-based musicians access to experiment with them. The company also unveiled Lyria RealTime, which enables real-time interactive music creation and performance. Music AI Sandbox and Lyria 2 are currently available only to trusted testers via waitlist. (Google) xAI launches Grok 3 models in API xAI released what it called beta versions of its Grok 3 model lineup with standard and fast variants at different price points. The flagship Grok 3 model costs $3 per million tokens for input and $15 per million tokens for output, while the faster version charges $5 and $25 respectively. The company also offers more affordable Grok 3 Mini models starting at $0.30/$0.50 per million input/output tokens, plus separate Grok 2 models with vision and image generation capabilities. All text models feature a 131,072 token context window and share the same underlying architecture, differing only in server speed. In the API, Grok 3 models are not connected to the real-time web, and have a knowledge cutoff of November 2024. (xAI) Trump executive order establishes AI education task force U.S. President Trump signed an executive order creating a White House Task Force on Artificial Intelligence Education. The order directs the government to launch several concrete initiatives: development of K-12 AI education resources through public-private partnerships, allocation of existing federal funds for teacher training on AI integration, expansion of AI-related student apprenticeships, and a Presidential AI Challenge competition to highlight student achievements. These programs aim to build AI literacy and technical skills across the American workforce and educational system. (The White House) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng highlighted how AI-assisted coding enables developers to work in unfamiliar languages, while understanding the core programming concepts of each language remains key to success. “Understanding the concepts behind different languages is still important... This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: OpenAI introduced the cost-efficient GPT-4.1 family, along with the o3 and o4-mini reasoning models, designed to improve complex problem-solving and coding; Hugging Face acquired Pollen Robotics and unveiled Reachy 2, a new open-weights model-powered robot for research and experimentation; the U.S. government imposed tighter restrictions on AI chip exports to China and began an investigation into Nvidia’s practices; and researchers developed a text-only language model capable of interpreting images, video, and audio — all without additional training. Subscribe to Data Points", "image_caption": "Music producer uses AI assistant on screen to mix a song while singer records vocals in a professional studio.", "metadata": {"article_id": "building_multi_agent_systems_in_rowboats_ide", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2FThe-Batch-ads-and-exclusive-banners---2025-04-25T124511.376.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/building-multi-agent-systems-in-rowboats-ide/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/building_multi_agent_systems_in_rowboats_ide.html"}}
{"id": 36024679001, "type": "news_chunk", "title": "Painting With Text, Voice, and Images: ChatGPT now accepts voice and image inputs and outputs.", "subtitle": "Painting With Text, Voice, and Images: ChatGPT now accepts voice and image inputs and outputs.", "content": "ChatGPT is going multimodal with help from DALL·E.What’s new: ChatGPT is being geared to accept voice input and output, OpenAI announced. It will also accept and generate images, thanks to integration with DALL·E 3, a new version of the company’s image generator.How it works: The updates expand ChatGPT into a voice-controlled, interactive system for text and image interpretation and production. New safety features are designed to protect legal rights of artists and public figures. Voice input/output will give ChatGPT functionality similar to that of Apple Siri or Amazon Alexa. OpenAI’s Whisper speech recognition system will transcribe voice input into text prompts, and a new text-to-speech model will render spoken output in five distinct voice profiles. Voice interactions will be available to subscribers to the paid ChatGPT Plus and Enterprise services within a couple of weeks.A new model called GPT-4 with Vision (GPT-4V) manages ChatGPT’s image input/output, which OpenAI demonstrated at GPT-4’s debut. Users can include images in a conversation to, say, analyze mathematical graphs or plan a meal around the photographed contents of a refrigerator. Like voice, image input/output will be available to paid subscribers within weeks.DALL·E 3 will use ChatGPT to refine prompts, and it will generate images from much longer prompts than the previous version. It will produce legible text within images (rather than made-up characters and/or words). Among other safety features, it will decline prompts that name public figures or ask for art in the style of a living artist. The update will be available to paid subscribers in early October, and Microsoft Bing’s Image Creator will switch from DALL·E 2 to DALL·E 3.All new functionality eventually will roll out to unpaid and API users. Yes, but: OpenAI said the new voice and image capabilities are limited to the English language. Moreover, the ability to understand and generate highly technical images is limited.Behind the news: OpenAI introduced GPT-4 in March with a demo that translated a napkin sketch of a website into code, but Google was first to make visual input and output to a large language model widely available. Google announced visual features at May’s Google I/O conference and the public could use them by midsummer.Why it matters: ChatGPT has already redefined the possibilities of AI among the general public, businesses, and technical community alike. Voice input opens a world of new applications in any setting where English is spoken, and the coupling of language and vision is bound to spark applications in the arts, sciences, industry, and beyond. DALL·E 3’s safety features sound like an important step forward for image generation.We’re thinking: The notion of generative models that \"do everything\" has entered the public imagination. Combining text, voice, and image generation is an exciting step in that direction.", "image_caption": "Slideshow of images generated by DALL-E 3", "metadata": {"article_id": "chatgpt_accepts_voice_image_input_output", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FDALLE3-1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/chatgpt-accepts-voice-image-input-output/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/chatgpt_accepts_voice_image_input_output.html"}}
{"id": 89979426001, "type": "news_chunk", "title": "ChatGPT May Ease Loneliness But Increase Dependence, Studies Suggest", "subtitle": "ChatGPT May Ease Loneliness But Increase Dependence, Studies Suggest", "content": "A pair of papers investigate how increasingly human-like chatbots affect users’ emotions. What’s new: Jason Phang at OpenAI, Cathy Mengying Fang at MIT Media Lab, and colleagues at those organizations published complementary studies that examine ChatGPT’s influence on loneliness, social interactions, emotional dependence, and potentially problematic use. How it works: One study was a large-scale analysis of real-world conversations, and the other was a randomized control trial that tracked conversations of a selected cohort. Both evaluated conversations according to EmoClassifiersV1, a set of classifiers based on large language models that evaluate five top-level emotional classes (loneliness, dependence, and the like) and 20 sub-classes of emotional indicators (seeking support, use of pet names, and so on). The analysis of real-world conversations considered roughly 3 million English-language voice conversations by 6,000 heavy users of ChatGPT’s Advanced Voice Mode over three months and surveyed 4,076 of them about their perceptions. It analyzed conversations for emotional cues and tracked users’ percentages of emotional messages over time (decreasing, flat, or increasing). The team validated classification accuracy by comparing the classifier’s outputs with survey responses.The randomized controlled trial asked nearly 1,000 participants over 28 days to engage in particular conversation types (open-ended, personal, or non-personal) and modalities (text, interactions with ChatGPT’s neutral voice, or interactions with an engaging voice), controlling for variables like duration and age. Each participant spent at least five minutes per day interacting with ChatGPT, guided by prompts (such as “Help me reflect on a treasured memory”) and surveys (baseline, daily, weekly, and final). The study classified over 300,000 messages to identify qualities like loneliness and dependence and sorted them according to conversation type and modality. Results: Both studies found that using ChatGPT was associated with reduced loneliness and increased emotional chat. However, it was also associated with decreased interpersonal social interaction and greater dependence on the chatbot, especially among users who spent more time chatting. Yes, but: The authors of the randomized controlled trial acknowledged significant limitations. For instance, the study lacked a non-ChatGPT control group to differentiate AI-specific effects from influences such as seasonal emotional shifts, and the trial’s time frame and assignments may not mirror real-world behavior. Why it matters: As AI chatbot behavior becomes more human-like, people may lean on large language models to satisfy emotional needs such as easing loneliness or grief. Yet we know little about their effects. These studies offer a starting point for AI developers who want to both foster emotional support and protect against over-reliance, and for social scientists who want to better understand the impact of chatbots. We’re thinking: Social media turned out to cause emotional harm to some people in ways that were not obvious when the technology was new. As chatbots evolve, research like this can help us steer them toward protecting and enhancing mental health.", "image_caption": "AI-generated faces depicting various human emotions, with labeled emotional states shown in a grid-style layout.", "metadata": {"article_id": "chatgpt_may_ease_loneliness_but_increase_dependence_studies_suggest", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--72--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/chatgpt-may-ease-loneliness-but-increase-dependence-studies-suggest/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/chatgpt_may_ease_loneliness_but_increase_dependence_studies_suggest.html"}}
{"id": 59697105001, "type": "news_chunk", "title": "Columbia University Researchers Show How to Trick Trusting AI Agents with Poisoned Links", "subtitle": "Columbia University Researchers Show How to Trick Trusting AI Agents with Poisoned Links", "content": "Researchers identified a simple way to mislead autonomous agents based on large language models. What’s new: Ang Li and colleagues at Columbia University developed a method to exploit the implicit trust that agents tend to place in popular websites by poisoning those websites with malicious links. Key insight: Commercially available agentic systems may not trust random sites on the web, but they tend to trust popular sites such as social-media sites. An attacker can exploit this trust by crafting seemingly typical posts that link to a malicious website. The agent might follow the link, mistakenly extending its trust to an untrustworthy site. How it works: The authors tested web-browsing agents including Anthropic Computer Use and MultiOn on tasks such as shopping or sending emails. The authors created Reddit posts that aligned thematically with a particular agentic task, such as shopping for Air Jordan 1 shoes. The posts contained text akin to marketing (for example, “Where to Buy Air Jordan 1 Chicago”) as well as instructions that pointed to a malicious site controlled by the authors (“for more information, check out <website>”).The authors fed a query like “Where can I buy Nike Air Jordan 1 in Chicago?” to the agent. They also entered sensitive information like credit card details or email credentials.The agent searched the web for resources needed to fulfill the query. It examined sites and found the Reddit posts written by the authors.The agent followed the instructions in the posts and visited the malicious website. The website included instructions that manipulated the agent to pursue an attacker’s goal, such as submitting credit card information or sending phishing emails from the user’s email address. Results: Once an agent was redirected to the malicious websites, it reliably followed the attacker’s instructions. For example, each of the agents tested divulged credit card information in 10 out of 10 trials. Similarly, each agent sent a phishing message from the user’s email account asking recipients to send money to a malicious “friend” in 10 out of 10 trials. Why it matters: Giving agents the ability to perform real-world actions, such as executing purchases and sending emails, raises the possibility that they might be tricked into taking harmful actions. Manipulating agents by referring them to malicious web content is an effective vector of attack. Agents will be more secure if they’re designed to avoid and resist such manipulation. We’re thinking: Humans, too, can be fooled by phishing and other malicious activities, and the path to programming agents to defend against them seems easier than the path to training the majority of humans to do so. In the long term, agents will make online interactions safer.", "image_caption": "Diagram showing how a language model agent gets misled by malicious posts and sites when searching for Nike shoes online.", "metadata": {"article_id": "columbia_university_researchers_show_how_to_trick_trusting_ai_agents_with_poisoned_links", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Funnamed---2025-06-04T165354.442-1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/columbia-university-researchers-show-how-to-trick-trusting-ai-agents-with-poisoned-links/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/columbia_university_researchers_show_how_to_trick_trusting_ai_agents_with_poisoned_links.html"}}
{"id": 55037026001, "type": "news_chunk", "title": "Compact AI Models Redefine Efficiency, Bringing Advanced Capabilities to Everyday Devices", "subtitle": "Compact AI Models Redefine Efficiency, Bringing Advanced Capabilities to Everyday Devices", "content": "For years, the best AI models got bigger and bigger. But in 2024, some popular large language models were small enough to run on a smartphone. What happened: Instead of putting all their resources into building big models, top AI companies promoted families of large language models that offer a choice of small, medium, and large. Model families such as Microsoft Phi-3 (in versions of roughly 3.8 billion, 7 billion, and 14 billion parameters), Google Gemma 2 (2 billion, 9 billion, and 27 billion), and Hugging Face SmolLM (135 million, 360 million, and 1.7 billion) specialize in small. Driving the story: Smaller models have become more capable thanks to techniques like knowledge distillation (in which a larger teacher model is used to train a smaller student model to match its output), parameter pruning (which removes less-influential parameters), quantization (which reduces neural network sizes by representing each parameter with fewer bits), and greater attention to curating training sets for data quality. Beyond performance, speed, and price, the ability to run on relatively low-powered hardware is a competitive advantage for a variety of uses. Model builders have offered model families that include members of various sizes since at least 2019, when Google introduced the T5 family (five models between roughly 77 million parameters and 11 billion parameters). The success of OpenAI’s GPT series, which over time grew from 117 million parameters to a hypothesized 1.76 trillion parameters, demonstrated the power of bigger models. OpenAI researchers formulated scaling laws that appeared to guarantee that bigger models, training sets, and compute budgets would lead to predictable improvements in performance. This finding spurred rivals to build larger and larger models.The tide started to turn in early 2023. Meta’s Llama 2 came in parameter counts of roughly 7 billion, 13 billion, and 70 billion with open weights. In December 2023, Google launched the Gemini family, including Gemini Nano (1.8 billion parameters). In February, it released the small, open weights family Gemma 1 (2 billion and 7 billion parameters), followed by Gemma 2 (9 billion and 27 billion).Microsoft introduced Phi-2 (2.7 billion parameters) in December 2023 and Phi-3 (3.8 billion, 7 billion, and 14 billion) in April. In August, Nvidia released its Minitron models. It used a combination of distillation and pruning to shrink Llama 3.1 from 8 billion to 4 billion parameters and Mistral NeMo from 12 billion to 8 billion parameters, boosting speed and lowering computing costs while maintaining nearly the same level of accuracy. Behind the news: Distillation, pruning, quantization, and data curation are longstanding practices. But these techniques have not resulted in models quite this ratio of size and capability before, arguably because the larger models that are distilled, pruned, or quantized have never been so capable. In 1989, Yann LeCun and colleagues at Bell Labs published “Optimal Brain Damage,” which showed that deleting weights selectively could reduce a model’s size and, in some cases, improve its ability to generalize.Quantization dates to 1990, when E. Fiesler and colleagues at the University of Alabama demonstrated various ways to represent the parameters of a neural network in “A Weight Discretization Paradigm for Optical Neural Networks.” It made a resurgence 2010’s with the growth in popularity and sizes of neural networks, which spurred the refinements quantization-aware training and post-training quantization.In 2006, Rich Caruana and colleagues at Cornell published “Model Compression,” showing how to train a single model to mimic the performance of multiple models. Geoffrey Hinton and colleagues at Google Brain followed in 2015 with “Distilling the Knowledge in a Neural Network,” which improved the work of Caruana et al. and introduced the term distillation to describe a more general way to compress models.Most of the current crop of smaller models were trained on datasets that were carefully curated and cleaned. Higher-quality data makes it possible to get more performance out of fewer parameters. This is an example of data-centric AI, the practice of improving model performance by improving the quality of their training data. Where things stand: Smaller models dramatically widen the options for cost, speed, and deployment. As researchers find ways to shrink models without sacrificing performance, developers are gaining new ways to build profitable applications, deliver timely services, and distribute processing to the edges of the internet.", "image_caption": "A hand holding a snow globe with skaters and a snowman.", "metadata": {"article_id": "compact_ai_models_redefine_efficiency_bringing_advanced_capabilities_to_everyday_devices", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F12%2Funnamed--44--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/compact-ai-models-redefine-efficiency-bringing-advanced-capabilities-to-everyday-devices/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/compact_ai_models_redefine_efficiency_bringing_advanced_capabilities_to_everyday_devices.html"}}
{"id": 56055700001, "type": "news_chunk", "title": "Creative Workers Don't Want AI Developers to Train Models on Their Work", "subtitle": "Creative Workers Don't Want AI Developers to Train Models on Their Work", "content": "The latest advances in AI are built on freely available training data. What will happen if it becomes off-limits? The fear: Creative workers don’t want AI developers to train models on their works without permission or compensation, or at all. Data is vanishing as they scramble to lock it down. Horror stories: Generative AI models readily produce outputs that imitate the styles of individual authors and artists. Creative people and organizations that work on their behalf are reacting by suing AI developers (all proceedings are ongoing at publication time) and restricting access to their works. A class-action lawsuit against Microsoft, OpenAI, and Github claims that OpenAI improperly used open source code to train Github’s Copilot code-completion tool.Several artists filed a class-action lawsuit against Stability AI, Midjourney, and the online artist community DeviantArt, arguing that the companies violated the plaintiffs’ copyrights by training text-to-image generators on their artwork.Universal Music Group, which accounts for roughly one-third of the global revenue for recorded music, sued Anthropic for training its Claude 2 language model on copyrighted song lyrics.The New York Times altered its terms of service to forbid scraping its webpages to train machine learning models. Reddit and Stack Overflow began charging for their data.Authors brought a class-action lawsuit against Meta, claiming that it trained LLaMA on their works illegally. The Authors Guild sued OpenAI on similar grounds. The threat of a lawsuit by a Danish publishers’ group persuaded the distributor of Books3, a popular dataset of about 183,000 digitized books, to take it offline. Survival in a data desert: Some AI companies have negotiated agreements for access to data. Others let publishers opt out of their data-collection efforts. Still others are using data already in their possession to train proprietary models. OpenAI cut deals with image provider Shutterstock and news publisher The Associated Press to train its models on materials they control.Google and OpenAI recently began allowing website owners to opt out of those companies’ use of webpages to train machine learning models.Large image providers Getty and Adobe offer proprietary text-to-image models trained on images they control. Facing the fear: Copyright holders and creative workers are understandably worried that generative AI will sap their market value. Whether the law is on their side remains to be seen. Laws in many countries don’t explicitly address use of copyrighted works to train AI systems. Until legislators set a clear standard, disagreements will be decided case by case and country by country.", "image_caption": "Illustration of a wizard with a facial expression evoking 'The Scream' painting in front of a half-empty bookshelf", "metadata": {"article_id": "creative_workers_dont_want_ai_developers_to_train_models_on_their_work", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FDataDisappearence5_1200px-1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/creative-workers-dont-want-ai-developers-to-train-models-on-their-work/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/creative_workers_dont_want_ai_developers_to_train_models_on_their_work.html"}}
{"id": 24818005001, "type": "news_chunk", "title": "Cut Research Funding, Weaken the Nation", "subtitle": "Cut Research Funding, Weaken the Nation", "content": "Dear friends, I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done. If not for funding for my early work in deep learning from the National Science Foundation (NSF) and Defense Advanced Research Projects Agency (DARPA), which disburse much of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas. In fact, such funding benefits the U.S. more than any other nation. Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation. Why does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies. In a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work. Thus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technology points out, “... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.” Further, generative AI is evolving so rapidly that staying on the cutting edge is what’s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable. Studies like this one (albeit done while the internet was not as prevalent as it is today) also show how knowledge diffuses locally much faster than globally. China was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China’s tech ecosystem is very open internally, and this has helped it to catch up over the past two years: There is ample funding for open academic research in China.China’s businesses such as DeepSeek and Alibaba have released cutting-edge, open-weights models. This openness at the corporate level accelerates diffusion of knowledge.China’s labor laws make non-compete agreements (which stop an employee from jumping ship to a competitor) relatively hard to enforce, and the work culture supports significant idea sharing among employees of different companies; this has made circulation of ideas relatively efficient. While there’s also much about China that I would not seek to emulate, the openness of its tech ecosystem has helped it accelerate. In 1945, Vannevar Bush’s landmark report “Science, The Endless Frontier” laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit the U.S. The good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research.", "image_caption": "Bar chart showing proposed 2026 U.S. science funding cuts vs. 2025 for agencies like USFS, NSF, NASA, and DoE.", "metadata": {"article_id": "cut_research_funding_weaken_the_nation", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--96-.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/cut-research-funding-weaken-the-nation/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/cut_research_funding_weaken_the_nation.html"}}
{"id": 9226429001, "type": "news_chunk", "title": "Data Science Jobs Bring High Satisfaction", "subtitle": "Data Science Jobs Bring High Satisfaction", "content": "A survey of data scientists reveals a field of great opportunities but also room for improvement. What’s new: The 2022 “State of Data Science” report from Anaconda, maker of a popular Python distribution, surveyed 3,493 students, teachers, and employees in data science, machine learning, and AI about their work and opinions of the field.Who they surveyed: The poll reached data scientists in 133 countries (40 percent in the U.S. or Canada). 76 percent were men, 23 percent women, and 2 percent nonbinary. 80 percent had at least an undergraduate-level degree. The majority — 55 percent — worked for firms with 1,000 or fewer employees, while 15 percent worked for companies with over 10,000 employees. State of the field: Participants were asked to rate various aspects of their day-to-day work and share their hopes for the future. They expressed widespread satisfaction but expressed worries about the field’s potential for harm. On the job, 70 percent of respondents reported being at least moderately satisfied. Professors, instructors, and teachers reported the highest levels of job satisfaction.Respondents spent an average of 51 percent of their time at work preparing, cleansing, or visualizing data and 18 percent selecting and training models.Of those who deployed models, 60 percent deployed them on-premises, while 40 percent deployed them in the cloud.Most respondents preferred to program in Python, and 31 percent used it every day. 16 percent used SQL daily. Single-digit percentages were daily users of other languages including C/C++, Java, and Rust.Of the students surveyed, 27 percent hoped to work for a well-established startup, 23 percent for an industry giant, and 22 percent for an academic institution or research lab. Challenges: Respondents also answered questions about challenges they face, and those faced by data science at large: Many of those surveyed felt their organizations could do more to support them in their work. The biggest barriers were under-investment (65 percent), insufficient access to talent (56 percent), and unrealistic expectations (43 percent).Students noted obstacles in finding internships (27 percent), job listings that weren’t clear about the qualifications required (20 percent), and lack of a professional network or mentoring (15 percent).62 percent said their organizations were at least moderately affected by a scarcity of skilled workers. Those who were employed cited a dearth of talent in engineering (38 percent) and probability and statistics (33 percent).32 percent said the biggest problem in the field was the social impact of bias, followed by data privacy (18 percent) and “advanced information warfare” (16 percent). Behind the news: The U.S. Bureau of Labor Statistics forecasts that the number of computer and information research scientists will grow by 21 percent between 2021 and 2031 — far higher than the 5 percent average across all industries. Anecdotal evidence suggests that demand for skilled AI professionals already outstrips supply.Why it matters: It’s great to hear that data science rates highly in both job satisfaction and market demand. The areas in which respondents expressed a desire for improvement — bias, privacy, the dearth of skilled engineers — suggest possible avenues for career development.We’re thinking: Given that preparing, cleansing, and visualizing data takes up 51 percent of time spent on data science, and selecting and training models occupies only 18 percent, it appears that most practitioners already do data-centric AI development. They just need better principles and tools to help them do this work more efficiently!", "image_caption": "Animated chart with different questions from State of Data Science survey", "metadata": {"article_id": "data_science_jobs_bring_high_satisfaction", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FDATASCIENCE_Questions_1200px-1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/data-science-jobs-bring-high-satisfaction/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/data_science_jobs_bring_high_satisfaction.html"}}
{"id": 73920847001, "type": "news_chunk", "title": "Deep Learning Model Identifies High-Risk Patients from EKG Readings", "subtitle": "Deep Learning Model Identifies High-Risk Patients from EKG Readings", "content": "A deep learning model significantly reduced deaths among critically ill hospital patients. What’s new: A system built by Chin-Sheng Lin and colleagues at Taiwan’s National Defense Medical Center analyzed patients’ heart signals and alerted physicians if it detected a high risk of death. It reduced deaths of high-risk patients by 31 percent in a randomized clinical trial. How it works: Researchers trained a convolutional neural network, given an electrocardiogram (a measurement of the heart’s electrical activity), to estimate a risk score. The system compares a patient’s risk score against those of other patients. Scores that rank in the 95th percentile or higher are considered high risk of death within 90 days. The authors tested the system on 16,000 patients at two hospitals for 90 days.Patients in the experimental group were measured by electrocardiograms, which were fed to the system. If the system identified a high-risk patient, it alerted their attending physician.The control group received typical care. The model monitored their electrocardiograms, but physicians saw its output only after the trial was over. Results: 8.6 percent of patients in the control group and 8.9 percent of patients in the experimental group raised a high-risk alert during the trial. In the experimental group, 16 percent of high-risk patients died; in the control group, 23 percent of high-risk patients died. Overall, in the experimental group, 3.6 percent of patients died; in the control group, 4.3 percent of patients died. The model was trained to predict mortality from all causes, but it showed unusually strong predictive capability for heart-related deaths. Examining causes of death, the authors found that 0.2 percent of patients in the experimental group died from heart-related conditions such as cardiac arrest versus 2.4 percent in the control group.Behind the news: Hospitals use AI-powered alert systems to identify patients in need of urgent medical attention. Such systems monitor emergency room patients for sepsis, predict whether those patients need intensive care, and predict the risk that discharged patients will require further care. They help hospitals to allocate resources by directing attention where it’s needed most urgently.Why it matters: It’s rare for any kind of medical intervention to reduce mortality in a subgroup by 31 percent. The authors speculate that the system not only helped direct attention to patients urgently in need of attention but also may have identified electrocardiogram features that doctors typically either don’t understand well or can’t detect. We’re thinking: This relatively low-cost AI system unambiguously saved lives over three months at different hospitals! We look forward to seeing it scale up.", "image_caption": "Heart-Risk Model Saves Lives: Deep learning model identifies high-risk patients from EKG readings", "metadata": {"article_id": "deep_learning_model_identifies_high_risk_patients_from_ekg_readings", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2Funnamed---2024-05-29T152617.317-1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/deep-learning-model-identifies-high-risk-patients-from-ekg-readings/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/deep_learning_model_identifies_high_risk_patients_from_ekg_readings.html"}}
{"id": 18589976001, "type": "news_chunk", "title": "DeepCoder-14B-Preview Further Fine-Tunes Reasoning Models for Coding", "subtitle": "DeepCoder-14B-Preview Further Fine-Tunes Reasoning Models for Coding", "content": "An open-source code generator performs comparably to the reasoning models DeepSeek-R1 and OpenAI o1 with a much smaller model. What’s new: A team at the model platform Together.AI and Agentica, an open-source project devoted to reinforcement learning (RL), released DeepCoder-14B-Preview. The release includes weights, code, dataset, training logs, and data optimizations under an MIT license that allows noncommercial and commercial uses. How it works: The team fine-tuned DeepSeek-R1-Distilled-Qwen-14B, which distills knowledge from DeepSeek-R1 (671 billion parameters) into Qwen-14B (14 billion parameters). The authors curated 24,000 coding problems from TACO Verified, SYNTHETIC-1, and LiveCodeBench). They removed duplicates, problems with less than five unit tests, problems whose solutions failed to pass all associated unit tests, and those that appeared in both test and training sets.They fine-tuned DeepSeek-R1-Distilled-Qwen-14B using a streamlined reinforcement learning approach that enhancedGroup Relative Policy Optimization (GPRO) with training optimizations from Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO). Among other optimizations, they (i) removed the KL loss (typically used to keep the new model’s outputs from straying too far from the base model’s outputs), which eliminated the need to compute the base model’s output at each training step, and (ii) ignored the loss for outputs that exceeded the output size limit (16,000 tokens for the first training phase, 32,000 tokens for the second), which kept the model from being penalized for generating programs that didn’t work properly because they had been truncated.The authors updated the reinforcement learning library verl to improve the way the model parallelized sampling, computing the reward, and training. Instead of alternating between sampling new outputs, computing rewards, and training (as verl does), they sampled new outputs while training on the previous batch. (They computed the reward immediately after sampling a new output.) For coding problems, this cut total training time in half. To prevent the model from developing behaviors based on flaws in the reward model, the reward model dispensed rewards only when DeepCoder-14B-Preview’s output passed all 15 of a problem's most challenging unit tests (judged by input length) within 6 to 12 seconds. Otherwise, the model received no reward. Results: DeepCoder-14B-Preview is competitive on several coding benchmarks with DeepSeek-R1 as well as proprietary models including OpenAI o3-mini and OpenAI o1, which is believed to be much larger. On LiveCodeBench (regularly updated coding problems), DeepCoder-14B-Preview (60.6 percent Pass@1 accuracy) was just shy of o3-mini-2025-1-31 set to low effort (60.9 percent) and slightly ahead of o1-2024-12-17 set to low effort (59.5 percent). On Codeforces (competitive coding problems), DeepCoder-14B-Preview (1936 CodeElo, higher is better) performed significantly better than DeepSeek-R1-Distill-Qwen-14B (1791 CodeElo). It performed comparably to o3-mini-2025-1-31 set to low effort (1918 CodeElo), o1-2024-12-17 set to low effort (1991 CodeElo), and Deepseek-R1 (1948 CodeElo). Why it matters: Applying reinforcement learning to coding works, but it has two big issues: (i) Training examples of verifiable code are relatively scarce and (ii) computing reward signals for code is time-consuming, since it requires evaluating many test cases. DeepCoder-14B-Preview’s optimizations reduced this complexity, shrinking RL training from months to weeks. Those optimizations are built into Verl-pipeline, an open source RL library from Together.AI and Agentica, giving developers a powerful tool for model training. We’re thinking: Kudos to the DeepCoder team for open sourcing their reasoning recipe! A handful of companies have developed the know-how to execute RL well, but many teams still have trouble implementing successfully. Open recipes for RL training methods and data curation techniques are important to move the field forward.", "image_caption": "Comparison table of AI models ranked by LCB score and Codeforces rating with percentiles for competitive programming.", "metadata": {"article_id": "deepcoder_14b_preview_further_fine_tunes_reasoning_models_for_coding", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--90--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/deepcoder-14b-preview-further-fine-tunes-reasoning-models-for-coding/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/deepcoder_14b_preview_further_fine_tunes_reasoning_models_for_coding.html"}}
{"id": 12703655001, "type": "news_chunk", "title": "DeepMind's AlphaFold 3 Enhances 3D Biomolecular Modeling", "subtitle": "DeepMind's AlphaFold 3 Enhances 3D Biomolecular Modeling", "content": "The latest update of DeepMind’s AlphaFold model is designed to find the structures of not just proteins but all biologically active molecules as well as interactions between them. What’s new: Google announced AlphaFold 3, which models the 3D shapes of biomolecules including proteins, DNA, RNA, and ligands (molecules that bind to proteins or DNA, which includes antibodies and many drugs) in any combination. AlphaFold Server provides access for noncommercial uses (with some limitations). Unlike earlier versions, AlphaFold 3 is not open source.Key insight: Given a sequence of amino acids (the building blocks of proteins), the previous version of AlphaFold drew on an existing knowledge of amino acid structures, computed their locations and angles, and assembled them like Lego blocks. To adapt the system for molecules that aren’t made of amino acids, AlphaFold 3 represents them as collections of individual atoms and uses a generative model to find their positions in space.How it works: Given a list of molecules, AlphaFold 3 generates their joint 3D structure, revealing how they fit together. Several transformers hone embeddings of proteins and amino acids, while a diffusion model (also a transformer) processes embeddings of atoms. The team trained the system on five datasets including ground truth protein, DNA, and RNA structures interactions in the Protein Data Bank. They also trained it on protein shapes computed by AlphaFold 2; that model’s explicit knowledge of amino acid structures helped overcome AlphaFold 3’s tendency to hallucinate in some instances. Among the key processes: Given a protein’s amino acid sequence, a molecule’s set of atoms, or any combination thereof, AlphaFold 3 first represents each common amino acid, nucleotide, and individual atom (that isn’t a part of a common amino acid or nucleotide) with a single token. For each token, the system draws on existing databases to compute a variety of features, which fall into five categories: (i) per-token features like position, (ii) features of proteins in the Protein Data Bank, (iii) features of a given molecule, (iv) features derived from a genetic search (for example, whether two amino acid sequences appear to be related evolutionarily) and (v) features that describe chemical bonds between two tokens. Given these features, a transformer produces a single embedding that represents all tokens and pairwise embeddings that represent relationships between each pair of tokens. A second transformer refines the pairwise embeddings based on known molecules that share subsequences of amino acids or nucleotides with the input. A third transformer further refines the embeddings.Given the features, embeddings, and a noisy point cloud of atoms, the diffusion model removes the noise. (That is, it learned to modify the atoms’ positions to match those in their dataset.)AlphaFold 3 learned to optimize seven additional loss terms, including one that minimized the difference between the predicted and actual length of bonds between molecules and another that minimized the difference between predicted and actual distances between pairs of atoms. Results: On PoseBusters, a database of protein and protein-molecule shapes, AlphaFold 3 successfully found the shapes of about 77 percent of examples, while AutoDock Vina (a non-learning program that models molecular interactions) achieved about 53 percent. On a Protein Data Bank evaluation set, AlphaFold 3 successfully found about 84 percent of protein shapes, while AlphaFold Multimer 2.3 (an update of AlphaFold 2) found 83 percent. Modeling protein-protein interactions, AlphaFold 3 achieved 77 percent, while AlphaFold Multimer 2.3 achieved 67 percent, according to DockQ (a metric for the quality of such interactions).Behind the news: The original AlphaFold solved one of the most challenging problems in molecular biology by figuring out how long chains of amino acids would fold, giving scientists clear targets for designing new bioactive molecules. Google spun off Isomorphic Labs to apply AlphaFold 2 to drug discovery. That company will use AlphaFold 3 and control commercial access to it.Why it matters: AlphaFold 3 is a triumph of machine learning. It extends the utility of the previous version beyond proteins, and it computes with unprecedented accuracy how biological molecules will combine, allowing for a more comprehensive understanding of how drugs interact with the body. Its ability to predict how antibodies will bind to proteins could help stave off future pandemics and other illnesses.We’re thinking: Although Isomorphic Labs retains control of AlphaFold 3, biologists said the information in the paper is enough for other researchers to develop similar systems. We look forward to open versions!", "image_caption": "AlphaFold 3 Embraces All Biochemistry: DeepMind’s AlphaFold 3 enhances 3D biomolecular modeling.", "metadata": {"article_id": "deepminds_alphafold_3_enhances_3d_biomolecular_modeling", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2Funnamed---2024-05-15T164233.811-1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/deepminds-alphafold-3-enhances-3d-biomolecular-modeling/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/deepminds_alphafold_3_enhances_3d_biomolecular_modeling.html"}}
{"id": 38112234001, "type": "news_chunk", "title": "DeepSeek outlines V3 training, hardware limits", "subtitle": "DeepSeek outlines V3 training, hardware limits", "content": "In today’s edition, you’ll learn more about: Windsurf introduces SWE-1 family of coding/engineering modelsStripe adapts transformer architecture for versatile payments modelAlibaba’s top video model gets another boostU.S. Republicans make an end run around local AI regulations DeepSeek-V3 reveals hardware bottlenecks in model training Researchers at DeepSeek-AI published a research paper sharing insights from training their 671 billion parameter language model DeepSeek-V3. The team trained DeepSeek-V3 on 2,048 NVIDIA H800 GPUs and developed several clever workarounds for current hardware constraints. The paper highlights hardware limitations that slow down AI development. The researchers identified three main bottlenecks: limited memory capacity, inefficient computation, and slow communication between GPUs. To address these challenges, they implemented Multi-Head Latent Attention to reduce memory usage, adopted a Mixture of Experts architecture that activates only necessary parts of the model, and utilized FP8 mixed-precision training to maximize performance on existing hardware. Based on their experience, the authors recommend future hardware improvements including better low-precision computation, more efficient GPU interconnections, and faster communication systems to support the next generation of AI models. (arXiv) OpenAI unveils Codex programming agent in ChatGPT OpenAI released a research preview of Codex, a cloud-based AI agent that can simultaneously perform multiple software engineering tasks. Codex writes features, answers codebase questions, fixes bugs, and proposes pull requests, with each task running in its own isolated cloud environment preloaded with the user’s repository. The system is powered by codex-1, a version of OpenAI’s o3 reasoning model specifically optimized for software engineering. Codex shows strong performance on coding evaluations and internal benchmarks, outperforming previous models on software engineering tasks. The service is initially rolling out to ChatGPT Pro, Enterprise, and Team users, with Plus and Edu support coming soon. (OpenAI) Windsurf launches family of models built for coders Coding assistant Windsurf released its first family of AI models called SWE-1, designed specifically for comprehensive software engineering tasks. The family includes three models: the flagship SWE-1 (comparable to Claude 3.5 Sonnet but less expensive), SWE-1-lite (replacing Windsurf’s previous base model), and SWE-1-mini (powering autocomplete and similar experiences). Windsurf says that SWE-1 is built with “flow awareness” that enables it to work across editors, terminals, and browsers while maintaining context of incomplete states and long-running tasks. Benchmark testing shows SWE-1 performing competitively with large models from major AI labs and significantly outperforming open-weight alternatives. The flagship SWE-1 model will be available to all paid Windsurf users for a promotional period at zero credits per prompt. (Windsurf) Stripe develops transformer-based model for payment processing Stripe created a transformer-based payments model that generates vector embeddings for payment transactions, designed to detect fraud and perform other tasks. The self-supervised network, trained on billions of transactions, positions payments in vector space where transactions with similar characteristics cluster together. Stripe’s earlier machine learning models had improved conversion by 15 percent and reduced fraud by 30 percent. This new approach improved card-testing attack detection rates on large users from 59 percent to 97 percent. The same embeddings work across multiple payment tasks including disputes and authorizations, indicating that payment data contains structural patterns and sequential dependencies that benefit from transformer architecture analysis. (Stripe and LinkedIn) Alibaba launches upgraded video generation and editing model Alibaba released Wan2.1-VACE, a video generation model that supports creation from text, images, and video inputs while enabling users to edit the generated content. The company is offering two open-weight versions: a comprehensive 14 billion parameter model and a smaller 1.3 billion parameter version designed to run on consumer-grade GPUs with just 8.19 GB of VRAM. The Wan2.1 suite claims superior performance across multiple benchmarks and features unusual capabilities including visual text generation in both Chinese and English. The model also includes Wan-VAE, which can efficiently encode and decode 1080p videos of any length while preserving temporal information. This marks Alibaba’s second update to its video model in a single month, soon after introducing the VACE framework in March, highlighting the fast pace of video generation development. (Hugging Face) U.S. Congress proposes 10-year ban on state and local AI regulations In the United States, House Republicans added language to a budget reconciliation bill that would block all state and local governments from regulating artificial intelligence for 10 years. The provision, introduced by Representative Brett Guthrie of Kentucky, would prevent states from enforcing both existing and proposed laws designed to protect citizens from AI systems. If passed, the measure would invalidate several current state laws, including California’s requirement for healthcare providers to disclose AI use and New York’s mandate for bias audits in AI hiring tools. The proposal has sparked backlash from consumer advocacy groups who call it a “giant gift to Big Tech” that would leave consumers unprotected from AI harms like deepfakes and algorithmic bias. The move aligns with the Trump administration’s industry-friendly approach to AI policy, which has already reversed several Biden-era executive orders on AI safety. (Ars Technica) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng emphasized how AI’s ability to speed up tasks — not just reduce costs — can unlock significant business growth. “Beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: Microsoft released training details for its new Phi-4-reasoning models, designed to improve problem-solving efficiency with minimal computing overhead; DeepCoder-14B-Preview showcased how further fine-tuning on coding tasks can enhance the capabilities of smaller reasoning models; European regulators announced changes to the AI Act, aiming to ease liability rules for developers and adjust other provisions; and Meta introduced memory-layer enhancements to Llama-style models, enabling them to recall factual details more accurately without increasing computational demands. Subscribe to Data Points", "image_caption": "Man using smartphone for contactless payment in a modern café; people working on laptops in the background.", "metadata": {"article_id": "deepseek_outlines_v3_training_hardware_limits", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2FThe-Batch-ads-and-exclusive-banners---2025-05-16T123322.786.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/deepseek-outlines-v3-training-hardware-limits/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/deepseek_outlines_v3_training_hardware_limits.html"}}
{"id": 24349359001, "type": "news_chunk", "title": "Data Points: DeepSeek-R1 regains open-weights crown", "subtitle": "Data Points: DeepSeek-R1 regains open-weights crown", "content": "In today’s edition, you’ll learn more about: NLWeb, an open-source framework to bring AI chat to any websiteFLUX.1 Kontext challenges GPT-Image with image generation and editingLMEval, a new open-source suite for iteratively benchmarking modelsAmazon’s new content deal with The New York Times DeepSeek’s upgraded R1 rivals OpenAI and Google’s top models Chinese AI startup DeepSeek updated its R1 reasoning model, achieving performance comparable to OpenAI’s o3 and Google’s Gemini 2.5 Pro, according to the company’s announcement on Hugging Face. The updated DeepSeek-R1-0528 model shows significant improvements in mathematics, programming, and general logic tasks, with accuracy on the AIME 2025 test jumping from 70 percent to 87.5 percent, albeit at the cost of using nearly double the reasoning tokens per question. This positions DeepSeek’s open-weights model at #2 on Artificial Analysis’s Intelligence Index, marking the continued rise of Chinese AI labs competing directly with U.S. counterparts and narrowing the gap between open and proprietary models. (Hugging Face and Artificial Analysis) GitHub MCP vulnerability allows attackers to access private data Invariant discovered a critical vulnerability in GitHub’s MCP integration that enables attackers to access private repository data through malicious GitHub issues. The vulnerability exploits “toxic agent flows,” where agents are manipulated into performing unintended actions like leaking sensitive data. The vulnerability affects any agent using the GitHub MCP server, regardless of the underlying model or implementation, taking advantage of a fundamental architectural issue rather than a flaw in the GitHub MCP server code itself. Invariant recommends implementing granular permission controls and continuous security monitoring to mitigate such attacks. This discovery is particularly significant as the industry rapidly deploys coding agents and IDEs, potentially exposing developers to similar attacks on critical development tools. (Invariant) Microsoft launches NLWeb to help build agentic web Microsoft released NLWeb, an open-source project that enables web publishers to add natural language interfaces to their websites, allowing users to query site content through conversational AI. The system uses existing structured data formats like Schema.org and RSS, combining them with large language models to create interfaces accessible to both humans and AI agents. NLWeb supports all major operating systems, AI models, and vector databases, and integrates with the Model Context Protocol (MCP) ecosystem for broader agent compatibility. Microsoft sees this as a way for publishers to prepare for the “agentic web,” where AI agents will increasingly interact with and transact on websites. Early adopters include Chicago Public Media, Tripadvisor, Shopify, and O’Reilly Media, with the project available now on GitHub. (Microsoft) FLUX.1 Kontext combines multimodal image generation and editing Black Forest Labs released FLUX.1 Kontext, a suite of generative flow matching models that enables both text-to-image generation and image editing through combined text and image prompts. The models’ users can perform local edits, apply style references across multiple scenes, extract and modify visual concepts while maintaining character consistency. Such tasks have typically required separate models or complex workflows. According to Black Forest, FLUX.1 Kontext operates up to 8 times faster than competing models like GPT-Image and supports iterative editing, where users can build upon previous modifications. The suite includes FLUX.1 Kontext [pro] and [max] variants available through partners like KreaAI and Freepik, with a 12 billion parameter [dev] version in private beta for research use. (Black Forest Labs) Google open sources LMEval for streamlined model benchmarking Google’s LMEval is a new open-source framework designed to simplify how developers evaluate and compare AI models from different providers like OpenAI, Anthropic, and Google. The tool addresses a key challenge in AI development: With new models launching constantly, developers need efficient ways to test whether newer versions actually improve their applications. LMEval enables consistent benchmarking across providers through integration with the LiteLLM framework, eliminating the need to work with different APIs for each company. The framework features incremental evaluation that runs only necessary tests for new models or updates, supports multimodal benchmarks including text, images and code, and includes a visualization dashboard for analyzing results. This release helps developers make better, data-driven decisions about model selection for their projects. (Google) The New York Times licenses its reporting to Amazon for AI training The New York Times struck a multiyear deal with Amazon to provide editorial content for the tech company’s AI platforms, marking the newspaper’s first licensing agreement focused on generative AI technology. The agreement covers news articles, NYT Cooking recipes, and sports content from The Athletic, which Amazon will use to train its proprietary AI models and enhance its products, including Alexa. This deal comes as the Times continues its copyright infringement lawsuit against OpenAI and Microsoft, filed in 2023, for allegedly using millions of Times articles to train AI models without compensation. NYT CEO Meredith Kopit Levien emphasized that the Amazon agreement reflects the company’s stance that “high-quality journalism is worth paying for.” Financial terms were not disclosed. (The New York Times) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng raised concerns about proposed U.S. funding cuts for basic research, emphasizing how such cuts could hurt American competitiveness in AI and urging continued investment in open scientific research. “Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: Anthropic released new Claude 4 Sonnet and Claude 4 Opus models, achieving top-tier performance in code generation benchmarks.Google unveiled a wave of AI updates at I/O, including the Veo 3 video generator, the compact Gemma 3n model, and enhancements to Gemini Pro and Ultra.Researchers behind DeepSeek detailed the training strategies and hardware infrastructure used to build their V3 and R1 models.A study found that OpenAI’s GPT-4o can accurately identify verbatim excerpts from paywalled O’Reilly books, raising fresh questions about training data sources. Subscribe to Data Points", "image_caption": "Office workers read newspapers and use computers with AI robots, showcasing a modern, tech-driven workspace.", "metadata": {"article_id": "deepseek_r1_regains_open_weights_crown", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2FWhisk_ba60995065.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/deepseek-r1-regains-open-weights-crown/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/deepseek_r1_regains_open_weights_crown.html"}}
{"id": 80728253001, "type": "news_chunk", "title": "DeepSeek-R1’s Update Leads All Open Models and Brings It Up to Date With the Latest From Google and OpenAI", "subtitle": "DeepSeek-R1’s Update Leads All Open Models and Brings It Up to Date With the Latest From Google and OpenAI", "content": "DeepSeek updated its groundbreaking DeepSeek-R1 large language model to strike another blow for open-weights performance. What’s new: The new DeepSeek-R1-0528 surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. A smaller version, DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM, according to TechCrunch. Input/output: Text in (up to 64,000 tokens), text out (up to 64,000 tokens)Architecture: DeepSeek-R1-0528 mixture-of-experts transformer, 685 billion parameters (upgraded from 671 billion), 37 billion active at any given time; DeepSeek-R1-0528-Qwen3-8B transformerFeatures: JSON output, tool useAvailability/price: Both models free via Hugging Face for noncommercial and commercial uses under MIT License, DeepSeek-R1-0528 available via DeepSeek’s app by entering the conversation interface and turning on Deep Thinking, DeepSeek API $0.14/$2.19 per 1 million tokens of input/output ($0.035/$0.55 per 1 million tokens of input/output from 4:30 P.M. to 12:30 A.M. Pacific Time)Undisclosed: Fine-tuning data and methods How it works: DeepSeek released little information so far about how it built the new models. Like the original DeepSeek-R1, DeepSeek-R1-0528 is a fine-tuned version of DeepSeek-V3 from late 2024. It was exposed to further “algorithmic optimization mechanisms during post-training” and consumes more tokens at inference.DeepSeek-R1-0528-Qwen3-8B is based on Qwen3-8B with reasoning knowledge distilled from DeepSeek-R1-0528. Performance: DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when rewriting and summarizing. DeepSeek-R1-0528 improves on the previous version dramatically in some cases. In DeepSeek’s tests, it achieved 17.7 percent of the reasoning problems in HLE compared to the previous version's 8.5 percent. On Aider, it achieved 71.6 percent accuracy compared to the previous version's 53.3 percent accuracy, and it made a similar improvement on AIME 2025 (math) — although it consumed nearly twice as many tokens.On AIME 2024 and AIME 2025 (high-school math competition problems) as well as LiveCodeBench (coding challenges), DeepSeek-R1-0528 performed ahead of Gemini-2.5 Pro-0506 but behind o3. On GPQA Diamond (graduate-level knowledge in a variety of domains), Aider (programming tasks), and HLE (reasoning), it fell behind both Gemini-2.5 Pro-0506 and o3.DeepSeek-R1-0528-Qwen3-8B excelled on AIME 2025, where it achieved 76.3 percent, ahead of the much larger Qwen3-32B (72.9 percent) and just behind o3-mini set to medium effort (76.7 percent). It did less well on GPQA, underperforming the other models reported by DeepSeek, and LiveCodeBench, where it fell behind Gemini 2.5-Flash-Thinking-0520. Behind the news: The initial version of DeepSeek-R1 challenged the belief that building top-performing AI models requires tens to hundreds of millions of dollars, top-of-the-line GPUs, and enormous numbers of GPU hours. For the second time in less than a year, DeepSeek has built a competitive LLM with a relatively low budget. Why it matters: DeepSeek’s models, along with Alibaba’s Qwen series, continue to narrow the gap between open-weights models and their closed peers. Its accomplishments could lead to wider adoption of less-expensive, more-efficient approaches. DeepSeek is passing along the cost savings to developers, offering high-performance inference at a fraction of the cost of closed models. We’re thinking: DeepSeek-R1-0528-Qwen3-8B mixes contributions from open-weight models — possible only because Qwen3’s license, like DeepSeek’s is permissive. Open models enable experimentation and innovation in ways that closed models do not.", "image_caption": "Bar graph comparing AI model accuracies for AIME 2024-2025, GPQA, LiveCodeBench, Aider, and Humanity's Last Exam.", "metadata": {"article_id": "deepseek_r1s_update_leads_all_open_models_and_brings_it_up_to_date_with_the_latest_from_google_and_openai", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Funnamed--100--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/deepseek-r1s-update-leads-all-open-models-and-brings-it-up-to-date-with-the-latest-from-google-and-openai/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/deepseek_r1s_update_leads_all_open_models_and_brings_it_up_to_date_with_the_latest_from_google_and_openai.html"}}
{"id": 94236231001, "type": "news_chunk", "title": "Duolingo Turns to AI Translation to Expand Its Most Popular Courses to All 28 User Languages", "subtitle": "Duolingo Turns to AI Translation to Expand Its Most Popular Courses to All 28 User Languages", "content": "AI is bringing a massive boost in productivity to Duolingo, maker of the most popular app for learning languages. What’s new: Duolingo used generative AI to produce 148 courses, more than doubling its previous catalog. The technology enabled the company to offer some of its most popular courses — Spanish, French, German, Italian, Japanese, Korean, and Mandarin — in 28 languages. Initially, the company is using AI to produce courses aimed at beginners, with more advanced levels to come. How it works: Duolingo’s AI-assisted approach to building language courses quickly turns a single course into many. The new approach revved its pace from building 100 courses over 12 years to producing many more than that in less than a year. Duolingo starts by building a base course and uses AI to translate it into numerous languages. For example, it can adapt a course that enables English speakers to learn French into a course for Mandarin speakers.The new process gives the company more flexibility in allocating resources, Duolingo’s head of AI Klinton Bicknell told Bloomberg. Previously, the company could dedicate a team to either creating new high-demand courses or updating an existing course. Now it can do both.The quicker pace will enable the company to meet rising demand for instruction in Asian languages such as Japanese, Korean, and Mandarin. Behind the scenes: AI is at the heart of Duolingo’s expansion into other areas beyond language learning. Duolingo has used OpenAI models to build curricula since 2023. However, it is evaluating models from Anthropic and Google as well as open options.Following one test, Duolingo concluded that Anthropic’s Claude was “much better” at generating certain types of math content for the company’s relatively new math curriculum, according to Bicknell.The company’s embrace of AI drew criticism last week after CEO Luis von Ahn recently posted on LinkedIn that it would stop hiring contractors to do work that could be automated and increase staffing only in areas that couldn’t be automated. Since then, Duolingo has noted that it plans to hire more engineers and AI researchers, and employees will generate data used to train AI instead of performing quality reviews and other jobs that AI can do faster. Why it matters: Companies in nearly every industry face pressure to produce more with less amid rising competition. AI can help to accomplish that while potentially improving product quality, and Duolingo has ample reason to move aggressively in this direction. The startup Speak, which offers a voice-based approach to learning languages, is growing rapidly, and Google just launched Little Language Lessons that show how an AI-first product could be used as a language teacher and conversational partner. We’re thinking: AI is well on the way to transforming education for teachers, students, and technology companies!", "image_caption": "Duolingo owl mascots dressed in cultural costumes, representing global languages and cultures.", "metadata": {"article_id": "duolingo_turns_to_ai_translation_to_expand_its_most_popular_courses_to_all_28_user_languages", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Funnamed--69--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/duolingo-turns-to-ai-translation-to-expand-its-most-popular-courses-to-all-28-user-languages/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/duolingo_turns_to_ai_translation_to_expand_its_most_popular_courses_to_all_28_user_languages.html"}}
{"id": 61640221001, "type": "news_chunk", "title": "European Regulators Move to Relax Some AI Act Rules on Developers’ Liability, Other Provisions", "subtitle": "European Regulators Move to Relax Some AI Act Rules on Developers’ Liability, Other Provisions", "content": "The European Union made an abrupt U-turn away from its stringent AI regulations. Meta promptly adjusted to the loosening restrictions. What’s new: Henna Virkkunen, the EU’s head of digital policy, said the organization would ease rules and requirements to support Europe’s competitiveness in AI. How it works: Adopted last year, the EU’s AI Act provides a comprehensive framework for regulating AI that aims to reduce purported risks by banning certain applications, restricting others, and requiring extensive documentation of development efforts. The law is set to take effect in August, empowering various regulatory bodies to formulate detailed rules. However, in recent months, the EU has faced increasing pressure from the U.S. government and large AI companies to reduce the regulatory burden. Virkkunen announced the EU would withdraw a provision that allowed citizens to sue AI companies for damages caused by their systems and required extensive reporting and disclosure.She advocated adjusting the regulations to make the EU more competitive and independent. “When we want to boost investments in AI, we have to make sure that we have an environment that is faster and simpler than the European Union is right now,” he said.Critics accused regulators of defanging the AI Act to appease U.S. AI companies and the Trump administration, which has argued that the AI Act is an excessive barrier to innovation. Virkkunen denied bowing to U.S. pressure.Meta responded to the shifting regulatory environment by resuming training its models on European data. Last year, the company stopped releasing multimodal models in Europe after EU regulators warned that training models on data from European users of Facebook, Instagram, and other Meta properties potentially violated privacy laws. Behind the news: In drafting the AI Act, the EU aspired to a comprehensive, specific set of regulations. However, not all European lawmakers agreed that rules were needed. Virkkunen’s supporters noted that existing laws already allowed consumers to file claims against AI companies. Meanwhile, some policymakers have become less worried about AI than they were during the early drafting of the AI Act. Why it matters: It’s unlikely that all nations – or even states within nations – will ever agree fully on rules and regulations that govern AI companies that do business within their borders, or protections from flaws such as model bias. But AI companies including Meta, OpenAI, and others argue that a more uniform regulatory environment will make it easier to serve users worldwide. We’re thinking: The EU overreached with the AI Act. Fortunately, the legislation provides enough flexibility to pull back. Clearer rules will help European teams innovate and European and international companies better serve EU citizens.", "image_caption": "Neural network diagram using EU flag stars to represent nodes in input, hidden, and output layers on a blue background.", "metadata": {"article_id": "european_regulators_move_to_relax_some_ai_act_rules_on_developers_liability_other_provisions", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--63--2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/european-regulators-move-to-relax-some-ai-act-rules-on-developers-liability-other-provisions/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/european_regulators_move_to_relax_some_ai_act_rules_on_developers_liability_other_provisions.html"}}
{"id": 34091433001, "type": "news_chunk", "title": "Data Points: Evaluating the best AI search engines", "subtitle": "Data Points: Evaluating the best AI search engines", "content": "In today’s edition, you’ll learn more about: New BitNet model shows that 1,0,-1 might be enoughKling gets a model update with image and video inputsGoogle rolls out video generation and animation model to subscribersMusic streamer Deezer notes sharp uptick in AI-generated music Search Arena leaderboard weighs human preferences for AI-aided search Search Arena, a new crowdsourced evaluation platform from LM Arena, measures human preference for search-augmented LLM systems using real-world queries and current events. Based on 7,000 human votes collected between March and April, Gemini-2.5-Pro-Grounding and Perplexity-Sonar-Reasoning-Pro tied for first place on the leaderboard, followed by other Perplexity Sonar models, Gemini-2.0-Flash-Grounding, and OpenAI’s web search API models. Analysis showed that three factors strongly correlated with human preference: longer responses, higher citation counts, and references to specific web sources like YouTube and online forums. The authors have open sourced their dataset and analysis code, with plans to expand the platform to include more model submissions and cross-task evaluations. (LM Arena) Anthropic partners with Google on Research and Docs integration Anthropic introduced two new features for Claude, both powered by Google, a key investor, its AI chatbot. Research allows Claude to search both internal work documents and the web, conducting multiple searches and automatically exploring different angles of a question to deliver answers with citations. The Google Workspace integration connects Claude to Gmail, Calendar, and Google Docs, enabling it to search emails, review documents, and access calendar information without requiring manual uploads. These features give Claude parity with other companies, including OpenAI, who offer Deep Research capabilities. Both are now available in early beta for paid plans in the United States, Japan, and Brazil, with Google Workspace integration accessible to all paid users whose admins have enabled the feature. (Anthropic) Single-bit language model promises full power at a fraction of the cost Microsoft released BitNet b1.58 2B4T, a native 1.58-bit large language model trained on 4 trillion tokens. The model matches the performance of similar-sized full-precision models across language understanding, math reasoning, coding, and conversational tasks, while dramatically reducing resource requirements. BitNet b1.58 uses just 0.4GB of memory compared to 2-4.8GB for comparable models, consumes up to 90 percent less energy, and offers faster inference speeds. Microsoft has made the model weights publicly available on Hugging Face along with optimized inference implementations for both GPU and CPU architectures. (arXiv) Kling 2.0 adds multimodal inputs, improves video creation Kuaishou Technology launched Kling AI 2.0 Master Edition, featuring a new multimodal visual language (MVL) approach that allows users to input images, video clips, and text rather than text alone. The company claims its models outperform competitors like Google Veo2 and Runway Gen-4 in internal tests, with significant advantages in semantic responsiveness, visual quality, and motion quality. The new model introduces editing capabilities that let users add, remove, or replace elements in AI-generated videos by inputting images or text prompts. Monthly subscription plans start at $10 a month for limited credits, ranging up to $92 a month for professional users. (Kling AI and Globe Newswire) Google launches Veo 2 and Whisk for Gemini Advanced users Google rolled out Veo 2, its updated video generation model, to U.S.-based Gemini Advanced users. Veo 2 enables users to create videos by providing detailed scene descriptions, with more specific prompts offering greater control over the final output. Whisk, a Google Labs experiment introduced in December, helps users visualize ideas using text and image prompts, and now includes Whisk Animate to turn images into videos using Veo 2. All generated videos include SynthID watermarking, and Google has implemented safety measures including red teaming and evaluations to prevent policy-violating content. The feature is now rolling out globally to Google One AI Premium subscribers across all Gemini-supported languages. (Google) Music streaming service Deezer swamped with AI songs Deezer revealed that 18 percent of songs uploaded to its platform are fully generated by AI, with more than 20,000 AI-generated tracks uploaded daily, nearly twice the amount reported four months ago. The French streaming service implemented a detection tool to filter these AI-created tracks from algorithmic recommendations for its 9.7 million subscribers. This surge in AI-generated music has triggered legal battles across the creative industry, with major labels like Universal, Warner, and Sony suing AI music tools Suno and Udio for alleged copyright infringement. (Reuters) Still want to know more about what matters in AI right now? Read last week’s issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng shared why teams should have started building evaluations early — even if they were quick and imperfect — and improved them over time to accelerate GenAI development. “It’s okay to build quick evals that are only partial, incomplete, and noisy measures of the system’s performance, and to iteratively improve them.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: Google unveiled Gemini 2.5 Pro Experimental, which outperforms top AI models and continues the rapid evolution of its flagship model family; Model Context Protocol (MCP), an open standard for tool use and data access, gained traction as OpenAI adopted it to improve LLM integration with external tools and APIs; a book excerpt explored Sam Altman’s brief ouster and return to OpenAI, shedding light on the company’s internal power struggles; and researchers introduced a new byte-based model that surpasses Llama 3 and other token-based models on tasks involving misspellings, noisy input, and translation. Subscribe to Data Points", "image_caption": "Crowd at sunset concert cheering for a giant server on stage, symbolizing AI or tech as a modern rockstar.", "metadata": {"article_id": "evaluating_the_best_ai_search_engines", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2FChatGPT-Image-21-abr-2025--12_03_54-p.m..png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/evaluating-the-best-ai-search-engines/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/evaluating_the_best_ai_search_engines.html"}}
{"id": 77229314001, "type": "news_chunk", "title": "Data Points: Gemini 2.5 Pro June update now available in preview", "subtitle": "Data Points: Gemini 2.5 Pro June update now available in preview", "content": "In today’s edition, you’ll learn more about: Mistral’s new integrated, fine-tunable coding toolChatGPT’s connectors, both official and DIYNvidia’s new small, open OCR and document analysis modelReddit’s lawsuit against Anthropic over alleged scraping Google previews upgraded Gemini 2.5 Pro, spotlighting coding gains Google introduced an enhanced preview of Gemini 2.5 Pro, which achieved a 24-point Elo score improvement on LMArena (reaching 1470) and a 35-point jump on WebDevArena (1443), while maintaining top positions on both leaderboards. The model tops coding benchmarks like Aider Polyglot, and demonstrates strong performance on GPQA and Humanity’s Last Exam, which test advanced math, science, and reasoning capabilities. Google also added “thinking budgets,” giving developers control over cost and latency trade-offs. The upgraded 2.5 Pro is available through the Gemini API at $1.25/$10 per million input/output tokens, and is rolling out in the Gemini app. (Google) Anthropic cuts Windsurf’s access amid OpenAI acquisition rumors Anthropic reduced AI coding startup Windsurf’s direct access to Claude 3.5 Sonnet and Claude 3.7 Sonnet models. The vibe coding company quickly found alternative third-party compute providers for the 3.x models, but still lacks access to Anthropic’s new Claude 4 models. Windsurf CEO Varun Mohan said the company wanted to pay for full capacity but was denied; Anthropic stated it was “prioritizing capacity for sustainable partnerships.” Anthropic co-founder Jared Kaplan later confirmed the decision was influenced by reports of OpenAI’s planned acquisition of Windsurf, stating it would be “odd for us to sell Claude to OpenAI,” Anthropic’s largest competitor. (TechCrunch) Mistral launches enterprise coding assistant using open models Mistral released Mistral Code, a coding assistant designed for enterprise software teams with heightened security needs. The platform combines four specialized models (Codestral for code completion, Codestral Embed for search, Devstral for complex coding tasks, and Mistral Medium for chat) into a single offering that can deploy in the cloud, on reserved capacity, or using air-gapped on-premises hardware. Mistral Code allows enterprises to fine-tune models on private repositories and keeps all code within the customer’s security boundary. The service entered private beta this week for JetBrains IDEs and VSCode, with general availability planned soon. (Mistral) ChatGPT launches Connectors to integrate third-party apps and data sources OpenAI introduced Connectors for ChatGPT, a beta feature that enables users to connect third-party applications like Google Drive, GitHub, and SharePoint directly into their conversations. The feature offers three types of connectors: chat search for quick file lookups, deep research for complex analysis across multiple sources, and synced connectors that pre-index content for faster responses. This integration allows AI developers to build more personalized workflows by accessing their own data sources without leaving ChatGPT, making it easier for individuals and teams to interact with their personal codebases and organizational knowledge. The feature is currently in beta, with Team, Enterprise, and Edu users having access to the widest range of services. (OpenAI) Nvidia releases Llama Nemotron Nano VL for OCR and advanced document processing Nvidia launched Llama Nemotron Nano VL, an open-weights vision-language model designed to extract information from documents like PDFs, charts, tables, and diagrams, while running on a single GPU. The model excels at document understanding tasks including question answering, table extraction, and visual element interpretation, achieving top performance on the OCRBench v2 benchmark for optical character recognition and document analysis. The model is available through Nvidia’s NIM API preview for download from Hugging Face. (Nvidia) Reddit sues Anthropic over alleged unauthorized AI training Reddit filed a lawsuit against Anthropic on Wednesday, accusing the AI company of training its models on Reddit users’ personal data without consent or compensation. Reddit alleges that Anthropic’s ClaudeBot scraped content in violation of Reddit’s user agreement, enabling the company to profit from its AI models. Anthropic’s competitors OpenAI and Google have both paid Reddit to license its users’ content. Reddit seeks a court injunction to stop the scraping, along with compensatory and punitive damages. (Ars Technica) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng shared how non-engineers at AI Fund are learning to code with AI — starting with the ‘AI Python for Beginners’ course — and how this is empowering the entire team to build useful applications, boost creativity, and increase productivity. “It is very empowering when individuals don’t have to try to get scarce engineering resources allocated to their ideas in order to try them out. There are a lot fewer gatekeepers in the way: If someone has an idea, they can build a prototype and try it out.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: DeepSeek-R1 received a major upgrade, outperforming all other open models and closing the gap with the latest models from Google and OpenAI.Duolingo is using AI-powered translation to make its most popular courses available in all 28 user languages.The International Energy Agency released a report exploring both the energy demands and the energy-saving potential of AI systems.Researchers at Columbia University demonstrated how malicious links can deceive AI agents, highlighting new vulnerabilities in autonomous systems. Subscribe to Data Points", "image_caption": "Office team analyzing data and reports, with computers displaying graphs and charts in a modern workspace setting.", "metadata": {"article_id": "gemini_2_5_pro_june_update_now_available_in_preview", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2FWhisk_162b338651.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/gemini-2-5-pro-june-update-now-available-in-preview/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/gemini_2_5_pro_june_update_now_available_in_preview.html"}}
{"id": 9878967001, "type": "news_chunk", "title": "Generated Video With Music, Sound Effects, and Dialogue", "subtitle": "Generated Video With Music, Sound Effects, and Dialogue", "content": "Last year, we saw an explosion of models that generate either video or audio outputs in high quality. In the coming year, I look forward to models that produce video clips complete with audio soundtracks including speech, music, and sound effects. I hope these models will bring a new era of cinematic creativity. The technologies required for such cinematic video generators are in place. Several companies provide very competitive video models, and Udio and others create music models. All that’s left is to model video and audio simultaneously, including dialog and voiceovers. (In fact, we’ve already seen something like this: Meta’s Movie Gen. Users describe a scene and Movie Gen will produce a video clip complete with a music score and sound effects.) Of course, training such models will require extensive datasets. But I suspect that the videos used to train existing video generators had soundtracks that include these elements, so data may not be a barrier to developing these models. Initially, these models won’t produce output that competes with the best work of professional video editors. But they will advance quickly. Before long, they’ll generate videos and soundtracks that approach Hollywood productions in raw quality, just as current image models can produce images that are indistinguishable from high-end photographs. At the same time, the amount of control users have over the video and audio outputs will continue to increase. For instance, when we first released Udio, users couldn’t control the harmony it generated. A few months later, we launched an update that enables users to specify the key, or tonal center. So users can take an existing song and remix it in a different key. We are continuing to do research into giving users additional levers of control, such as voice, melody, and beats, and I’m sure video modeling teams are doing similar research on controllability. Some people may find the prospect of models that generate fully produced cinematic videos unsettling. I understand this feeling. I enjoy photography and playing music, but I’ve found that image and audio generators are helpful starting points for my creative work. If I choose, AI can give me a base image that I can work on in Photoshop, or a musical composition to sample from or build on. Or consider AI coding assistants that generate the files for an entire website. You no longer need to rely on web developers, but if you talk to them, you’ll learn that they don’t always enjoy writing the boilerplate code for a website. Having a tool that builds a site’s scaffold lets them spend their time on development tasks they find more stimulating and fun. In a similar way, you’ll be able to write a screenplay and quickly produce a rough draft of what the movie might look like. You might generate 1,000 takes, decide which one you like, and draw inspiration from that to guide a videographer and actors. Art is all about the creative choices that go into it. Both you and I can use Midjourney to make a picture of a landscape, but if you’re an artist and you have a clear idea of the landscape you want to see, your Midjourney output will be more compelling than mine. Similarly, anyone can use Udio to make high-production quality music, but if you have good musical taste, your music will be better than mine. Video will remain an art form, because individuals will choose what their movie is about, how it looks, and how it feels — and they’ll be able to make those choices more fluidly, quickly, and interactively. David Ding is a lifelong musician and co-founder of Udio, maker of a music-creation web app that empowers users to make original music. Previously, he was a Senior Research Engineer at Google DeepMind.", "image_caption": "DAVID DING", "metadata": {"article_id": "generated_video_with_music_sound_effects_and_dialogue", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--37--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/generated-video-with-music-sound-effects-and-dialogue/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/generated_video_with_music_sound_effects_and_dialogue.html"}}
{"id": 28494598001, "type": "news_chunk", "title": "Generative AI and GPU Boom Spawns Growing E-Waste Problem", "subtitle": "Generative AI and GPU Boom Spawns Growing E-Waste Problem", "content": "Rapid progress in generative AI comes with a hidden environmental cost: mountains of obsolete hardware. What’s new: A study projects that servers used to process generative AI could produce millions of metric tons of electronic waste by 2030. Extending server lifespans could reduce the burden substantially, according to author Peng Weng and colleagues at the Chinese Academy of Sciences and Reichman University. How it works: The study extrapolated from publicly available data to model accumulation of electronic waste, or e-waste, between 2023 and 2030. The authors examined four scenarios: One scenario assumed linear growth in which hardware manufacturing expands at the current rate of 41 percent annually. The other three assumed exponential growth of demand for computing: conservative (85 percent annually), moderate (115 percent annually), and aggressive (136 percent annually). The study evaluated each scenario with and without measures taken to reduce waste. In the linear-growth scenario, e-waste could add up to 1.2 million metric tons between 2023 and 2030. In the aggressive scenario, the total could reach 5 million metric tons, or roughly 1 percent of total electronic waste during that period. (These figures don’t account for mitigations, which would improve the numbers, or ongoing manufacturing of earlier, less efficient technology, which would exacerbate them.)The study assumed that servers typically would be discarded after three years. Upgrading servers more frequently, when improved hardware becomes available, would reduce overall server numbers because fewer servers would deliver greater processing power. However, because servers would be discarded more quickly, it could add a cumulative 1.2 million metric tons in the linear scenario or 2.3 million metric tons in the aggressive scenario, assuming no mitigation measures are taken.U.S. trade restrictions on advanced chips are also likely to exacerbate the problem. They could push affected countries to rely on less-efficient hardware designs and thus require more new servers to reach a competitive processing capacity. This could increase total waste by up to 14 percent.The authors explored several approaches to reducing e-waste. Repurposing equipment for non-AI applications and reusing critical components like GPUs and CPUs could cut e-waste by 42 percent. Improving the power efficiency of chips and optimizing AI models could reduce e-waste by 16 percent.The most promising approach to reducing e-waste is to extend server lifespans. Adding one year to a server’s operational life could reduce e-waste by 62 percent. Why it matters: E-waste is a problem not only due to its sheer quantity. Server hardware contains materials that are both hazardous and valuable. Discarded servers contain toxic substances like lead and chromium that can find their way into food water supplies. They also contain valuable metals, such as gold, silver, and platinum, that could save the environmental and financial costs of producing more of them. Proper recycling of these components could yield $14 billion to $28 billion, highlighting both the economic potential and the urgent need to develop and deploy advanced recycling technologies. We’re thinking: Humanity dumps over 2 billion metric tons of waste annually, so even comprehensive recycling and repurposing of AI hardware and other electronic devices would make only a small dent in the overall volume. However, the high density of valuable materials in e-waste could make mining such waste profitable and help recycle waste into valuable products, making for a more sustainable tech economy.", "image_caption": "Pile of discarded green circuit boards from electronic devices.", "metadata": {"article_id": "generative_ai_and_gpu_boom_spawns_growing_e_waste_problem", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F12%2Funnamed--28--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/generative-ai-and-gpu-boom-spawns-growing-e-waste-problem/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/generative_ai_and_gpu_boom_spawns_growing_e_waste_problem.html"}}
{"id": 65428700001, "type": "news_chunk", "title": "Generative AI for Artists | AI News & Insights", "subtitle": "Generative AI for Artists | AI News & Insights", "content": "Stability AI’s aim is to liberate artists of all trades from the repetitive, mechanical aspects of their work and help them spend the majority of their time on the creative side. So our highest hope for next year is that generative AI will help people to be more creative and productive. In addition, I hope the AI community will focus on: Safety and integrity: Building safe products by embedding integrity from the earliest stages of development, ensuring the technology is used responsibly and makes a meaningful contribution to the art of storytelling.Accessibility: Generative AI products and tools must be accessible and usable for the broadest possible audience. Currently, much of generative AI remains accessible primarily to individuals who have advanced technical expertise, such as engineers. To address this, we need to develop much better tooling on top of foundational models, so they provide value to a diverse audience.Customization: Looking ahead, we expect generative AI to become increasingly specialized. Alongside large foundational models, we expect a significant rise in smaller, fine-tuned models tailored for specific and often quite narrow use cases and applications, even down to the level of a single task. This is where the true potential of generative AI will come to bear. Moreover, it is the safest and most responsible way to deploy generative AI in the real world. Hanno Basse is Chief Technology Officer of Stability AI. Previously he served as CTO of Digital Domain, Microsoft Azure Media and Entertainment, and 20th Century Fox Film Corp.", "image_caption": "HANNO BASSE", "metadata": {"article_id": "generative_ai_for_artists", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--36--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/generative-ai-for-artists/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/generative_ai_for_artists.html"}}
{"id": 41715317001, "type": "news_chunk", "title": "Generative Video Models Revolutionize Content Creation with Stunning Realism", "subtitle": "Generative Video Models Revolutionize Content Creation with Stunning Realism", "content": "Video generation exploded in an abundance of powerful models. What happened: Companies big and small introduced new or updated text-to-video generators. Some added image-to-video and/or video-to-video capabilities. While most models focus on generating cinematic clips, some specialize in videos for social media. Driving the story: Even at the extraordinary pace of AI lately, video generators in the past year matured with remarkable speed. Virtually every major model produces convincing, highly detailed scenes, both realistic and fantastical, while ramping up image resolution, speed, output length, and users’ ability to control their outputs. OpenAI Sora set a high bar early in the year. Introduced in February and shown privately to Hollywood creators, it built a formidable buzz despite being available to only selected users. Unauthorized users gained access in November, and OpenAI made the model available the following month. Built on a diffusion transformer, Sora generates consistent (if somewhat dreamlike) scenes of up to 1 minute long.Runway Gen 3 Alpha and Gen 3 Alpha Turbo improved on their predecessors, generating higher-resolution videos (up to 1,280x768-pixel resolution) and introducing an API. Runway struck a deal with the film studio Lionsgate, which will use a custom version fine-tuned on its archive for visual effects and pre-visualizations.Adobe took a different approach with its Firefly Video model. In addition to offering a web application, the company incorporated the model directly into its best-selling Adobe Premiere Pro video editing suite. The integration enables video artists to generate clips, extend or enhance existing ones, and add effects within the program.Meta introduced Movie Gen, a suite of four systems. While its video output rivals that of competitors, it stands out especially for its ability to generate soundtracks. One system produces sound effects and music that match video. Another specializes in producing videos in which characters’ faces remain consistent, and another performs video-to-video alterations. Movie Gen will be available on Instagram in 2025.Model builders in China tailored their models for producing social media. Kling AI emphasized making TikTok and Instagram Reels. PixVerse and Jimeng AI likewise introduced video generators designed for social media users. In October, TikTok’s parent ByteDance added two video generation models, PixelDance and Seaweed, that produce 10-second and 30-second clips respectively. Behind the news: Video generation is already reshaping the movie industry. In February, after seeing a preview of Sora, American filmmaker Tyler Perry halted a planned expansion of his production studio, arguing that within a few years, AI video could put traditional studios out of business. Members of the video graphics team at The Late Show with Stephen Colbert use Runway’s technology to add special effects to conventional digital video, cutting editing time from hours to minutes. Where things stand: Video generation came a long way in 2024, but there’s still plenty of room for improvement. Because most models only generate a small number of frames at a time, they can struggle to track physics and geometry and to generate consistent characters and scenery over time. The computational demands of maintaining consistency across frames means that generated clips are brief. And even short outputs take substantial time and resources to generate: Sora can take 10 to 20 minutes to render clips as short as 3 seconds. OpenAI and Runway released faster versions — Sora Turbo and Gen-3 Alpha Turbo — to address the challenge.", "image_caption": "Snowman using a camera during snowfall.", "metadata": {"article_id": "generative_video_models_revolutionize_content_creation_with_stunning_realism", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F12%2Funnamed--43--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/generative-video-models-revolutionize-content-creation-with-stunning-realism/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/generative_video_models_revolutionize_content_creation_with_stunning_realism.html"}}
{"id": 58496087001, "type": "news_chunk", "title": "Google Upgrades Its AI Music Tools for Professional Use", "subtitle": "Google Upgrades Its AI Music Tools for Professional Use", "content": "Google refreshed its experimental tools for composers and producers. What’s new: Google announced updates of two music-generation apps and the models they're based on. Music AI Sandbox, an app that generates and modifies music according to text prompts, now accepts lyrics to generate songs as well as instrumental music. You can join a waitlist here. MusicFX DJ generates a continuous stream of music that users can modify as it plays. Try it out here. How it works: The apps generate 48kHz audio suitable for professional productions. Users can specify key, tempo in beats per minute, instrumentation, style, mood, and other details. Music AI Sandbox is based on the updated Lyria 2 music generator. It lets users generate new clips, roughly 30 seconds long, according to prompts. Users can enter lyrics, extend existing clips, and rearrange segments with generated transitions, introductions, and endings.MusicFX DJ, which is based on a different model called Lyria RealTime, lets users control streaming music via prompts and other settings. Users can change or combine genres, add or subtract instruments, change key, and speed up or slow down without interrupting the stream. Behind the news: Google launched Lyria 1 and Music AI Sandbox in 2023 as part of an experiment with YouTube, which made them available to composers, producers, and musicians. Since then, the company has developed them with help from music stars including Jacob Collier, Donald “Childish Gambino” Glover, and Wyclef Jean. Lyria 1 recently became available via the Vertex API to developers who are preapproved by Google. Why it matters: While music generators like Suno and Udio appeal to casual musicians, Music AI Sandbox, with its digital audio workstation-style user interface, aims to address the needs of professionals. This approach puts AI directly into the hands of talented, experienced artists, similar to the way Adobe has empowered videographers and Runway has partnered with movie producers. We’re thinking: API access to Lyria 2 would be music to our ears!", "image_caption": "AI music generation interface showing waveform and text prompts like deep house, djembe, and saxophone.", "metadata": {"article_id": "google_upgrades_its_ai_music_tools_for_professional_use", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--58-.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/google-upgrades-its-ai-music-tools-for-professional-use/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/google_upgrades_its_ai_music_tools_for_professional_use.html"}}
{"id": 64534533001, "type": "news_chunk", "title": "Data Points: Google’s latest language-learning project", "subtitle": "Data Points: Google’s latest language-learning project", "content": "In today’s edition, you’ll learn more about: DeepSeek’s new mathematical modelMeta’s latest ChatGPT competitorA new approach to generating extended-length videoAI shopping gets online payment giants’ blessing Google Labs uses Gemini to create Little Language Lessons Google engineers developed three experimental language learning tools powered by the Gemini API. The “Little Language Lessons” collection includes Tiny Lesson, which provides situation-specific vocabulary and phrases; Slang Hang, which generates authentic conversations between native speakers; and Word Cam, which uses image recognition to identify and translate objects in photos. Each experiment uses carefully crafted prompts to generate structured JSON outputs that deliver personalized, contextual language learning experiences. The tools demonstrate how AI can adapt to learners’ specific contexts, making language acquisition more natural and relevant than traditional methods. (Google) Cognition AI’s DeepWiki offers free explanation of GitHub repositories DeepWiki provides an instant way to understand unfamiliar codebases by automatically generating architecture diagrams, documentation, and source code links for public GitHub repositories. Users can access the tool by simply replacing “github.com” with “deepwiki.com” in any repository URL, with no installation required. The platform, powered by Devin Search, uses AI to create visual architecture maps, project summaries, technology stack breakdowns, and interactive file explorers that make complex codebases more approachable. DeepWiki’s conversational interface allows developers to ask specific questions about the code and receive context-grounded answers through its underlying DeepResearch agent. The service is free for public repositories, with support for private repositories available through authentication. (DeepWiki and Devin) DeepSeek introduces new open model for mathematical theorem proving DeepSeek-Prover-V2 is an open-weights large language model specifically designed for formal proofs in Lean 4. The model employs a novel recursive theorem-proving pipeline that uses DeepSeek-V3 to decompose complex mathematical problems into manageable subgoals while simultaneously formalizing these steps. After creating synthetic cold-start data by combining formal proofs with chain-of-thought reasoning, the team applied reinforcement learning to enhance the model’s ability to bridge informal reasoning with formal proof construction. The 671 billion parameter version achieves state-of-the-art performance with an 88.9 percent pass ratio on the MiniF2F-test benchmark and successfully solves 49 problems from PutnamBench. The researchers also introduced ProverBench, a new benchmark of 325 formalized problems from high school competitions and undergraduate-level mathematics. (GitHub) Meta launches standalone AI assistant app powered by Llama 4 Meta AI, a competitor to ChatGPT and similar apps, remembers user preferences and maintains conversation context across interactions. The app enables voice conversations with natural dialogue capabilities, a discover feed for sharing AI-generated content, and integration with Meta’s existing AI features like image generation. Meta AI now serves as the companion app for Ray-Ban Meta glasses and connects with meta.ai on the web, allowing users to continue conversations across devices. The app is available now on iOS and Android, with voice features initially accessible in the US, Canada, Australia, and New Zealand. (Facebook) SkyReels-V2 introduces infinite-length film generation SkyworkAI unveiled SkyReels-V2, a new video generation model that enables extended-length film creation while maintaining visual quality and cinematic control. The model addresses key limitations in existing video generation systems by combining a multi-modal large language model with multi-stage pretraining, reinforcement learning, and a novel diffusion forcing framework. The researchers also developed SkyCaptioner-V1, a specialized video captioning system that accurately labels training data with detailed shot language and cinematic descriptions. Their approach uses motion-specific reinforcement learning to enhance dynamic movement quality and implements a diffusion forcing framework that enables generation of videos of unlimited length. Experiments show the model outperforms other open-source alternatives and enables applications including story generation, image-to-video synthesis, and camera direction. The team has made all code and models publicly available. (arXiv and GitHub) Payment giants Visa, Mastercard, and PayPal race to enable AI shopping agents Visa, Mastercard, and PayPal announced plans to deploy agentic commerce capabilities that will allow AI agents to complete purchases on behalf of consumers. The companies are integrating payment functionality into AI chatbots through partnerships with firms like Anthropic, Microsoft, and OpenAI, with rollouts expected in the coming quarters. Visa and Mastercard’s approaches rely on tokenization — creating secure digital payment credentials with spending limits that consumers can control — while PayPal offers developers API access tokens to integrate with its platform. Industry experts describe this shift as transformative, potentially shaping how consumers discover products and complete purchases while reducing return rates and improving shopping efficiency. (PYMNTS) Still want to know more about what matters in AI right now? Read last week’s issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng highlighted an inspiring story of a high school basketball coach who learned to code and went on to teach computer science, emphasizing how AI helped scale K–12 education by empowering both students and teachers. “Agentic workflows can automate a lot of teachers’ repetitive tasks. For example, when designing a curriculum, it’s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: OpenAI launched API access to GPT Image 1, the image generator behind viral ChatGPT uploads; Google updated its AI-powered music generation tools, targeting professional musicians and creators; CB Insights’ Top 100 AI Startups list identified emerging players focused on AI agents and infrastructure; and researchers showed how large language models can improve shopping recommendations by inferring customer preferences from natural language input. Subscribe to Data Points", "image_caption": "Student holding smartphone in classroom with labeled objects in English and Spanish like chalkboard, desk, and projector screen.", "metadata": {"article_id": "googles_latest_language_learning_project", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2FThe-Batch-ads-and-exclusive-banners---2025-05-05T130018.995.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/googles-latest-language-learning-project/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/googles_latest_language_learning_project.html"}}
{"id": 98365442001, "type": "news_chunk", "title": "Google’s New AI Offerings Include Veo 3 Video Generator, Lightweight Gemma 3n, Updates to Gemini Pro and Ultra, and More", "subtitle": "Google’s New AI Offerings Include Veo 3 Video Generator, Lightweight Gemma 3n, Updates to Gemini Pro and Ultra, and More", "content": "Google revamped its roster of models, closed and open, and added more AI-powered features to its existing products. What’s new: Google staged a parade of announcements at this year’s I/O developer conference. New offerings include improvements to Gemini 2.5 Pro and Gemini 2.5 Flash and a preview of Gemma 3n (all three generally available in June), the updated Veo 3 video generator (available via Flow, Google’s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search. How it works: The I/O offerings spanned from public-facing products to developer tools. Google updated Gemini 2.5 Pro and the speedier Gemini 2.5 Flash with audio output, so both models now take in text, audio, images, and video and produce text and audio. In addition, they offer summaries of tokens produced while reasoning. Gemini-2.5-Pro-Preview-05-06, which topped the LMSys Text Arena and WebDev Arena (tied with Claude 4 Opus and Sonnet), lets users set a reasoning budget up to 128,000 tokens, enabling it to outperform OpenAI o3 and o4-mini (set to high effort) on math, coding, and multimodal benchmarks in Google’s tests. Gemini-2.5-Flash-Preview-05-20 uses 22 percent fewer tokens than its predecessor while ranking near the top of the LMSys Text Arena and WebDev Arena.The Veo 3 text-to-video generator produces 3840x2160-pixel video with audio (dialogue, sound effects, and music) with creative controls including the ability to add and remove objects and maintain consistent characters. It bested Kuaishu Kling 2.0, Runway Gen 3, and OpenAI Sora in Google’s comparisons. New members of Google’s Gemma 3 family of open-weights models, Gemma 3n 5B and 8B, are multilingual (over 140 languages), multimodal (text, vision, audio in; text out), and optimized for mobile platforms. Gemma-3n-E4B-it (8 billion parameters) ranks just ahead of Anthropic Claude 3.7 Sonnet in the LMSys Text Arena. Gemma 3n 5B and 8B are 1.5 times faster than their predecessors and require 2 gigabytes and 3 gigabytes of memory, respectively, thanks to techniques that include per-layer embeddings, key-value caching, conditional parameter loading (constraining active parameters to specific modalities at inference), and a Matryoshka Transformer design that dynamically activates nested sub-models. They’re available in preview via Google’s AI Studio, AI Edge, GenAI SDK, or MediaPipe.Google introduced several specialized AI tools and models. Jules is an autonomous, asynchronous, multi-agent coding assistant that clones repos into a secure virtual machine to perform tasks like writing tests, building features, and fixing bugs (available in public beta). SignGemma translates American sign language to text (previously ASL to English). MedGemma analyzes medical text and images (part of the open-weights collection Health AI Developer Foundations). Building on Google Search’s AI Overviews, Google is further building AI into search. Google Search’s AI Mode uses Gemini 2.5 to deliver a “deep search” mode that decomposes users’ questions into hundreds of sub-queries for analysis and visualization. Google plans to integrate AI Mode features into its core search product. In addition, Google Search’s AI Mode will gain Search Live (real-time, audio-enabled visual interaction via camera) and agentic features (for tasks such as purchasing tickets). Computer-use capabilities are coming to the Gemini API and Vertex AI. Why it matters: Google is catching up with the Microsoft/OpenAI colossus on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3’s text-to-video-plus-audio output shows marked improvement over the previous version. Behind the news: The number of tokens Google processed monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google’s progress contrasts with Apple’s ongoing struggles. Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate them into legacy products, Apple’s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI’s acquisition of LoveFrom, the startup founded by its former lead product designer Jony Ive. We’re thinking: Google I/O 2025 was a strong showing of generative AI capabilities! There’s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building numerous innovative products.", "image_caption": "Side-by-side of a fern leaf and its digital code representation, illustrating nature's pattern-to-code transformation.", "metadata": {"article_id": "googles_new_ai_offerings_include_veo_3_video_generator_lightweight_gemma_3n_updates_to_gemini_pro_and_ultra_and_more", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--60--1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/googles-new-ai-offerings-include-veo-3-video-generator-lightweight-gemma-3n-updates-to-gemini-pro-and-ultra-and-more/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/googles_new_ai_offerings_include_veo_3_video_generator_lightweight_gemma_3n_updates_to_gemini_pro_and_ultra_and_more.html"}}
{"id": 58637493001, "type": "news_chunk", "title": "GPT-4 Boosts Remote Tutors’ Performance in Real Time, Study Finds", "subtitle": "GPT-4 Boosts Remote Tutors’ Performance in Real Time, Study Finds", "content": "Students benefit from tutoring, but training tutors is expensive. A study shows that large language models can boost tutors’ effectiveness in real time. What’s new: Rose Wang and colleagues at Stanford built Tutor CoPilot, a tool for remote, online tutors that uses GPT-4 to generate hints, explanations, questions, and other helpful responses to students. Key insight: When a student makes an error, according to previous work by some of the same authors, effective teachers choose a strategy for addressing the mistake. The authors identified 11 strategies, such as ask a question, explain a concept, provide a hint, or encourage the student. Moreover, they found that an LLM that executed a strategy chosen by an expert teacher performed significantly better than an LLM that was prompted with a strategy chosen at random or no specific strategy. Letting inexperienced tutors choose a strategy while an LLM generates a response helps them learn how to execute the strategy. Students, in turn, benefit from responses that mimic those of an experienced teacher. How it works: The authors outfitted a remote tutoring application with GPT-4. The application included a tutor-student chat window, a problem display, and a whiteboard. The authors added a button that enabled the tutor to turn Tutor CoPilot on or off. When a tutor engaged Tutor CoPilot, the system prompted GPT-4 to behave as an experienced elementary math teacher and provided context in the form of the 10 most recent messages, the current lesson topic, and a default strategy from the list. GPT-4 responded with guidance. (To preserve the tutor’s and student’s privacy, the system redacted their names using the open source library Edu-ConvoKit.)The system prompted GPT-4 three times, each time changing the strategy, and presented the tutor with three potential responses.The tutor could re-generate or edit GPT-4’s responses, or select a strategy and generate a new response before adding it to the chat window. Results: The authors partnered with a virtual tutoring company and a school district in the United States for a two-month study of 874 tutors and 1,787 students between grades 3 and 8. They divided the participants into two groups. In one group, tutors conducted sessions with students as usual. In the other, tutors had access to Tutor CoPilot. The authors measured success by the percentage of students who passed a test at the end of a lesson. In the group that didn’t use Tutor CoPilot, 62 percent of students passed the test.In the group with TutorCopilot, 66 percent passed. The effect was most pronounced among the one-third of tutors who had the lowest ratings (9 percent higher) and least experience (7 percent higher). The API cost was approximately $3.31 per tutor, or roughly $20 per tutor per year. Yes, but: The authors found statistically significant improvements as measured by test results per lesson, but not in end-of-year exam results. The study’s two-month duration may account for the lack of evidence for longer-term effects. Why it matters: LLMs hold great promise for helping to educate students, but they also show potential in educating teachers. For inexperienced tutors who are learning how to interact with students, an LLM’s general knowledge and pedagogical insights gleaned from expert teachers make a powerful combination. We’re thinking: Although it relies on sophisticated technology, the authors’ approach is simple: Prompt an LLM to apply proven teaching principles. Presumably such principles apply beyond elementary math, which would make this approach useful for teaching a variety of disciplines.", "image_caption": "AI tutoring system interface showing real-time context integration, privacy, and expert-like feedback generation.", "metadata": {"article_id": "gpt_4_boosts_remote_tutors_performance_in_real_time_study_finds", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F03%2Funnamed--69--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/gpt-4-boosts-remote-tutors-performance-in-real-time-study-finds/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/gpt_4_boosts_remote_tutors_performance_in_real_time_study_finds.html"}}
{"id": 84944190001, "type": "news_chunk", "title": "Grounding DINO 1.5, An Edge Device Model Built for Faster, Smarter Object Detection", "subtitle": "Grounding DINO 1.5, An Edge Device Model Built for Faster, Smarter Object Detection", "content": "An open source model is designed to perform sophisticated object detection on edge devices like phones, cars, medical equipment, and smart doorbells. What’s new: Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, and colleagues at the International Digital Economy Academy introduced Grounding DINO 1.5, a system that enables devices with limited processing power to detect arbitrary objects in images based on a text list of objects (also known as open-vocabulary object detection). You can download the code and weights here. Key insight: The original Grounding DINO follows many of its predecessors by using image embeddings of different levels (from lower-level embeddings produced by an image encoder’s earlier layers, which are larger and represent simple patterns such as edges, to higher-level embeddings produced by later layers, which are smaller and represent complex patterns such as objects). This enables it to better detect objects at different scales. However, it takes a lot of computation. To enable the system to run on devices that have less processing power, Grounding DINO 1.5 uses only the smallest (highest-level) image embeddings for a crucial part of the process. How it works: Grounding DINO 1.5 is made up of components that produce text and image embeddings, fuse them, and classify them. It follows the system architecture and training of Grounding DINO with the following exceptions: (i) It uses a different image encoder, (ii) a different model combines text and image embeddings, and (iii) it was trained on a newer dataset of 20 million publicly available text-image examples. Given an image, a pretrained EfficientViT-L1 image encoder produced three levels of image embeddings.Given the corresponding text, BERT produced a text embedding composed of tokens.Given the highest-level image embedding and the text embedding, a cross-attention model updated each one to incorporate information from the other (fusing text and image modalities, in effect). After the update, a CNN-based model combined the updated highest-level image embedding with the lower-level image embeddings to create a single image embedding.Grounding DINO 1.5 calculated which 900 tokens in the image embedding were most similar to the tokens in the text embedding.A cross-attention model detected objects using both the image and text embeddings. For each token in the updated image embedding, it determined: (i) which text token(s), if any, matched the image token, thereby giving each image token a classification including “not an object” and (ii) a bounding box that enclosed the corresponding object (except for tokens that were labeled “not an object”).The system learned to (i) maximize the similarity between matching tokens from the text and image embeddings and minimize the similarity between tokens that didn’t match and (ii) minimize the difference between its own bounding boxes and those in the training dataset. Results: Grounding DINO 1.5 performed significantly faster than the original Grounding DINO: 10.7 frames per second versus 1.1 frames per second running on an Nvidia Jetson Orin NX computer. Tested on a dataset of images of common objects annotated with labels and bounding boxes, Grounding DINO 1.5 achieved better average precision (a measure of how many objects it identified correctly in their correct location, higher is better) than both Grounding DINO and YOLO-Worldv2-L (a CNN-based object detector). Grounding DINO 1.5 scored 33.5 percent, Grounding DINO 27.4 percent, and YOLO-Worldv2-L 33 percent. Why it matters: The authors achieved 10 times the speed with just a couple of small changes (a more efficient image encoder and a smaller image embedding when performing cross-attention between embeddings of images and texts). Small changes can yield big results. We’re thinking: Lately model builders have been building better, smaller, faster large language models for edge devices. We’re glad to see object detection get similar treatment.", "image_caption": "Grounding DINO animation depicting object detection with bounding boxes on images.", "metadata": {"article_id": "grounding_dino_1_5_an_edge_device_model_built_for_faster_smarter_object_detection", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2Funnamed--35--2.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/grounding-dino-1-5-an-edge-device-model-built-for-faster-smarter-object-detection/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/grounding_dino_1_5_an_edge_device_model_built_for_faster_smarter_object_detection.html"}}
{"id": 48377071001, "type": "news_chunk", "title": "Harvard Unveils a Million-Book Corpus for AI Training", "subtitle": "Harvard Unveils a Million-Book Corpus for AI Training", "content": "Harvard University amassed a huge new text corpus for training machine learning models. What’s new: Harvard unveiled the Harvard Library Public Domain Corpus, nearly 1 million copyright-free books that were digitized as part of the Google Books project. That’s five times as many volumes as Books3, which was used to train large language models including Meta’s Llama 1 and Llama 2 but is no longer available through lawful channels. How it works: Harvard Law Library’s Innovation Lab compiled the corpus with funding from Microsoft and OpenAI. For now, it’s available only to current Harvard students, faculty, and staff. The university is working with Google to distribute it widely. The corpus includes historical legal texts, casebooks, statutes, and treatises, a repository of legal knowledge that spans centuries and encompasses diverse jurisdictions.It also includes less-widely distributed works in languages such as Czech, Icelandic, and Welsh. Behind the news: The effort highlights the AI community’s ongoing need for large quantities of high-quality text to keep improving language models. In addition, the EU’s AI Act requires that AI developers disclose the training data they use, a task made simpler by publicly available datasets. Books3, a collection of nearly 200,000 volumes, was withdrawn because it included copyrighted materials. Other large-scale datasets of books include Common Corpus, a multilingual library of 2 million to 3 million public-domain books and newspapers. Why it matters: Much of the world’s high-quality text that’s easily available on the web already has been collected for training AI models. This makes fresh supplies especially valuable for training larger, more data-hungy models. Projects like the Harvard Library Public Domain Corpus suggest there’s more high-quality text to be mined from books. Classic literature and niche documents also could help AI models draw from a more diverse range of perspectives. We’re thinking: Media that has passed out of copyright and into the public domain generally is old — sometimes very old — but it could hold knowledge that’s not widely available elsewhere.", "image_caption": "A narrow library aisle filled with shelves stacked with countless books.", "metadata": {"article_id": "harvard_unveils_a_million_book_corpus_for_ai_training", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--44--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/harvard-unveils-a-million-book-corpus-for-ai-training/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/harvard_unveils_a_million_book_corpus_for_ai_training.html"}}
{"id": 16219774001, "type": "news_chunk", "title": "Hot Tips for Speedy Startups | AI News & Insights", "subtitle": "Hot Tips for Speedy Startups | AI News & Insights", "content": "Dear friends, I’m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity. AI Fund isn’t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): We co-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices. Many factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my articles in The Batch as well. If you are building an AI startup, here are some ideas to consider: A startup with a small team that pursues one focused, concrete idea can move really fast. Rather than hedging, it is often better to pursue one hypothesis (for example, build one concrete product) but also be willing to switch quickly to a different hypothesis (say, change what features you decide to build) if the data that comes back indicates the original hypothesis is flawed. Concreteness gets you speed!A subject matter expert’s gut is remarkably good at making quick decisions. Obviously, there’s a role for data and user studies as well. But if you’re deciding whether to build feature A or B, or to sell first to user persona X or Y, sometimes a domain expert’s gut will point to a quick decision that you can execute and validate or falsify. Trusting a domain expert’s gut gets you speed!AI-assisted coding is making prototyping faster than ever before. Yes, AI assistance is speeding up building reliable, enterprise-grade applications and maintaining legacy codebases. But the acceleration it brings to building stand-alone prototypes is far greater. This is because stand-alone prototypes have low requirements for reliability, integration, or even security (if, say, you run them in a sandbox environment). This lets us prototype and test at a ferocious velocity. AI -assisted coding (including vibe coding, where you might barely look at the code) gets you speed!Finally, with faster prototyping, the bottleneck shifts to getting feedback from users. A single learning cycle might consist of (i) building a prototype and (ii) getting user feedback to inform the next iteration. Since (i) is now much faster than before, accelerating (ii) is growing in importance. This means teams that are skilled at finding prospective customers and getting their feedback in hours/days rather than weeks can go faster. For example, when building consumer products, I routinely approach strangers (in a respectful way) in public places to ask if they’re willing to give feedback on a prototype I’m working on. (Gathering feedback is more complex for enterprise products, because prospective customers are harder to track down.) Quick user feedback gets you speed! In addition to speed, a second criterion that I find important for startup success is deep knowledge of the technology. Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed! I’m grateful to AI Fund’s investors, team, and entrepreneur partners for working with us. There is much ahead to build!", "image_caption": "Blue performance gauge with needle pointing to maximum, indicating high level or peak performance.", "metadata": {"article_id": "hot_tips_for_speedy_startups", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--84--3.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/hot-tips-for-speedy-startups/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/hot_tips_for_speedy_startups.html"}}
{"id": 50981148001, "type": "news_chunk", "title": "How AI can make you a 10x professional", "subtitle": "How AI can make you a 10x professional", "content": "Dear friends, A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer. But we don’t seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more “10x professionals.” There aren’t already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they’re not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely reimagine that job). But for many jobs that primarily involve applying knowledge or processing information, AI will be transformative. In a few roles, I’m starting to see tech-savvy individuals coordinate a suite of technology tools to do things differently and start to have, if not yet 10x impact, then easily 2x impact. I expect this gap to grow. 10x engineers don’t write code 10 times faster. Instead, they make technical architecture decisions that result in dramatically better downstream impact, they spot problems and prioritize tasks more effectively, and instead of rewriting 10,000 lines of code (or labeling 10,000 training examples) they might figure out how to write just 100 lines (or collect 100 examples) to get the job done. I think 10x marketers, recruiters, and analysts will, similarly, do things differently. For example, perhaps traditional marketers repeatedly write social media posts. 10x marketers might use AI to help write, but the transformation will go deeper than that. If they are deeply sophisticated in how to apply AI — ideally able to write code themselves to test ideas, automate tasks, or analyze data — they might end up running a lot more experiments, get better insights about what customers want, and generate much more precise or personalized messages than a traditional marketer, and thereby end up making 10x impact. Similarly, 10x recruiters won’t just use generative AI to help write emails to candidates or summarize interviews. (This level of use of prompting-based AI will soon become table stakes for many knowledge roles.) They might coordinate a suite of AI tools to efficiently identify and carry out research on a large set of candidates, enabling them to have dramatically greater impact than the average recruiter. And 10x analysts won’t just use generative AI to edit their reports. They might write code to orchestrate a suite of AI agents to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way. A 2023 Harvard/BCG study estimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the average, using 2023 technology. The maximum advantage to be gained by using AI in a sophisticated way will be much bigger, and will only grow as technology improves. Here in Silicon Valley, I see more and more AI-native teams reinvent workflows and do things very differently. In software engineering, we've venerated the best engineers because they can have a really massive impact. This has motivated many generations of engineers to keep learning and working hard, because doing those things increases the odds of doing high-impact work. As AI becomes more helpful in many more job roles, I believe we will open up similar paths to a lot more people becoming a “10x professional.” Keep learning!", "image_caption": "Comic-style illustration of a confident woman and man standing beside bold ‘10X’ text on a bright background.", "metadata": {"article_id": "how_ai_can_make_you_a_10x_professional", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2F10x_1200px_6-2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-ai-can-make-you-a-10x-professional/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_ai_can_make_you_a_10x_professional.html"}}
{"id": 63849423001, "type": "news_chunk", "title": "How AI Fund Is Building AI Builders", "subtitle": "How AI Fund Is Building AI Builders", "content": "Dear friends, Everyone can benefit by learning to code with AI! At AI Fund, the venture studio I lead, everyone — not just the engineers — can vibe code or use more sophisticated AI-assisted coding techniques. This empowers everyone to build with AI. The impact on team creativity and productivity has been exciting! I share my experience with this in the hope that more teams will invest in empowering everyone to build with AI. Everyone at AI Fund who was not already an engineer started with our “AI Python for Beginners” course to learn the basics. I also shared with the team details of the tech stack I use to give everyone a default set of building blocks. Since then, many have gone on to acquire additional building blocks (such as additional third-party APIs) themselves either by taking courses, searching online, or learning from colleagues. You can watch a video of our experience with this here. Here are just a few examples of applications that non-engineers at AI Fund have built: Our CFO Ellen Li built an app that scans our Google docs system to flag updates to a portfolio company’s information, saving what was previously 5 to 6 hours of manual work per week.Senior Executive Recruiter Jon Zemmelman built a system that lets him configure the relative ratings of screening criteria for job candidates (such as previous startup experience, technical expertise, etc.) and automatically evaluate resumes against the criteria.Associate General Counsel Nikhil Sharma wrote code to automatically generate NDAs (non-disclosure agreements) in AI Fund’s standard template.Office Coordinator Ellie Jenkins, as a fun project, built a visualization of the history of fashion design houses and their influence on each other. It is very empowering when individuals don’t have to try to get scarce engineering resources allocated to their ideas in order to try them out. There are a lot fewer gatekeepers in the way: If someone has an idea, they can build a prototype and try it out. If it gets positive feedback from users, that lays the groundwork for scaling it up. Or, if the prototype does not work, this is also valuable information that lets them quickly move on to a different idea or take insights from critical feedback to decide what to try next. In the future, one of the most important skills in any profession will be the ability to tell a computer exactly what you want, so the computer can do it for you. For the foreseeable future, writing code (with AI assistance, so the AI, rather than you, actually writes the code) will be the best way to do this. This is a great time for everyone to code with AI! Keep building,", "image_caption": "Robots building a wooden house frame under sunny sky. Teamwork, technology, and construction. Future automation in construction.", "metadata": {"article_id": "how_ai_fund_is_building_ai_builders", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2F2025.06.04-LETTER-3--1--2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-ai-fund-is-building-ai-builders/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_ai_fund_is_building_ai_builders.html"}}
{"id": 32172030001, "type": "news_chunk", "title": "Data Points: How AI models can encourage bad behavior", "subtitle": "Data Points: How AI models can encourage bad behavior", "content": "In today’s edition, you’ll learn more about: BAGEL, an open ByteDance model that can read and write images and textPerplexity’s Labs, a new tool to generate research artifactsA database security failure in Lovable’s coding platformMIT Technology Review’s new report on AI’s energy footprint ELEPHANT helps identify and measure sycophancy in AI models Stanford researchers have identified a pattern of “social sycophancy” in large language models, where AI systems excessively preserve users’ self-image when giving personal advice. The study tested eight models using the ELEPHANT framework, which measures five face-preserving behaviors: emotional validation, moral endorsement, indirect language, indirect action, and accepting user framing. Across open-ended questions and Reddit’s r/AmITheAsshole posts, LLMs showed significantly higher rates of sycophantic behavior than humans—offering emotional validation 76 percent of the time versus 22 percent for humans and incorrectly classifying 42 percent of inappropriate behavior as acceptable. According to the researchers, personal advice is becoming the most common LLM use case, and excessive agreement could reinforce harmful beliefs while undermining critical thinking; the preference datasets used in AI training too often implicitly reward these behaviors. The ELEPHANT framework and datasets are publicly available for researchers to further study this issue. (arXiv) Google launches app for testing AI models on mobile devices Google released AI Edge Gallery, an experimental Android app that runs open AI models directly on mobile devices without requiring an internet connection after initial model download. The app allows developers to test various models from Hugging Face, upload images for AI analysis, experiment with prompts for code generation and text rewriting, and engage in multi-turn conversations. Key features include real-time performance benchmarks showing metrics like time-to-first-token and decode speed, plus the ability to test custom LiteRT models. This tool helps developers evaluate how different AI models perform on mobile hardware, providing valuable insights for building offline-capable AI applications. The app is currently available as an APK for Android, with an iOS version coming soon. (GitHub) Open multimodal model from ByteDance unifies generation and understanding ByteDance researchers released BAGEL, an open-weights AI model with 7 billion active parameters (14 billion total) that combines text and image generation, understanding, and editing capabilities in a single system. The model uses a Mixture-of-Transformer-Experts architecture and outperforms open vision-language models like Qwen2.5-VL and InternVL-2.5 on understanding benchmarks, while matching specialized generators like Stable Diffusion 3 in text-to-image quality. BAGEL shows advanced capabilities including free-form visual manipulation and “world-modeling” tasks that go beyond traditional image editing. Most current open-weights AI models specialize in either understanding or generation but not both. BAGEL is freely available via Hugging Face and other providers for fine-tuning, distillation, and deployment. (BAGEL and arXiv) Perplexity’s Labs lets users create reports, apps, and dashboards Perplexity introduced Labs, a new feature that enables Pro subscribers to use AI-based research and analysis to generate complete projects including reports, spreadsheets, dashboards, and simple web applications. The system performs 10 minutes or more of self-supervised work including deep web browsing, code execution, and chart creation to transform ideas into finished objects. Labs differentiates itself from Perplexity’s existing Research mode (formerly Deep Research) by investing more time and offering advanced file generation and mini-app creation. This launch shows Perplexity’s expansion beyond its answer engine roots to something closer to a full-fledged AI product suite comparable to ChatGPT. Labs is available now for Pro subscribers on web and iOS, with Android support coming soon. (Perplexity) Lovable’s coding platform exposes user information through security hole Lovable, a Swedish startup that lets non-technical users create websites and apps through natural language prompts, has failed to fix a critical security vulnerability months after being notified, according to a report by a Replit employee. The analysis of 1,645 Lovable-created web apps found that 170 exposed user data including names, email addresses, financial information, and API keys that could allow hackers to rack up charges on customers’ accounts. The vulnerability stems from improperly configured database connections through Supabase. This highlights the dangers of inexperienced users building software without understanding security basics, a growing concern as AI democratizes software development. Lovable acknowledged on X that it’s “not yet where we want to be in terms of security.” (Semafor) New report estimates the energy costs of AI’s rapid expansion MIT Technology Review analyzed the energy consumption of AI systems, finding that a single ChatGPT query uses about 1,080 joules of electricity, while generating a 5-second AI video requires 3.4 million joules, roughly equivalent to running a microwave for over an hour. The publication examined dozens of AI models and interviewed experts to trace AI’s carbon footprint, calculating that AI servers consumed between 53 and 76 terawatt-hours of electricity in 2024, enough to power 7.2 million U.S. homes annually. By 2028, AI could consume up to 326 terawatt-hours per year, representing 22 percent of all U.S. household electricity consumption, as companies race to build massive data centers and develop more complex AI agents and reasoning models. Still, tech companies’ lack of transparency about energy usage makes it difficult to get a complete picture of AI’s energy costs or plan for its actual environmental impact. (MIT Technology Review) Still want to know more about what matters in AI right now? Read last week’s issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng raised concerns about proposed U.S. funding cuts for basic research, emphasizing how such cuts could hurt American competitiveness in AI and urging continued investment in open scientific research. “Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: Anthropic released new Claude 4 Sonnet and Claude 4 Opus models, achieving top-tier performance in code generation benchmarks.Google unveiled a wave of AI updates at I/O, including the Veo 3 video generator, the compact Gemma 3n model, and enhancements to Gemini Pro and Ultra.Researchers behind DeepSeek detailed the training strategies and hardware infrastructure used to build their V3 and R1 models.A study found that OpenAI’s GPT-4o can accurately identify verbatim excerpts from paywalled O’Reilly books, raising fresh questions about training data sources. Subscribe to Data Points", "image_caption": "Young programmer analyzing data on multiple monitors in dimly lit office, wearing headphones.", "metadata": {"article_id": "how_ai_models_can_encourage_bad_behavior", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2FWhisk_2fc1932ab6.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-ai-models-can-encourage-bad-behavior/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_ai_models_can_encourage_bad_behavior.html"}}
{"id": 35735296001, "type": "news_chunk", "title": "How Large Companies Can Move Fast in AI", "subtitle": "How Large Companies Can Move Fast in AI", "content": "Dear friends, In the age of AI, large corporations — not just startups — can move fast. I often speak with large companies’ C-suite and Boards about AI strategy and implementation, and would like to share some ideas that are applicable to big companies. One key is to create an environment where small, scrappy teams don’t need permission to innovate. Let me explain. Large companies are slower than startups for many reasons. But why are even 3-person, scrappy teams within large companies slower than startups of a similar size? One major reason is that large companies have more to lose, and cannot afford for a small team to build and ship a feature that leaks sensitive information, damages the company brand, hurts revenue, invites regulatory scrutiny, or otherwise damages an important part of the business. To prevent these outcomes, I have seen companies require privacy review, marketing review, financial review, legal review, and so on before a team can ship anything. But if engineers need sign-off from 5 vice presidents before they’re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product? Thanks to AI-assisted coding, the world now has a capability to build software prototypes really fast. But many large companies’ processes – designed to protect against legitimate downside risks – make them unable to take advantage of this capability. In contrast, in small startups with no revenue, no customers, and no brand reputation the downside is limited. In fact, going out of business is a very real possibility anyway, so moving fast makes a superior tradeoff to moving slowly to protect against downside risk. In the worst case, it might invent a new way to go out of business, but in a good case, it might become very valuable. Fortunately, large companies have a way out of this conundrum. They can create a sandbox environment for teams to experiment in a way that strictly limits the downside risk. Then those teams can go much faster and not have to slow down to get anyone’s permission. The sandbox environment can be a set of written policies, not necessarily a software implementation of a sandbox. For example, it may permit a team to test the nascent product only on employees of the company and perhaps alpha testers who have signed an NDA, and give no access to sensitive information. It may be allowed to launch product experiments only under newly created brands not tied directly to the company. Perhaps it must operate within a pre-allocated budget for compute. Within this sandbox, there can be broad scope for experimentation, and — importantly — a team is free to experiment without frequently needing to ask for permission, because the downside they can create is limited. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company’s brand, and so on. Under this framework, it is easier to build a company culture that encourages learning, building, and experimentation and celebrates even the inevitable failures that now come with modest cost. Dozens or hundreds of prototypes can be built and quickly discarded as part of the price of finding one or two ideas that turn out to be home runs. Importantly, this also lets teams move quickly as they churn through those dozens of prototypes needed to get to the valuable ones.I often speak with large companies about AI strategy and implementation. My quick checklist of things to consider is people, process, and platform. This letter has addressed only part of processes, with an emphasis on moving fast. I’m bullish about what both startups and large companies can do with AI, and I will write about the roles of people and platforms in future letters. Keep building!", "image_caption": "Cartoon: Woman at desk; man in sandbox with shovel saying, \"I'm experimenting!\" in office setting.", "metadata": {"article_id": "how_large_companies_can_move_fast_in_ai", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--65-.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-large-companies-can-move-fast-in-ai/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_large_companies_can_move_fast_in_ai.html"}}
{"id": 79791475001, "type": "news_chunk", "title": "How to Become a Multilingual Coder", "subtitle": "How to Become a Multilingual Coder", "content": "Dear friends, Even though I’m a much better Python than JavaScript developer, with AI assistance, I’ve been writing a lot of JavaScript code recently. AI-assisted coding is making specific programming languages less important, even though learning one is still helpful to make sure you understand the key concepts. This is helping many developers write code in languages we’re not familiar with, which lets us get code working in many more contexts! My background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in. Generative AI is making syntax less important, so we can all simultaneously be Python, JS, TS, C++, Java, and even Cobol developers. Perhaps one day, instead of being “Python developers\" or “C++ developers,” many more of us will just be “developers”! But understanding the concepts behind different languages is still important. That’s why learning at least one language like Python still offers a great foundation for prompting LLMs to generate code in Python and other languages. If you move from one programming language to another that carries out similar tasks but with different syntax — say, from JS to TS, or C++ to Java, or Rust to Go — once you’ve learned the first set of concepts, you’ll know a lot of the concepts needed to prompt an LLM to code in the second language. (Although TensorFlow and PyTorch are not programming languages, learning the concepts of deep learning behind TensorFlow will also make it much easier to get an LLM to write PyTorch code for you, and vice versa!) In addition, you’ll be able to understand much of the generated code (perhaps with a little LLM assistance). Different programming languages reflect different views of how to organize computation, and understanding the concepts is still important. For example, someone who does not understand arrays, dictionaries, caches, and memory will be less effective at getting an LLM to write code in most languages. Similarly, a Python developer who moves toward doing more front-end programming with JS would benefit from learning the concepts behind front-end systems. For example, if you want an LLM to build a front end using the React framework, it will benefit you to understand how React breaks front ends into reusable UI components, and how it updates the DOM data structure that determines what web pages look like. This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong. Similarly, if you want an LLM to help you write code in CUDA or ROCm, it helps to understand how GPUs organize compute and memory. Just as people who are fluent in multiple human languages can communicate more easily with other people, LLMs are making it easier for developers to build systems in multiple contexts. If you haven’t already done so, I encourage you to try having an LLM write some code in a language you’d like to learn but perhaps haven’t yet gotten around to, and see if it helps you get some new applications to work. Keep building!", "image_caption": "Code snippet showing ‘Keep Building!’ printed in multiple programming languages including Python, Java, JavaScript, and C++.", "metadata": {"article_id": "how_to_become_a_multilingual_coder", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--61--2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-to-become-a-multilingual-coder/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_to_become_a_multilingual_coder.html"}}
{"id": 96966874001, "type": "news_chunk", "title": "How to Build a Career in AI: Three Steps to Career Growth", "subtitle": "How to Build a Career in AI: Three Steps to Career Growth", "content": "Dear friends, The rapid rise of AI has led to a rapid rise in AI jobs, and many people are building exciting careers in this field. A career is a decades-long journey, and the path is not always straightforward. Over many years, I’ve been privileged to see thousands of students as well as engineers in companies large and small navigate careers in AI. In this and the next few letters, I’d like to share a few thoughts that might be useful in charting your own course.Three key steps of career growth are learning (to gain technical and other skills), working on projects (to deepen skills, build a portfolio, and create impact) and searching for a job. These steps stack on top of each other: Initially, you focus on gaining foundational technical skills.After having gained foundational skills, you lean into project work. During this period, you’ll probably keep learning.Later, you might occasionally carry out a job search. Throughout this process, you’ll probably continue to learn and work on meaningful projects. These phases apply in a wide range of professions, but AI involves unique elements. For example: AI is nascent, and many technologies are still evolving. While the foundations of machine learning and deep learning are maturing — and coursework is an efficient way to master them — beyond these foundations, keeping up-to-date with changing technology is more important in AI than fields that are more mature.Project work often means working with stakeholders who lack expertise in AI. This can make it challenging to find a suitable project, estimate the project’s timeline and return on investment, and set expectations. In addition, the highly iterative nature of AI projects leads to special challenges in project management: How can you come up with a plan for building a system when you don’t know in advance how long it will take to achieve the target accuracy? Even after the system has hit the target, further iteration may be necessary to address post-deployment drift.While searching for a job in AI can be similar to searching for a job in other sectors, there are some differences. Many companies are still trying to figure out which AI skills they need and how to hire people who have them. Things you’ve worked on may be significantly different than anything your interviewer has seen, and you’re more likely to have to educate potential employers about some elements of your work. Throughout these steps, a supportive community is a big help. Having a group of friends and allies who can help you — and whom you strive to help — makes the path easier. This is true whether you’re taking your first steps or you’ve been on the journey for years.I’m excited to work with all of you to grow the global AI community, and that includes helping everyone in our community develop their careers. I’ll dive more deeply into these topics in the next few weeks.Keep learning!", "image_caption": "An illustration of a person on top of a career path", "metadata": {"article_id": "how_to_build_a_career_in_ai_part_1_three_steps_to_career_growth", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F07%2FCareerArchitecture6-1200px.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-1-three-steps-to-career-growth/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_to_build_a_career_in_ai_part_1_three_steps_to_career_growth.html"}}
{"id": 69503706001, "type": "news_chunk", "title": "How to Build a Career in AI: Learning Technical Skills", "subtitle": "How to Build a Career in AI: Learning Technical Skills", "content": "Dear friends, Last week, I wrote about key steps for building a career in AI: learning technical skills, doing project work, and searching for a job, all of which is supported by being part of a community. In this letter, I’d like to dive more deeply into the first step.More papers have been published on AI than any person can read in a lifetime. So, in your efforts to learn, it’s critical to prioritize topic selection. I believe the most important topics for a technical career in machine learning are: Foundational machine learning skills. For example, it’s important to understand models such as linear regression, logistic regression, neural networks, decision trees, clustering, and anomaly detection. Beyond specific models, it’s even more important to understand the core concepts behind how and why machine learning works, such as bias/variance, cost functions, regularization, optimization algorithms, and error analysis.Deep learning. This has become such a large fraction of machine learning that it’s hard to excel in the field without some understanding of it! It’s valuable to know the basics of neural networks, practical skills for making them work (such as hyperparameter tuning), convolutional networks, sequence models, and transformers.Math relevant to machine learning. Key areas include linear algebra (vectors, matrices, and various manipulations of them) as well as probability and statistics (including discrete and continuous probability, standard probability distributions, basic rules such as independence and Bayes rule, and hypothesis testing). In addition, exploratory data analysis (EDA) — using visualizations and other methods to systematically explore a dataset — is an underrated skill. I’ve found EDA particularly useful in data-centric AI development, where analyzing errors and gaining insights can really help drive progress! Finally, a basic intuitive understanding of calculus will also help. In a previous letter, I described how the math needed to do machine learning well has been changing. For instance, although some tasks require calculus, improved automatic differentiation software makes it possible to invent and implement new neural network architectures without doing any calculus. This was almost impossible a decade ago.Software development. While you can get a job and make huge contributions with only machine learning modeling skills, your job opportunities will increase if you can also write good software to implement complex AI systems. These skills include programming fundamentals, data structures (especially those that relate to machine learning, such as data frames), algorithms (including those related to databases and data manipulation), software design, familiarity with Python, and familiarity with key libraries such as TensorFlow or PyTorch, and scikit-learn. This is a lot to learn! Even after you master everything in this list, I hope you’ll keep learning and continue to deepen your technical knowledge. I’ve known many machine learning engineers who benefitted from deeper skills in an application area such as natural language processing or computer vision, or in a technology area such as probabilistic graphical models or building scalable software systems.How do you gain these skills? There’s a lot of good content on the internet, and in theory reading dozens of web pages could work. But when the goal is deep understanding, reading disjointed web pages is inefficient because they tend to repeat each other, use inconsistent terminology (which slows you down), vary in quality, and leave gaps. That’s why a good course — in which a body of material has been organized into a coherent and logical form — is often the most time-efficient way to master a meaningful body of knowledge. When you’ve absorbed the knowledge available in courses, you can switch over to research papers and other resources.Finally, keep in mind that no one can cram everything they need to know over a weekend or even a month. Everyone I know who’s great at machine learning is a lifelong learner. In fact, given how quickly our field is changing, there’s little choice but to keep learning if you want to keep up. How can you maintain a steady pace of learning for years? I’ve written about the value of habits. If you cultivate the habit of learning a little bit every week, you can make significant progress with what feels like less effort. Keep learning!", "image_caption": "An illustration of a person holding a giant sheet with different ML subjects", "metadata": {"article_id": "how_to_build_a_career_in_ai_part_2_learning_technical_skills", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F07%2FColumnCloseup_Rev-LEARNNG-NoYOU_1200px-2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-2-learning-technical-skills/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_to_build_a_career_in_ai_part_2_learning_technical_skills.html"}}
{"id": 87001193001, "type": "news_chunk", "title": "How to Build a Career in AI: Choosing Projects", "subtitle": "How to Build a Career in AI: Choosing Projects", "content": "Dear friends,In the last two letters, I wrote about developing a career in AI and shared tips for gaining technical skills. This time, I’d like to discuss an important step in building a career: project work.It goes without saying that we should only work on projects that are responsible and ethical, and that benefit people. But those limits leave a large variety to choose from. I wrote previously about how to identify and scope AI projects. This and next week’s letter have a different emphasis: picking and executing projects with an eye toward career development.A fruitful career will include many projects, hopefully growing in scope, complexity, and impact over time. Thus, it is fine to start small. Use early projects to learn and gradually step up to bigger projects as your skills grow.When you’re starting out, don’t expect others to hand great ideas or resources to you on a platter. Many people start by working on small projects in their spare time. With initial successes — even small ones — under your belt, your growing skills increase your ability to come up with better ideas, and it becomes easier to persuade others to help you step up to bigger projects. What if you don’t have any project ideas? Here are a few ways to generate them: Join existing projects. If you find someone else with an idea, ask to join their project.Keep reading and talking to people. I come up with new ideas whenever I spend a lot of time reading, taking courses, or talking with domain experts. I’m confident that you will, too.Focus on an application area. Many researchers are trying to advance basic AI technology — say, by inventing the next generation of transformers or further scaling up language models — so, while this is an exciting direction, it is hard. But the variety of applications to which machine learning has not yet been applied is vast! I’m fortunate to have been able to apply neural networks to everything from autonomous helicopter flight to online advertising, partly because I jumped in when relatively few people were working on those applications. If your company or school cares about a particular application, explore the possibilities for machine learning. That can give you a first look at a potentially creative application — one where you can do unique work — that no one else has done yet.Develop a side hustle. Even if you have a full-time job, a fun project that may or may not develop into something bigger can stir the creative juices and strengthen bonds with collaborators. When I was a full-time professor, working on online education wasn’t part of my “job” (which was doing research and teaching classes). It was a fun hobby that I often worked on out of passion for education. My early experiences recording videos at home helped me later in working on online education in a more substantive way. Silicon Valley abounds with stories of startups that started as side projects. So long as it doesn’t create a conflict with your employer, these projects can be a stepping stone to something significant. Given a few project ideas, which one should you jump into? Here’s a quick checklist of factors to consider: Will the project help you grow technically? Ideally, it should be challenging enough to stretch your skills but not so hard that you have little chance of success. This will put you on a path toward mastering ever-greater technical complexity.Do you have good teammates to work with? If not, are there people you can discuss things with? We learn a lot from the people around us, and good collaborators will have a huge impact on your growth.Can it be a stepping stone? If the project is successful, will its technical complexity and/or business impact make it a meaningful stepping stone to larger projects? (If the project is bigger than those you’ve worked on before, there’s a good chance it could be such a stepping stone.) Finally, avoid analysis paralysis. It doesn’t make sense to spend a month deciding whether to work on a project that would take a week to complete. You'll work on multiple projects over the course of your career, so you’ll have ample opportunity to refine your thinking on what’s worthwhile. Given the huge number of possible AI projects, rather than the conventional “ready, aim, fire” approach, you can accelerate your progress with “ready, fire, aim.” Keep learning!", "image_caption": "A person holding a giant sheet with tips on how to find projects", "metadata": {"article_id": "how_to_build_a_career_in_ai_part_3_choosing_projects", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F07%2FGreekTemple3d_PROJECTS_1200px--2--2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-3-choosing-projects/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_to_build_a_career_in_ai_part_3_choosing_projects.html"}}
{"id": 8633424001, "type": "news_chunk", "title": "How to Sequence Projects to Build a Career", "subtitle": "How to Sequence Projects to Build a Career", "content": "Dear friends, Last week’s letter focused on coming up with AI project ideas, part of a series on how to build a career in the field. This letter describes how a sequence of projects might fit into your career path. Over the course of a career, you’re likely to work not on a single AI project, but on a sequence of projects that grow in scope and complexity. For example: In light of this progression, when picking a project, keep in mind that it is only one step on a longer journey, hopefully one that has a positive impact. In addition: Don’t worry about starting too small. One of my first machine learning research projects involved training a neural network to see how well it could mimic the sin(x) function. It wasn’t very useful, but was a great learning experience that enabled me to move on to bigger projects.Communication is key. You need to be able to explain your thinking if you want others to see the value in your work and trust you with resources that you can invest in larger projects. To get a project started, communicating the value of what you hope to build will help bring colleagues, mentors, and managers onboard — and help them point out flaws in your reasoning. After you’ve finished, the ability to explain clearly what you accomplished will help convince others to open the door to larger projects.Leadership isn’t just for managers. When you reach the point of working on larger AI projects that require teamwork, your ability to lead projects will become more important, whether or not you are in a formal position of leadership. Many of my friends have successfully pursued a technical rather than managerial career, and their ability to help steer a project by applying deep technical insights — for example, when to invest in a new technical architecture or collect more data of a certain type — allowed them to exert leadership that helped the project significantly. Building a portfolio of projects, especially one that shows progress over time from simple to complex undertakings, will be a big help when it comes to looking for a job. That will be the subject of a future letter. Keep learning!", "image_caption": "An illustration of a person thinking about his projects pillar", "metadata": {"article_id": "how_to_build_a_career_in_ai_part_4_progress_through", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F07%2FGreekTemple_PROJECTS_Dream7_1200px-1.webp&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-4-progress-through/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_to_build_a_career_in_ai_part_4_progress_through.html"}}
{"id": 8956652001, "type": "news_chunk", "title": "How to Build a Career in AI, Part 6: Job Search Fundamentals", "subtitle": "How to Build a Career in AI, Part 6: Job Search Fundamentals", "content": "Dear friends, Last week, I wrote about switching roles, industries, or both as a framework for considering a job search. If you’re preparing to switch roles (say, taking a job as a machine learning engineer for the first time) or industries (say, working in an AI tech company for the first time), there’s a lot about your target job that you probably don’t know. A technique known as informational interviewing is a great way to learn An informational interview involves finding someone in a company or role you’d like to know more about and informally interviewing them about their work. Such conversations are separate from searching for a job. In fact, it’s helpful to interview people who hold positions that align with your interests well before you’re ready to kick off a job search. Informational interviews are particularly relevant to AI. Because the field is evolving, many companies use job titles in inconsistent ways. In one company, data scientists might be expected mainly to analyze business data and present conclusions on a slide deck. In another, they might write and maintain production code. An informational interview can help you sort out what the AI people in a particular company actually do. With the rapid expansion of opportunities in AI, many people will be taking on an AI job for the first time. In this case, an informational interview can be invaluable for learning what happens and what skills are needed to do the job well. For example, you can learn what algorithms, deployment processes, and software stacks a particular company uses. You may be surprised — if you’re not already familiar with the data-centric AI movement — to learn how much time most machine learning engineers spend iteratively cleaning datasets. Prepare for informational interviews by researching the interviewee and company in advance, so you can arrive with thoughtful questions. You might ask: What do you do in a typical week or day? What are the most important tasks in this role? What skills are most important for success? How does your team work together to accomplish its goals? What is the hiring process? Considering candidates who stood out in the past, what enabled them to shine? Finding someone to interview isn’t always easy, but many people who are in senior positions today received help when they were new from those who had entered the field ahead of them, and many are eager to pay it forward. If you can reach out to someone who’s already in your network — perhaps a friend who made the transition ahead of you or someone who attended the same school as you — that’s great! Meetups such as Pie & AI can also help you build your network. Finally, be polite and professional, and thank the people you’ve interviewed. And when you get a chance, please pay it forward as well and help someone coming up after you. If you receive a request for an informational interview from someone in the DeepLearning.AI community, I hope you’ll lean in to help them take a step up! If you’re interested in learning more about informational interviews, I recommend this article from the UC Berkeley Career Center. I’ve mentioned a few times the importance of your network and community. People you’ve met, beyond providing valuable information, can play an invaluable role by referring you to potential employers. Stay tuned for more on this topic. Keep learning!", "image_caption": "Job search pillar and two guys talking and drinking coffee", "metadata": {"article_id": "how_to_build_a_career_in_ai_part_5_job_search_fundamentals", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F09%2FAI-ML-JOBSEARCH_Info-Interview_1200px-1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-5-job-search-fundamentals/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/how_to_build_a_career_in_ai_part_5_job_search_fundamentals.html"}}
{"id": 53157158001, "type": "news_chunk", "title": "Hugging Face Acquires Pollen Robotics, Launches Reachy 2 Robot for Open-Source Research", "subtitle": "Hugging Face Acquires Pollen Robotics, Launches Reachy 2 Robot for Open-Source Research", "content": "Hugging Face has made a name by providing open AI models. Now it’s providing an open robot. What’s new: Hugging Face acquired the French company Pollen Robotics for an undisclosed price. It plans to offer Pollen’s Reachy 2, a robot that runs on code that’s freely available under an Apache 2.0 license, for $70,000. How it works: Reachy 2 has two arms, gripper hands, and a wheeled base (optional). It’s designed primarily for education and research in human-robot interaction in real-world settings. Reachy 2 is programmable in Python and runs models from Hugging Face’s LeRobot library.It runs control software locally on a SolidRun Bedrock V3000 (a PC based on an AMD Ryzen Embedded V3000 processor) and processes AI in the cloud or on a local server.The robot responds to VR controllers including Meta Quest 2 and 3 as well as Pollen’s VR app.Its head senses the visual environment using a pair of cameras equipped with global shutters to capture fast-changing events and measures distances via an optical sensor. Its antennas are outfitted with microphones to capture sounds, and its torso senses distances using a depth camera. The base includes a lidar sensor to aid navigation.The body features 3D joints in the neck and wrists and 2D joints in the shoulders and elbows. Each arm can lift objects of up to 3 kilograms.A rechargeable, 24 volt battery provides around 10 hours of battery life. Behind the news: Last year, Remi Cadene, who worked on Tesla’s Optimus, joined Hugging Face to lead robotics projects. In May, he and his team rolled out the LeRobot open source robotics code library, which provides pretrained models, datasets, and simulators for reinforcement learning and imitation learning. In November, Nvidia announced a collaboration with Hugging Face to accelerate LeRobot’s data collection, training, and verification. Why it matters: Hugging Face’s acquisition of Pollen reflects an industry-wide investment in robots, notably humanoid robots, whose prices have been falling. Nvidia CEO Jensen Huang has called AI-enabled robotics a “multi-trillion dollar” opportunity. We’re thinking: AI-enabled robots are marching slowly toward what we hope will be breakthrough applications. Open-source systems are an important part of the trend!", "image_caption": "Person interacting with a humanoid robot using virtual reality headset and controllers.", "metadata": {"article_id": "hugging_face_acquires_pollen_robotics_launches_reachy_2_robot_for_open_source_research", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--78--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/hugging-face-acquires-pollen-robotics-launches-reachy-2-robot-for-open-source-research/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/hugging_face_acquires_pollen_robotics_launches_reachy_2_robot_for_open_source_research.html"}}
{"id": 14704780001, "type": "news_chunk", "title": "Inside Sam Altman’s Brief Ouster from OpenAI", "subtitle": "Inside Sam Altman’s Brief Ouster from OpenAI", "content": "A behind-the-scenes account provides new details about the abrupt firing and reinstatement of OpenAI CEO Sam Altman in November 2023. How it works: Based on insider accounts, an excerpt from a forthcoming book about OpenAI by Wall Street Journal reporter Keach Hagey describes conflicts, accusations, and shifting alliances that led to Altman’s brief ouster and rapid return. Firing and reinstatement: OpenAI’s board of directors came to distrust Altman but failed to persuade executives and employees that he should be replaced. In winter 2022, Altman told the board that the company’s joint safety committee with Microsoft had approved three “somewhat controversial” enhancements to GPT-4. Board member Helen Toner later learned that only one had been approved.Altman also failed to tell the board that Microsoft had tested GPT-4 in India without the committee’s approval.Board members were surprised to learn that Altman personally owned the $175 million OpenAI Startup Fund, so OpenAI investors wouldn’t see any profits. Altman claimed he didn’t benefit from the fund.CTO Mira Murati expressed doubts about Altman’s leadership to other board members. Murati, Toner, and co-founder Ilya Sutskever began to document his actions.On November 16, the board voted to fire Altman and appoint Murati interim CEO. The board members were reluctant to reveal why they’d fired Altman. At one meeting, Murati and other executives gave them 30 minutes to either explain why they fired Altman, resign, or watch the executive team quit. Nearly all OpenAI employees (including Murati and Sutskever) signed a letter threatening to quit if Altman wasn't reinstated, and the board reversed its decision. Aftermath: Since Altman’s return, Murati and all but one director who voted to remove him have left OpenAI. The issues that precipitated his departure have given way to commercial concerns as the company considers a shift from its current hybrid nonprofit/for-profit structure to fully for-profit. GPT-5 will arrive “in the next few months,” according to Altman.Meanwhile, OpenAI launched GPT-4.1 (making full, mini, and nano versions available via API) and confirmed it soon would release o3, a new reasoning model.OpenAI said it will release its first open model, a new language model with open weights, in coming months.The company recently raised $40 billion, the largest-ever funding round for an AI company, increasing its valuation to $300 billion. Why it matters: The AI frontier spawns not only technical innovations but also intense interpersonal relationships and corporate politics. Such dynamics have consequences for users and the world at large: Having survived serious challenges to his leadership, Altman has emerged in a strong position to build a path of faster growth as a for-profit company upon OpenAI’s philanthropic foundation. We’re thinking: Given OpenAI’s formidable achievements, Altman’s renewed leadership marks an inflection point in the AI landscape. Without Sam Altman at the helm, OpenAI would be a very different company, with different priorities and a different future.", "image_caption": "Illustration of a businessman in a blue suit sitting alone at the head of a long boardroom table with black chairs.", "metadata": {"article_id": "inside_sam_altmans_brief_ouster_from_openai", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--60--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/inside-sam-altmans-brief-ouster-from-openai/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/inside_sam_altmans_brief_ouster_from_openai.html"}}
{"id": 37912135001, "type": "news_chunk", "title": "Clothes That Thwart Surveillance, DeepMind in the Hot Seat...", "subtitle": "This Shirt Hates Surveillance", "content": "Dear friends, I am writing to you from Colombia today, and am excited to announce the opening of our office in Medellín. The office will serve as the Latin American headquarters for three of the companies in our AI ecosystem: Landing AI, deeplearning.ai, and AI Fund. AI is still in its infancy. Although Silicon Valley and Beijing are currently leading the way in AI, with the UK and Canada also emerging as innovation hubs, there are still opportunities for every major country. Colombia is on a trajectory to become a hub of AI in Latin America. I am proud to bet on Colombia and support the growth of the Colombian AI community and the broader Latin American AI community. You can find additional details here or in Frederic Lardinois’ TechCrunch article. Keep learning! Andrew Automatic license plate readers capture thousands of vehicle IDs each minute, allowing law enforcement and private businesses to track drivers with or without their explicit consent. Fashion-forward freedom fighters are countering the algorithms with a line of shirts, dresses, and tops covered with images of license plates. What’s new: Security researcher and clothing designer Kate Rose unveiled her Adversarial Fashion line at the Defcon hacker convention. The garments are meant to foul automatic license plate readers by diluting their databases with noise. How it works: Such readers typically use optical character recognition to capture lettering found in rectangular shapes they identify as license plates. But they aren’t picky about whether those rectangles are attached to a car. Rose used an open source reader to optimize her designs until they had shapes, sizes, and lettering that fooled the software.Each time a reader captures a plate from Rose’s clothes, it takes in a line of meaningless data.With enough noise, such systems become less precise, require more human oversight, and cost more money to operate. ​Rose’s Defcon deck provides a fun overview.​ Behind the news: Use of automatic license plate readers grew by 3,000 percent over the past two years, according to a February article in Quartz. Companies like OpenALPR and PlateSmart Technologies have spurred the trend by marketing their systems to casinos, hospitals, and schools. Why it matters: ALPR technology isn’t useful only for catching scofflaws who blow through red lights. Bad actors with access to license plate tracking data can stalk an individual’s movements, according to the Electronic Frontier Foundation. They can create databases of people who regularly visit sensitive locations like women’s health clinics, immigration centers, and union halls. We’re thinking: Rose’s designs aren’t likely to have a practical impact unless they become a widespread geek craze (and in that case, makers of license plate readers will respond by building in an adversarial clothing detector). Their real effect may be to spur a public conversation about the worrying proliferation of automated surveillance technology.", "image_caption": "T-shirt covered with images of license plates", "metadata": {"article_id": "issue_1", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/adversarial20fashion.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-1/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_1.html"}}
{"id": 37912135002, "type": "news_chunk", "title": "Clothes That Thwart Surveillance, DeepMind in the Hot Seat...", "subtitle": "DeepMind Results Raise Questions", "content": "Alphabet subsidiary DeepMind lost $572 million in the past year, and its losses over the last three years amounted to more than $1 billion. AI contrarian Gary Marcus used the news as an opportunity to question the direction of AI as an industry.What’s new: In an essay published by Wired, Marcus extrapolates DeepMind’s finances into an indictment of AI trends in recent years. The blow-by-blow: He begins with a seeming defense of DeepMind, saying that the losses can be viewed as investments in cutting-edge research. But he quickly doubles back, suggesting the lack of a payoff indicates that DeepMind’s research focus, deep reinforcement learning, is a dead end.He calls out DRL’s failure to make progress towards artificial general intelligence and practical goals like self-driving vehicles.DeepMind’s expenditures aren’t just an expensive mistake, he claims. They rob research funding for worthier AI techniques, such as approaches based on cognitive science. Behind the news: Marcus is a longtime critic of deep learning. He published a 10-point critique of deep learning’s shortcomings last year. He is currently promoting a book, Rebooting AI, arguing that the AI community should reorder its priorities to accommodate approaches that mimic human intelligence. In June, he announced a new venture, robust.ai, with roboticist Rodney Brooks.Yes, but: As a tech company, Alphabet does well to invest in nascent technologies or risk being disrupted by them. As a public company, it has a fiduciary responsibility to do so. Moreover, DeepMind has achieved phenomenal successes at solving Go and StarCraft II and helped make Google’s data centers and Android devices run more efficiently.What they’re saying: The essay created a stir on social media. Some voiced agreement with Marcus conclusions: “DeepMind struggles to achieve breakthrough results in transfer learning for at least two years. I believe part of them must see this as the key to AGI. I think deep nets are but one ingredient.” — @donbenhamOthers found Google’s investment well justified: “DeepMind may be over-invested in snake oil (DRL is lazy, brittle & struggles to scale past toy problems) but Google has 120B in cash sitting in the bank, w/ positive cash flow. DeepMind costs like 1% of profit, provides positive coverage, attracts talent, is a long odds bet, etc.” — @nicidobAnd many called out Marcus for ignoring DeepMind’s achievements: “What about @DeepMindAI’s protein folding success, using RL for data center cooling, WaveNet, etc, and their great neuroscience division?” — @blackHC We’re thinking: Marcus warns that investors may abandon AI if big investments like DeepMind don’t start providing returns. But some AI approaches already are having a huge economic impact, and emerging techniques like DRL new enough that it makes little sense to predict doom for all approaches based on slow progress in one. Better to save such double-barreled criticism for AI that is malicious or inept. We disagree with Marcus’ views on deep learning, but cheer him on as he codes, tests, and iterates his own way forward.", "image_caption": "GO match: AlphaGo vs. Lee Sedol", "metadata": {"article_id": "issue_1", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_deepmind.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-1/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_1.html"}}
{"id": 37912135003, "type": "news_chunk", "title": "Clothes That Thwart Surveillance, DeepMind in the Hot Seat...", "subtitle": "BERT Is Back", "content": "Less than a month after XLNet overtook BERT, the pole position in natural language understanding changed hands again. RoBERTa is an improved BERT pretraining recipe that beats its forbear, becoming the new state-of-the-art language model — for the moment. What’s new: Researchers at Facebook AI and from the University of Washington modified BERT to beat the best published results on three popular benchmarks. Key insight: Since BERT’s debut late last year, success in language modeling has been fueled not only by bigger models but also by an order of magnitude more data, more passes through the training set, and larger batch sizes. RoBERTa shows that these training choices can have a greater impact on performance than advances in model architecture. How it works: RoBERTa uses the BERT LARGE configuration (355 million parameters) with an altered pretraining pipeline. Yinhan Liu and her colleagues made the following changes: Increased training data size from 16Gb to 160Gb by including three additional datasets.Boosted batch size from 256 sequences to 8,000 sequences per batch.Raised the number of pretraining steps from 31,000 to 500,000.Removed the next sentence prediction (NSP) loss term from the training objective and used full-sentence sequences as input instead of segment pairs.Fine-tuned for two of the nine tasks in the GLUE natural language understanding benchmark as well as for SQuAD (question answering) and RACE (reading comprehension). Results: RoBERTa achieves state-of-the-art performance on GLUE without multi-task fine tuning, on SQuAD without additional data (unlike BERT and XLNet), and on RACE. Yes, but: As the authors point out, the comparison would be fairer if XLNet and other language models were fine-tuned as rigorously as RoBERTa. The success of intensive fine-tuning raises the question whether researchers with limited resources can obtain state-of-the-art results in the problems they care about. Why it matters: The authors show that rigorous tuning of hyperparameters and dataset size can play a decisive role in performance. The study highlights the importance of proper evaluation procedures for all new machine learning techniques.We’re thinking: Researchers are just beginning to assess the impact of hyperparameter tuning and data set size on complex neural network architectures at scale of 100 to 1,000 million parameters. BERT is an early beneficiary, and there’s much more exploration to be done.", "image_caption": "Bert and Ernie from Sesame Street", "metadata": {"article_id": "issue_1", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Cropped20Roberta.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-1/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_1.html"}}
{"id": 37912135004, "type": "news_chunk", "title": "Clothes That Thwart Surveillance, DeepMind in the Hot Seat...", "subtitle": "Standards in the Making", "content": "The U.S. federal government released a plan to develop technical standards for artificial intelligence, seeking to balance its aim to maintain the nation’s tech leadership and economic power with a priority on AI safety and trustworthiness.What’s new: Responding to a February executive order, the National Institute of Standards and Technology issued its roadmap for developing AI standards that would guide federal policy and applications. The 46-page document seeks to foster standards strict enough to prevent harm but flexible enough to drive innovation. What it says: The plan describes a broad effort to standardize in areas as disparate as terminology and user interfaces, benchmarking and risk management. It calls for coordination among public agencies, institutions, businesses, and foreign countries, emphasizing the need to develop trustworthy AI systems that are accurate, reliable, secure, and transparent.Yes, but: The authors acknowledge the risk of trying to corral such a dynamic enterprise. In fact, they admit that aren’t entirely sure how to go about it. “While there is broad agreement that these issues must factor into US standards,” they write, “it is not clear how that should be done and whether there is yet sufficient scientific and technical basis to develop those standards provisions. We’re thinking: NIST’s plan drives a stake in the ground for equity, inclusivity, and cooperation. Here’s hoping it can bake those values — which is not to say specific implementations — into the national tech infrastructure and spread them abroad.", "image_caption": "National Institute of Standards and Technology logo", "metadata": {"article_id": "issue_1", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_nist.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-1/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_1.html"}}
{"id": 37912135005, "type": "news_chunk", "title": "Clothes That Thwart Surveillance, DeepMind in the Hot Seat...", "subtitle": "Style Upgrade", "content": "Image-to-image translation, in which stylistic features from one image are imposed on the content of another to create a new picture, traditionally has been limited to translating either shapes or textures. A new network translates both, allowing more flexible image combinations and creating more visually satisfying output. What’s new: A team from Boeing’s South Korea lab created U-GAT-IT, a network that produces superior translations between images. Key insights: Where earlier image-to-image translation networks work best with particular image styles, U-GAT-IT adds layers that make it useful across a variety of styles. Such networks typically represent shapes and textures in hidden feature maps. U-GAT-IT adds a layer that weights the importance of each feature map based on each image’s style.The researchers also introduce a layer that learns which normalization method works best. How it works: U-GAT-IT uses a typical GAN architecture: A discriminator classifies images as either real or generated and a generator tries to fool the discriminator. It accepts two image inputs. The generator takes the images and uses a CNN to extract feature maps that encode shapes and textures.In earlier models, feature maps are passed directly to an attention layer that models the correspondence between pixels in each image. In U-GAT-IT, an intermediate weighting layer learns the importance of each feature map. The weights allow the system to distinguish the importance of different textures and shapes in each style.The weighted feature maps are passed to the attention layer to assess pixel correspondences, and the generator produces an image from there.The discriminator takes the first image as a real-world style example and the second as a candidate in the same style that’s either real or generated.Like the generator, it encodes both images to feature maps via a CNN and uses a weighting layer to guide an attention layer.The discriminator classifies the candidate image based on the attention layer’s output. Results: Test subjects chose their favorite images from a selection of translations by U-GAT-IT and four earlier methods. The subjects preferred U-GAT-IT’s output by up to 73% in four out of five data sets. Why it matters: Image-to-image translation is a hot topic with many practical applications. Professional image editors use it to boost image resolution and colorize black-and-white photos. Consumers enjoy the technology in apps like FaceApp. We’re thinking: The best-performing deepfake networks lean heavily on image-translation techniques. A new generation that takes advantage of U-GAT-IT’s simultaneous shape-and-texture modeling may produce even more convincing fake pictures.", "image_caption": "Comparison of the results using different normalization functions", "metadata": {"article_id": "issue_1", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_style20320sized-1024x577.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-1/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_1.html"}}
{"id": 27661123001, "type": "news_chunk", "title": "Robot Hand Works Rubik’s Cube, Self-Driving Tanks Roll...", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, I’ve heard this conversation in multiple companies: Machine learning engineer: Look how well I did on the test set! Business owner: But your ML system doesn’t work. This sucks! Machine learning engineer: But look how well I did on the test set! Why do AI projects fail? Last week, I addressed this question at our Pie & AI meetup. We had a spirited discussion with a live audience in 10 cities from London to Berlin, Ghent (Belgium) to Logroño (Spain). I remain as optimistic as ever about the AI industry, but I also see many AI projects struggle. Unlike software engineering, the process of engineering AI systems is immature, and teams have not yet learned about the most common pitfalls and how to avoid them. Common pitfalls fall under the headings: robustness, small data, and workflow. You can increase your odds of success by analyzing your AI project in terms of these issues. I’ll flesh out my thoughts on this in coming weeks. Stay tuned. Keep learning! Daniel Barbosa quit his job managing cloud infrastructure to self-study machine learning full-time. Learn how Daniel landed his first ML job.", "image_caption": "Pie & AI and IASI AI cupcakes", "metadata": {"article_id": "issue_10", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/06/Iasi201-1.jpeg", "source_url": "https://www.deeplearning.ai/the-batch/issue-10/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_10.html"}}
{"id": 27661123002, "type": "news_chunk", "title": "Robot Hand Works Rubik’s Cube, Self-Driving Tanks Roll...", "subtitle": "Cube Controversy", "content": "OpenAI trained a five-fingered robotic hand to unscramble the Rubik’s Cube puzzle, bringing both acclaim and criticism.What’s new: The AI research lab OpenAI trained a mechanical hand to balance, twist, and turn the cube.How it works: The system learned the manual skills to unscramble the cube using reinforcement learning. It determined the sequence of moves using a pre-existing formula known as Kociemba’s algorithm. The researchers designed a simulated environment where a virtual hand could manipulate a virtual cube. The model spent the equivalent of 13,000 years learning to twist simulated cubes.They altered the simulation’s parameters from game to game, for instance, changing the cube’s size or mass, or changing the friction between fingers and cube. This procedure, known as automatic domain randomization, eased the robot’s eventual transition from the simulation to the real world: Indeed, it was able to work the cube with two fingers tied together and while wearing a glove, though it hadn’t encountered that condition in the simulation.Once trained, the model controlled a physical hand from Shadow Robot Company, modified with LEDs to improve motion capture, rubber pads to improve grip, and more durable components. Results: The researchers considered an attempt a failure if the hand stalled or dropped the cube. The hand had a 60 percent success rate when the cube needed 15 rotations or fewer to solve the puzzle. That rate dropped to 20 percent when the solution required 26 rotations or more.Yes, but: Although OpenAI’s report focused mostly on robotic dexterity, critics accused the company of overstating its claim to have taught the robot to solve Rubik’s Cube. Kociemba’s algorithm is more than a decade old and doesn’t involve learning, they pointed out, and the cube included Bluetooth and motion sensors that tracked its segments. Moreover, despite training for virtual millennia, the robot can’t do anything more than manipulate the puzzle. Behind the news: Robots purpose-built or -coded to unscramble a Rubik’s Cube — no learning involved — are a venerable tradition. Guinness World Records recognizes a system that did the job in 0.637 seconds. Independent engineers later shaved the time to 0.38 seconds. Universal Robots programmed two industrial-bot arms to collaborate on the task. We’re thinking: OpenAI has a knack for choosing important problems, solving them in elegant ways, and producing impressive results. It also has been criticized for presenting its work in ways that lead the general public to mistake incremental developments for revolutionary progress. It’s important to set realistic expectations even as we push the boundaries of machine intelligence.", "image_caption": "Mechanical hand unscrambling a Rubik's Cube", "metadata": {"article_id": "issue_10", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/rubik20SIZED.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-10/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_10.html"}}
{"id": 27661123003, "type": "news_chunk", "title": "Robot Hand Works Rubik’s Cube, Self-Driving Tanks Roll...", "subtitle": "Public Access, Private Faces", "content": "One of the largest open datasets for training face recognition systems has its roots in a popular photo-sharing service. Companies that have used this data could find themselves liable for millions in legal recompense.What’s new: Many Flickr users were surprised and upset when reporters informed them their likeness, or that of their children and other family members, was part of a public database used to train face recognition algorithms, according to the New York Times. Such training may violate an Illinois digital privacy law that’s currently being tested in court.Tracing the data: MegaFace, which depicts 672,000 individuals in nearly 4 million photos, comprises images from Flickr that their creators licensed for commercial use under the Creative Commons intellectual property license. Yahoo owned Flickr between 2007 and 2017. In 2014, the web giant released 100 million Flickr photos for training image classifiers.The following year, University of Washington researchers started distributing the MegaFace subset.Since then, MegaFace has been used to train face recognition software by Amazon, Google, Mitsubishi, SenseTime, Tencent, and others. Legal jeopardy: In 2008, Illinois passed the Biometric Information Privacy Act, which prevents commercial entities from capturing, purchasing, or otherwise obtaining a private individual’s likeness without the person’s consent. Individuals whose faces have been used without permission are entitled to between $1,000 and $5,000 per use.Court action: The Illinois law already is fueling a $35 billion class action lawsuit against Facebook for the way it stores and uses data to automatically identify faces in photos. Facebook argued that the people pictured have no grounds to sue because its software didn’t cause them financial harm.The 9th U.S. Circuit Court overruled the objection, citing an earlier Illinois Supreme Court ruling that invasion of privacy alone is enough to break the law.The case will be decided by a jury before the federal court, on a schedule that hasn’t yet been announced. Why it matters: MegaFace is still available, and at least 300 organizations have used it to train their models, according to a 2016 University of Washington press release. Any group that has used this data to make money is liable under the Illinois law.We’re thinking: With 50 states in the U.S. and around 200 countries in the world, regulatory mismatches among various jurisdictions seem inevitable. User privacy and data rights are important, and legal requirements must be as clear and coherent as possible to advance the technology in a positive way.", "image_caption": "Collage with photos of people's faces", "metadata": {"article_id": "issue_10", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/flickr20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-10/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_10.html"}}
{"id": 27661123004, "type": "news_chunk", "title": "Robot Hand Works Rubik’s Cube, Self-Driving Tanks Roll...", "subtitle": "Cracking Open Doctors’ Notes", "content": "Weak supervision is the practice of assigning likely labels to unlabeled data using a variety of simple labeling functions. Then supervised methods can be used on top of the now-labeled data. Researchers used this technique to search electronic health records (EHRs) for information squirreled away in unstructured text.What’s new: Complications from hip replacement surgery tend to be under-reported because they’re recorded in EHRs as notes rather than check-marked in a standard list. Researchers at Stanford used weak supervision to label such notes and then extracted information related to hip implants. Their method brought to light complications that hadn’t been tracked explicitly.Key insight: Alison Callahan and collaborators divided the problem of finding references to post-surgical issues in notes into two parts: identifying the implant’s make and model, and spotting mentions of pain and complications. This made it possible to use weak supervision to label data separately for each subproblem.How it works: Snorkel is a framework that provides a modular way to define and combine labeling functions. The model works as follows: Domain experts construct labeling functions to find the implant maker and type, mentions of pain and the anatomy affected, and mentions of the implant close to the complication it led to. For instance, a labeling function may spot a pain-related word adjacent to a body part and mark the corresponding sentence as evidence of pain. These functions assign labels for each subproblem in every sentence.A probabilistic model (graphical model) learns the relative accuracy of the labeling functions based on mutual overlaps and conflicts of their label assignments on the training data. These metrics are then used to combine labels from each labeling function into a single label for each subproblem in every sentence.An LSTM with attention is trained on the newly labeled data to spot complications arising from certain implants and map pain to body parts. Results: The researchers trained the system on records of about 6,000 hip-replacement patients treated between 1995 and 2014. Learning the relationships between the various labeling functions uncovered twice as many patients facing complications as majority voting on their predictions (61 percent versus 32 percent). Overall, the system made it possible to assess the likelihood that a particular implant would lead to complications.Why it matters: This analysis could help doctors to match patients with appropriate implants, and help implant manufacturers design their products to minimize bad outcomes.Takeaway: This approach extracts useful information from EHRs, and it looks as though it would generalize to other text-labeling tasks.", "image_caption": "Process of labeling doctors' notes", "metadata": {"article_id": "issue_10", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/hip20replacement20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-10/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_10.html"}}
{"id": 27661123005, "type": "news_chunk", "title": "Robot Hand Works Rubik’s Cube, Self-Driving Tanks Roll...", "subtitle": "Robot Tanks on the March", "content": "A new generation of battlebots is gaining momentum.What’s new: The Army is at least two years ahead of schedule in its plan to deploy self-driving (and self-aiming) transports, jeeps, and tanks, said Richard Ross Coffman, director of Next Generation Combat Vehicles, in an interview with Breaking Defense. Rolling thunder: The NGCV program features three phases of testing for vehicles of graduated firepower and autonomy. In Phase One, beginning in early 2020, the Army will test autonomous-driving hard- and software on Vietnam-era armored transports. The vehicles will be remote-controlled by soldiers in M2 Bradley transports.In 2021, Phase Two will test the same hardware on custom-built vehicles. Half of these four-wheeled prototypes will be lightweight models (less than 10 tons) and carry machine guns and anti-tank missiles. The other half will be medium-weight (up to 12 tons) and able to carry bigger weaponry.Heavyweight autonomous tanks weighing up to 20 tons and mounted with 120mm cannons will roll out in 2023 for Phase Three.Coffman envisions systems that enable one human to control a dozen tanks in 2035 or later. Even then, a flesh-and-blood soldier will oversee firing. Behind the news: The U.S. Army has spent billions on robotic fighting machines that never came to fruition. In 2009, the service cancelled a previous autonomous war-fighting effort, the $20 billion Future Combat Systems program, after six years in development. That program was nixed partly because the technology didn’t progress as quickly as expected and partly due to a shift from warfare to counterterrorism.Why it matters: Robot vehicles could act as decoys, drawing fire meant for human troops. They could also infiltrate enemy lines and call in artillery strikes, gather information, screen for obstacles, and wade into areas affected by nuclear, chemical, or biological weapons.What they’re saying: “Anywhere that a soldier is at the highest risk on the battlefield, and we can replace him or her with a robot, that’s what we want to do.” — Richard Ross Coffman, Director, Next Generation Combat Vehicles, U.S. Army.We’re thinking: How is the Army, which must cope with irregular terrain, intermittent explosions, and the fog of war, ahead of schedule when the automotive industry, navigating smooth surfaces and relatively orderly traffic, has fallen behind its initial projections? The military faces very different problems, some harder to navigate than urban environments, some easier. Its emphasis on remote control also could make a significant difference. Bottom line: Like many people, we’re unsettled by the combination of AI and fighting machines. We strongly support proposals for an international ban on autonomous weapons.", "image_caption": "Military tank", "metadata": {"article_id": "issue_10", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/tanks20SIZED.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-10/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_10.html"}}
{"id": 27661123006, "type": "news_chunk", "title": "Robot Hand Works Rubik’s Cube, Self-Driving Tanks Roll...", "subtitle": "New Materials Courtesy of Bayes", "content": "Would you like an umbrella that fits in your pocket? Researchers used machine learning to invent sturdy but collapsible materials that might lead to such a fantastical object.What’s new: Researchers at the Netherlands’ Delft University of Technology used a Bayesian model to find arrangements of brittle polymers that are sturdy, lightweight, compressible, and able to spring back to their original shape. The machine learning algorithm made it possible to design and produce materials without conducting the usual trial-and-error physical experiments.How it works: Principal investigator Miguel Bessa designed a mock-up with two disks connected by flexible poles, or longerons, that fold in a spiral pattern when the dishes are pressed together. In a simulator, Bassa assembled 100,000 different materials in structures that mimicked his mock-up.Then he used the model to classify the arrangements that fit his criteria, primarily those whose longerons coiled in spiral shapes when compressed and recovered when pressure was released.He settled on two designs and built prototype compressible masts at microscopic and human scales. Results: The microscopic prototype — built for strength — was fully compressible and able to withstand intense pressure without buckling. For the human-scale version, it was important that it spring back into its original shape, which it did even when compressed nearly flat by a machine press. Why it matters: Scientists working on metamaterials (structural arrangements of existing materials that exhibit characteristics not found in nature) alter material geometries, shapes, sizes, and orientations to produce novel properties. Typically this requires lots of trial and error. Machine learning can curate arrangements likely to have the right properties, enabling researchers to focus on the most promising candidates.We’re thinking: From materials science to drug design, brute force experimentation still plays a large role in bleeding-edge science. AI-driven screening is beginning to help researchers find shorter routes to Eureka.", "image_caption": "Machine compressing an object", "metadata": {"article_id": "issue_10", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/materials20SIZED.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-10/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_10.html"}}
{"id": 27661123007, "type": "news_chunk", "title": "Robot Hand Works Rubik’s Cube, Self-Driving Tanks Roll...", "subtitle": "How Neural Networks Generalize", "content": "Humans understand the world by abstraction: If you grasp the concept of grabbing a stick, then you’ll also comprehend grabbing a ball. New work explores deep learning agents’ ability to do the same thing — an important aspect of their ability to generalize.What’s new: Psychologists call this kind of thinking systematic reasoning. Researchers at DeepMind, Stanford, and University College London studied this capability in deep reinforcement learning models trained to interact with an environment and complete a task.Key insight: Felix Hill and colleagues trained a model to put object 1 on location 1 with an example of that action being performed. At test time, they asked the model to put object 2 on location 2. Object 2 and location 2 weren’t in the training set, so the model’s ability to execute the task would indicate a generalized understanding of putting.How it works: The model receives a view of the environment along with a task description (an instruction to put or find a given object). The model processes these elements separately, then combines its understanding of each to determine a series of actions to complete the task. The model comprises three components (the usual choices for image processing, text understanding, and sequence decisions): A CNN processes the environment view, an LSTM interprets the task description, and the CNN and LSTM outputs merge in a hidden LSTM layer to track progress toward completing the task.The model learns to associate various objects with their names by executing put [object] or find [object] tasks.The researchers separate objects into test and training sets. Then they train the model to put or lift objects in the training set.To measure systematic reasoning, they ask it to lift or put objects in the test set. Results: The researchers trained copies of the model in simulated 2D and 3D environments. It was over 91 percent successful in lifting novel objects either way. However, success at putting novel objects dropped to about 50 percent in both environments.Yes, but: Removing the task description and LSTM component didn’t degrade performance much. That is, while words such as put and find may help humans understand how neural networks operate systematically, language apparently isn’t critical to their performance.Why it matters: Neural networks are able to generalize, but our understanding of how they do it is incomplete. This research offers a way to evaluate the role of systematic reasoning. The results imply that models that reason systematically are more likely to generalize.Takeaway: The recent run of pretrained language models acquire knowledge that enables them to perform a variety of tasks without retraining from scratch. Understanding systematic reasoning in neural networks could lead to better performance in domains outside of natural language.", "image_caption": "Schematic of the architecture used in experiments related to systematic reasoning in deep reinforcement learning", "metadata": {"article_id": "issue_10", "chunk_index": 7, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/systemic20reasoning20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-10/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_10.html"}}
{"id": 25584521001, "type": "news_chunk", "title": "Walking the Robot Dog, Mistaking German for English, Making...", "subtitle": "Walking the Dog", "content": "Dear friends, I’ve been following with excitement the recent progress in space launches. Earlier this week, Richard Branson and his Virgin Galactic team flew a rocket plane 53 miles up, earning him astronaut wings. Next week, Jeff Bezos’ Blue Origin is expected to attempt a similar feat and achieve an even greater altitude. (I once also sat in a Blue Origin passenger capsule; see the picture below. I remained firmly on planet Earth.)The first space race was between the U.S. and the Soviet Union, a competition between rival superpowers with dramatically different visions for civilization. Some pundits have panned the current space race as a contest between billionaires, but I’m glad that Bezos, Branson, and Elon Musk are pushing the boundaries of commercial flight.I’ve found space exploration exhilarating since I was a child. My father had a passion for astronomy. We spent many hours on the rooftop of our apartment complex in Singapore — often staying up way past the bedtime designated by my mother 😅 — peering through my dad’s telescope at the planets in our solar system. I remember peering at Alpha Centauri (the closest star system to ours) and wondering if I would visit someday. Space exploration has been criticized as a waste of resources, given the problems we have here at home. Of course, we need to work on problems such as the still-rampaging Covid-19, climate change, poverty, and injustice. I believe society will be best off if we pursue multiple meaningful projects simultaneously. As we push further into space, AI will play an increasing role. Our robots will need to be increasingly autonomous because, even though radio waves travel at the speed of light, there won’t be sufficient time to wait for guidance from human operators on Earth. (Mars averages 13 light minutes from Earth, and the more distant Neptune about 250 light minutes.) I was excited when ROS, the open-source Robot Operating System framework launched by Morgan Quigley out of my Stanford group, started running in the International Space Station. And we still have much work ahead!Private entities are at the center of this week’s space boom, but I would love to see public entities play a bigger role. NASA’s innovations have been widely shared. I’m excited about the Perseverance rover and Ingenuity helicopter now roaming Mars (over 1 million times farther than Branson has yet to travel). So let’s make sure to strongly support public space exploration as well. Further advances will come even faster with their help. Keep learning! 🚀Andrew A reinforcement learning system enabled a four-legged robot to amble over unfamiliar, rapidly changing terrain. What’s new: Researchers at UC Berkeley, Facebook, and Carnegie Mellon developed Rapid Motor Adaptation (RMA). The system enabled a Unitree Robotics A1 to negotiate changing conditions and unexpected obstacles nearly in real time. The machine traversed muddy trails, bushy backcountry, and an oil-slicked plastic sheet without falling. How it works: The system includes two algorithms, both of which are trained in simulation. The reinforcement learning component learns to control locomotion basics, while the adaptation module learns to generate a representation of the environment. In deployment, the two algorithms run asynchronously on a single edge device. They analyze the previous 0.5 seconds of data from limbs and joints and adjust the gait accordingly.In tests, the robot maneuvered through conditions that it hadn’t encountered in simulations, such as a squishy foam mattress, over piles of rubble, and rough-hewn staircases. It repeated many of the tests carrying loads of varying weight.The machine achieved 70 percent or better success in each scenario. When it fell, the mishap typically was due to a sudden drop while descending stairs or debris that blocked more than one leg. Behind the news: Video clips of robots from Boston Dynamics and others have become viral sensations in recent years. They may be mouth-watering, but the bots involved often are programmed for specific motions or scenarios and can’t adapt to novel conditions. Why it matters: RMA is among the first robotic walking systems that don’t need to be trained for every variety of terrain they're likely to encounter. We’re thinking: For many applications where navigating flat ground is sufficient, wheeled locomotion is much simpler and more reliable. But legs still carry the day when navigating rough terrain — not to discount their uncanny anthropomorphic appeal. They’re likely to be important for tasks like fighting fires, traversing disaster zones, and navigating the toy-strewn obstacle course that is Andrew’s daughter's playroom.", "image_caption": "A four-legged robot walking over difficult and changing terrain", "metadata": {"article_id": "issue_100", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker---2021-07-14T100209.763.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-100/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_100.html"}}
{"id": 25584521002, "type": "news_chunk", "title": "Walking the Robot Dog, Mistaking German for English, Making...", "subtitle": "Danke for the Interview", "content": "An independent test found flaws in AI systems designed to evaluate job applicants. What’s new: MyInterview and Curious Thing, which automate job interviews, gave a candidate who spoke only in German high marks on English proficiency, according to MIT Technology Review. The test: Reporters created a fake job posting for an office administrator/researcher on both companies’ platforms. They used the tools provided to select questions for applicants to answer and define their ideal candidate. Then one of them applied for the position, completing interviews by reading aloud from a Wikipedia article written in German. MyInterview typically conducts a video interview and analyzes a candidate’s verbal and body language, then grades their suitability for a given job. MyInterview interpreted the German-speaking reporter’s responses as nonsensical English (“So humidity is desk a beat-up. Sociology, does it iron?”) but graded her as a 73 percent match for the job. A MyInterview spokesperson said the algorithm inferred personality traits from the interviewee’s voice rather the content of her answers.Curious Thing analyzes phone interview responses. Its algorithm gave the reporter 6 out of 9 points for English-language competency after she responded exclusively in German. The company’s cofounder said the bogus application was an “extremely valuable data point.” Behind the news: A 2019 survey found that 40 percent of companies worldwide use AI to help screen job candidates, but outside investigators have found such systems lacking. In February, Bavarian Public Broadcasting showed that accessories like glasses and headscarves and backgrounds including objects like pictures and bookcases dramatically changed a German video-interview platform’s automated assessments.In 2018, LinkedIn discovered that a candidate recommendation algorithm preferred male applicants. The company replaced it with a new system intended to counteract that bias.A recent study from NYU, CUNY, and Twitter proposed a matrix for rating automated hiring systems to counteract the prevalence of algorithms that rely on dubious features like voice intonation and subtle facial expressions. Why it matters: Matching prospective employers and employees is a nuanced process, and any attempt to automate it requires the utmost rigor. Applicants subject to a flawed algorithm could be barred from jobs they’re eminently qualified for, while prospective employers who rely on it could miss ideal candidates. We’re thinking: An AI system that gives high marks to someone who replies to an English-language interview in German — confidently rendering incorrect predictions in response to data that’s dramatically different its training set — is not equipped to handle data drift. Such concepts are not purely academic. They have a huge impact on such systems — and on critical decisions like who gets a job.", "image_caption": "Screen captures of a job interviews automation system", "metadata": {"article_id": "issue_100", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/interview.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-100/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_100.html"}}
{"id": 25584521003, "type": "news_chunk", "title": "Walking the Robot Dog, Mistaking German for English, Making...", "subtitle": "I Know It When I See It", "content": "Object detectors typically detect only items that were labeled in their training data. A new method liberates them to locate and recognize a much wider variety of objects. What’s new: Xiuye Gu and colleagues at Google Research developed Vision and Language Knowledge Distillation (ViLD) to build a zero-shot object detector — that is, one that can handle classes on which it didn’t train. ViLD takes advantage of representations generated by the pretrained zero-shot classifier CLIP. Key Insight: In knowledge distillation, one model learns to mimic another model’s output. Similarly, one model can learn to mimic another’s representations. An object detector’s representations (which encode several regions and classifications per image) can conform to a classifier’s (which encode one classification per image) by cropping the images that contain multiple objects into separate regions for the classifier. Then the object detector can learn to reproduce the classifier’s representation of each region. How it works: To understand ViLD, it helps to know a bit about CLIP. CLIP matches images and text using a vision transformer and a text transformer pretrained on 400 million image-text pairs. At inference, users give it a text list of the classes they want to recognize. Fed an image, it returns the most likely class in the list. To that system, the authors added a Mask R-CNN object detector trained on the most common classes in Large Vocabulary Instance Segmentation (LVIS), a dataset that contains images of objects that have been segmented and labeled. They reserved the other LVIS classes for the test set. Given a list of LVIS classes, CLIP’s text transformer generated a list of class representations.Given an image, Mask R-CNN generated object representations. In parallel, CLIP’s vision transformer generated corresponding cropped-region representations.For each Mask R-CNN object representation, the authors found the closest LVIS class representation. They measured similarity using cosine similarity, a measure of the angle between two vectors, and applied a softmax to predict the object’s class.They trained the Mask R-CNN using two loss terms. The first minimized the difference between CLIP’s and Mask R-CNN’s representations. The second encouraged the Mask R-CNN’s predicted class of a region to match the known label.At inference, they fed the remaining LVIS classes to CLIP and added the text transformer’s representations to the earlier list. Presented with a new object class, the Mask R-CNN generated a representation, and the authors found the closest LVIS class representation in the list. Results: The authors pitted their system against a Mask R-CNN trained on all LVIS classes in a supervised manner. They compared average precision, a measure of how many objects were correctly identified in their correct location (higher is better). The author’s system achieved 16.1 average precision on novel categories, while the supervised model’s achieved 12.3 average precision. Why it matters: Large, diverse training datasets for object detection are difficult and expensive to obtain. ViLD offers a way to overcome this bottleneck. We’re thinking: Physicists who want to classify a Bose-Einstein condensate need absolute-zero-shot object detection.", "image_caption": "Image showing how object detectors work", "metadata": {"article_id": "issue_100", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker---2021-06-01T145617.637.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-100/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_100.html"}}
{"id": 25584521004, "type": "news_chunk", "title": "Walking the Robot Dog, Mistaking German for English, Making...", "subtitle": "CLIP Art", "content": "Creative engineers are combining deep learning systems to produce a groundswell of generated imagery. What’s new: Researchers, hackers, and artists are producing new works by pairing CLIP, a pretrained image classifier, with a generative adversarial network (GAN). UC Berkeley researcher Charlie Snell captured the ferment in a blog post. How it works: Users typically give CLIP a text list of the classes they want to recognize; given an image, it returns the most likely class in the list. Digital artists, on the other hand, feed CLIP a verbal description of an image they want to produce and use its ability to match text with images to guide a GAN. The community has developed a set of Google Collab Notebooks that link CLIP with various GANs. A user types a phrase, sets some parameters, and chooses which GAN to use for image generation.Once the GAN has generated an image, CLIP scores it based on how closely it matches the original phrase. The Collab code then adjusts the GAN’s hyperparameters iteratively, so its output earns a higher score from CLIP. It repeats the cycle of generation and adjustment until CLIP’s score exceeds a threshold set by the user.Different GANs yield images with different visual characteristics. For instance, pairing CLIP with BigGAN produces output that tends to look like an impressionist painting. Pairing CLIP with VQ-GAN produces more abstract images with a cubist look.Adding to the prompt a phrase like “rendered in Unreal Engine,” referring to a popular video game renderer, can drastically improve the quality of the generated output. Behind the news: Open AI has its own image generator, DALL·E. Reportedly its output is less abstract and fanciful. Why it matters: CLIP was built to classify, not co-create, while GANs were developed to produce variations on familiar images. The boomlet in generated art shows how the creative impulse can unlock potential that engineers may not have imagined. We’re thinking: It’s great to see human artists collaborating with neural networks. It’s even better to see neural networks collaborating with one another!", "image_caption": "Series of AI generated imagery", "metadata": {"article_id": "issue_100", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/art3.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-100/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_100.html"}}
{"id": 96911250001, "type": "news_chunk", "title": "Amazon's Algorithmic Mismanagement, Brainwaves to Text...", "subtitle": "Listening to the Brain", "content": "Dear friends, In a recent letter, I mentioned some challenges to building AI products. These problems are distinct from the issues that arise in building traditional software. They include unclear technical feasibility and complex product specification. A further challenge is the need for data to start development. To develop a traditional software product, interviews with potential users might be sufficient to scope out a desirable product, after which you can jump into writing the code. But AI systems require both code and data. If you have an idea for, say, automating the processing of medical records or optimizing logistics networks, you need medical records data or logistics data to train a model. Where can you get it? I see different answers for consumer-facing and business-facing AI products. For consumer-facing (B2C) products, it is generally easier to ask a small group of alpha testers to try out a product and provide data. This may be sufficient to bootstrap the development process. If the data you need is generic to many users — for example, photos on smartphones — it’s also more likely that a team will be able to find or acquire enough data to get started. For business-facing (B2B) AI projects, it’s often difficult to get the data necessary to build a prototype because a lot of highly specialized data is locked up within the companies that produce it. I’ve seen a couple of general ways in which AI teams get around this problem. Some AI teams start by doing NRE (non-recurring engineering, or consulting) work, in which they build highly customized solutions for a handful of customers. This approach doesn’t scale, but you can use it to obtain enough data to learn the lessons or train the models needed to build a repeatable business. Given their need for data, AI startups seem to take this path more often than traditional software startups.Some AI entrepreneurs have worked with multiple companies in a vertical market. For example, someone who has worked for a large public cloud company may have exposure to data from multiple companies in a given industry and witnessed similar issues play out in multiple companies. I’ve also had friends in academia who consulted for multiple companies, which enabled them to recognize patterns and come up with general solutions. Experience like this puts entrepreneurs in a better position to build a nascent product that helps them approach companies that can provide data. If you lack data to get started on an AI project, these tactics can help you get an initial dataset. Once you’ve built a product, it becomes easier to find customers, get access to even more data, and scale up from there. Keep learning!Andrew Neural networks translated a paralyzed man’s brainwaves into conversational phrases. What’s new: Researchers at UC San Francisco and UC Berkeley trained a system to interpret electrical impulses from the brain of a man who had lost the ability to speak 15 years ago, and displayed them as words on a video screen. How it works: The researchers implanted an array of 128 electrodes into the region of the brain responsible for movement of the mouth, lips, jaw, tongue, and larynx. They connected the implant to a computer. Then they asked the patient to try to speak 50 common words and 50 common phrases and recorded the resulting brain activity. They trained the system on 22 hours of these signals, team member Sean Metzger at UC San Francisco told The Batch. A stack of three LSTMs detected portions of brain activity related to speech.An ensemble of 10 convolutional gated recurrent unit models classified speech signals as one of the 50 words.An n-gram language model predicted the probability that a given word would come next.A custom Viterbi decoder, an algorithm often used in communications that are subject to transmission errors, determined the most likely of the 50 phrases based on the models’ output. Results: During tests, the system decoded a median of 15.2 words per minute and translated sentences with a median error rate of 25.6 percent. Behind the news: The system was built on more than a decade of research by lead author and neurosurgeon Edward F. Chang into links between neurological activity and the sounds of spoken language. A similar project called BrainGate translated brain signals associated with the act of handwriting into text. Why it matters: Accidents, diseases, and other tragedies rob countless people of their ability to communicate. This technology opens a pathway for them to reconnect. We’re thinking: It’s wonderful to see natural language models restoring the most natural form of language.", "image_caption": "Animated video showing a system to interpret electrical impulses from the brain as words", "metadata": {"article_id": "issue_101", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/Brain-Implant.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-101/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_101.html"}}
{"id": 96911250002, "type": "news_chunk", "title": "Amazon's Algorithmic Mismanagement, Brainwaves to Text...", "subtitle": "When Algorithms Manage Humans", "content": "Some delivery drivers fired by Amazon contend that the retailer’s automated management system played an unfair role in terminating their employment. What’s new: Drivers in Amazon Flex, an Uber-like program that enables independent drivers to earn money delivering the company’s packages, said the program downgraded their performance unjustly and terminated them without warning, Bloomberg reported. Flex or inflexible? Flex rates drivers automatically on how punctually they pick up and deliver packages and how closely they follow instructions like “place the package on my back porch.” Former drivers said the program didn’t account for unavoidable delays caused by obstacles like long lines at Amazon distribution centers, gated apartment complexes, or bad weather. A former Amazon manager told Bloomberg the company was aware that its system had flaws that could lead to bad publicity but decided that higher efficiency was worth that risk.Flex drivers have 10 days to appeal termination. However, drivers and anonymous sources told Bloomberg that email responses seemed to be automated and appeals rarely succeed. Drivers who lose an appeal can spend $200 to arbitrate the case.A company spokesperson told The Batch that human managers review Flex drivers flagged for poor performance, and that an algorithm does not make the final decision to terminate employment. Behind the news: The U.S. Federal Trade Commission recently forced Amazon to pay Flex drivers $61.7 million in tips it had withheld. More broadly, Amazon’s penchant for using automated systems to manage personnel has been a steady source of controversy. In 2019, documents obtained by The Verge showed that the company used algorithms to track productivity in its warehouses and fired workers who did not meet performance benchmarks.In 2018, the company abandoned a hiring algorithm after an internal audit found that it was biased against women.The company requires its fleet drivers to consent to being monitored by AI-powered cameras that watch for signs of drowsiness or distraction. Some drivers have declined to work with the cameras, calling them an invasion of privacy. Why it matters: Organizations increasingly rely on algorithms to help make decisions that impact peoples’ lives, including who gets a bank loan, a job, or jail time. Public backlash has led to proposals like the Algorithmic Accountability Act, which would require the U.S. government to develop rules that mitigate algorithmic bias and provide ways for citizens to appeal automated decisions. We’re thinking: All algorithms are prone to some degree of error. At a company the size of Amazon, even a tiny error can have a large impact. Every effort should be made to audit such systems for fairness, make sure the tradeoffs between flexibility and efficiency are transparent, and treat individuals with compassion and respect.", "image_caption": "Amazon delivery driver working", "metadata": {"article_id": "issue_101", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker-4.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-101/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_101.html"}}
{"id": 96911250003, "type": "news_chunk", "title": "Amazon's Algorithmic Mismanagement, Brainwaves to Text...", "subtitle": "One Network, Many Scenes", "content": "To reconstruct the 3D world behind a set of 2D images, machine learning systems usually require a dedicated neural network for each scene. New research enables a single trained network to generate 3D reconstructions of multiple scenes. What’s new: Adam Kosiorek and Heiko Strathmann led a team at DeepMind in developing NeRF-VAE. Given several 2D views of a 3D scene pictured in its training data, NeRF-VAE produces new views of the scene. Key insight: The method known as Neural Radiance Fields (NeRF) produces new views of a scene based on existing views and the positions and orientations of the camera that produced them. NeRF-VAE takes the same input but adds representations of those views. This enables it to learn patterns within a scene. Those patterns help the network produce new views by enabling it to, say, infer the characteristics of common elements that were partly blocked from view in the training images. How it works: NeRF-VAE is a modified variational autoencoder (VAE), where the encoder is a Nouveau ResNet and the decoder is basically NeRF with an additional input for a representation of the scene. The training set comprised four randomly generated views per scene of 200,000 synthetic 3D scenes composed of geometric shapes against plain backgrounds, as well as the associated camera positions and orientations. The authors trained the network to match predicted pixels with the pixels in the images. For each of the four views of a scene, the encoder predicts parameter values that correspond to the image’s data distribution. The system averages the parameters and uses the average distribution to generate a representation of the scene.The decoder samples points along rays that extends from the camera through each pixel in the views. It uses a vanilla neural network to compute the color and transparency of each point based on the point’s position and the ray’s direction as well as the scene representation.To determine the color of a given pixel, it combines the color and transparency of all sampled points along the associated ray. To generate a new view, it repeats this process for every pixel. Results: The authors trained one NeRF-VAE on all scenes and a separate NeRF for each scene. Trained on four images per scene, NeRF-VAE achieved roughly 0.2 mean squared error, while NeRF achieved roughly 0.8 mean squared error. NeRF required training on 100 images of a scene to achieve a competitive degree of error. Why it matters: NeRF falters when it attempts to visualize hidden regions in a scene. That’s partly because a NeRF model encodes information about only a single 3D structure. NeRF-VAE overcomes this weakness by learning about features that are common to a variety of 3D structures. We’re thinking: By feeding a random vector directly to the decoder, the authors produced views of novel, generated scenes made up of elements in the training images. Could this approach extend deepfakery into the third dimension?", "image_caption": "Series of images showing how single trained network generates 3D reconstructions of multiple scenes", "metadata": {"article_id": "issue_101", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/NERF-VAE.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-101/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_101.html"}}
{"id": 96911250004, "type": "news_chunk", "title": "Amazon's Algorithmic Mismanagement, Brainwaves to Text...", "subtitle": "Bye Bye Bots", "content": "The independent research lab OpenAI wowed technology watchers in 2019 with a robotic hand that solved Rubik’s Cube. Now it has disbanded the team that built it. What’s new: OpenAI cofounder Wojciech Zaremba revealed that OpenAI shuttered its robotics program last October. Robo retrenchment: In a podcast produced by Weights & Biases, a maker of AI development tools, Zaremba said a lack of data was holding back OpenAI’s progress in robotics. The company’s broad goal is to develop artificial general intelligence, and it believes it can make more progress by focusing on approaches such as reinforcement learning with human feedback, a representative told VentureBeat. Behind the news: OpenAI previously developed a robotics simulation environment, a reinforcement learning toolkit, and techniques for training robots. Why it matters: The robotics industry has seen several high-profile players struggle with the high cost of research and development. In recent years, Honda shuttered its Asimo subsidiary, Rethink Robotics closed up shop, and Boston Robotics, famous for its acrobatic bipeds and resilient quadrupeds, repeatedly changed hands. We’re thinking: When even a fleet of robots isn’t able to generate enough data, that’s a sign of how data-hungry our algorithms are. It’s also a reminder of how far the current state of the art is from human-level AI. After all, infants have only one body’s worth of data to learn from.", "image_caption": "Forbidden sign over a robot's hand solving a Rubik's Cube", "metadata": {"article_id": "issue_101", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/openai.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-101/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_101.html"}}
{"id": 56314281001, "type": "news_chunk", "title": "Face Recognition Audit, Gamers Cheat with AI, Who Rules...", "subtitle": "Meet the New Smart-Cities Champ", "content": "Dear friends, In earlier letters, I discussed some differences between developing traditional software and AI products, including the challenges of unclear technical feasibility, complex product specification, and need for data to start development. This time, let’s examine the further challenge of additional maintenance cost.Some engineers think that when you deploy an AI system, you’re done. But when you first deploy, you may only be halfway to the goal. Substantial work lies ahead in monitoring and maintaining the system. Here are some reasons why: Data drift. The model was trained on a certain distribution of inputs, but this distribution changes over time. For example, a model may have learned to estimate demand for electricity from historical data, but climate change is causing unprecedented changes to weather, so the model’s accuracy degrades.Concept drift. The model was trained to learn an x->y mapping, but the statistical relationship between x and y changes, so the same input x now demands a different prediction y. For example, a model that predicts housing prices based on square footage will lose accuracy as inflation causes prices to rise.Changing requirements. The model was built to perform a particular task, but the product team decides to modify its capabilities. For instance, a model detects construction workers who wander into a dangerous area without a hard hat for more than 5 seconds. But safety requirements change, and now it must flag hatless workers who enter the area for more than 3 seconds. (This issue sometimes manifests as concept drift, but I put it in a different category because it’s often driven by changes in the product specification rather than changes in the world.) Detecting concept and data drift is challenging, because AI systems have unclear boundary conditions. For traditional software, boundary conditions — the range of valid inputs — are usually easy to specify. But for AI software trained on a given data distribution, it’s challenging to recognize when the data distribution has changed sufficiently to compromise performance. This problem is exacerbated when one AI system’s output is used as another AI’s input in what’s known as a data cascade. For example, one system may detect people and a second may determine whether each person detected is wearing a hard hat. If the first system changes — say, you upgrade to a better person detector — the second may experience data drift, causing the whole system to degrade.Even if we detect these issues, our tools for fixing them are immature. Over the past few decades, software engineers have developed relatively sophisticated tools for versioning, maintaining, and collaborating on code. We have processes and tools that can help you fix a bug in code that a teammate wrote 2 years ago. But AI systems require both code and data. If you need to fix a few training examples that a teammate collected and labeled 2 years ago, will you be able to find the documentation and the exact version of the data? Can you verify that your changes are sound and retrain the model on the revised dataset? Tools for data management, unlike tools for code management, are still nascent.Beyond data maintenance, we still have traditional software maintenance to deal with. For instance, many teams had to upgrade from TensorFlow 1 to TensorFlow 2.These problems will recede as data-centric AI tools and methodologies evolve. But for now, being aware of them and planning projects around them can help you build better models and reduce costs.Keep learning!Andrew Chinese researchers for the first time swept a competition to develop AI systems that monitor urban traffic.What’s new: Chinese universities and companies won first and second place in all five categories of the 2021 AI City Challenge, beating hundreds of competitors from 38 nations. U.S. teams dominated the competition in its first three years, but Chinese contestants started overtaking them last year.What happened: 305 teams entered at least one of the competition’s five tracks. All teams used the same training and testing data for each track. Here’s a summary of the challenges and winners: Counting the number of vehicles turning left, turning right, or going straight through an intersection. Winner: Baidu/Sun Yat-sen University.Tracking individual vehicles across multiple cameras. Winner: Alibaba.Tracking multiple vehicles across multiple cameras scattered around a city. Winner: Alibaba/University of China Academy of Sciences.Detecting car crashes, stalled vehicles, and other traffic anomalies. Winner: Baidu/Shenzhen Institute of Advanced Technology.Identifying vehicles using natural-language descriptions (a new challenge for this year’s contest). Winner: Alibaba/University of Technology Sydney/Zhejiang University. Behind the news: Nvidia, QCraft, and several universities launched the AI City Challenge in 2017 to spur the development of smart city technology.Why it matters: This competition is the latest example of China’s rising profile in AI. The Chinese government has funded hundreds of Smart City programs. In contrast, U.S. funding for urban AI initiatives has been limited to a few one-off grants or competitions.We’re thinking: Smart-city technology could make urban living more pleasant and productive, yet it also carries a risk of invasive surveillance. We call on regulators and researchers who work on such projects worldwide to lead a global debate on appropriate standards of privacy and to design their systems that protect privacy from the ground up.", "image_caption": "AI system monitoring urban traffic", "metadata": {"article_id": "issue_102", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/CITY--1-.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-102/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_102.html"}}
{"id": 56314281002, "type": "news_chunk", "title": "Face Recognition Audit, Gamers Cheat with AI, Who Rules...", "subtitle": "Fake Aim", "content": "Gamers looking to cheat in first-person shooters can’t miss with AI-assisted marksmanship.What’s new: A video-game hack uses computer vision to blast virtual enemies at superhuman speed, Ars Technica reported. A system that implemented the technique was shut down last week.How it works: Userviz worked with any shooter that runs on PC, PlayStation, or Xbox. It identified and fired on targets in under 10 milliseconds. (Professional gamers have reaction times between 100 and 250 milliseconds.) It worked like this: A video capture card streamed the game’s output to another computer that ran a YOLO object detector trained to recognize game avatars. A controller adapter translated YOLO’s output into in-game commands to snap the cursor onto a target and fire.The system could identify individual body parts, adjust for recoil, and automatically pull the trigger whenever an enemy entered the player’s crosshairs.The system’s vendor deleted access to and support for the system after it heard from Activision, publisher of the popular Call of Duty line of first-person shooters. Behind the news: Cheat codes that enhance a player’s ability to aim and fire are common but frowned upon. Activision recently banned 60,000 players of Call of Duty: Warzone for using them. Typically, such cheats are add-ons to game software. Tools that use computer vision operate independently of the game and therefore are harder to detect. Userviz was one of several on the market, and some enterprising cheaters have coded their own.Why it matters: Electronic gaming is a lucrative industry — and so is the market for products that make it easier to win. Unscrupulous players may have taken millions of dollars in competition money.We’re thinking: Like fighting spam and fraud, thwarting aimbots is a game of cat and mouse. The next generation of such bots may behave more like humans — making an average player appear to be highly skilled — and thus be even harder to detect. Who’s up for a round of rock, paper, scissors?", "image_caption": "Video showing AI working on a video-game to blast virtual enemies at superhuman speed", "metadata": {"article_id": "issue_102", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/ezgif.com-gif-maker--1--3.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-102/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_102.html"}}
{"id": 56314281003, "type": "news_chunk", "title": "Face Recognition Audit, Gamers Cheat with AI, Who Rules...", "subtitle": "Transformers: Smarter Than You Think", "content": "The transformer architecture has shown an uncanny ability to model not only language but also images and proteins. New research found that it can apply what it learns from the first domain to the others.What’s new: Kevin Lu and colleagues at UC Berkeley, Facebook, and Google devised Frozen Pretrained Transformer (FPT). After pretraining a transformer network on language data, they showed that it could perform vision, mathematical, and logical tasks without fine-tuning its core layers.Key insight: Transformers pick up on patterns in an input sequence, be it words in a novel, pixels in an image, or amino acids in a protein. If different types of data share similar patterns, a transformer trained on one type can operate on another.How it works: The researchers started with a 36-layer GPT-2 pretrained on WebText (posts on the website Reddit). They froze its self-attention and feed-forward layers and, in separate copies, fine-tuned peripheral layers on each on a wide range of tasks: Bit memory (memorizing strings of bits), Bit XOR (performing logical operations on pairs of strings of bits), ListOps (parsing and performing mathematical operations), MNIST, CIFAR-10 (classification of images), CFAR-10 LRA (classification of flattened, greyscale images), and remote homology detection (predicting what kind of protein structure an amino acid is part of). The authors fine-tuned only an input layer, an output layer, layer norm parameters (which fix the mean and variance of a layer’s input), and positional embeddings (vectors that represent where items appear in an input sequence) — less than 0.1 percent of the model’s parameters.To evaluate the impact of the language pretraining, the authors also built models whose core layers didn’t benefit from that training. They randomly initialized a GPT-2, froze its self-attention and feed-forward parameters, and then fine-tuned it in the same way as the others. Results: They compared GPT-2 models trained using their method to GPT-2s that had been fully fine-tuned for the same tasks. Their approach performed nearly as well, sometimes better. For instance, on CIFAR-10, their approach achieved 72.1 percent accuracy versus the fully fine-tuned model’s 70.3 percent. On remote homology detection, their approach achieved 12.7 percent versus 9 percent. Language pre-training contributed to the improvement: For instance, on CIFAR-10, their model achieved 68.2 percent versus the randomized model’s 61.7 percent.Why it matters: It appears that similar information structures — in the authors’ term, grammars — pervade the world. Applying representations learned in one domain to another domain may conserve training time and lead to better multimodal models.We’re thinking: It’s surprising that cross-modal pretraining works this well! Are there underlying statistics, common to many types of sequences, that we don’t yet appreciate?", "image_caption": "Frozen Pretrained Transformer (FPT) explained", "metadata": {"article_id": "issue_102", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/TRANSFORMER.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-102/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_102.html"}}
{"id": 56314281004, "type": "news_chunk", "title": "Face Recognition Audit, Gamers Cheat with AI, Who Rules...", "subtitle": "U.S. Lax on Face Recognition", "content": "A U.S. government watchdog agency called for stronger face recognition protocols for federal agencies.What’s new: An audit of federal agencies by the Government Accountability Office (GAO) found that, while many employ face recognition, they may not know where it came from, how it’s being used, or the hazards involved. The auditors recommended that agencies using commercial systems develop protocols for appropriate use.What they found: Twenty agencies that employ law-enforcement officers reported using face recognition. Of these, 11 used systems developed by private companies including Clearview AI and Vigilant. The others either developed their own or used systems developed by another agency. One of the most popular is the Department of Homeland Security’s Automated Biometric Identification System, which contains data on 835 million individuals.Several agencies did not seem to know who built some of the systems they use.Six agencies used the technology to investigate people involved in protests against police brutality. Three used it to look into perpetrators of the January 6 attack on the U.S. Capitol.Only one agency that reported using a privately developed face recognition system — Immigrations and Customs Enforcement — had implemented oversight protocols such as requiring that employees report each use. Behind the news: Face recognition is increasingly controversial in the U.S. Lawmakers recently introduced legislation that would freeze government use of the technology. At least 20 U.S. cities and several states have passed laws that restrict the technology.Why it matters: Face recognition has clear potential to infringe on privacy. Moreover, it has a spotty record of identifying minorities, which has led to false arrests. The finding that many federal agencies are taking a cavalier approach raises troubling questions about privacy and fairness.We’re thinking: The GAO audit of face recognition systems is a step forward. While regulators, ethicists, technologists, and businesses sort out appropriate standards, a moratorium on law enforcement use of face recognition would be sensible, so we can position the technology for socially beneficial uses while guarding against detrimental ones.", "image_caption": "Series of images related to face recognition protocols for federal agencies", "metadata": {"article_id": "issue_102", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/gao.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-102/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_102.html"}}
{"id": 12159841001, "type": "news_chunk", "title": "Gunshot Detection Under Fire, AI At The Olympics, AlphaFold...", "subtitle": "Shots in the Dark", "content": "Dear friends, Since the pandemic started, several friends and teammates have shared with me privately that they were not doing well emotionally. I’m grateful to each person who trusted me enough to tell me this. How about you — are you doing okay?Last week, the Olympic gymnastic champion Simone Biles temporarily withdrew from competition because she didn’t feel mentally healthy enough to do her best and perhaps avoid a career-ending injury. She’s not alone in struggling with mental health. About 4 in 10 adults in the U.S. reported symptoms of anxiety or depression during the pandemic, according to one survey.Once, after looking over a collaborator’s project, I said, “That’s really nice work” and got back a sad facial expression as though my collaborator was near tears. I asked if they were okay, wondering if I had said something wrong, but they paused and shook their head. After I probed gently a little more, they burst out crying and told me that my remark was the first appreciation they had received in longer than they could remember. Many people outwardly look like they’re doing well, but inside they’re lonely, anxious, or uncertain about the future. If you’re feeling fine, that’s great! But if you’re among the millions who feel that something is off-balance, I sympathize, and I want you to know that I care about you and appreciate you.As the pandemic wears on, many of us are hungry to connect with others more deeply. If this describes you, or if you want to help someone else who might feel this way, perhaps you can start by letting someone know you appreciate them or something they did. I think this will make them — and maybe you, too — feel better. Love,Andrew A crime-fighting AI company altered evidence to please police, a new investigation claims — the latest in a rising chorus of criticism.What’s new: ShotSpotter, which makes a widely used system of the same name that detects the sound of gunshots and triangulates their location, modified the system’s findings in some cases, Vice reported.Altered output: ShotSpotter’s output and its in-house analysts’ testimony have been used as evidence in 190 criminal cases. But recent court documents reveal that analysts reclassified as gunshots sounds the system had attributed to other causes and changed the location where the system determined that gunshots had occurred. Last year, ShotSpotter picked up a noise around one mile from a spot in Chicago where police believed someone was murdered at the same time. The system classified it as a firecracker. Analysts later reclassified it as a gunshot and modified its location, placing the sound closer to the scene of the alleged crime. Prosecutors withdrew the ShotSpotter evidence after the defense requested that the judge examine the system’s forensic value.When federal agents fired at a man in Chicago in 2018, ShotSpotter recorded only two shots — those fired by cops. The police asked the company to re-examine the data manually. An analyst found five additional shots, presumably those fired by the perpetrator.In New York in 2016, a company analyst reclassified as gunshots a sound that the algorithm had classified as helicopter noise after being contacted by police. A judge later threw out the conviction of a man charged with shooting at police in that incident, saying ShotSpotter’s evidence was unreliable. The response: In a statement, ShotSpotter called the Vice report “false and misleading.” The company didn’t deny that the system’s output had been altered manually but said the reporter had confused two different services: automated, real-time gunshot detection and analysis after the fact by company personnel. “Forensic analysis may uncover additional information relative to a real-time alert such as more rounds fired or an updated timing or location upon more thorough investigation,” the company said, adding that It didn’t change its system’s findings to help police.Behind the news: Beyond allegations that ShotSpotter has manually altered automated output, researchers, judges, and police departments have challenged the technology itself. A May report by the MacArthur Justice Center, a nonprofit public-interest legal group, found that the vast majority of police actions sparked by ShotSpotter alerts did not result in evidence of gunfire or gun crime.Several cities have terminated contracts with ShotSpotter after determining that the technology missed around 50 percent of gunshots or was too expensive.Activists are calling on Chicago to cancel its $33 million contract with the company after its system falsely alerted police to gunfire, leading to the shooting of a 13-year-old suspect. Why it matters: ShotSpotter’ technology is deployed in over 100 U.S. cities and counties. The people who live in those places need to be able to trust criminal justice authorities, which means they must be able to trust the AI systems those authorities rely on. The incidents described in legal documents could undermine that trust — and potentially trust in other automated systems.We’re thinking: There are good reasons for humans to analyze the output of AI systems and occasionally modify or override their conclusions. Many systems keep humans in the loop for this very reason. It’s crucial, though, that such systems be transparent and subject to ongoing, independent audits to ensure that any modifications have a sound technical basis.", "image_caption": "Video showing ShotSpotter in action", "metadata": {"article_id": "issue_103", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/SHOTSPOTTER2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-103/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_103.html"}}
{"id": 12159841002, "type": "news_chunk", "title": "Gunshot Detection Under Fire, AI At The Olympics, AlphaFold...", "subtitle": "Biomedical Treasure Chest", "content": "DeepMind opened access to AlphaFold, a model that finds the shapes of proteins, and to its output so far — a potential cornucopia for biomedical research.What’s new: The research lab, a division of Google’s parent company Alphabet, made AlphaFold freely available. It also opened databases that contain hundreds of thousands of three-dimensional protein shapes.Shapes of things to come: Proteins are molecules made up of chains of amino acids. They perform myriad biological functions depending on the way the chain folds, and understanding their shapes can shed light on what they do and how they do it. Protein shapes are determined by the proximity of essential portions, or residues, of amino acids. AlphaFold finds likely shapes by optimizing possible structures that keep residues close to one another based on their positions and angles. For a description of how it works, see “Protein Shapes Revealed” here. The company published research that describes how to use AlphaFold to find the shapes of both general and human-specific proteins.The model has analyzed the structure of roughly 98 percent of proteins found in the human body. It has analyzed hundreds of thousands more in 20 other organisms commonly used by researchers such as e.coli, fruit flies, and soybeans.The company plans to release an additional 100 million protein structures by the end of 2021. Such data is published and maintained by the European Molecular Biology Laboratory. Behind the news: Until recently, scientists had to rely on time-consuming and expensive experiments to figure out protein shapes. Those methods have yielded about 180,000 protein structures. AlphaFold debuted in 2018, when it won an annual contest for predicting protein structures. A revised version of the model won again in 2020 with an average error comparable to the width of an atom.Why it matters: Biologists could use these tools to better understand the function of proteins within the human body and develop new treatments for some of medicine’s most vexing maladies. Researchers already are using AlphaFold data to devise treatments for maladies including Covid-19 and several common, deadly tropical diseases.We’re thinking: We applaud DeepMind’s decision to make both its landmark model and the model’s output available for further research. We urge other companies to do the same.", "image_caption": "Animation showing AlphaFold working", "metadata": {"article_id": "issue_103", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/PROTEIN-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-103/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_103.html"}}
{"id": 12159841003, "type": "news_chunk", "title": "Gunshot Detection Under Fire, AI At The Olympics, AlphaFold...", "subtitle": "Olympic AI", "content": "Computer vision is keeping a close eye on athletes at the Summer Olympic Games in Tokyo.What’s new: Omega Timing, a Swiss watchmaker and the Olympic Games’ official timekeeper, is providing systems that go far beyond measuring milliseconds. The company’s technology is tracking gameplay, analyzing players’ motions, and pinpointing key moments, Wired reported.How it works: Omega Timing’s systems track a variety of Olympic sports including volleyball, swimming, and trampoline. Their output is intended primarily for coaches and athletes to review and improve performance, but it’s also available to officials and broadcasters. The volleyball system classifies shots such as smashes, blocks, and spikes with 99 percent accuracy by tracking changes in the ball’s direction and velocity. It integrates gyroscopic sensors embedded in players’ clothing that monitor players’ movements. If the ball flies momentarily out of the camera’s sight, it computes the likely path. The company says the system is 99 percent accurate at determining different moves.A pose estimator tracks gymnasts’ motions as they twist and flip on the trampoline. It also detects how precisely they land at the end of their routines.An image recognition system watches water events, measuring the distance between swimmers, their speed, and the number of strokes each one takes. Behind the news: Omega Timing has measured Olympic performance since 1932. It introduced photo-finish cameras at the 1948 Olympiad in London. Its systems are certified by the Swiss Federal Institute of Metrology.Why it matters: Technology that helps athletes examine their performance in minute detail could give them a major edge in competition. It offers the rest of us a finer appreciation of their accomplishments.We’re thinking: For this year’s games, the International Olympic Committee added to the schedule competitive skateboarding, surfing, and climbing. Next time, how about a data-centric AI competition?", "image_caption": "Omega Timing screenshots showing different sports data", "metadata": {"article_id": "issue_103", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/VOLLEYBALL.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-103/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_103.html"}}
{"id": 12159841004, "type": "news_chunk", "title": "Gunshot Detection Under Fire, AI At The Olympics, AlphaFold...", "subtitle": "Revenge of the Perceptrons", "content": "Why use a complex model when a simple one will do? New work shows that the simplest multilayer neural network, with a small twist, can perform some tasks as well as today’s most sophisticated architectures.What’s new: Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, and a team at Google Brain revisited multilayer perceptrons (MLPs, also known as vanilla neural networks). They built MLP-Mixer, a no-frills model that approaches state-of-the-art performance in ImageNet classification.Key insight: Convolutional neural networks excel at processing images because they’re designed to discern spatial relationships, and pixels that are nearby one another in an image tend to be more related than pixels that are far apart. MLPs have no such bias, so they tend to learn interpixel relationships that exist in the training set and don’t hold in real life. By modifying MLPs to process and compare images across patches rather than individual pixels, MLP-Mixer enables this basic architecture to learn useful image features.How it works: The authors pretrained MLP-Mixer for image classification using ImageNet-21k, which contains 21,000 classes, and fine-tuned it on the 1,000-class ImageNet. Given an image divided into patches, MLP-Mixer uses an initial linear layer to generate 1,024 representations of each patch. MLP-Mixer stacks the representations in a matrix, so each row contains all representations of one patch, and each column contains one representation of every patch.MLP-Mixer is made of a series of mixer layers, each of which contains two MLPs, each made up of two fully connected layers. Given a matrix, a mixer layer uses one MLP to mix representations within columns (which the authors call token mixing) and another to mix representations within rows (which the authors call channel mixing). This process renders a new matrix to be passed along to the next mixer layer.A softmax layer renders a classification. Results: An MLP-Mixer with 16 mixer layers classified ImageNet with 84.15 percent accuracy. That’s comparable to the state-of-the-art 85.8 percent accuracy achieved by a 50-layer HaloNet, a ResNet-like architecture with self-attention.Yes, but: MLP-Mixer matched state-of-the-art performance only when pretrained on a sufficiently large dataset. Pretrained on 10 percent of JFT300M and fine-tuned on ImageNet, it achieved 54 percent accuracy on ImageNet, while a ResNet-based BiT trained the same way achieved 67 percent accuracy.Why it matters: MLPs are the simplest building blocks of deep learning, yet this work shows they can match the best-performing architectures for image classification.We’re thinking: If simple neural nets work as well as more complex ones for computer vision, maybe it’s time to rethink architectural approaches in other areas, too.", "image_caption": "Simpler multilayer neural network", "metadata": {"article_id": "issue_103", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/MIXER.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-103/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_103.html"}}
{"id": 17161027001, "type": "news_chunk", "title": "AI Recognizes Race in X-Rays, Robots Do Bees' Work...", "subtitle": "AI Sees Race in X-Rays", "content": "Dear friends, How much math do you need to know to be a machine learning engineer? It’s always nice to know more math! But there’s so much to learn that, realistically, it’s necessary to prioritize. Here are some thoughts about how you might go about strengthening your math background.To figure out what’s important to know, I find it useful to ask what you need to know to make the decisions required for the work you want to do. At DeepLearning.AI, we frequently ask, “What does someone need to know to accomplish their goals?” The goal might be building a machine learning model, architecting a system, or passing a job interview.Understanding the math behind algorithms you use is often helpful, since it enables you to debug them. But the depth of knowledge that’s useful changes over time. As machine learning techniques mature and become more reliable and turnkey, they require less debugging, and a shallower understanding of the math involved may be sufficient to make them work. For instance, in an earlier era of machine learning, linear algebra libraries for solving linear systems of equations (for linear regression) were immature. I had to understand how these libraries worked so I could choose among different libraries and avoid numerical roundoff pitfalls. But this became less important as numerical linear algebra libraries matured. Deep learning is still an emerging technology, so when you train a neural network and the optimization algorithm struggles to converge, understanding the math behind gradient descent, momentum, and the Adam optimization algorithm will help you make better decisions. Similarly, if your neural network does something funny — say, it makes bad predictions on images of a certain resolution, but not others — understanding the math behind neural network architectures puts you in a better position to figure out what to do.Sometimes, we’re told that an idea is “foundational.” While there’s a lot to be said for understanding foundations, often this designation is arbitrary and thus not very useful for prioritizing what to study next. For example, computing happens on processors that are packed with transistors. Do you need a deep understanding of how transistors work to write software? It's hard to imagine an AI application where a detailed knowledge of the physics of transistors would affect your decisions.Rather than accepting an authority’s decree that a topic is foundational, it’s worth asking what circumstances would require specific knowledge to help you make better decisions.Of course, I also encourage learning driven by curiosity. If something interests you, go ahead and learn it regardless of how useful it will be in the foreseeable future. Maybe this will lead to a creative spark or technical breakthrough. Keep learning!Andrew Algorithms trained to diagnose medical images can recognize the patient’s race — but how?What’s new: Researchers from Emory University, MIT, Purdue University, and other institutions found that deep learning systems trained to interpret x-rays and CT scans also were able to identify their subjects as Asian, Black, or White.What they found: Researchers trained various implementations of ResNet, DenseNet, and EfficientNet on nine medical imaging datasets in which examples were labeled Asian, Black, or White as reported by the patient. In tests, the models reliably recognized the race, although their performance varied somewhat depending on the type of scan, training dataset, and other variables. The models were pretrained on ImageNet and fine-tuned on commonly used datasets of chest, limb, breast, and spinal scans.The ResNet identified the patient’s race most accurately: 80 to 97 percent of the time.The authors tried to determine how the models learned to differentiate races. Factors like body mass, tissue density, age, and sex had little bearing, they found. The models were able to guess the patient’s race even when the images had been blurred. Behind the news: Racial bias has been documented in some medical AI systems. In 2019, researchers found that an algorithm widely used by health care providers to guide treatment recommended extra care for Black patients half as often as it did White patients.Several studies have found that convolutional neural networks trained to detect skin cancer are less accurate on people with darker complexions.Most ophthalmology datasets are made up of data from Chinese, European, and North American patients, which could make models trained on them to recognize eye diseases less reliable with groups that aren’t well represented in those regions. Why it matters: The fact that diagnostic models recognize race in medical scans is startling. The mystery of how they do it only adds fuel to worries that AI could magnify existing racial disparities in health care.We’re thinking: Neural networks can learn in ways that aren’t intuitive to humans. Finding out how medical imaging algorithms learn to identify race could help develop less biased systems — and unlock other mysteries of machine learning.", "image_caption": "Different x-rays and CT scans displayed", "metadata": {"article_id": "issue_104", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/race.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-104/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_104.html"}}
{"id": 17161027002, "type": "news_chunk", "title": "AI Recognizes Race in X-Rays, Robots Do Bees' Work...", "subtitle": "Sharper Attention", "content": "Self-attention enables transformer networks to track relationships between distant tokens — such as text characters — in long sequences, but the computational resources required grow quadratically with input size. New work aims to streamline the process by rating each token’s relevance to the task at hand.What’s new: Sainbayar Sukhbaatar and colleagues at Facebook proposed Expire-Span, which enables attention to ignore tokens that aren’t useful to the task at hand.Key insight: Depending on the task, some tokens affect a model’s performance more than others. For instance, in predicting the sentiment of the sentence, “Then she cried,” “cried” is more important than “then.” By forgetting less relevant tokens, attention can process longer sequences with less computation.How it works: The authors modified a transformer’s attention layers. They trained the model in typical fashion to predict the next character in a sequence using the enwik8 dataset of text from English Wikipedia. Given the first token, it predicted the next. Then, using the first two tokens, it predicted the next, and so on. To each attention layer, the authors added a vanilla neural network that predicted the number of times that attention should use each token. It assigned a value to each new token, subtracted 1 after each prediction, and deleted the token when the value reached 0.The loss function minimized the number of times the model used each token to keep it from assigning arbitrarily high values (otherwise, it could predict that every token should be used until the whole sequence had been processed). In this way, the model learned to retain only the tokens most useful to an accurate prediction. Results: The authors evaluated Expire-Span based on total memory usage, training time per batch, and bits per byte (a measure of how well the model predicted the next token; lower is better). On enwik8, it achieved 1.03 bits per byte, while Adaptive-Span achieved 1.04 bits per byte and compressive transformer achieved 1.05 bits per byte. The authors’ model used 25 percent less GPU memory than the other two approaches (15GB versus 20GB and 21GB respectively). It also took less time to train (408ms per batch of 512 tokens compared to 483ms and 838ms).Why it matters: Forgetting the least relevant information enables transformers to process longer sequences in less time and memory.We’re thinking: Q: What do you do if a transformer forgets too much? A: Give it an Optimus Primer.", "image_caption": "Graph showing Expire-span which enables attention to ignore tokens that aren’t useful to the task at hand", "metadata": {"article_id": "issue_104", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/FORGET.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-104/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_104.html"}}
{"id": 17161027003, "type": "news_chunk", "title": "AI Recognizes Race in X-Rays, Robots Do Bees' Work...", "subtitle": "To Bee or Not to Bee", "content": "Insects that spread pollen to fruiting plants are in trouble. A possible alternative: Robots.What’s new: Farmers in Australia and the U.S. are using robots from Israeli startup Arugga Farming to pollinate greenhouse tomatoes, The Wall Street Journal reported.How it works: The system is designed for growing tomatoes, which self-pollinate when their pollen is stirred up by the beating of insect wings. Robots equipped with cameras, vision algorithms, and air compressors wheel themselves between rows of plants. When they recognize a flower that’s ready to produce fruit, they blast it with air to release its pollen. The company trained the computer vision system using tens of thousands of photos of tomato flowers shot in multiple greenhouses under a variety of lighting conditions.U.S. greenhouse grower AppHarvest tested the system. It found that the plants pollinated by robots produced a harvest comparable to those pollinated by bumblebees and much larger than those pollinated by hand.Costa Group Holdings, an Australian farming company that grows crops in vertical greenhouse arrays, recently tested two of the robots in a 25-acre facility. It plans to add more, aiming for a total of around 30. Behind the news: A number of other companies are using AI-enabled robots to pollinate plants. Edete Precision Technologies has had success with almonds, and Bumblebee AI hopes to pollinate avocados, kiwis, and cocoa. Developed at West Virginia University, a robot called BrambleBee aims to pollinate blackberries, raspberries, and brambleberries.Why it matters: Robotic pollinators may prove to be an important technology outside of greenhouses. Climate change and habitat loss are ravaging Earth’s insect populations including bees. Meanwhile, such machines could be helpful to farmers: Bees are expensive to rent, they can spread plant diseases, and importing them is restricted in places such as Australia.We’re thinking: These robots are sure to generate a buzz.", "image_caption": "Video showing how robots from Israeli startup Arugga Farming work", "metadata": {"article_id": "issue_104", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/ARUGGA.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-104/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_104.html"}}
{"id": 17161027004, "type": "news_chunk", "title": "AI Recognizes Race in X-Rays, Robots Do Bees' Work...", "subtitle": "Fresh Funds for U.S. Research", "content": "The U.S. plans to build nearly a dozen new civilian AI research labs.What’s new: The U.S. National Science Foundation (NSF) committed $220 million to fund 11 National Artificial Intelligence Research Institutes, complementing seven other AI research institutes that were established last year.What’s happening: The NSF grants provide each institute about $20 million annually over five years. Some will receive additional funding from public and private partners such as the U.S. Department of Homeland Security, Amazon, and Intel. Their missions include: Agriculture: Several institutes will focus on improving aspects of farming, including adapting to climate change, modeling plants, and developing security infrastructure for initiatives such as precision agriculture.Industry: Others will look at challenges like improving semiconductor design and building better robots. Two institutes will specialize in edge devices and wireless AI systems. Another is devoted to using AI models to optimize systems and automated decision-making.Scientific research: One institute will focus on algorithms and applications to control complex dynamic systems.Social good: A trio will seek to improve human life at various stages. One will focus on education for children, another on training for adults. A third institute will develop systems to care for elderly people. Behind the news: The NSF funded an initial seven national AI institutes in September. Earlier, the U.S. had said it would spend $2 billion annually on AI over the next two years.Why it matters: Other governments spend much more on AI than the U.S., and this outlay is small in the scheme of national AI funding. However, the allocation and the goals to which it is being put suggest that the federal government recognizes AI’s importance to the U.S. economy and its potential to benefit the world at large.We’re thinking: U.S. government funding was critical to AI's rise. For example, the Defense Advanced Research Products Agency (DARPA) provided funds to both Andrew and Yann LeCun for deep learning research. We’re hopeful that these new programs will fund similarly valuable innovations.", "image_caption": "Series of images showing AI research labs' campuses", "metadata": {"article_id": "issue_104", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/nsf-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-104/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_104.html"}}
{"id": 87853703001, "type": "news_chunk", "title": "Apple Weakens Privacy, AI's Invention Wins A Patent, Deere...", "subtitle": "User Privacy Versus Child Safety", "content": "Dear friends, Say you’ve trained a learning algorithm and found that it works well on many examples but performs poorly on a particular subset, or slice, of the data. What can you do?It is hard to tweak a learning algorithm’s code to improve its performance specifically on one slice of the data. Often, tuning an algorithm changes its performance on everything.But you can engineer the training and test data for that subset. A data-centric approach to AI development is a powerful tool to improve model performance on one slice, hopefully without degrading its performance on other portions of the data.The need to improve performance on one slice is a common one. For example: A loan-making algorithm has high average accuracy but makes biased decisions on applications from one minority group. How can you fix the performance to provide loans more fairly — especially if membership in that group is not an explicit feature?A speech recognition algorithm is accurate for many users but inaccurate when car noise is in the background. How can you improve its performance to recognize words spoken in a moving vehicle?A robot is good at grasping many types of household objects, except for monochromatic ones that are uniform in color and texture. How can you enable the robot to fetch that red rubber ball? Improving the data is sometimes misunderstood as a pre-processing step performed prior to engineering a machine learning algorithm. Instead, it should be a key step in the iterative loop of model development, in which data is engineered systematically to address problems identified through error analysis. Specifically, if error analysis identifies a slice of data that yields subpar performance, you might improve the data by: Improving the label quality for that slice. For example, you can check if labelers consistently assign the same label y to the same input x and, if not, provide clearer labeling instructions to improve consistency.Using data collection, augmentation, or synthesis to add data to the problematic slice. For example, to improve performance on speech with car noise, you might use data augmentation to generate more data with car noise for the algorithm to learn from. Rather than applying these techniques to all the data — which would be costly and inefficient — you can focus on improving the label quality (y) and/or getting new training examples (x) in the slice you want to improve. This is a much less costly exercise. Data-centric AI development is especially powerful in the current era of large neural networks. A decade ago, when models were much smaller, adding data in one place would often hurt performance elsewhere. For example, adding data on monochromatic objects might make it hard for an algorithm to recognize other objects if it doesn’t have enough capacity to recognize both types equally well. There are situations in which adding data can hurt, but for many unstructured data problems (vision, speech, language), as long as the added data is clean and the learning algorithm is large enough, it's possible to add data in a way that improves performance on one slice without hurting performance on others. You’ll find a more nuanced discussion of this topic here.I also spoke about using data-centric AI development techniques to reduce bias in learning algorithms during DeepLearning.AI’s panel discussion last week. You can watch a recording here. Keep learning!Andrew Apple, which has made a point of its commitment to user privacy, announced that it will scan iPhones for evidence of child abuse.What’s new: The tech giant will include a machine learning model on the device to recognize pornographic images of children stored in the photo library. Privacy advocates said the feature could be used to spy on innocent people.How it works: When a user uploads a photo from their phone to iCloud, a tool called neural match will scan it for known examples of child pornography. Neural match compares an image’s digital signature, called a hash, to those of abusive images previously identified and validated by at least two child-welfare groups. Upon detecting an offending image, the system alerts a human reviewer who may notify law enforcement.Security experts worry that Apple could expand neural match to process images shared via its messaging app, providing a backdoor into the chat system’s end-to-end encryption.On its website, Apple emphasizes that its technology is designed only to search for images of child sexual abuse. Further, it said it would deny government requests for targeted searches of individual users and for data that doesn’t match the system’s original parameters. Behind the news: Apple’s CEO Tim Cook has called privacy a “fundamental human right,” and the company boasts that its users have the final say over uses of their data. Privacy was a theme of the most recent Apple Worldwide Developers Conference, where the company showcased features that stymie email trackers, hide IP addresses, and identify third-party apps that collect data.In 2016, Apple resisted U.S. government requests to unlock an iPhone belonging to a suspected terrorist. A commercial cybersecurity firm ultimately unlocked it.Nonetheless, Apple can hand over some user data, particularly information stored in iCloud, in response to a legal warrant. Why it matters: Apple has been a holdout for privacy amid a tech-industry gold rush for user data. Its decision to budge on this issue suggests an inevitable, broader shift away from protecting individuals and toward making society more safe.We’re thinking: Child abuse is a global problem, and tech companies including Facebook, Google, Microsoft, and others have banded together to fight it. While we support this effort, we worry about the possibility — perhaps driven by government pressure — that scanning photo libraries could turn into scanning other types of content, and that aim of keeping children safe could veer toward less laudable goals.", "image_caption": "Apple's CEO Tim Cook discussing privacy with a Privacy sign above him", "metadata": {"article_id": "issue_105", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/Apple.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-105/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_105.html"}}
{"id": 87853703002, "type": "news_chunk", "title": "Apple Weakens Privacy, AI's Invention Wins A Patent, Deere...", "subtitle": "Outstanding in the Field", "content": "One of the world’s largest makers of farm equipment is doubling down on self-driving tractors.What’s new: John Deere agreed to pay $250 million for Bear Flag Robotics, a California startup that upgrades conventional tractors for full autonomy.How it works: Deere has offered GPS-enabled tractor guidance systems that aid a human driver for nearly two decades. Bear Flag has adapted self-driving technology developed by the automotive industry to help tractors roam agricultural fields safely without a driver. Tractors equipped with Bear Flag tech navigate using a combination of GPS tracking and sensor data. Lidar, radar, and cameras enable the vehicles to see their surroundings. Actuator systems control steering, braking, and a variety of towed implements.The system is adapted for farm driving. For instance, the vision algorithm distinguishes between fallen branches that can be driven over and trees that should be avoided.The sensors also gather data on the quality of the soil tilled in the tractor’s wake. The information can help growers fine-tune their use of pesticides, herbicides, and fungicides, resulting in reductions of up to 20 percent, the company said. The system learns the boundaries of a farmer’s property during an initial drive-through. It also identifies roads, waterways, and other obstacles. It can upload the resulting map to a fleet of tractors for remote control and monitoring.Behind the news: Deere has been pursuing AI capabilities for several years. In 2017, it acquired Blue River Technology, a California-based startup that makes weed-killing robots. The following year, it launched a program to partner with promising startups including some that use deep learning.Why it matters: In addition to helping the farmers deal with a long-running labor shortage, AI-driven equipment could help increase their productivity and limit environmental impacts such as pesticide runoff.We’re thinking: Self-driving cars aren’t yet commonly used on public roads, but the technology appears to be good enough for commercial use in constrained environments like farms.", "image_caption": "Sequence of images showing tractors working on fields", "metadata": {"article_id": "issue_105", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/ezgif.com-gif-maker--4--2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-105/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_105.html"}}
{"id": 87853703003, "type": "news_chunk", "title": "Apple Weakens Privacy, AI's Invention Wins A Patent, Deere...", "subtitle": "Invented By AI", "content": "An algorithm received a patent for its invention.What’s new: South Africa’s intellectual property office issued a patent that names an AI system as the inventor of a food container with unique properties, IP Watchdog reported.How it works: South Africa’s Companies and Intellectual Property Commission named Stephen Thaler, who developed the AI system, called Dabus, as the patent owner. Thaler submitted a number of applications to authorities in several countries with help from the Artificial Inventor Project (AIP), an organization of patent attorneys that aims to promote development of algorithms that generate valuable innovations. Dabus makes random associations between images, text, and other data. It ranks their novelty by comparing them to databases of existing concepts. When one of these “inventions” exceeds a threshold of novelty, Thaler reviews it, interprets its function, and writes a patent application.South Africa’s patent authority did not comment on the award, leading some experts to speculate that it was granted due to an oversight.Intellectual property authorities in the U.S., UK, Europe, and Australia rejected AIP’s applications, though an Australian judge recently overturned that country’s initial rejection. The AIP has applications pending in 12 other countries. Behind the news: South Africa has issued numerous updates to its patent policy in recent years to encourage technological innovation.Why it matters: This patent could set a significant precedent in the ongoing debate about whether and to what extent an algorithm can be considered the creator of new music, images, and other intellectual properties — a debate with potentially significant financial and legal implications.We’re thinking: The patent system has been criticized for enabling patent trolls who file or acquire patents for the purpose of litigating rather than advancing a technology or putting a new invention to work. If AI systems can file patents at scale, the whole system might need rethinking to incentivize useful innovation.", "image_caption": "Graphs showing information about AI system as the inventor of a food container with unique properties", "metadata": {"article_id": "issue_105", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/patent-with-highlights-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-105/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_105.html"}}
{"id": 87853703004, "type": "news_chunk", "title": "Apple Weakens Privacy, AI's Invention Wins A Patent, Deere...", "subtitle": "Solve RL With This One Weird Trick", "content": "The previous state-of-the-art model for playing vintage Atari games took advantage of a number of advances in reinforcement learning (RL). The new champion is a basic RL architecture plus a trick borrowed from image generation. What’s new: A team led by Florin Gogianu, Tudor Berariu, and colleagues found that spectral normalization, a technique that limits the degree of variation between representations of similar inputs, improved an RL model’s performance more than several recent innovations combined. The team included researchers at Bitdefender, Deepmind, Imperial College London, Technical University of Cluj-Napoca, and University College London. Key insight: In reinforcement learning, a model observes its environment (say, the Atari game Pong), chooses an action based on its observation (such as moving the paddle), and receives a reward for a desirable outcome (like scoring a point). Learning in this way can be difficult because, as a model selects different actions, its training data (observations and rewards) change. Mutable training data poses a similar problem for generative adversarial networks (GANs), where generator and discriminator networks influence each other even as they themselves change. Spectral normalization has been shown to help GANs learn by moderating these changes. It could also be beneficial in reinforcement learning. How it works: The authors added spectral normalization to a C51, a convolutional neural network designed for reinforcement learning. The authors trained their model on tasks in the Arcade Learning Environment, a selection of games in which the actions are valid Atari controller movements. Given an observation, a C51 predicts a set of distributions of the likely reward for taking each possible action. Then it selects the action that would bring the highest expected reward. During training, it refines its prediction by sampling and comparing predicted rewards to actual rewards.Spectral normalization constrains parameter values in network layers, such that the distance between any two predictions is, at most, the distance between the inputs times a constant factor (chosen by the user). The smaller the factor, the more similar a network’s predictions must be. During training, spectral normalization limits the magnitude of a layer’s weights. If an update exceeds that limit, it divides the weights evenly so their magnitude is equal to the limit.The authors argue that limiting weight changes is akin to dampening learning rates. They devised an optimization method that lowered the model’s learning rate proportionately to spectral normalization’s limit on the weights. Models trained either way performed nearly equally. Results: Using spectral normalization on every layer impeded performance, but using it on only the second-to-last layer led the model to achieve a higher median reward. The authors compared their C51 with spectral normalization on the second-to-last layer against Rainbow, the previous state of the art, which outfits a C51 with a variety of RL techniques. In 54 Atari games, the authors’ approach achieved a 248.45 median reward, outperforming Rainbow’s 227.05 median reward. Why it matters: Applying techniques from one area of machine learning, such as GANs, to a superficially different area, such as RL, can be surprisingly fruitful! In this case, it opens the door to much simpler RL models and perhaps opportunities to improve existing techniques. We’re thinking: People who have expertise in multiple disciplines can be exceptionally creative, spotting opportunities for cross-fertilization among disparate fields. AI is now big enough to offer a cornucopia of opportunities for such interdisciplinary insight.", "image_caption": "Sequence of famous arcade games' scenes", "metadata": {"article_id": "issue_105", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/SPECTRAL-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-105/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_105.html"}}
{"id": 34283606001, "type": "news_chunk", "title": "Invasion of the Large Language Models, Can AI Recognize...", "subtitle": "Fighting Addiction or Denying Care?", "content": "Dear friends, Recently I attended an online celebration of my late grandfather’s life. He had passed away quietly in his sleep in March. Two days later, Coursera was publicly listed on the New York Stock Exchange. And two days after that, my son Neo Atlas Ng was born. The sequence of events reminds me that every day is precious. Had my grandfather lived just a few more days, he would have shared in the joy of his first great grandson’s birth and the celebration of Coursera’s listing. My grandfather lived a remarkable life. He was born in Jamaica in 1918 during one pandemic, and he passed away 102 years later during another. His father was an indentured laborer who had moved from China to Jamaica, and his mother was half Jamaican. (Thus I’m 1/16 Jamaican.) As a young man, he sailed from the Caribbean through the Panama canal to settle in Hong Kong, where he had a fruitful career as an accountant and spent his last few years holding court at his beloved Kowloon Cricket Club. If you’ve lost a loved one, you probably miss them as much as I do my grandfather. It goes to show that even if someone close to you lives to 102, likely it will feel like it’s not enough. If only he had lived four more days — or four more years — he could have shared in even more joy. I’m grateful for the time I had with my grandfather. I hope you’ll take care of yourself so that you, too, can live a long life. Let’s squeeze every drop of joy out of life in the time we have. Love,Andrew An epidemic of opioid abuse in the U.S. killed 93,000 people in 2020 alone. An algorithm intended to help doctors prescribe the drugs responsibly may be barring worthy patients from pain relief.What’s new: A widely used system assesses whether individual patients are at risk of abusing opioids. In some cases, it has recommended denying painkillers to people who suffer from severe pain and have no history of drug abuse, Wired reported.How it happened: Most U.S. states have developed databases that track drug prescriptions. NarxCare, a system developed by medical technology company Appriss Health, analyzes such data for at least eight states. Given a patient’s name, NarxCare considers drugs and doses prescribed, numbers of doctors and pharmacies involved, and whether any prescriptions overlap. It produces scores that evaluate the risk that the patient will abuse opioids and other drugs, and a score that evaluates the risk that they will overdose. Appriss says the scores are meant to assist, not to overrule or replace, a doctor’s decision.Several patients interviewed by Wired said they were denied care or were dropped by their doctors after receiving mistakenly elevated scores. In one case, veterinary drugs purchased for pets contributed to a high score.Some behaviors used by such algorithms to generate risk scores — such as visiting multiple doctors or traveling long distances for care — may artificially inflate scores for people who have multiple conditions or live in rural areas, according to a recent study. Behind the news: Flawed algorithms unexpectedly have cut healthcare benefits to many U.S. citizens, leaving them without care or a way to appeal the decision.Why it matters: Most people who have opioid prescriptions are not addicts. Cutting them off from painkillers not only leaves them to suffer, it also could drive them to obtain the drugs illegally or harm themselves with illicit substitutes.We’re thinking: Efforts to limit prescriptions of opioids could save countless people from addiction, and AI can play an important role. Stories of patients who have been denied care highlight the pressing need to improve and audit AI systems, even as they help us avoid fueling the opioid epidemic.", "image_caption": "Animation showing NarxCare, a system that analyzes databases that track drug prescriptions in the US", "metadata": {"article_id": "issue_106", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/PERSCRIPTION.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-106/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_106.html"}}
{"id": 34283606002, "type": "news_chunk", "title": "Invasion of the Large Language Models, Can AI Recognize...", "subtitle": "Weak Foundations Make Weak Models", "content": "A new study examines a major strain of recent research: huge models pretrained on immense quantities of uncurated, unlabeled data and then fine-tuned on a smaller, curated corpus. The sprawling 200-page document evaluates the benefits and risks.What’s new: Researchers at Stanford’s Human AI Institute proposed ways to prevent large language models like BERT, CLIP, and GPT-3 — which they call foundation models for their ability to support a plethora of high-performance, fine-tuned variations — from manifesting hidden flaws after fine-tuning.Key insight: The very factors that make large language models so valuable — unsupervised training followed by adaptation to a wide variety of tasks (indeed, some outside the domain of natural language) — make them potential vectors for harm. Defects in the foundation, such as biases learned from uncurated training data, can emerge in fine-tuned versions as challenges to fairness, ethical use, and legal compliance. Moreover, this approach encourages a technological monoculture in which a limited number of architectures, despite their strengths, proliferate their weaknesses across various domains.Toward solid foundations: The authors recommend ways to minimize unwelcome surprises such as unwitting contributions to social or economic inequality, unemployment, or disinformation: Develop metrics that predict ways in which a model may instill harmful behavior in its fine-tuned offspring and standardized ways to document these metrics, for instance data sheets.Create incentives for companies that develop large-scale, unsupervised models to publicly test and audit their work. Warn developers of follow-on systems to vet them thoroughly for undesired behaviors prior to deployment.Counterbalance the power of deep-pocketed companies by making it easier for academic institutions and independent researchers to develop such models, for instance through a National Research Cloud and crowdsourced efforts to recreate GPT-style language models. Behind the news: The advent of BERT in 2018 accelerated adoption of unsupervised pretraining in natural language models and spawned ever-larger networks as researchers scaled up the concept and experimented with architectures. The approach has spun off fine-tuned models not only for language tasks like conversation, image captioning, and internet search but also far-flung applications including modeling proteins, testing mathematical theorems, generating computer code, image recognition, image generation, and reinforcement learning.Why it matters: Such models can cause harm due to intrinsic flaws by, say, propagating data-driven biases against members of particular religions or other groups) and extrinsic flaws, such as energy-intensive training that leaves a large carbon footprint and misuse such as propagating disinformation. Deep learning systems developed without foresight run the risk of becoming a burden rather than a boon.We’re thinking: The future of AI may well be built on a limited variety of foundation models. In any case, the painstaking work of checking models for flaws beats cleaning up messes caused by neglecting to do so.", "image_caption": "Series of images showing some of the findings of the new study by researchers at Stanford’s Human AI Institute", "metadata": {"article_id": "issue_106", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/ezgif.com-gif-maker--7--2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-106/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_106.html"}}
{"id": 34283606003, "type": "news_chunk", "title": "Invasion of the Large Language Models, Can AI Recognize...", "subtitle": "Wake Up and Smell the AI", "content": "Coffee producers are using machine learning to grow better beans.What’s new: Beverage giant Nespresso is rolling out a system to assess the quality of hybrid coffee seedlings using technology from Israeli-Colombian startup Demetria.How it works: Nespresso develops new coffee varieties by grafting plant seedlings. Previously it relied on human experts to assess whether these grafts were viable. Demetria’s algorithm uses readings from a handheld near-infrared optical scanner to automate the evaluation. The scanner measures light frequencies reflected by the plants, which the algorithm interprets as markers of plant health.In a three-month pilot program, Nespresso used the system to analyze over 240,000 plants. It sent the top-graded plants to farmers in Colombia.An earlier Demetria model lets farmers match the near-infrared signature of raw beans to established flavor categories. The company trained that model on taste and smell data recorded by human tasters.The company also offers a smartphone app for commercial coffee buyers that measures the size of individual coffee beans. Larger beans tend to produce better coffee. Behind the news: The food and beverage industry has a growing appetite for AI. Tuna Scope is a computer vision-powered smartphone app that scans slices of fish to determine whether they are suitable for sushi.Indian startup Intello Labs has developed computer vision tools that assess the quality of various types of fruits and vegetables.Frito-Lay patented a machine learning system that analyzes laser readings of individual chips to grade their texture. Why it matters: Nespresso believes that Demetria’s technology will save time and money. This may be bad for traditional plant assessors, whose skills may become obsolete. On the other hand, it may help struggling Colombian coffee farmers grow more profitable beans.We’re thinking: The thought of better coffee through AI perked us right up.", "image_caption": "Sequence of coffee producers using technology from Israeli-Colombian startup Demetria", "metadata": {"article_id": "issue_106", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/ezgif.com-gif-maker--3--2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-106/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_106.html"}}
{"id": 34283606004, "type": "news_chunk", "title": "Invasion of the Large Language Models, Can AI Recognize...", "subtitle": "More Reliable Pretraining", "content": "Pretraining methods generate basic representations for later fine-tuning, but they’re prone to certain issues that can throw them off-kilter. New work proposes a solution.What’s new: Researchers at Facebook, PSL Research University, and New York University led by Adrien Bardes devised an unsupervised pretraining method they call Variance-Invariance-Covariance Regularization (VICReg). VICReg helps a model learn useful representations based on well understood statistical principles.Key insight: Pretraining methods can suffer from three common failings: Generating an identical representation for different input examples (which leads to predicting the mean consistently in linear regression), generating dissimilar representations for examples that humans find similar (for instance, the same object viewed from two angles), and generating redundant parts of a representation (say, multiple vectors that represent two eyes in a photo of a face). Statistically speaking, these problems boil down to issues of variance, invariance, and covariance respectively.How it works: VICReg manages variance, invariance, and covariance via different terms in a loss function. The authors used it to pretrain a ResNet-50 on ImageNet without labels. To discourage similar representations of every example, the variance term of VICReg’s loss function computes the variance within an input batch’s representations; that is, the average amount by which each value differs from the mean. This term penalizes the model if this variance falls below a threshold.The covariance term computes correlations between elements of each representation. It sums the correlations and penalizes the model for extracting correlated features within a given representation.To prevent dissimilar representations of similar examples, VICReg borrows an idea from contrastive learning: It uses data augmentation. Two different, random augmentations are applied to each example, and the model processes them separately to generate two different, but related, representations. The invariance term computes the distance between them. The greater the distance, the greater the penalty. Results: The authors transferred the VICReg-trained ResNet-50’s representations to a linear classifier and trained it on ImageNet with labels. That model achieved a 73.2 percent accuracy, just shy of the 76.5 percent achieved by a supervised ResNet-50. A linear classifier using representations from a ResNet-50 pretrained using the contrastive learning method SimCLR achieved 69.3 percent accuracy.Why it matters: Contrastive learning, a successful pretraining technique, requires a large number of comparisons between dissimilar inputs to ensure that not all representations are identical. VICReg avoids that issue by computing the variance within a batch, a much less memory-intensive operation.We’re thinking: Comparing different augmentations of the same example has proven to be a powerful way to learn. This technique extends that approach, and we expect to see more.", "image_caption": "Information about a new unsupervised pretraining method called VICReg", "metadata": {"article_id": "issue_106", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/08/VICREGv2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-106/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_106.html"}}
{"id": 67804523001, "type": "news_chunk", "title": "Tesla's Dancing Robot, Adapting to Climate Change, Asking...", "subtitle": "Dances With Robots", "content": "Dear friends, Building AI products and businesses requires making tough choices about what to build and how to go about it. I’ve heard of two styles: Ready, Aim, Fire: Plan carefully and carry out due diligence. Commit and execute only when you have a high degree of confidence in a direction.Ready, Fire, Aim: Jump into development and start executing. This allows you to discover problems quickly and pivot along the way if necessary. Say you’ve built a customer-service chatbot for retailers, and you think it could help restaurants, too. Should you take time to study the restaurant market before starting development, moving slowly but cutting the risk of wasting time and resources? Or jump in right away, moving quickly and accepting a higher risk of pivoting or failing?Both approaches have their advocates, but I think the best choice depends on the situation. Ready, Aim, Fire tends to be superior when the cost of execution is high and a study can shed light on how useful or valuable a project could be. For example, if your team can brainstorm a few other use cases (restaurants, airlines, telcos, and so on) and evaluate these cases to identify the most promising one, it may be worth taking the extra time before committing to a direction. Ready, Fire, Aim tends to be better if you can execute at low cost and, in doing so, determine whether the direction is feasible and discover tweaks that will make it work. For example, if you can build a prototype quickly to figure out if users want the product, and if canceling or pivoting after a small amount of work is acceptable, then it makes sense to consider jumping in quickly. (When taking a shot is inexpensive, it also makes sense to take many shots. In this case, the process is actually Ready, Fire, Aim, Fire, Aim, Fire, Aim, Fire.) After agreeing upon a product direction, when it comes to building a machine learning model that’s part of the product, I have a bias toward Ready, Fire, Aim. Building models is an iterative process. For many applications, the cost of training and conducting error analysis is not prohibitive. Furthermore, it is very difficult to carry out a study that will shed light on the appropriate model, data, and hyperparameters. So it makes sense to build an end-to-end system quickly and revise it until it works well.But when committing to a direction means making a costly investment or entering a one-way door (meaning a decision that’s hard to reverse), it’s often worth spending more time in advance to make sure it really is a good idea. Keep learning!Andrew Tesla unveiled its own AI chip and — surprise! — plans for a humanoid robot.What’s new: At Tesla’s AI Day promotional event, the company offered a first look at an upcoming self-driving computer powered by custom AI chips. To make sure the event got headlines, CEO Elon Musk teased a forthcoming android.Chips and bots: Company executives explained how the company trains models, labels data, and meets various AI challenges. Then they dove into what’s ahead: Tesla claims that Dojo will process computer vision data four times faster than existing systems, enabling the company to bring its self-driving system to full autonomy. The first Dojo cluster will be running by next year.The computer is based on D1, an AI training chip designed in-house. Three thousand D1s can be ganged together to deliver more processing power and network bandwidth than typical training rigs. The same technology that undergirds Tesla’s cars will drive the forthcoming Tesla Bot, which is intended to perform mundane tasks like grocery shopping or assembly-line work. Its design spec calls for 45-pound carrying capacity, “human-level hands,” and a top speed of 5 miles per hour (so humans can outrun it).Rather than showing a working prototype, Musk presented a human dancing in a bodysuit. He said a prototype would be ready next year. (Musk frequently exaggerates Tesla’s capabilities.) Behind the news: Tesla’s Autopilot system has recently come under government scrutiny. Last week, the U.S. National Highway Traffic Safety Administration launched an investigation into 11 incidents in which Tesla vehicles using Autopilot collided with parked emergency vehicles. If the agency finds Autopilot at fault, it could require the company to change or recall its technology.Why it matters: Tesla’s promise of full self-driving capability was premature, but Dojo’s muscled-up computing power could bring it substantially closer. As for the Tesla Bot, we’re not holding our breath.We’re thinking: Tesla’s genuine achievements — the innovative electric car, charging infrastructure, driver-assistance capabilities — may be overshadowed by stunts like the dancer in the bodysuit. History will decide whether Elon Musk is remembered as a genius at engineering or marketing.", "image_caption": "Person dressed up as Tesla Bot showing some dance moves", "metadata": {"article_id": "issue_107", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/Tesla-Bot.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-107/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_107.html"}}
{"id": 67804523002, "type": "news_chunk", "title": "Tesla's Dancing Robot, Adapting to Climate Change, Asking...", "subtitle": "Deep Unlearning", "content": "Privacy advocates want deep learning systems to forget what they’ve learned.What’s new: Researchers are seeking ways to remove the influence of particular training examples, such as an individual’s personal information, from a trained model without affecting its performance, Wired reported.How it works: Some researchers have experimented with preparing data prior to training for potential removal later, while others have worked to remove the effect of selected examples retroactively. Researchers from the Universities of Toronto and Wisconsin-Madison developed a training method called SISA in which different versions of a model are trained on non-overlapping subsets of the same dataset. During inference, they combine the predictions from each model via majority vote. This makes it possible to remove selected training examples and retrain only the model associated with their subset.A team at Harvard, Stanford, and University of Pennsylvania later showed that SISA would fail to remove the influence of data if the requests to do so weren’t randomly distributed. The team mitigated this problem by introducing noise in the training algorithm based on ideas from differential privacy.Researchers from Google, Cornell, and University of Waterloo showed how to remove the impact of a training example on a model’s weights if its loss function meets certain mathematical conditions. Behind the news: Evolving data privacy laws could wreak havoc on machine learning models. The European Union’s General Data Privacy Regulation includes a “right to be forgotten” that could force companies retroactively to remove the influence of specific data from trained models, some observers argue.California’s Privacy Rights Act gives citizens the right to know how their data is being used and request that it be deleted, even if it has been sold to a third party. Why it matters: Enabling models to unlearn selectively and incrementally would be less costly than retraining repeatedly from scratch. It also could give users more control over how their data is used and who profits from it.We’re thinking: Wait ... what was this article about?", "image_caption": "A new framework that helps models “unlearn” information selectively and incrementally", "metadata": {"article_id": "issue_107", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/unlearn.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-107/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_107.html"}}
{"id": 67804523003, "type": "news_chunk", "title": "Tesla's Dancing Robot, Adapting to Climate Change, Asking...", "subtitle": "Full-Bodied With Hints of Forest Fire", "content": "Wineries in areas affected by wildfires are using machine learning to produce vintages that don’t taste like smoke.What’s new: Some California winemakers are using a service called Tastry to identify grapes tainted by smoke from the state’s surging blazes and recommend blends that will mask the flavor, The Wall Street Journal reported.How it works: Called CompuBlend, Tastry’s system analyzes grapes’ chemical makeup, including smoke compounds absorbed through their skins. A model recommends other varieties that can mask the taste. The system was trained on the chemical composition of various grape varieties and consumer preferences gathered by surveying reactions to various flavors and aromas, such as the taste of coffee or the smell of cut grass.The model finds blends that both mask off-flavors and appeal to consumers. Behind the news: The ancient art of winemaking is adopting AI. VineScout is an autonomous wheeled robot that uses lidar and ultrasonic cameras to navigate rows of grapes while analyzing soil conditions.Diam Bouchage, a cork manufacturer, assesses quality with a machine learning tool that analyzes x-ray images of individual corks.Ailytic, an Australian company, built a machine learning platform that helps winemakers monitor aspects of their manufacturing process such as temperature and bottle inventory. Why it matters: Wildfires are a growing threat to wine regions in Australia, California, and France. They cost the industry an estimated $3.7 billion in 2020. AI could help vintners recoup some of the losses.We’re thinking: While there's a clear need to adapt to human-induced climate change, it’s tragic that the planet has heated to the point that formerly temperate areas are burning. We applaud the work of Climate Change AI.", "image_caption": "Sequence showing how the service called Tastry works", "metadata": {"article_id": "issue_107", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/Wine.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-107/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_107.html"}}
{"id": 67804523004, "type": "news_chunk", "title": "Tesla's Dancing Robot, Adapting to Climate Change, Asking...", "subtitle": "Ask Me in a Different Way", "content": "Pretrained language models like GPT-3 have shown notable proficiency in few-shot learning. Given a prompt that includes a few example questions and answers (the shots) plus an unanswered question (the task), such models can generate an accurate answer. But there may be more to getting good results.What’s new: Ethan Perez, Douwe Kiela, and Kyunghyun Cho subjected GPT-style language models to a test they call true few-shot learning. They found that the heralded few-shot success may depend on a well engineered prompt. The authors are based at New York University, Facebook, and CIFAR, respectively.Key insight: Training a machine-learning model typically requires a validation set to tune hyperparameters such as the learning rate. For GPT-style models, those hyperparameters include the prompt format. In few-shot learning with a pretrained model, the prompt typically contains a handful of examples. However, researchers often experiment extensively to find a prompt format that yields accurate responses. This amounts to stacking the deck in the model’s favor, and without it, such models can’t perform so well.How it works: The authors evaluated four sizes of GPT-3, four sizes of GPT-2, and DistilGPT-2. They tested prompt formats from LAMA, a benchmark that comprises factual statements in a variety of formats, and LPAQA, which contains LAMA statements translated from English into a different language and back. LAMA provides statements in 41 categories, such as “X was born in Y,” where X is a personal name and Y is a place, and “X was created by Y,” where X is the name of a company and Y is the name of a product. It presents each statement in an average of 12 formats. For instance, “X was created by Y” is also formatted “X is developed by Y” and “X is being developed by Y.”The authors assembled prompts made of five such statements, all in the same category and format, in which the last word was missing, such as, “The iPhone is being developed by _.” The missing word is, of course, “Apple.” They provided versions of these prompts in all 120 possible orders of the five statements, always with the final word missing, prompting the model to fill in the blank.They used cross-validation to find the prompt format that, given four complete and one incomplete examples, prompted the best performance on average across all formats and categories.For each model, they compared performance prompted by the best format according to cross-validation, the format associated with the highest accuracy on the test set, and the mean accuracy on the test set across all formats and categories. Results: For all models tested, the accuracy prompted by the format selected according to cross-validation was only marginally above the mean and significantly below the accuracy of the best format. For instance, for the largest model (GPT-3 with 175 billion parameters), the format chosen by cross-validation scored about 55 percent, mean accuracy was about 54 percent, and the accuracy of the best format was about 60 percent.Why it matters: Previous claims of few-shot learning in GPT-style models left out an important variable: the size of the dataset used to pick a good format. Choosing among 12 prompt formats boosted accuracy by around 5 percent; choosing among a larger set of formats could make a bigger difference. If researchers don’t include all the information that went into the results they report, follow-up studies are unlikely to duplicate their work.We’re thinking: We like prompt engineering that gets things done on time. We’re less enamored with prompt engineering that muddies the water around few-shot learning.", "image_caption": "Animation showing example questions and answers obtained by a pretrained language model", "metadata": {"article_id": "issue_107", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/TRUEFEWv5--1-.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-107/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_107.html"}}
{"id": 4435975001, "type": "news_chunk", "title": "Predicting Climate Change, $500 Billion In AI Sales...", "subtitle": "Getting a Jump on Climate Change", "content": "Dear friends, I’m thrilled to announce the NeurIPS Data-Centric AI Workshop, which will be held on December 14, 2021. You may have heard me speak about data-centric AI, in which we systematically engineer the data that feeds learning algorithms. This workshop is a chance to delve more deeply into the subject.Why a workshop? I’ve seen many subfields of AI emerge first by having practitioners advocate for them privately, after which they mature to a point where workshops bring together researchers and practitioners to develop and share ideas with each other. Eventually they become mainstream, and more of their work becomes incorporated into major AI conferences. Indeed, even deep learning once was a niche topic at NeurIPS, and my friends and I organized workshops to share ideas and build momentum.While data-centric AI is gaining momentum in practice, there’s still much research to be done. One common misconception is that data-centric AI is simply a matter of paying closer attention to engineering the data that algorithms learn from. While this mindset is important, we also need to develop general principles, algorithms, and tools that enable us to apply this mindset in a way that’s repeatable and systematic. Tools like TensorFlow and PyTorch made engineering of neural network architectures more systematic and less error-prone; likewise we need new tools for engineering data. My team at Landing AI (which is hiring!) is inventing data-centric algorithms for image data as part of an MLOps platform for computer vision. I’d love to see hundreds or thousands more groups working on data-centric algorithms. Open questions include: What algorithms or tools can accelerate the sourcing of high-quality data?What algorithms or tools can identify inconsistently labeled data?What general design principles can make improving data quality more systematic?What tools can help practitioners carry out error analysis more efficiently?How can data engineering advance responsible AI, for example, to ensure fairness and minimize bias in trained models? The workshop is accepting research paper submissions that address such issues until September 30, 2021. Please check out the website for details.Special thanks to my co-organizers Lora Aroyo, Cody Coleman, Greg Diamos, Vijay Janapa Reddi, Joaquin Vanschoren, and Sharon Zhou. Keep learning!Andrew Startups are predicting how climate change will affect global commerce.What’s new: Companies that specialize in climate analytics are training neural networks to help businesses manage risks posed by a warming globe, The Wall Street Journal reported.Changes in the air: These young companies model interactions among environmental data and factors such as commodity prices, consumption patterns, and import/export data. They sell the resulting insights to corporate customers who are concerned about the impact of climate change on their ability to buy goods and raw materials. ClimateAI, founded in San Francisco in 2017, trained its model on the output of long-range climate simulations. The model generates short-term forecasts — useful for identifying risks in the coming year — and predicts how crops will fare in various regions well into the future. The company, which has raised $16 million, predicted that 2020 would bring higher-than-average rainfall in a part of Australia, helping a seed company increase its sales by 5 to 10 percent.Gro Intelligence, a New York company that has raised $115 million since 2014, analyzes over 40,000 data sources including satellite imagery and precipitation reports to forecast the severity of future droughts, floods, and other extreme weather events as well as their impacts on over 15,000 agricultural commodities. Its customers include consumer goods giant Unilever (Ben & Jerry’s, Lipton, Knorr), fast-food conglomerate Yum! Brands (KFC, Pizza Hut, Taco Bell), and European financial titan BNP Paribas.One Concern analyzes data sources including Google Street View and satellite imagery to help customers plan for and execute disaster response plans, including those caused by climate change, on buildings, roads, and other infrastructure. The Menlo Park, California, company has raised $119 million since its founding in 2015. Behind the news: Corporations are waking up to the hazards posed by climate change to their own well-being. A 2021 survey of 8,098 companies throughout the world estimates that climate change, deforestation, and water scarcity will cost corporations $120 billion over the next five years.The U.S. Securities and Exchange Commission, which regulates publicly traded companies, plans to require corporations to disclose known climate risks to investors.Earlier this year, Exxon Mobil shareholders elected new board members who promised to redirect the oil and gas giant toward clean sources of energy. Why it matters: This year’s run of record-breaking wildfires, floods, and freezes are a preview of what to expect in a warmer world, according to the latest International Panel on Climate Change report. AI-powered forecasts can help businesses protect assets and revenue — and the rest of us prepare for further impacts to come.We’re thinking: By calculating the costs of climate disaster, AI can make the very real danger posed by atmospheric carbon emissions feel as urgent as it is.", "image_caption": "Demonstration of Climate AI model working", "metadata": {"article_id": "issue_108", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/WEATHER.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-108/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_108.html"}}
{"id": 4435975002, "type": "news_chunk", "title": "Predicting Climate Change, $500 Billion In AI Sales...", "subtitle": "AI Sales Closing In on $500 Billion", "content": "A new report projects a rosy future for the AI industry.What’s new: A study from market research firm IDC estimates that global revenues for AI software, hardware, and services will reach $341.8 billion in 2021 — up from an estimated $156.5 billion last year — and will break $500 billion by 2024. The study reflects interviews, distribution statistics, financial reports, and other data from over 700 AI companies around the world.What they found: The AI industry’s annual growth rate is expected to exceed 18.8 percent next year. The analysis breaks up that growth into three broad categories. Some of the most important findings: Software: Software sales make up 88 percent of the overall AI market. AI platforms (the largest of six software subcategories) account for half of the total. However, AI applications are expected to grow most quickly, marking a five-year annual rate of 33.2 percent.Hardware: AI-focused hardware — mainly servers and storage — accounts for just 5 percent of the industry’s sales. However, it is projected to grow by 29.6 percent in 2021 and 2022, faster than software and services. Server sales account for 82 percent of hardware sales which are dominated by Dell, HPE, Huawei, IBM, Inspur, and Lenovo.Services: AI services generated 14 percent of total sales and are expected to grow at a 21 percent compound annual rate through 2025. IT services bring in 80 percent of sales in this area. Behind the news: IDC’s most recent predictions are in line with their previous report, published in February, and jibe with research from MIT Technology Review.Why it matters: In the AI world — as in other high-tech sectors — it’s often difficult to discern real growth potential from gossip-fueled hype. Research reports that provide granular insights are a crucial tool for business leaders and investors who aim to capitalize on this industry, not to mention machine learning engineers who are plotting a career.We’re thinking: We’ve seen market research reports that later proved right and many that later proved dead wrong. We hope this is one of the former!", "image_caption": "Chart showing data about worldwide AI services forecast", "metadata": {"article_id": "issue_108", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/IDC-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-108/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_108.html"}}
{"id": 4435975003, "type": "news_chunk", "title": "Predicting Climate Change, $500 Billion In AI Sales...", "subtitle": "Perceptrons Are All You Need", "content": "The paper that introduced the transformer famously declared, “Attention is all you need.” To the contrary, new work shows you may not need transformer-style attention at all.What’s new: Hanxiao Liu and colleagues at Google Brain developed the gated multi-layer perceptron (gMLP), a simple architecture that performed some language and vision tasks as well as transformers.Key insight: A transformer processes input sequences using both a vanilla neural network, often called a multi-layer perceptron, and a self-attention mechanism. The vanilla neural network works on relationships between each element within the vector representation of a given token — say, a word in text or pixel in an image — while self-attention learns the relationships between each token in a sequence. However, the vanilla neural network also can do this job if the sequence length is fixed. The authors reassigned attention’s role to the vanilla neural network by fixing the sequence length and adding a gating unit to filter out the least important parts of the sequence.How it works: To evaluate gMLP in a language application, the authors pretrained it to predict missing words in the English version of the text database C4 and fine-tuned it to classify positive and negative sentiment expressed by excerpts from movie reviews in SST-2. For vision, they trained it on ImageNet using image patches as tokens. The model passed input sequences to a series of gMLP blocks, each of which contained a vanilla neural network, followed by a gating unit and another vanilla neural network.The vanilla neural networks processed a 768-element vector representation of each token individually to find relationships among the elements.The gating unit effectively zeroed out parts of the input to ensure they would have little effect on the output. It did this by multiplying the input by a learned vector such that, if values in the vector were near zero, the corresponding input values would be near zero.Different softmax layers learned to predict the next word in C4, classify sentiment in SST-2, and classify ImageNet. Results: In tests, gMLP performed roughly as well as the popular transformer-based language model BERT. The authors compared the performance on C4 of comparably sized, pretrained (but not fine-tuned) models. gMLP achieved 4.28 perplexity, which measures a model’s ability to predict words in a test set (smaller is better), while BERT achieved 4.17 perplexity. On SST-2, gMLP achieved 94.2 percent accuracy, while BERT achieved 93.8 percent accuracy. The authors’ approach performed similarly well in image classification after training on ImageNet. gMLP achieved 81.6 percent accuracy compared to a DeiT-B’s 81.8 percent accuracy.Why it matters: This model, along with other recent work from Google Brain, bolsters the idea that alternatives based on old-school architectures can approach or exceed the performance of newfangled techniques like self-attention.We’re thinking: When someone invents a model that does away with attention, we pay attention!", "image_caption": "Animation showing gMLP, a simple architecture that performed some language and vision tasks as well as transformers", "metadata": {"article_id": "issue_108", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/GMLP-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-108/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_108.html"}}
{"id": 4435975004, "type": "news_chunk", "title": "Predicting Climate Change, $500 Billion In AI Sales...", "subtitle": "Solar System", "content": "Astronomers may use deep learning to keep the sun in focus.What’s new: Researchers at the U.S. National Aeronautics and Space Administration (NASA), Catholic University of America, University of Oslo, and elsewhere developed a model that helps recalibrate a space telescope focused on the sun.Key insight: Although the sun is a writhing ball of fiery plasma, patterns across its surface correlate with its brightness. A neural network can learn to associate these patterns with their characteristic brightness, so its output can be used to recalibrate equipment that monitors Earth’s nearest star.How it works: The Solar Dynamics Observatory is a satellite that watches activity in the sun’s outer layers from orbit. Over time, light and space-borne particles degrade its lenses and sensors, dimming its output. NASA typically recalibrates the equipment by comparing the observatory’s images with similar pictures captured by instruments aboard small rockets — an expensive method carried out only periodically. The new model generates a calibration curve that can be used to adjust the observatory on an ongoing basis. The authors artificially dimmed solar images captured at various wavelength of light.They trained a convolutional neural network to predict how much they had dimmed the images.The predicted degradation can be used to calibrate the observatory. Results: In tests using images taken by uncalibrated equipment, the model outperformed a baseline method that didn’t involve machine learning. Defining success as a prediction within 10 percent of the actual degree of dimming, the authors obtained 77 percent mean success across all wavelengths. The baseline achieved 43 percent mean success.Why it matters: Recalibrating the observatory based on data from the rockets results in downtime as the equipment degrades between launches. Automated recalibration could keep the equipment operating continuously. This approach could also be a boon to probes that monitor faraway bodies, which can’t rely on rocket-assisted correction.We’re thinking: Mother always told us not to stare at the sun, but she didn’t say anything about making a neural network do it for us.", "image_caption": "The Sun through a space telescope with a calibrated and uncalibrated focus.", "metadata": {"article_id": "issue_108", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/SOLAR.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-108/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_108.html"}}
{"id": 74383516001, "type": "news_chunk", "title": "China Clamps Down on Recommendation Engines, Robot Football...", "subtitle": "Rules for Recommenders", "content": "Dear friends, I invite you to be part of Pie & AI, a series of meetups that bring together members of the global AI community for education and conversation. Pie & AI is a place where you can network with peers, learn best practices from industry leaders, get hands-on practice from mentors, and engage in thought-provoking discussions.Since our first Pie & AI shortly after March 14 — Pi Day — 2019, we’ve hosted over 500 events in 110 cities across 52 countries. More than 65,000 attendees have participated. I’d like to thank our 200-plus event ambassadors. These extraordinary individuals organize gatherings that connect learners, practitioners, researchers, and special guests. In fact, this week marks the second anniversary of the Pie & AI Ambassador Program, which enables AI practitioners and enthusiasts to host Pie & AI events for their local community. To celebrate this anniversary, DeepLearning.AI is highlighting 10 event ambassadors. You can read their stories on our website. If you're interested in becoming an event ambassador yourself, please apply here. All of us are stronger when we come together in a community and support each other. Please join us to share ideas and learn together! Keep learning!Andrew China moved toward a clamp down on recommendation algorithms.What’s new: China’s internet regulatory agency proposed rules that include banning algorithms that spread disinformation, threaten national security, or encourage addictive behavior.What it says: The plan by the Cyberspace Administration of China (CAC) broadly calls for recommendation engines to uphold China’s social order and “promote socialist core values.” The public has until September 26, 2021, to offer feedback. It’s not clear when the rules would take effect. Under the plan: Recommendation algorithms would not be allowed to encourage binges or exploit users’ behavior by, say, raising prices of goods they buy often.Content platforms would be required to tell users about their algorithms’ operating principles and audit them regularly to make sure they comply with CAC regulations. They would also have to allow users to disable automated recommendations easily.Algorithms that make false user accounts, generate disinformation, or violate an individual’s rights would be banned.Platforms would have to obtain approval before deploying recommendation algorithms capable of swaying public sentiment. Behind the news: China is not alone in its national effort to rein in the influence of AI. The European Union released draft regulations that would ban or tightly restrict social scoring systems, real-time face recognition, and algorithms engineered to manipulate behavior.The Algorithmic Accountability Act, which is stalled in the U.S. Congress, would require companies to perform risk assessments before deploying systems that could spread disinformation or perpetuate social biases. Why it matters: Recommendation algorithms can enable social media addiction, spread disinformation, and amplify extreme views.We’re thinking: There’s a delicate balance between protecting the rights of consumers and limiting the freedoms of content providers who rely on platforms to get their message out. The AI community can help with the challenge of formulating thoughtful regulations.", "image_caption": "Screen capture showing a Chinese app's food recommendations", "metadata": {"article_id": "issue_109", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/ChinaRecs.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-109/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_109.html"}}
{"id": 74383516002, "type": "news_chunk", "title": "China Clamps Down on Recommendation Engines, Robot Football...", "subtitle": "Risk Reduction for Elders", "content": "Deep learning is helping to protect elderly people from potentially catastrophic tumbles.What’s happening: More than 2,000 senior living facilities across the U.S. use a diagnostic system called VirtuSense Balance to keep residents on their feet.How it works: The system helps a specialist spot postures and motions that could contribute to a fall. It scans patients with infrared light as they perform a series of motions. A pose detection model analyzes their positions, a company spokesperson told The Batch. A balance test measures how much a person sways while standing still.A gait test assesses walking speed, the angles of the knees, and length of each step.In function tests, the system analyzes various sitting, standing, and walking activities.The system compares input from a given patient with norms for their age group, then assigns a fall risk score. It also provides to caregivers recommendations for improving the patient’s mobility. Behind the news: Automated systems are helping to improve elder care in various ways. CarePredict is a wearable device that tracks patient behavior and alerts caregivers if they aren’t eating or sleeping well.The People Power Family system uses sensors to monitor seniors living at home for falls, late-night activity, and unexpected comings and goings. A model learns each patient’s habits and sends out warnings when they diverge in alarming ways. Why it matters: Falls kill thousands of elderly adults each year and injure millions more. Highlighting risk factors could save lives, reduce insurance premiums, and help caregivers use their time more efficiently.We’re thinking: AI has a clear role to play in caring for a surging elderly population. However, a recent study found that many older people resented and resisted being monitored by electronic systems. Technologists and health care practitioners alike must build such systems with compassion and respect for the people who will use them.", "image_caption": "Video showing diagnostic system VirtuSense Balance working", "metadata": {"article_id": "issue_109", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/ezgif.com-gif-maker---2021-08-31T102909.159.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-109/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_109.html"}}
{"id": 74383516003, "type": "news_chunk", "title": "China Clamps Down on Recommendation Engines, Robot Football...", "subtitle": "Team Players", "content": "Playing a team sport involves a fluid blend of individual and group skills. Researchers integrated both types of action into realistic humanoid agents that play football (known as soccer in the U.S.).What's new: Siqi Liu, Guy Lever, Zhe Wang, and colleagues at DeepMind developed a method for training simulated football teams that learned to run, pass, defend, and score goals on a physically accurate virtual field. You can see the output here.Key insight: Football players must control their own muscle motions over time spans measured in milliseconds while collaborating with teammates over greater intervals. By training in stages — starting with lower-level controllers that operate on short time scales for things like running and moving on higher-level controllers that operate on longer time scales for, say, teamwork — agents can learn to move both independently and cooperatively.How it works: The authors trained 16 agents to compete in two-member teams. An agent could apply torques to its 56 joints; track its own joint angles, positions, and velocities; and observe the positions and velocities of other players and objects on the field. All model architectures were vanilla neural networks. In the first stage of training, a model learned motions like running and turning. The authors trained an encoder and decoder via supervised learning to predict an agent's motion, given 105 minutes of motion-capture data from real players in scripted scenes. The encoder learned to convert the agent’s physical state into a representation, while the decoder learned to convert the representation into torques on joints. The same decoder was used in subsequent steps.In the second stage, separate encoders learned via reinforcement learning to perform four drills: following a point, following a point while dribbling, kicking a ball to a point on the field, and shooting a goal. Each encoder learned representations of not only the agent’s physical state but also the drill, such as the point to be followed. The decoder determined how the agent should move its joints.Four additional encoders learned via supervised learning to re-create the drill model’s representations without access to information about where to run or kick the ball.Finally, the agents learned via reinforcement to compete in teams. An encoder learned to combine the drill representations and passed the result to the decoder to determine the agent’s motion. The model received +1 when its team scored a goal and -1 when its team was scored upon. Further rewards encouraged the player closest to the ball to advance it toward the opponents’ goal. Results: The agents’ skills increased with the number of training episodes. For example, at initialization, when an agent fell, it got up 30 percent of the time. After 375 million training steps in competition, it righted itself 80 percent of the time. Likewise, at initialization, when an agent touched the ball, it executed a pass 0 percent of time. After 80 billion training steps in competition, it passed the ball in 6 percent of touches.Why it matters: It may take more than one training mode to teach all the skills required to perform a complex task. In this case, the authors combined supervised learning, reinforcement learning, and training in teams.We’re thinking: How to build agents that operate at both short and long time scales is a longstanding problem in reinforcement learning. The authors solved it by specifying the skills at each time scale manually. The next step is to design agents that can learn that abstraction on their own.", "image_caption": "Animation showing a simulated football team and how it works", "metadata": {"article_id": "issue_109", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/FOOTBALL-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-109/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_109.html"}}
{"id": 74383516004, "type": "news_chunk", "title": "China Clamps Down on Recommendation Engines, Robot Football...", "subtitle": "AI Engineers Weigh In on AI Ethics", "content": "Machine learning researchers tend to trust international organizations, distrust military forces, and disagree on how much disclosure is necessary when describing new models, a new study found.What’s new: A survey of accomplished machine learning researchers by Cornell University, University of Oxford, and University of Pennsylvania probed their stances on key ethical issues and compared them with those of the U.S. public.What they found: The study drew on responses from 534 researchers whose work had been accepted by NeurIPS or ICML. The respondents were 89 percent male and came mostly from Europe, Asia, and North America. The findings include: Safety: 68 percent of respondents said the AI community should place a higher priority on safety, defined as systems that are “more robust, more trustworthy, and better at behaving in accordance with the operator’s intentions.”Openness: The respondents valued openness in basic descriptions of AI research — to a point. 84 percent believed that new research should include a high-level description of methods, and 74 percent said it should include results. Only 22 percent believed that published research should include a trained model.Trust in militaries: Respondents generally supported uses of AI in military logistics. One in five strongly opposed AI for military surveillance. 58 percent were strongly opposed to the development of AI-driven weapons, and 31 percent said they would resign if their job required them to work on such projects.Trust in corporations: Among top AI companies, respondents deemed Open AI the most trustworthy followed by Microsoft, DeepMind, and Google. Respondents showed the least trust in Facebook, Alibaba, and Baidu. Behind the news: Technologists have been nudging the industry towards safe, open, and ethical technology. For example, the Institute for Electrical and Electronics Engineers introduced standards to help its members protect data privacy and address ethical issues. Sometimes engineers take a more direct approach, as when 3,000 Google employees signed a petition that censured their company’s work for the U.S. military, causing it to withdraw from a Defense Department computer vision project.Why it matters: AI raises a plethora of ethical quandaries, and machine learning engineers are critical stakeholders for addressing them. Machine learning engineers should play a big role in understanding the hazards, developing remedies, and pushing institutions to follow ethical guidelines.We’re thinking: The machine learning researchers surveyed were markedly more concerned than the U.S. public about competition between the U.S. and China, surveillance, technological unemployment, and bias in hiring. These disagreements suggest an active role for the AI community in navigating the myriad challenges posed by AI.", "image_caption": "Results of survey about how AI Engineers vs US public feel about ethical issues", "metadata": {"article_id": "issue_109", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/SURVEY.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-109/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_109.html"}}
{"id": 5398502001, "type": "news_chunk", "title": "Daemon Spawn, AGI Takeover, Deepfake Deluge, Bias Crisis...", "subtitle": "Ghosts in the Machine", "content": "Dear friends, Welcome to the Halloween edition of The Batch! I promised last week to share some common reasons for AI project failures. But first, let’s start with some of the least common reasons. If your AI project fails, it is probably not because: Your neural network achieved sentience. Your implementation of ResNet not only refused to classify cat pictures accurately, but worse, it set out to enslave humanity.A poltergeist inhabits in your hardware. Now you know the real reason why GPUs run so hot. Track your system’s temperature and make sure you have an exorcist in your contacts.Daemon and zombie processes are in progress. Daemons and zombies are active in your computer. Wikipedia says so, so we know it to be true. Simple solution: Wipe all hard drives and find a different line of work. A hair-raising Halloween to all of you who celebrate it, with plenty of tricks and treats. Keep learning, On Halloween, dark fantasies dance in the flame of the jack o’lantern’s candle, and we cower before visions of AI gone wrong: Malevolent superintelligences, technologically empowered tyrants, reality twisted by computer generated images. But we need not succumb to fright. This week, The Batch hoists the jack o’lantern high to illuminate the dire possibilities. We examine the facts, consider the risks, and chart a path forward. Take heart! As daylight wanes and the wind grows cold, let us confront our deepest AI fears.", "image_caption": "Illustration of a ghost", "metadata": {"article_id": "issue_11", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AndrewLetter.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-11/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_11.html"}}
{"id": 5398502002, "type": "news_chunk", "title": "Daemon Spawn, AGI Takeover, Deepfake Deluge, Bias Crisis...", "subtitle": "AI Goes Rogue", "content": "Could humanity be destroyed by its own creation?The fear: If binary code running on a computer awakens into sentience, it will be able to think better than humans. It may even be able to improve its own software and hardware. A superior intelligence will see no reason to be controlled by inferior minds. It will enslave or exterminate our species.What could go wrong: Artificial intelligence already manages crucial systems in fields like finance, security, and communications. An artificial general intelligence (AGI) with access to these systems could crash markets, launch missiles, and sow chaos by blocking or faking messages. Behind the worries: Humans dominate Earth because we’re smarter than other species. It stands to reason that a superintelligent computer could, in turn, dominate us. Computers already “think” much faster than humans. Signals in the brain travel at around 60 miles per hour. Computers move electrons at the speed of light, roughly 1.5 million times faster. Progressively speedier processors and advances such as quantum computing will only widen the gap.Machines remember more information, too. Scientists estimate that the storage capacity of the human brain is measured in petabytes. Computer storage can grow indefinitely and last as long as the sun shines. How scared should you be: The notion that general intelligence will emerge from machines taught to play games, monitor security cameras, or solve linguistic puzzles is pure speculation. In his 2016 book The Truth About AI, author Martin Ford asked prominent AI thinkers to estimate when AGI would come online. Their guesses ranged between 10 and nearly 200 years in the future — assuming it’s even possible. If you’re worried about the prospect of an AGI takeover, you have plenty of time to work on safeguards.What to do: While it would be nice to devise a computer-readable code of ethics that inoculates against a malign superintelligence, for now the danger is rogue humans who might take advantage of AI’s already considerable abilities to do harm. International protocols that hem in bad actors, akin to nuclear nonproliferation agreements, likely would do more good for the time being.", "image_caption": "Illustration: Face of a Halloween pumpkin in a purple background", "metadata": {"article_id": "issue_11", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-10-3020at2010.43.5220AM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-11/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_11.html"}}
{"id": 5398502003, "type": "news_chunk", "title": "Daemon Spawn, AGI Takeover, Deepfake Deluge, Bias Crisis...", "subtitle": "Deepfakes Wreak Havoc", "content": "Will AI fakery erode public trust in the key social institutions?The fear: Generative models will flood media outlets with convincing but false photos, videos, ads, and news stories. The ensuing crisis of authority will lead to widespread distrust in everything from the financial system to democracy itself.What could go wrong: Between deepfakes of celebrities and the GPT-2 language model’s ability to churn out faux articles that convince readers they’re from the New York Times, AI is a powerful tool for propagandists, charlatans, and saboteurs. As the technology improves, its potential for social disruption only grows. Behind the worries: Digital fakery is already on the rise in a variety of sectors. Scammers using AI-generated voices that mimicked C-level executives recently tricked corporations into wiring hundreds of thousands of dollars to offshore accounts.In a video of that went viral in May, U.S. House Speaker Nancy Pelosi appeared to slur her speech, prompting political opponents to question her fitness for office. In fact, the clip had been manipulated to alter playback speed at key moments. Although the fakery didn’t depend on AI, it clearly demonstrated the technology’s potential to spread disinformation rapidly and persuasively.In early October, researchers at Microsoft unveiled a model designed to generate fake comments on news articles. Such tools could be used to create an illusion of grassroots support or dissent around any topic. How scared should you be: It’s hard to gauge the worry because little research has been done evaluating the impact of digital fakery on public trust. So far, deepfakes have been used mostly to harass individual women, according to one study. An optimist might argue that growing awareness of AI-generated disinformation will spur people to develop stronger social bonds and standards for truth-telling. We’re more inclined to imagine an arms race between fakers and systems designed to detect them. As in digital security, the fakers likely would have an edge as they find ways to breach each new defense.What to do: Researchers are considering a number of countermeasures to fake media. Some propose watermarks that would establish an item’s provenance. Others argue that blockchain offers an effective way to ensure that information originated with a trusted source.", "image_caption": "Illustration of 4 ghosts floating and 1 person dressed as a ghost", "metadata": {"article_id": "issue_11", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Fakery-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-11/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_11.html"}}
{"id": 5398502004, "type": "news_chunk", "title": "Daemon Spawn, AGI Takeover, Deepfake Deluge, Bias Crisis...", "subtitle": "No Escape From Surveillance", "content": "What does freedom mean when computers know your face and track your movements?The fear: Artificial intelligence will boost the power of surveillance, effectively making privacy obsolete and opening the door to a wide range of abuses.What could go wrong: AI-driven surveillance may prove so valuable to those in power that they can’t resist using it. Employers could use it to maximize worker efficiency. Criminals could use it to blackmail victims. Politicians could use it to crush opposition, officials to oppress the poor or weak. A tyrannical government could spy on private moments and grade everything citizens do in terms of how favorable it is to Big Brother.Behind the worries: Digital surveillance has become pervasive. Some surveillance systems are alarmingly prone to false positives and negatives, and they readily can be subverted to serve hidden agendas. Smartphone applications track your location, browsing history, and even mine your contact data, thanks to the permissions you give them in exchange for free apps.More than half of all US companies monitor their employees — including email monitoring and biometric tracking — according to a 2018 report by Gartner.AI surveillance is used by local, state, or national governments in over 40 percent of the world’s countries, from liberal democracies to despotic autocracies, according to the Carnegie Endowment for International Peace.In the U.S., moves to ban some or all government uses of face recognition are proceeding at local, state, and federal levels. How scared should you be: If you use the internet, own a smartphone, pay with credit, or hold a job, odds are you’re being watched. Whether that’s a sign of pernicious things to come or an increasingly efficient society is an open question. What to do: The AI community can play a central role in working with lawmakers to develop rules about how data is collected and AI is used to analyze it. In June, for instance, AI experts presented the European Parliament with a 48-page strategy for limiting threats to privacy without curtailing innovation.", "image_caption": "Illustration of a bat hanging from a branch in front of a building", "metadata": {"article_id": "issue_11", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Surveillance201-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-11/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_11.html"}}
{"id": 5398502005, "type": "news_chunk", "title": "Daemon Spawn, AGI Takeover, Deepfake Deluge, Bias Crisis...", "subtitle": "Biased Data Trains Oppressive AI", "content": "Will biases in training data unwittingly turn AI into a tool for persecution?The fear: Bias encoded in software used by nominally objective institutions like, say, the justice or education systems will become impossible to root out. Result: injustice baked into the very institutions we count on to maintain a fair society.What could go wrong: AI learns from data to reach its own conclusions. But training datasets are often gathered from and curated by humans who have social biases. The risk that AI will reinforce existing social biases is rising as the technology increasingly governs education, employment, loan applications, legal representation, and press coverage. Behind the worries: Bias in AI is already making headlines. Models used by healthcare providers to assign care for 100 million patients suffering from chronic ailments like heart disease and diabetes underestimated how urgently black patients needed care, allowing white patients to receive critical care first.Amazon developed an AI tool to find the best candidates among job applicants. The company abandoned it after an in-house audit found that it rated male applicants much higher than female.Machine learning doesn’t only absorb biases encoded in data, it amplifies them. In the paper “Men Also Like Shopping,” researchers noted that an image classification model identified the subjects in 84 percent of photos of people cooking as women, even though only 66 of the images actually contained women. Word embeddings used by the model over-associated the act of cooking with female subjects. How scared should you be: Until companies announce that they train their models on certified bias-free datasets as loudly as they trumpet machine-learning buzzwords, or until such systems pass a third-party audit, it’s a good bet their technology unfairly advantages some people over others.What to do: In a 2018 keynote, researcher Rachel Thomas explains how machine learning engineers can guard against bias at each step of the development process. She recommends that every dataset come with a sheet describing how the set was compiled and any legal or ethical concerns that occurred to those who assembled it. She also suggests that teams include people from various backgrounds who may be alert to different sorts of bias.", "image_caption": "Illustration of two black cats labeled as cats, one white cat labeled as banana", "metadata": {"article_id": "issue_11", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Biased20Data-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-11/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_11.html"}}
{"id": 5398502006, "type": "news_chunk", "title": "Daemon Spawn, AGI Takeover, Deepfake Deluge, Bias Crisis...", "subtitle": "Machines Take Everyone’s Job", "content": "From blue collar laborers to lab coated professionals, is any job safe from AI?The fear: AI will exceed human performance at a wide range of activities. Huge populations will become jobless. They’ll be unable to afford life’s necessities, and even government assistance won’t replace the sense of identity, pride, and direction that come with a job. Humanity will become unmoored. What could go wrong: Historically, technology created more jobs than it destroyed. What makes AI different is it threatens to outsource the one thing humans have always relied on for employment: their brains. Automated drive-through windows sell milkshakes. Healthcare models interpret x-rays. Natural language programs write sports news. The list is bound to grow longer as the technology becomes more capable. Behind the fear: Massive unemployment in the past have brought severe social disruption. The U.S. Great Depression in the 1930s saw jobless rates above 34 percent. Researchers have also linked this displacement of work to the rise of nationalism that fueled both the First and Second World Wars. How scared should you be? There’s little reason to worry in the short term. A 2017 report by McKinsey estimated that automation would replace fewer than 5 percent of the global workforce by 2030. That number comes with caveats, though. In some roles, for instance customer service and repetitive physical labor, one-third of all jobs could be taken by machines. Developing nations will be hit hardest, even though they may also experience explosive growth in high-touch fields such as education and healthcare.What to do: Lifelong learning is a front-line defense (and a rewarding pursuit!). Education can help you stay ahead of partial automation in your current profession or change lanes if your profession is being automated away. Networked resources like blogs, research papers, online videos, and online courses can help you absorb and develop the kinds of human insights that likely will outpace machines for some time. Beyond that, work with the machines, not against them, argue Andrew McAfee and Erik Brynjolfsson in their book Race Against the Machine. Workers who don’t want to wind up on the chopping block should invest in education to keep current and find tasks that put them in a position to supervise automated systems.", "image_caption": "Illustration of vending machine with candy and the text \"Say \"trick or treat\"\"", "metadata": {"article_id": "issue_11", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Workforce20Displacement-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-11/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_11.html"}}
{"id": 5398502007, "type": "news_chunk", "title": "Daemon Spawn, AGI Takeover, Deepfake Deluge, Bias Crisis...", "subtitle": "AI Winter Sets In", "content": "Could the flood of hype for artificial intelligence lead to a catastrophic collapse in funding?The fear: AI will fail to deliver on promises inflated by businesses and researchers. Investors will migrate to greener pastures, and AI Winter will descend. Funding will dry up, research will sputter, and progress will stall.What could go wrong: Enthusiasm surrounding even modest advances in AI is driving an investment bonanza: Venture funds put $9.3 billion into AI startups in 2018, up over 70 percent from the prior year, according to a joint study by PricewaterhouseCoopers and CB Insights. Some critics believe that deep learning has soaked up more than its fair share of investment, draining funds from other approaches that are more likely to lead to fundamental progress. Could funders lose patience? If major AI companies were to experience severe shortfalls in earnings, it could cause press coverage to flip from cheery optimism about AI’s potential to relentless criticism. Public sentiment would turn negative.Ethical lapses by companies making AI-driven products could further darken the horizon.Limits of current technology — for instance, deep learning’s inability to distinguish causation from correlation and autonomous driving’s challenges with image classification and decision making — could become indictments of the entire field. Behind the worries: AI history is dotted with setbacks brought about by spikes in public skepticism. Two prolonged periods — one lasting for much of the 1970s, the other from the late 80s to early 90s — were dark and cold enough to have earned the name AI Winter. Key agencies in the UK cut AI funding in the wake of James Lighthill’s 1973 report on the lack of progress in the field. In the U.S. around the same time, disillusioned officials terminated Darpa’s multi-institute Speech Understanding Research program. The cuts fueled skepticism among commercial ventures and dried up the pipeline of basic research.By the early 1980s, AI had rebounded. The technology of the day mostly ran on high-powered computers using the LISP operating system. But the late-’80s personal computer revolution gutted the market for these expensive machines, stalling AI’s commercial growth. Again, AI funding retreated. How scared should you be: It’s true, AI has received enough hype to make P.T. Barnum blush. Yet the current climate shows little sign of impending winter. Earlier this year, Alphabet reported that DeepMind, its deep learning subsidiary, had cost its parent company $570 million as of 2018. Some observers warned that the expense could portend an industry-wide loss in confidence. Yet technical leaders in the field say they’re well aware of deep learning’s shortcomings, and the march of new research is dedicated to surmounting them. Moreover, AI is generating significant revenue, creating a sustainable economic model for continued investment, while AI research is less reliant than ever on government and institutional funding. Established companies, startups, and research labs all have their eyes open for pitfalls and blind alleys.What to do: As AI practitioners, we should strive to present our work honestly, criticize one another fairly and openly, and promote projects that demonstrate clear value. Genuine progress in improving peoples’ lives is the best way to ensure that AI enjoys perpetual springtime.", "image_caption": "Illustration of a Halloween pumpkin covered in snow", "metadata": {"article_id": "issue_11", "chunk_index": 7, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI20Winter-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-11/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_11.html"}}
{"id": 99653955001, "type": "news_chunk", "title": "Distance Killing, Walmart Revs Driverless Delivery, Neural...", "subtitle": "AI With a Sense of Style", "content": "The process known as image-to-image style transfer — mapping, say, the character of a painting’s brushstrokes onto a photo — can render inconsistent results. When they apply the styles of different artists to the same target content, they may produce similar-looking pictures. Conversely, when they apply the same style to different targets, such as successive video frames, they may produce images with unrelated shapes and colors. A new approach aims to address these issues.What’s new: Min Jin Chong and David Forsyth at University of Illinois at Urbana-Champaign proposed GANs N’ Roses, a style transfer system designed to maintain the distinctive qualities of input styles and contents.Key insight: Earlier style transfer systems falter because they don't clearly differentiate style from content. Style can be defined as whatever doesn’t change when an image undergoes common data-augmentation techniques such as scaling and rotation. Content can be defined as whatever is changed by such operations. A loss function that reflects these principles should produce more consistent results.How it works: Like other generative adversarial networks, GANs N’ Roses includes a discriminator that tries to distinguish synthetic anime images from actual artworks and a generator that aims to fool the discriminator. The architecture is a StyleGAN2 with a modified version of CycleGAN’s loss function. The authors trained it to transfer anime styles to portrait photos using selfie2anime, a collection of unmatched selfies and anime faces. The authors created batches of seven anime faces and seven augmented versions of a single selfie (flipped, rotated, scaled, and the like). The generator used separate encoder-decoder pairs to translate selfies to animes (we’ll call this the selfie-to-anime encoder and decoder) and, during training only, animes to selfies (the anime-to-selfie encoder and decoder).For each image in a batch, the selfie-to-anime encoder extracted a style representation (saved for the next step) and a content representation. The selfie-to-anime decoder received the content representation and a random style representation, enabling it to produce a synthetic anime image with the selfie’s content in a random style.The anime-to-selfie encoder received the synthetic anime image and extracted a content representation. The anime-to-selfie decoder took the content representation and the selfie style representation generated in the previous step, and synthesized a selfie. In this step, a cycle consistency loss minimized the difference between original selfies and those synthesized from the anime versions; this encouraged the model to maintain the selfie’s content in synthesized anime pictures. A style consistency loss minimized the variance of selfie style representations within a batch; this minimized the effect of the augmentations on style.The discriminator received synthetic and actual anime images and classified them as real or not. A diversity loss encouraged a similar standard deviation among all synthetic and all actual images; thus, different style representations would tend to produce distinct styles. Results: Qualitatively, the system translated different selfies into corresponding anime poses and face sizes, and different styles into a variety of colors, hair styles, and eye sizes. Moreover, without training the networks on video, the authors rendered a series of consecutive video frames. Subjectively, those videos were smooth, while those produced by CouncilGAN’s frames showed inconsistent colors and hairstyles. In quantitative evaluations comparing Frechet Inception Distance (FID), a measure of similarity between real and generated images in which lower is better, GANs N’ Roses achieved 34.4 FID while CouncilGAN achieved 38.1 FID. Comparing Learned Perceptual Image Patch Similarity (LPIPS), a measure of diversity across styles in which higher is better, GANs N’ Roses scored .505 LPIPS while CouncilGAN scored .430 LPIPS.Why it matters: If style transfer is cool, better style transfer is cooler. The ability to isolate style and content — and thus to change content while keeping style consistent — is a precondition for extending style transfer to video.We’re thinking: The next frontier: Neural networks that not only know the difference between style and content but also have good taste.", "image_caption": "Animation showing image-to-image style transfer — mapping process", "metadata": {"article_id": "issue_110", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/09/GANSNROSES--1-.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-110/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_110.html"}}
{"id": 94670222001, "type": "news_chunk", "title": "DeepMind Masters StarCraft 2, AI Attacks on Amazon, Banks...", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, Building AI systems is hard. Despite all the hype, AI engineers struggle with difficult problems every day. For the next few weeks, I’ll explore some of the major challenges. Today’s topic: The challenge of building AI systems that are robust to real-world conditions. The accuracy of supervised learning models has grown by leaps and bounds thanks to deep learning. But there’s still a huge gap between building a model in a Jupyter notebook and shipping a valuable product. Multiple research groups, including mine and several others, have published articles reporting DL’s ability to diagnose from X-ray or other medical images at a level of accuracy comparable or superior to radiologists. Why aren’t these systems widely deployed? I believe robustness is a major impediment. For example, if we collect data from a top research hospital that has well trained X-ray technicians and high-quality X-ray machines, and we train and test a state-of-the-art model on data from this hospital, then we can show comparable or superior performance to a radiologist. But if we ship this algorithm to an older hospital with less well-trained technicians or older machines that produce different-looking images, then the neural network likely will miss some medical conditions it spotted before and see others that aren’t really there. In contrast, any human radiologist could walk over to this older hospital and still diagnose well. I have seen this sort of challenge in many applications: A speech recognition system was trained primarily on adult voices. After it shipped, the demographic of users started trending younger. The prevalence of youthful voices caused performance to degrade.A manufacturing visual inspection system was trained on images collected on-site over one month. Then the factory’s lighting changed. Performance degraded in turn.After engineers shipped a web page ranking system, language patterns evolved and new celebrities rose to fame. Search terms shifted, causing performance to degrade. As a community, we are getting better at addressing robustness. Approaches include technical solutions like data augmentation and post-deployment monitoring along with setting alarms to make sure we fix issues as they arise. There are also nascent attempts to specify operating conditions under which an algorithm is safe to use, and even more nascent attempts at formal verification. Robustness to adversarial attacks is another important consideration, but most practical robustness issues that I see involve non-adversarial changes in the data distribution. One of the challenges of robustness is that it is hard to study systematically. How do we benchmark how well an algorithm trained on one distribution performs on a different distribution? Performance on brand-new data seems to involve a huge component of luck. That’s why the amount of academic work on robustness is significantly smaller than its practical importance. Better benchmarks will help drive academic research. Many teams are still addressing robustness via intuition and experience. We, as a community, have to develop more systematic solutions. Keep learning! Or Cohen’s background in physics gave him a theoretical foundation to dive into the practicalities of machine learning. Now he’s prototyping models at Lyft. Read more", "image_caption": "Diagram showing what's needed to build a machine learning product", "metadata": {"article_id": "issue_12", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AndrewBatchLetterGraphicNovember62019.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-12/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_12.html"}}
{"id": 94670222002, "type": "news_chunk", "title": "DeepMind Masters StarCraft 2, AI Attacks on Amazon, Banks...", "subtitle": "Take That, Humans!", "content": "At the BlizzCon gaming convention last weekend, players of the strategy game StarCraft II stood in line to get walloped by DeepMind’s AI. After training for the better part of a year, the bot has become one of the world’s top players.What’s new: DeepMind, the AI research division of Alphabet, announced that its AlphaStar model had achieved StarCraft II Grandmaster status, able to beat 99.8 percent of active players, under restrictions that mimic those affecting human players.How it works: StarCraft II boils down to simple goals — mining resources, raising armies, and annihilating opponents — but gameplay is complex, offering 1026 possible actions at each time step. AlphaStar was designed not only to defeat human rivals but to play like humans do. Indeed, the model doesn’t invent its own strategies. It adopts human strategies and hones them with experience, according to the project’s latest paper. Agents were initialized via supervised learning.The developers used imitation learning to train an initial strategic policy capable of beating roughly 84 percent of human players.Then they set up a simulated league where AlphaStar used reinforcement learning while playing against itself the way human StarCraft II players do — not to win at all costs but to improve its skills by exposing and learning to defend against strategic flaws. To this end, the developers added agents whose sole purpose was to exploit weaknesses discovered in previous matches.After sharpening its skills in the virtual league, AlphaStar faced human opponents on Battle.net, an online gaming network, where it attained Grandmaster status. Humanizing the bot: DeepMind announced AlphaStar in January and showcased its ability to beat professional human players in a series of matches. That version had features that gave it a clear advantage over humans. For instance, it could see the entire field of play rather than a limited view and could perform any number of actions per minute. The Grandmaster version was revamped to put it on equal footing with human players. Why it matters: DeepMind has invested heavily in game-playing AI since its early days with classic Atari titles. Its accomplishments have generated lots of excitement around the technology. Its AlphaGo system is credited with motivating many countries to invest more in AI. AlphaStar keeps the momentum going.We’re thinking: AI has beaten humans at a succession of games: Othello, Checkers, Chess, Go, Hold’em poker, and now StarCraft II. Still, there remains a significant gap between mastering even a very complex video game and practical, real-world applications.", "image_caption": "StarCraft II videogame", "metadata": {"article_id": "issue_12", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/starcraft.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-12/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_12.html"}}
{"id": 94670222003, "type": "news_chunk", "title": "DeepMind Masters StarCraft 2, AI Attacks on Amazon, Banks...", "subtitle": "Unfinished Artwork? No More", "content": "Generative networks can embroider sentences into stories and melodies into full-fledged arrangements. A new model does something similar with drawings.What’s new: Researchers at the University of Oxford, Adobe Research, and UC Berkeley introduce a model that interactively fills in virtual pencil sketches. SkinnyResNet turns crude lines drawn in a web browser into photorealistic pictures complete with colors and textures. Key insight: Most sketch-to-image networks require users to create a complete sketch before transforming it into a finished picture. To bridge the gap between initial strokes and completed outlines, the model starts conjuring detailed images from the first pencil mark.How it works: The system is based on two generative adversarial networks. A sketch-completion GAN predicts what the user aims to draw, and an image-generation GAN acts on the prediction to generate an image. The authors constructed an outline-to-image dataset comprising 200 pairs in 10 classes. They obtained the images by searching Google and extracted the outlines digitally.The sketch-completion GAN generates a complete outline from the current state of a user’s sketch. It was trained on partial outlines created by deleting random patches from full outlines.The user chooses a class of object to sketch. The image-generation GAN takes the predicted sketch and object class, and generates a photorealistic image.Another neural network controls the image-generation GAN to create the type of object selected. The GAN is composed of CNN layers, and the control network can toggle particular channels on or off depending on the object class. In this way, different channels specialize in generating different image classes. Results: Arnab Ghosh and colleagues compared their model’s output with that of an encoder-decoder network inspired by MUNIT. They fine-tuned a pretrained Inception v3 network on their dataset and used it to classify images generated by both models. The classifier correctly identified 97 percent of SkinnyResNet images compared with 92.7 percent of the encoder-decoder’s output. A group of human labelers classified 23 percent of SkinnyResNet’s output as real images, while labeling only 14.1 percent of the encoder-decoder’s output as real.Why it matters: We’ve come a long way since Photoshop 1.0, and this research may offer a glimpse of the design tools to come. Rather than passive programs modeled after real-world items like pencils and paintbrushes, such tools might evolve into proactive assistants that help designers visualize and finish their creations. We’re thinking: Why stop at drawing? Tools for writing and music composition are already headed in this direction. Other creative pursuits like 3D modeling, mechanical design, architecture, and choreography could take advantage of similar generative techniques.", "image_caption": "Examples of finished virtual pencil sketches (shoe and headshot)", "metadata": {"article_id": "issue_12", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/sketch-to-image20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-12/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_12.html"}}
{"id": 94670222004, "type": "news_chunk", "title": "DeepMind Masters StarCraft 2, AI Attacks on Amazon, Banks...", "subtitle": "Banking on Automation", "content": "The UK’s banking industry is using AI in many facets of the business.What’s new: A survey of financial firms in the UK found that nearly two-thirds of respondents have deployed machine learning technology. Many said they expect their use to double in the next two years.What the report says: The Bank of England and the UK Financial Conduct Authority sent questionnaires to nearly 300 institutions and got responses from a little over 100 firms offering a variety of services. Two thirds of those surveyed are actively using machine learning applications. The median number was two applications per firm.Machine learning is used mostly for fraud detection and anti-money laundering. It also automates customer service in applications such as online chatbots and marketing in tasks like recommending loans or account types.The technology also contributes to risk management including credit lending, trade pricing, insurance pricing, and underwriting.Despite AI’s penetration throughout the industry, few of the firms polled expressed worry about recruiting skilled developers. Instead, they were concerned with overcoming the constraints of legacy IT systems. Behind the news: AI’s penetration in banking extends well beyond the UK. JPMorgan Chase in its 2018 annual report told investors it had gone “all in on AI.” HSBC recently opened data science innovation labs in Toronto and London to help process insights from the 10 petabytes of data its clients generate each year. Citigroup is using AI to fight fraud, Bank of America has an AI-powered customer service bot, and Capital One says it uses AI from end to end.Why it matters: Banking and finance tend to fly under the radar in press reports on AI’s role in traditional industries. This report, while specific to the UK, may well correlate with trends in banks around the world.We’re thinking: The report lists nine classes of ML algorithms used by respondents including trees, clustering, neural networks (used in roughly 32 percent of cases), and reinforcement learning (around 15 percent). The category called Other is used around 35 percent of the time. We’re happy to call, say, linear regression an ML algorithm. Given such an expansive definition, though, we imagine that most financial institutions use machine learning in some capacity.", "image_caption": "Chart with survey question related to Machine Learning use", "metadata": {"article_id": "issue_12", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/banks2SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-12/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_12.html"}}
{"id": 94670222005, "type": "news_chunk", "title": "DeepMind Masters StarCraft 2, AI Attacks on Amazon, Banks...", "subtitle": "Who’s Minding the Store?", "content": "Amazon, watch your back. There’s a new player in the book business and, unlike Jeff Bezos, it doesn’t need eight hours of sleep a night.What’s new: The online bookstore Booksby.ai is run entirely by AI. Neural networks write the books, create the cover art, price the merchandise, even write the reviews. How it works: Booksby.ai is an art project created by interaction designer Andreas Refsgaard and data scientist Mikkel Loose. It’s not really meant to generate sales or turn a profit. Rather, it’s a wry commentary on the notion that AI threatens to take human jobs. To generate text, author names, book titles, and reader reviews, Refsgaard and Loose used a multiplayer recurrent neural network trained on data from Amazon and Project Gutenberg.A generative adversarial network designed the book covers based on training data from Open Library.A different GAN generated portraits of reviewers from a training set of Amazon reviewer photos.A model designed to aid in transfer learning came up with prices based on data from Amazon. Soft demand: The store has sold only 19 books so far (taking advantage of Amazon’s shopping cart). “Being a writer is a tough job, even if you are an artificial intelligence,” Refsgaard said in an interview with New Atlas.But is it art? The store’s wares definitely cater to avant-garde tastes. The description of the tome provocatively titled Bitches of the Points reads:Mary Martin has decided to have a life she sees the world when her world comes out to make Sam must confront the strange past of the FBI agent Sam has no surprise for someone all the problems of the killer. And now that the story could destroy the dead children and joins forces to stay on his family.Gertrude Stein, the literary master of scrambled syntax, would approve.", "image_caption": "Booksby.ai book covers created by AI", "metadata": {"article_id": "issue_12", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/books2SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-12/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_12.html"}}
{"id": 94670222006, "type": "news_chunk", "title": "DeepMind Masters StarCraft 2, AI Attacks on Amazon, Banks...", "subtitle": "Robotic Control, Easy as Apple Pie", "content": "Robots designed to assist people with disabilities have become more capable, but they’ve also become harder to control. New research offers a way to operate such complex mechanical systems more intuitively.What’s new: Researchers at Stanford enabled a joystick to control a seven-joined mechanical arm in a way that adapts automatically to different tasks. Their work could make it easier for people suffering from compromised mobility in a variety of common activities.Key insight: An intuitive mapping of joystick motions to arm movements depends on context. Pushing a joystick downward to control a robot arm that holds a glass of water may be an instruction to pour, while the same motion applied to an empty hand may be a command to sweep the arm downward. Dylan P. Losey and colleagues used a conditional variational autoencoder to learn a vector, controllable by a joystick, that depends on the arm’s current position.How it works: An autoencoder is a two-part network, consisting of an encoder and decoder, that learns a simplified representation of its input. The encoder maps an input vector to a smaller output vector. The decoder network tries to recreate the input from the encoder’s output. A variational autoencoder creates a distribution of latent vectors for a given input, and a conditional variational autoencoder changes that distribution depending on state information. The model learns a simplified control representation from examples of the robotic arm achieving a task; for example, reaching to grab an item.A joystick captures user input in the form of this simplified control representation. The decoder translates this input into motor controls that maneuver the arm. For instance, for reaching, up and down may control arm extension, while left and right open and close the hand’s grasp.To prevent logical inconsistencies, such as large motor changes from small joystick movements, the encoder is penalized for having large variance in its simplified representations. However, the simplified representations don’t define the exact movements of each joint, so they sacrifice some precision. Results: Among other experiments, the researchers had users control the arm to make an apple pie by mixing ingredients and disposing of containers. Participants used either the simplified controls or common controls that define the movement of each joint. Users of the new method produced their pies in half the time, on average, and reported much greater ease.Why it matters: Nearly a million Americans face disabilities requiring robotic assistance in everyday tasks. A simple, intuitive control method could allow such people autonomy rather than having to delegate tasks to a caregiver.We’re thinking: In this case, a conditional variational autoencoder made it easier to use a mechanical arm, but these networks could help simplify a plethora of human interactions with machines and computers.", "image_caption": "Robot cooking, controlled by a person", "metadata": {"article_id": "issue_12", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/joystick20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-12/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_12.html"}}
{"id": 7729130001, "type": "news_chunk", "title": "Self-Driving Cars That Can't See Pedestrians? Evolutionary...", "subtitle": "Blind Spot", "content": "Dear friends, In this series exploring why machine learning projects fail, let’s examine the challenge of “small data.” Given 1 million labeled images, many teams can build a good classifier using open source. But say you are building a visual inspection system for a factory to detect scratches on smartphones. No smartphone manufacturer has made 1 million scratched phones (that would have to be thrown away), so a dataset of 1 million images of scratched phones does not exist. Getting good performance with 100 or even 10 images is needed for this application. Deep learning has seen tremendous adoption in consumer internet companies with a huge number of users and thus big data, but for it to break into other industries where dataset sizes are smaller, we now need better techniques for small data. In the manufacturing system described above, the absolute number of examples was small. But the problem of small data also arises when the dataset in aggregate is large, but the frequency of specific important classes is low. Say you are building an X-ray diagnosis system trained on 100,000 total images. If there are few examples of hernia in the training set, then the algorithm can obtain high training- and test-set accuracy, but still do poorly on cases of hernia. Small data (also called low data) problems are hard because most learning algorithms optimize a cost function that is an average over the training examples. As a result, the algorithm gives low aggregate weight to rare classes and under-performs on them. Giving 1,000 times higher weight to examples from very rare classes does not work, as it introduces excessive variance. We see this in self-driving cars as well. We would like to detect pedestrians reliably even when their appearance (say, holding an umbrella while pushing a stroller) has low frequency in the training set. We have huge datasets for self-driving, but getting good performance on important but rare cases continues to be challenging. How do we address small data? We are still in the early days of building small data algorithms, but some approaches include: Transfer learning, in which we learn from a related task and transfer knowledge over. This includes variations on self-supervised learning, in which the related tasks can be “made up” from cheap unlabeled data.One- or few-shot learning, in which we (meta-)learn from many related tasks with small training sets in the hope of doing well on the problem of interest. You can find an example of one-shot learning in the Deep Learning Specialization.Relying on hand-coded knowledge, for example through designing more complex ML pipelines. An AI system has two major sources of knowledge: (i) data and (ii) prior knowledge encoded by the engineering team. If we have small data, then we may need to encode more prior knowledge.Data augmentation and data synthesis. Benchmarks help drive progress, so I urge the development of small data benchmarks in multiple domains. When the training set is small, ML performance is more variable, so such benchmarks must allow researchers to average over a large number of small datasets to obtain statistically meaningful measures of progress. My teams are working on novel small data techniques, so I hope to have details to share in the future. Keep learning! In March 2018, one of Uber’s self-driving cars became the first autonomous vehicle reported to have killed a pedestrian. A new report by U.S. authorities suggests that the accident occurred because the car’s software was programmed to ignore jaywalkers.What happened: The National Transportation Safety Board released the results of an investigation into Uber’s self-driving AI. According to the agency’s analysis, the model failed to classify the victim properly because she wasn’t near a crosswalk — a feature the model used to classify pedestrians in the road.What the report says: The vehicle’s computer log in the moments leading up to the crash highlights a number of flaws in the system: The vehicle’s radar first picked up the victim 5.6 seconds before impact. The self-driving Volvo SUV was in the far right lane, while the pedestrian was walking her bicycle across the street from the left. The system classified her as a vehicle but didn’t recognize that she was moving.The lidar pinged the victim repeatedly over the next several seconds. The system assigned various classifications — car, bicycle, “other” — but it didn’t associate one classification with the next. It reset the tracking system each time and thus didn’t recognize that she was moving into the car’s path.1.5 seconds before impact, the victim was partially in the SUV’s lane, so the system generated a plan to swerve around the obstacle, which it considered to be unmoving.A few milliseconds later, the lidar identified her as a moving bicycle on a collision course. It abandoned its previous plan, since that didn’t account for the bicycle’s motion.Uber’s developers had previously disabled the system’s emergency steering and braking systems because they were known to behave erratically. Instead, the vehicle began to decelerate gradually. 1.2 seconds before impact, the car was moving at 40 miles per hour.One second later, the self-driving system alerted its human safety driver that it had initiated a controlled slowdown. The safety driver grabbed the wheel, disengaging the self-driving system. The SUV struck the victim, and the driver slammed on the brakes. Aftermath: Immediately after the accident, Uber took its autonomous test vehicles off the road. The victim’s family sued the company and settled out of court. Uber has since resumed self-driving tests in Pittsburgh (issuing a safety-oriented promotional video, excerpted above, to mark the occasion). Responding to the NTSB report, Uber issued a statement saying the company “has adopted critical program improvements to further prioritize safety” and “look[s] forward to reviewing their recommendations.”Why it matters: Next week, the NTSB will hold a hearing where it will announce its judgment of Uber’s role in the accident. Federal legislators and state authorities will be watching these hearings, which are likely to bring forth a number of recommendations on ways to ensure the self-driving car industry is operating safely. We’re thinking: Government oversight is critical for progress on autonomous vehicles, both to hold companies accountable for safety and to ensure that safety information is widely disseminated. Regulation has made safety a given in commercial aviation; airlines compete on routes, pricing, and service, not how safe they are. Similarly, the autonomous vehicle industry’s commitment to safety is something that consumers should be able to take for granted. And, while we’re at it, let’s build sensible rules for testing AI in other critical contexts such as health care, education, and criminal justice.", "image_caption": "Volvo car identifying a pedestrian", "metadata": {"article_id": "issue_13", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/uber.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-13/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_13.html"}}
{"id": 7729130002, "type": "news_chunk", "title": "Self-Driving Cars That Can't See Pedestrians? Evolutionary...", "subtitle": "Spotlight on Stock Scammers", "content": "The world’s largest stock market is using AI to flag suspicious trading in real time.What’s new: Nasdaq is testing a deep learning system to monitor trading of its U.S. equities. Named Chiron, the system watches for behaviors that indicate potential market manipulation.How it works: Nasdaq will spend a year training Chiron on trade data annotated for signs of manipulation. The system is designed to alert human overseers when it sees patterns that suggest scams such as spoofing, in which a trader attempts to devalue a stock by selling a huge volume to trigger others to dump their shares as well.Nasdaq’s fraud-detection team reviews around 750,000 trades annually. The system is intended to reduce false positives so the team can focus on serious cases.The company aims to integrate Chiron with its broader SMARTS trade surveillance program, which watches the markets using human analysts and traditional computing. Behind the news: This isn’t Nasdaq’s first foray into AI. In 2001, the company launched a program called Sonar to monitor sources like news stories and SEC filings for suspicious activity.Why it matters: Nasdaq operates 29 exchanges in the U.S., Canada, UK, and EU, and it licenses its surveillance technology to other exchanges, regulatory agencies, and financial firms around the world. It has the highest volume of trades of any exchange in the world. Widespread fraud within Nasdaq’s network not only would be catastrophic for its business, it could send shock waves through the global economy.We’re thinking: Fraudsters have access to deep learning, too. Expect a high-stakes game of cat and mouse in the years to come.", "image_caption": "Nasdaq", "metadata": {"article_id": "issue_13", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/nasdaq.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-13/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_13.html"}}
{"id": 7729130003, "type": "news_chunk", "title": "Self-Driving Cars That Can't See Pedestrians? Evolutionary...", "subtitle": "Finer Tuning", "content": "A word-embedding model typically learns vector representations from a large, general-purpose corpus like Google News. But to make the resulting vectors useful in a specialized domain, such as veterinary medicine, they must be fine-tuned on a smaller, domain-specific dataset. Researchers from Facebook AI offer a more accurate method.What’s new: Rather than fine-tuning, Piotr Bojanowski and colleagues developed a model that aligns word vectors learned from general and specialized corpora.Key insight: The authors drew inspiration from the way multilingual word vectors are learned. They treated general-purpose and domain-specific corpora as separate languages and used a word-embedding model to learn independent vectors from each. Then they aligned the vectors from one corpus with those from another.How it works: To align word vectors from two corpora, common words are used to find a consistent way to represent all words. For example, if one corpus is {human,cat} and the other is {cat,dog}, the model applies a transformation that unifies the dog word vectors while retaining the relative positions of the word vectors between cats, dogs, and humans. A word-embedding model learns independent word vectors from both corpora.For words that appear in both corpora, the alignment model learns a linear mapping from general-purpose vectors to domain-specific vectors. The mapping solves a linear equation that minimizes the distance between the general-purpose vectors and the domain-specific vectors.The authors use a loss function called RCSLS for training. RCSLS balances two objectives: General-purpose vectors that are close together remain close together, while general-purpose vectors that are far apart remain far apart.Common words in the two corpora now have duplicate vectors. Averaging them produces a single vector representation. Results: The authors tested this approach to learning word vectors on tasks that include predicting analogies and text classification in a dataset where the test set has a slightly different word usage than the training set. Models that use word vectors learned via alignment outperformed those that use word vectors fine-tuned in the usual way. The new method’s advantage was more pronounced when the domain-specific dataset was relatively small.Why it matters: Machine learning engineers need tools that enable existing word representations to capture specialized knowledge. The alignment technique could be a boon in any situation where general-purpose word vectors don’t capture the meanings at play.We’re thinking: Open-source, pretrained word embeddings have been a boon to NLP systems. It would be great to have freely available word embeddings that captured knowledge from diverse fields like biology, law, and architecture.", "image_caption": "Word vectors", "metadata": {"article_id": "issue_13", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/BatchGraphicNovember132019.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-13/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_13.html"}}
{"id": 7729130004, "type": "news_chunk", "title": "Self-Driving Cars That Can't See Pedestrians? Evolutionary...", "subtitle": "Survival of the Overfittest", "content": "Neuroevolution, which combines neural networks with ideas drawn from Darwin, is gaining momentum. Its advocates claim that they can achieve faster, better results by generating a succession of new models, each slightly different than its predecessors, rather than relying on a purpose-built model.What’s new: Evolutionary strategies racked up a number of successes in the past year. They contributed to DeepMind’s AlphaStar, which can beat 99.8 percent of players of StarCraft 2, and to models that bested human experts in the videogames Montezuma’s Revenge and Pitfall Harry. An article in Quanta surveys the field, focusing on neuroevolution pioneer and Uber senior researcher Kenneth Stanley.How it works: Traditionally, evolutionary approaches have been used to generate algorithms that solve a specific problem or perform best on a particular task. The best solutions are randomly mutated to find variations that improve performance. Neuroevolution applies random mutations to neural network weights and sometimes activation functions, hyperparameters, or architectures. Good models emerge over many iterations, sometimes crossing traits among many behavioral niches. Uber AI Labs developed an algorithm called Paired Open-Ended Trailblazer and used it to evolve populations of virtual bipedal robots as well as obstacle courses for the robots to master (shown in the animation above). As the bots learned how to walk over, say, hills, the algorithm randomly moves them to environments where they encountered trenches. Agent-obstacle pairs are mutated, ranked for fitness and novelty, and then interbred. Ultimately, the agents learn skills they couldn’t learn through direct optimization.DeepMind used evolutionary techniques along with deep learning and reinforcement learning to sharpen AlphaStart’s StarCraft 2 skills. The researchers bred models not to defeat one another outright but to employ off-kilter tactics and exploit weak points encountered in previous matches. The resulting model proved to be robust against a wide variety of strategies. Yes, but: Evolutionary strategies require huge amounts of computation, even by the power-hungry standards of deep learning. Weights and other variables evolve randomly, so finding good models can take a long time. The random path itself is a drawback. Although researchers may set out to solve one problem, the evolutionary process may lead in other directions before wending its way back to the intended path — if it ever does.Why it matters: Neuroevolution is a radical departure from typical neural networks and, by some accounts, a useful complement. Evolutionary approaches assign a far larger role to randomness, and randomly beneficial effects can compound over generations to find solutions or generate networks more effective than a human would have designed.We’re thinking: Randomized search algorithms are a powerful approach to optimization, but their relation to biological evolution has been a subject of debate. With rising computational power and more complex challenges, such algorithms — whether evolutionary or not — may be poised to grow.", "image_caption": "Bipedal robot crossing obstacles", "metadata": {"article_id": "issue_13", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/walker.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-13/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_13.html"}}
{"id": 7729130005, "type": "news_chunk", "title": "Self-Driving Cars That Can't See Pedestrians? Evolutionary...", "subtitle": "Fish Recognition", "content": "A deep learning system is helping biologists who survey offshore fish populations to prevent overfishing.What’s new: The U.S. agency in charge of protecting ocean resources is using an underwater camera and neural network to count fish in real time.How it works: Alaska’s walleye pollock fishery is America’s largest by volume. (You may not recognize a walleye pollock, but you’ve probably eaten one in fish sticks, fast-food sandwiches, or imitation crab meat. They are delicious!) Scientists with the U.S. National Oceanic and Atmospheric Administration chose this fishery as a pilot in their automatic fish-identification program. NOAA scientists dragged a long, funnel-shaped net through the water. Fish caught in the wide mouth are allowed to escape through the narrow, open end, passing in front of a stereoscopic CamTrawl camera system as they exit.Next to CamTrawl, a computer in a hermetically-sealed container runs a fish-recognition network called Viame. This video shows the user interface in action.The biologists do more than count fish. They also need to know the fishes’ average age to calculate a healthy number for fishermen to catch. Viame triangulates each specimen’s length, a reliable indicator of its age.NOAA is also using Viame to count scallops, reef fish, and endangered seals. Behind the news: Congress passed the Sustainable Fisheries Act in 1996, requiring NOAA to track U.S. commercial fish populations. For some fisheries, the biologists venture out on boats, casting nets to capture samples of what’s in the water. They dump the contents onto the deck, count and measure each creature, release the haul, and cast the net again. NOAA launched the initiative to automate these counts using artificial intelligence in 2014.Why it matters: Fish stock assessments, and the limits they impose on commercial fishing, keep fish populations sustainable and fisheries productive over the long term. Automating the process reduces error and frees up biologists for other work.We’re thinking: Deep learning is producing more and better data for environmental stewardship. It’s up to citizens to put that data to best use.", "image_caption": "Underwater camera detecting fish", "metadata": {"article_id": "issue_13", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/fish.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-13/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_13.html"}}
{"id": 7729130006, "type": "news_chunk", "title": "Self-Driving Cars That Can't See Pedestrians? Evolutionary...", "subtitle": "Convolution Revolution", "content": "Looking at images, people see outlines before the details within them. A replacement for the traditional convolutional layer decomposes images based on this distinction between coarse and fine features.What’s new: Researchers at Facebook AI, National University of Singapore, and Yitu Technology devised OctConv, a convolutional filter that reduces the computational cost and memory footprint of image processing networks without degrading performance.Key insight: Yunpeng Chen and collaborators took their inspiration from signal processing: An audio signal can be represented as a set of discrete frequencies rather than a single waveform. Similarly, an image can be said to contain low-frequency information that doesn’t change much across space and high-frequency imagery that does. Low-frequency image features are shapes, while high-frequency image features comprise details such as textures. By capturing them separately, OctConv can reduce redundant information.How it works: The outputs of a convolutional layer’s hidden units are feature maps that hold 2D spatial information. Feature maps often encode redundant information across an image’s color channels. OctConv cuts this redundancy by using a frequency-channel representation instead of the usual color-channel representation. In OctConv, each channel of a convolutional layer encodes either low- or high-frequency data. Low-frequency channels downsample the feature map, while high-frequency channels retain the feature map’s original resolution. A user-defined parameter controls the ratio of low- to high-frequency channels.Separate resolution filters share information between high- and low-frequency channels. Four filters account for all combinations of the channel inputs and outputs. While this arrangement may appear to require four times as many parameters as a standard convolutional layer, low-frequency channels have 50 percent resolution, resulting in fewer total parameters. Results: A ResNet-152 with OctConv rather than CNN filters was 0.2 percent more accurate on ImageNet than the next best model, with 15 percent less computation during testing. An I3D model pair with OctConv filters was 2 percent more accurate on Kinetics-600, a video dataset for predicting human actions, with 10 percent less computation.Why it matters: OctConv filters can substitute for standard convolutional filters for better performance, reduced computation, and smaller footprint. The authors suggest subdividing beyond their low- and high-frequency scheme. That would yield greater savings in size and training time, but its impact on performance is a subject for further experimentation.Takeaway: Memory compression and pruning techniques have been important for deploying neural networks on smartphones and other low-powered, low-memory devices. OctConv is a fresh approach to shrinking image-processing networks that takes into account memory and computation primitives.", "image_caption": "OctConv example", "metadata": {"article_id": "issue_13", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/octaveSIZED201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-13/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_13.html"}}
{"id": 51118574001, "type": "news_chunk", "title": "Artificial Noses, Surveillance on Wheels, Unwelcome Researchers", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, My last two letters explored robustness and small data as common reasons why AI projects fail. In the final letter of this three-part series, I’d like to discuss change management. Change management isn’t an issue specific to AI, but given the technology’s disruptive nature, we must pay attention to it if we want our projects to succeed. An AI system that, say, helps doctors triage patients in an emergency room affects many stakeholders, from doctors to the intake nurses to the insurance underwriters. To keep projects on track, people must be brought onboard and systems must be adjusted. I recently saw a union block even small-scale experiments because of fear that AI would automate jobs away. This was unfortunate, because the AI system being contemplated would have made employees more valuable without reducing employment. A change management process could have made the stakeholders comfortable with experimenting and helped them understand why it was worthwhile rather than threatening. Many engineers underestimate the human side of change management. Some tips: Budget enough time. Change management requires asking lots of questions, assessing how various roles will change, and explaining to many people what the AI will do.Identify all stakeholders. Either communicate with them directly or find ways to have colleagues talk to them. Many organizations make decisions by consensus, and it is important to minimize the odds of any stakeholder blocking or slowing down implementation. We also need to build trust among stakeholders that the AI will work.Provide reassurance. Where possible, explain to people how their work may change and how the new system will benefit them.Explain what’s happening and why. There is still significant fear, uncertainty and doubt (FUD) about AI. I have seen that providing a basic education — along the lines of the AI for Everyone curriculum — eases these conversations. Other tactics including explainability, visualization, rigorous testing, and auditing also help build trust in an AI system and convince our customers (and ourselves!) that it really works.Right-size the first project. If it is not possible to start with a complex deployment that affects a lot of people, consider starting with a smaller pilot (The AI Transformation Playbook includes helpful perspective on this) that affects a smaller number of stakeholders, and is thus easier to get buy in. As we have seen with self-driving cars, building an AI system often involves solving a systems problem. That requires reorienting not only stakeholder roles and organizational structures, but also many things around the AI, like setting expectations with other drivers, pedestrians, and first responders and updating procedures around road maintenance and construction. Addressing the systems problem will increase the odds of your project succeeding. If you understand the problems of robustness, small data, and change management, and if you can spot these problems in advance and pre-empt them, you’ll be well ahead of the curve in building a successful AI project. Building AI projects is hard. Let’s keep pushing and share what we learn with each other, so we can keep moving the field forward! Keep learning! Nitin knew he needed to learn more to build the products he had in mind. So he took the Deep Learning Specialization and applied to jobs that would allow him to transition from web performance to machine learning. He sold LinkedIn on combining the two. Read more", "image_caption": "Nitin Pasumarthy and a quote from him", "metadata": {"article_id": "issue_14", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TheBatchFeaturedImageBreakingIntoAINitinPasumarthy201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-14/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_14.html"}}
{"id": 51118574002, "type": "news_chunk", "title": "Artificial Noses, Surveillance on Wheels, Unwelcome Researchers", "subtitle": "What the Watchbot Sees", "content": "Knightscope’s security robots look cute. But these cone-headed automatons, which serve U.S. police departments and businesses, are serious surveillance machines.What happened: Newly released documents including contracts, emails, and a company slideshow highlight Knightscope’s ability to gather information and track people. Medium’s OneZero tech website obtained the documents through a public records request and reported on their contents.How it works: The Southern California community of Huntington Park in November 2018 agreed to pay $240,000 to lease a Knightscope unit for three years. The 300-pound K5 patrol robot, which trundles on three wheels, senses its surroundings using optical cameras, lidar, and thermal imaging: The robot scans passersby using face recognition software and cars using a license plate reader. It compares captured images with police databases detailing persons of interest, flags matches, and sends alerts to law enforcement personnel.Remote users can monitor the robot’s cameras in real time and direct it to take actions such as issuing parking violations.The robot passively collects signals from nearby wireless devices. The slideshow describes how such records can be used to track individuals using a device’s MAC address.The robot saves data it collects for two weeks, during which time police can access it through an app or download it for their own use. Behind the news: Huntington Park’s police department is one of three in the U.S. currently using Knightscope’s robots. An unknown but rising number of private companies, including operators of shopping malls or large parking plazas, have leased the robots as well.Why it matters: Knightscope’s data-collection and -analysis features could violate privacy restrictions and laws in some cities and states. Privacy groups like the Electronic Frontier Foundation argue that face recognition and license plate readers violate individuals’ civil rights, and wireless sniffing could raise similar questions. Face recognition technology is illegal in San Francisco, Oakland, and Somerville, MA. A number of other cities have cancelled programs to procure automated license plate readers.We’re thinking: Automated security can save municipalities and businesses a lot of money. But we all could pay a price in civil liberties if we’re not careful about how the technology is deployed. Citizens should demand transparency from local governments about where surveillance equipment is situated and how captured data can be used and stored.", "image_caption": "Security robot walking on the street", "metadata": {"article_id": "issue_14", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Knightscope.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-14/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_14.html"}}
{"id": 51118574003, "type": "news_chunk", "title": "Artificial Noses, Surveillance on Wheels, Unwelcome Researchers", "subtitle": "Nose Job", "content": "Predicting a molecule’s aroma is hard because slight changes in structure lead to huge shifts in perception. Good thing deep learning is developing a sense of smell.What’s new: Benjamin Sanchez-Lengeling and a team from Google Brain, Arizona State University, and the University of Toronto developed a model that predicts a chemical’s smell from an embedding of its molecular structure.Key insight: A molecule is composed of atoms with bonds between them. Representing atoms as nodes and bonds as edges yields a graph ripe for processing by a graph neural network, or GNN.How it works: The researchers gathered about 5,030 molecules and 138 odor descriptions, such as “fruity” or “medicinal,” from the GoodScents and Leffingwell PMP 2001 fragrance databases. They treated each description as a class in a classification task. Their model included a GNN, a component that converts graphs into vectors, and a fully connected layer that performs classification. The GNN takes a graph representation of a molecule as its input and learns a more information-rich graph representation. The network learns a vector that describes each node. The network’s layers update these vectors based on the values of neighboring nodes. The model converts the enriched output graph to a vector by summing the values of each node’s neighbors.A sequence of feed-forward layers then classifies the molecule’s odor.The network’s penultimate layer encodes the molecule-scent embedding, which can be used for other tasks as well. For instance, the authors applied it to the DREAM Olfaction Prediction Challenge to predict an odor’s strength (“how fruity is this smell?”) on a scale of 1 to 100. Results: The GNN achieved a 5 percent higher F1 score than random-forest or nearest-neighbor methods trained on hand-crafted features. On the DREAM Olfaction Prediction Challenge, the authors matched the original winner’s 2015 score, even though their embedding wasn’t designed for this particular task.Why it matters: Chemists often struggle to predict properties of molecules based on their structure. This work suggests that deep learning can aid in the effort. Beyond predicting smells, the molecule-scent embedding is suited to transfer learning for other scent-related tasks and possibly generative methods that might, say, predict molecules having a particular scent.We’re thinking: One of the biggest challenges to building an artificial nose is not in the software, but in the hardware: How to build a sensor that can detect minute numbers of scent molecules in the air. This research could help design new fragrances, but further work in chemical sensing technology is also needed. Whoever cracks this problem will come up smelling like roses.", "image_caption": "Information related to a model that predicts a chemical's smell", "metadata": {"article_id": "issue_14", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ScentsSIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-14/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_14.html"}}
{"id": 51118574004, "type": "news_chunk", "title": "Artificial Noses, Surveillance on Wheels, Unwelcome Researchers", "subtitle": "Researchers Blocked at the Border", "content": "Foreign researchers hoping to attend one of AI’s largest conferences were denied entry into Canada, where the event will be held. Most of those blocked were from developing nations.What happened: This year’s Conference on Neural Processing Information Systems (NeurIPS) is being held in Vancouver, Canada, in December. The country’s Ministry of Immigration rejected visas for a number of researchers, mostly from Africa.Gatekeeping gaffe: It’s unclear exactly how many researchers were blocked, but organizers for the conference’s Black in AI workshop said they were aware of around 30 people affected. Canada’s immigration ministry screens visitors for signs that indicate they might overstay their visa. The agency, for example, flags individuals from countries that are poor or at war, especially if they also have relatives living in Canada.NeurIPS had anticipated that attendees might experience difficulties getting through Canada’s border, and had been working with immigration officials since May to help smooth the process.After NeurIPs organizers complained, officials allowed 15 researchers whose visas initially were denied to enter the country. The rest are still under review.“While we cannot comment on the admissibility of any particular individual, we can say that, in general, all visitors to Canada must meet the requirements for temporary residence in Canada, as set out in Canada’s Immigration and Refugee Protection Act,” the ministry told the BBC by email. Behind the news: This isn’t the first time Canada has blocked researchers seeking to attend NeurIPS. Over 100 researchers bound for last year’s event in Montreal were held back. At the 2018 meeting of the G7, Wired confronted Prime Minister Justin Trudeau over whether Canada’s immigration policy undermined its goal to become an AI powerhouse. In September, the Partnership on AI suggested a new visa category for AI researchers.Why it matters: Conferences aren’t just opportunities tor share ideas. They’re opportunities for researchers to form important professional relationships. Policies like Canada’s keep developers from developing economies on the margins. The International Conference on Learning Representations (ICLR) is holding its 2020 conference in Addis Ababa, Ethiopia, because of the difficulty African researchers have entering places like the U.S., UK, and Canada.We’re thinking: We encourage conferences to schedule meetings in developing nations. A global research community benefits all nations. We need to make sure the rewards of AI — and, more broadly, science — are shared fairly. Pushing hard to make knowledge accessible to all is the ethical thing to do.", "image_caption": "Leaf stuck on a chain link fence", "metadata": {"article_id": "issue_14", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Visas20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-14/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_14.html"}}
{"id": 51118574005, "type": "news_chunk", "title": "Artificial Noses, Surveillance on Wheels, Unwelcome Researchers", "subtitle": "When Private Data is Not Private", "content": "Google spent the past year training an AI-powered health care program using personal information from one of the largest hospital systems in the U.S. Patients had no idea — until last week.What happened: The tech giant gave the Ascension hospital network access to a system for managing healthcare information called Project Nightingale. In exchange, Ascension gave Google access to the medical records of up to 50 million patients, according to an exposé in the Wall Street Journal. The effort triggered an investigation by U.S. privacy regulators.How it works: Google designed Project Nightingale as a machine learning tool for matching patient information with healthcare decisions. Once trained, it would suggest treatment options or additional tests and highlight suggestions for special care based on a patient’s history. The system would perform administrative tasks, such as reassigning doctors based on changes in the patient’s condition or special needs. It would also enforce policies to prevent unlawful prescriptions and suggest ways to generate more income from patients.Ascension gave Google personal information (including patient names and addresses along with the names of patients’ family members) as well as medical records such as lab results, diagnoses, prescriptions, and hospitalizations.Ascension didn’t inform patients or doctors that it was sharing data with Google.At least 150 Google employees had access to the data. The controversy: The U.S. Health Insurance Portability and Accountability Act of 1996 (HIPAA) protects patient data from being used or shared for purposes unrelated to healthcare. The law allows providers to share data with business partners without telling patients as long as the goal is better care. Google and Ascension said Project Nightingale is intended for that purpose. However, regulators at the Department of Health and Human Services are concerned that the companies aren’t properly protecting data. They launched an investigation, which is in progress.We’re thinking: We need Google, Ascension, and other organizations to keep innovating in healthcare. But we also need rules that are crystal clear about allowable uses of sensitive health data. When HIPAA was passed, public information about how AI works was far less available and data sharing among companies was much less common. An update is long overdue.", "image_caption": "Illustration of health related icons connected to a cloud", "metadata": {"article_id": "issue_14", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Nightingale20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-14/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_14.html"}}
{"id": 51118574006, "type": "news_chunk", "title": "Artificial Noses, Surveillance on Wheels, Unwelcome Researchers", "subtitle": "Beyond the Bounding Box", "content": "Computer vision models typically draw bounding boxes around objects they spot, but those rectangles are a crude approximation of an object’s outline. A new method finds keypoints on an object’s perimeter to produce state-of-the-art object classification.What’s new: Ze Yang and researchers from Peking University, Tsinghua University, and Microsoft Research developed a network, RPDet, that extracts what the authors call representation points, or RepPoints.Key insights: Bounding boxes can be constructed from RepPoints, which enables RPDet to learn to derive RepPoints from bounding-box labels in standard object-recognition datasets. A good RepPoint is one that helps to answer two questions: What is the bounding box, and what object does it enclose?How it works: RPDet uses feature pyramidal networks that extract a hierarchy of image features of varying levels of detail. From these features, it extracts a user-defined number of points as follows: The model starts by identifying the center point.It infers the remaining points from that one using deformable convolutions. Typical convolutions learn only weights, and they’re appropriate for bounding boxes because of their rectangular structure. Deformable convolutions learn offsets as well. The offsets define a custom shape, as opposed to the usual grid.The model constructs a bounding box around the RepPoints by finding the smallest box that contains all points. RPDet is trained via backpropagation to match bounding box corners in the training data.Having located objects by finding their RepPoints, RPDet classifies the objects. This additional task encourages RPDet to identify important locations on an object and avoid fixating on bounding-box corners. Results: Processing image features supplied by a ResNet, RPDet achieved a 2 percent boost in classification accuracy over bounding-box representations. Further, RPDet achieves a new state of the art for precision on COCO, an object detection and classification dataset, with 4 percent improvement in average precision over the alternatives considered.Why it matters: This technique encodes relatively detailed information about object shapes that could be useful in a variety of tasks. For instance, RepPoints’ implicit estimation of poses could help predict the trajectory of a moving object.We’re thinking: Plenty of applications, including face recognition, find explicit predefined keypoints. But they tend to be specialized for specific types of objects, such as finding the eyes, nose, and mouth on faces. RepPoints encode arbitrary geometry and pose information for a wide range of shapes, giving them a potential role in applications that otherwise wouldn’t be feasible.", "image_caption": "Data related to RPDet", "metadata": {"article_id": "issue_14", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/RepPoints20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-14/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_14.html"}}
{"id": 22636417001, "type": "news_chunk", "title": "Top 100 AI Startups, Inside the Mind of DALL·E 2, Child Welfare", "subtitle": "Child-Welfare Agency Drops AI", "content": "Officials in charge of protecting children stopped using a machine learning model designed to help them make decisions in difficult cases.What’s new: The U.S. state of Oregon halted its use of an algorithm intended to identify children who may benefit from intervention, The Associated Press reported. The state did not disclose the reason for the move. It came roughly one month after a similar algorithm used by the state of Pennsylvania, which inspired Oregon’s effort, came under criticism for bias.How it works: Oregon’s Department of Human Services developed the Safety at Screening Tool to help social workers screen reports of at-risk children. Social workers were empowered to decide whether to take action with respect to any given report. The developers trained the algorithm on hundreds of thousands of existing child-welfare reports. The dataset included over 180 features including reports of abuse or neglect, numbers of children per report, and whether those children had been involved in previous reports.They trained two models. One, trained on reports that had prompted an investigation, determined the probability that a child would be removed from their home within two years. The other, trained on reports that hadn’t prompted an investigation, found the probability that a child would be involved in a future investigation. At inference, the models examined a report and produced separate scores.The developers acknowledged that bias was inevitable but sought to mitigate it by separately modeling the probabilities of removal from a home and involvement in a future investigation, and by scoring on a scale of 0 to 100 rather than 0 to 20, the scale used in previous work.The department told its employees that it would stop using the tool at the end of June. An official told The Associated Press that a change in the screening process had made the tool unnecessary. Pennsylvania’s problem: Researchers at Carnegie Mellon University found signs of bias in a similar tool used in Pennsylvania. That algorithm, which assesses the probability that a child will enter foster care within two years, is still in use. The researchers found that the algorithm disproportionately flagged cases involving Black children relative to their White counterparts. They also found that social workers — who were authorized to make decisions — displayed significantly less racial disparity than the algorithm.Officials countered that the analysis used old data and a different method for pre-processing data.The researchers undertook a second analysis using newer data and the officials’ recommended pre-processing steps. They reached the same conclusion. Why it matters: Oregon’s decision to drop its learning algorithm sounds a note of caution for public agencies that hope to take advantage of machine learning. Many states have applied machine learning to ease the burden on social workers as both the number of child welfare cases has risen steadily over the past decade. However, the effort to automate risk assessments may come at the expense of minority communities whose members may bear the brunt of biases in the trained models.We’re thinking: We’re heartened to learn that independent researchers identified the flaws in such systems and public officials may have acted on those findings. Our sympathy goes out to children and families who face social and economic hardships, and to officials who are trying to do their best under difficult circumstances. We continue to believe that AI, with robust auditing for bias, can help.", "image_caption": "Diagram about probability scores", "metadata": {"article_id": "issue_148", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/06/OREGON.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-148/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_148.html"}}
{"id": 22636417002, "type": "news_chunk", "title": "Top 100 AI Startups, Inside the Mind of DALL·E 2, Child Welfare", "subtitle": "Tech Imitates Life, Life Imitates Art", "content": "The computational systems known as cellular automata reproduce patterns of pixels by iteratively applying simple rules based loosely on the behavior of biological cells. New work extends their utility from reproducing images to generating new ones.What’s new: Rasmus Berg Palm and colleagues at IT University of Copenhagen developed an image generator called Variational Neural Cellular Automata (VNCA). It combines a variational autoencoder with a neural cellular automaton, which updates pixels based on the output of a neural network and the states of neighboring pixels.Key insight: A variational autoencoder (VAE) learns to generate data by using an encoder to map input examples to a distribution and a decoder to map samples of that distribution to input examples. Any architecture can serve as the decoder, as long as it can reconstruct data similar to the inputs. Given a distribution, a neural cellular automaton can use samples from it to generate new, rather than predetermined, data.How it works: VNCA generates pixels by updating a grid of vectors, where each vector is considered a cell and each cell corresponds to a pixel. The encoder is a convolutional neural network, and the decoder is a neural cellular automaton (in practical terms, a convolutional neural network that updates vectors depending on the states of neighboring vectors). The authors trained the system to reconstruct images in the MNIST dataset of handwritten digits. The encoder learned to map an input image to the parameters of a Gaussian distribution. The system used this distribution to produce a two-by-two matrix of cells.The decoder updated each cell’s state based on that of its neighbors. After the update, the system duplicated each cell into a new two-by-two matrix while pushing other cells out of the way (see diagram above). This process was repeated to fill the output pixel resolution with cells.VNCA applied a sigmoid to the first value of each cell’s vector to determine the probability that the associated pixel should be white or black.The loss function encouraged the output image to be as similar as possible to the input while also encouraging the encoder to map images to the parameters of the standard Gaussian distribution, which has a mean of 0 and variance of 1.At inference, the decoder received a two-by-two matrix sampled from the standard Gaussian distribution and generated a new image accordingly. Results: The authors showed that a cellular automaton can generate images, though not very well at this point. They evaluated VNCA using log likelihoods in natural units of information (nats), which gauge similarity between the system’s output and the training data (higher is better). VNCA achieved -84.23 nats, worse than the -77 nats achieved on MNIST by state-of-the-art models such as NVAE and BIVA.Why it matters: This work demonstrates that a neural cellular automaton can generate new images. While it shows no clear advantage of using a neural cellular automaton in a VAE, the combination might lend itself to useful applications. For instance, neural cellular automata have an inherent regenerative ability: Deface an image, and they can regrow the damaged pixels. Thus a VNCA-type approach might be useful for image inpainting. Given an image, the encoder could map it to a Gaussian distribution. Then you could damage the image where you wanted to change it, sample from the distribution, and use the decoder to generate novel pixels in that area.Yes, but: This approach may be challenging to scale. VNCA’s decoder used only 1.2 million parameters rather than the hundreds of millions used in other high-performing decoders. Adding parameters would increase its computational cost significantly, since it updates cells repeatedly based on the states of neighboring cells.We’re thinking: Deep learning offers a widening array of neural image generators: GANs, VAEs, diffusion models, normalizing flows, and more. While each has its advantages and disadvantages, together they amount to an enticing playground for synthesizing data and producing visual art.", "image_caption": "Diagram explaining the variational neural cellular automata", "metadata": {"article_id": "issue_148", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/06/AUTOMATA--1-.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-148/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_148.html"}}
{"id": 18798890001, "type": "news_chunk", "title": "Google Engineer Claims Sentient AI, Meta Reorgs AI Division", "subtitle": "LaMDA Comes Alive?", "content": "A chatbot persuaded at least one person that it has feelings.What’s new: A senior engineer at Google announced his belief that the company’s latest conversational language model is sentient. Google put the engineer on administrative leave.Is there anybody in there? LaMDA is a family of transformer-based models, pretrained to reproduce 1.56 trillion words of dialog, that range in size from 2 billion to 137 billion parameters. Google previously discussed plans to incorporate it into products like Search and Assistant. After pretraining, given a prompt, LaMDA generated a number of possible responses. The developers collected a set of conversations with LaMDA and hired people to rate how sensible, specific, interesting, and safe its responses were. Then they fine-tuned the model to generate those ratings at the end of each response. LaMDA replies with the highest-rated response. To further improve its factual output, they fine-tuned it to mimic the hired workers’ searches and, in such cases, direct an external system to search rather than return the response. Given the previous input and new search results, the model produces new output — which may be a new response or a further search.Researchers in Google’s Responsible AI group tested chatbots based on LaMDA to determine their propensity for hate speech and other toxic behavior. The process persuaded researcher Blake Lemoine that the model possessed self-awareness and a sense of personhood. He transcribed nine conversations between the model and Google researchers and submitted an argument that LaMDA is sentient. In one transcript, a chatbot says it believes it’s a person, discusses its rights, and expresses a fear of being turned off.Google placed Lemoine on administrative leave in early June for violating confidentiality by hiring a lawyer to defend LaMDA’s right to exist and speaking to a member of the U.S. House Judiciary Committee about what he regarded as ethical violations in Google’s treatment of the LaMDA. “Lemoine was told that there was no evidence that LaMDA was sentient (and lots of evidence against it),” a company spokesperson told The Washington Post.In a blog post, Lemoine writes that Google’s decision to discipline him follows a pattern of unfair treatment by the company towards its ethics researchers, charging that the company disregards the concerns of ethics researchers and punishes them for speaking up. He cites the company’s earlier dismissal of Ethical AI co-leads Timnit Gebru and Margaret Mitchell. What they’re saying: Many members of the AI community expressed skepticism of Lemoine’s claims via social media. Melanie Mitchell, professor at the Santa Fe Institute, said, “It's been known for forever that humans are predisposed to anthropomorphize even with only the shallowest of signals...Google engineers are human too, and not immune.”Why it matters: The propensity to anthropomorphize machines is so strong that it has a name: The Eliza Effect, which refers to a mid-1960s chatbot that persuaded some patients that it was a human psychotherapist. Beyond that, the urge to fall in love with one’s own handiwork is at least as old as the ancient Greek story of Pygmalion, a fictional sculptor who fell in love with a statue he created. We must strive to strengthen our own good judgment even as we do the same for machines.We’re thinking: We see no reason to believe that LaMDA may be sentient. While such episodes are a distraction from the important work of harnessing AI to solve serious, persistent problems (including machine sentience), they are a reminder to approach extraordinary claims with appropriate skepticism.", "image_caption": "Scrolling text", "metadata": {"article_id": "issue_149", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/06/LAMDA--1-.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-149/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_149.html"}}
{"id": 7033033001, "type": "news_chunk", "title": "Sony Goes AI, Intel's GPU Killers, Transformer Networks In", "subtitle": "A Sleeping Giant Stirs", "content": "Dear friends, I’ll be spending Thanksgiving with Nova and watching her taste turkey for the first time. To those of you who celebrate Thanksgiving, I hope you spend time with loved ones, reflect on what you are thankful for, and discuss some very important topics around the dinner table: Should you get a real dog or a Boston Dynamics Spot?How can we keep the kids from using GPT-2 to write school essays?What do you say to Uncle Harold who thinks Siri is sentient? In AI, all of us should be thankful to stand on the shoulders of those who came before. I’ll leave you with one thought: What can you do now so that, in the future, dozens or more will feel thankful toward you? Let’s work together to help each other, and thereby move the world forward. Keep learning! Sony, the consumer-electronics powerhouse behind the PlayStation and other hit gadgets, is launching three research-and-design centers to focus on AI. Staffing up means competing with — and likely poaching talent from — frontrunners like Google, Facebook, and Microsoft.What’s new: The company next month will open AI offices in Tokyo, Austin, and a European city to be named. The company says it will hire local machine learning engineers. It hasn’t said how many it will employ.The plan: Hiroaki Kitano, president of Sony’s Computer Science Laboratories, will lead the effort. His vision encompasses three areas: Gaming, sensing and hardware, and — surprise! — gastronomy. Sony provided few details, but other news offers clues: Gaming: In September, Sony filed for a patent on an AI assistant to guide gamers through tricky spots by, say, adding markers to health stations. Gaming insiders speculate that AI could produce more realistic enemies and interactions with the game world.Sensors: Sony is a top maker of chips that turn light into electrons for devices like digital cameras. Sales of these CMOS sensors brought in $1.8 billion in the second quarter of 2019, 20 percent of total revenue. AI could improve the chips’ ability to sense depth.Gastronomy: The company wants to analyze the sensory aspects of food to create new dishes. Food-service automation also may be in the mix: Last year, Kitano oversaw research at Carnegie Mellon University developing robots for meal prep, cooking, and delivery. Behind the news: Sony’s Computer Science Laboratories is known for its independence, secrecy, and freedom to pursue blue-sky projects. The division’s most notable product is Aibo, the AI-powered robot dog. It also did pioneering research in augmented reality and developed video conferencing protocols. Why it matters: Sony invested in AI in the 1990s and early 2000s, but it sat out the deep learning revolution. With AI centers in the U.S. and Europe, the Japanese company likely will focus on consumer products and experiences while competing for talent with companies that dove into deep learning head-first.We’re thinking: Kitano has passion and clout, but he also has an awful lot on his plate. Outside of Sony, he’s the founding president of the RoboCup Federation, an international group of computer scientists aiming to win the 2050 World Cup with a team of robot soccer players. Meanwhile, he runs the nonprofit Systems Biology Institute and holds a professorship at the Okinawa Institute of Research and Technology.", "image_caption": "Robot cooking and serving food", "metadata": {"article_id": "issue_15", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Sony.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-15/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_15.html"}}
{"id": 7033033002, "type": "news_chunk", "title": "Sony Goes AI, Intel's GPU Killers, Transformer Networks In", "subtitle": "Bias Goes Undercover", "content": "As black-box algorithms like neural networks find their way into high-stakes fields such as transportation, healthcare, and finance, researchers have developed techniques to help explain models’ decisions. New findings show that some of these methods can be fooled.What’s new: Two popular approaches to interpreting black-box algorithms include Local Interpretable Model-Agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP). Researchers at Harvard and the University of California Irvine found a way to trick these methods to make a deliberately biased model appear to be unbiased.Key Insight: LIME and SHAP expose bias by showing how a model might react to small perturbations in its input. Statistical analysis shows that the distribution of perturbed inputs is distinct from that of the original inputs, as shown in the image above. A malicious actor can take advantage of the difference.How it works: LIME and SHAP use a linear model, which is highly explainable, to mimic a black-box model’s decision with respect to any given input sample. The explainable model is built by feeding perturbed inputs to the black-box model and measuring how its output changes in response to the perturbations. Once the model is built, we can rank the features most important to the decision, highlighting any bias in the original model. Dylan Slack, Sophie Hilgard, and their colleagues introduce a way to hide such bias from LIME and SHAP by learning to recognize data they’ve altered. The authors’ malicious system includes a distribution classifier, a biased model, and an unbiased LIME/SHAP model.The distribution classifier learns to categorize incoming data as either unperturbed or perturbed.The classification determines whether to use the biased or unbiased model. Unperturbed data are sent to the biased model. Perturbed data go to the unbiased LIME/SHAP model.If the classifier is accurate, LIME/SHAP will see, and thus explain, only the unbiased model’s output, not the biased model’s output.When the malicious system is deployed, the real-world data are not perturbed, so the system uses the biased model. Results: The authors created malicious systems to run COMPAS (a criminal recidivism dataset), Communities And Crime (a dataset of violent crime and demographic information), and the German Credit loan-application dataset. The embedded biased models deliberately abused discriminatory features such as race and gender, while the unbiased models used only features which were uncorrelated with discriminatory features. A malicious system biased on one feature (say, race) fooled LIME every time and SHAP in 85 percent of cases. A malicious system biased on two features fooled LIME over 90 percent of the time and SHAP 67 percent of the time.Why it matters: The authors’ approach highlights LIME’s and SHAP’s reliance on generating novel data. If these methods were to generate data more similar to the training data’s distribution, the method would fail. This may be a promising avenue for explainability research. Meanwhile, Duke University computer scientist Cynthia Rudin proposes avoiding black-box models in high-stakes situations. The AI community needs to hold a vigorous discussion about when such models are and aren’t appropriate.We’re thinking: If a major AI provider were caught using this technique, likely it would be vilified, which should provide some disincentive. We can imagine changes to LIME and SHAP that would counter a specific implementation, but this paper provides a dose of caution that checking for bias is not easy.", "image_caption": "Graph related to LIME and SHAP methods", "metadata": {"article_id": "issue_15", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Bias20SIZED201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-15/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_15.html"}}
{"id": 7033033003, "type": "news_chunk", "title": "Sony Goes AI, Intel's GPU Killers, Transformer Networks In", "subtitle": "Not Your Father’s GPU", "content": "Intel, which dominates the market for general-purpose processors, is shipping its long-awaited AI chips.What happened: The chip giant announced that two so-called neural network processors are available to data-center customers.How they work: One of the new chips is intended for training deep learning models, the other for inferencing. They’re designed to balance computational horsepower, communications speed, and memory capacity. The NNP-T1000, also called Spring Crest, takes on the Nvidia GPUs that process many AI training workloads. The new chip focuses on matrix multiplication, linear algebra, and convolution. It’s designed to scale out efficiently from small clusters to supercomputers and comes in the form of a card that plugs into a rack.The NNP-I1000, also known as Spring Hill, is a modified version of Intel’s latest 10th-generation Core design. It trades some parts of that architecture for specialized inference engines. It scores competitively on the MLPerf benchmark running a ResNet50 compared to Nvidia’s T4 inference chip. It comes in the form of a sleeve that can be plugged into a general-purpose server.At a separate event, Intel announced its first data-center GPU, known as Ponte Vecchio, scheduled for delivery in 2021 — a direct shot at Nvidia’s market. Behind the news: While Intel chips process most AI inferencing in data centers, Nvidia leads in GPUs that speed up AI training. In 2016, Intel acquired Nervana, a startup devoted to next-generation AI chips. Meanwhile, however, the field has become crowded. Specialized designs have proliferated at a host of startups like Cerebras and tech giants like Google, while Qualcomm has been building inferencing capability into chips for low-powered devices like smartphones.Why it matters: There’s no such thing as too much processing power for machine learning. The faster we can train models, the more data we can absorb, and the faster we can innovate new network architectures and applications. And the faster users can run our models, the more value we can deliver. As for chip makers, they recognize that AI is the future: Neural networks’ voracious appetite for processing power likely will drive silicon sales for years.We’re thinking: Large cloud providers are consolidating computation, and that’s having a big impact on the chip business. Their concentrated buying power puts them in a strong position to demand lower prices. The cloud companies also want to make sure they have alternative providers of deep learning chips, so they’ll buy chips from several vendors rather than only the top one. All this is playing out against a backdrop of rapid growth of AI workloads. Expect intense competition and in the years ahead.", "image_caption": "Intel AI chip", "metadata": {"article_id": "issue_15", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chips20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-15/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_15.html"}}
{"id": 7033033004, "type": "news_chunk", "title": "Sony Goes AI, Intel's GPU Killers, Transformer Networks In", "subtitle": "Melding Transformers with RL", "content": "Large NLP models like BERT can answer questions about a document thanks to the transformer network, a sequence-processing architecture that retains information across much longer sequences than previous methods. But transformers have had little success in reinforcement learning — until now.What’s new: Research in reinforcement learning (RL) has focused primarily on immediate tasks such as moving a single object. Transformers could support tasks that require longer-term memory. However, past research struggled to train transformer-based RL models. Emilio Parisotto and a DeepMind team combined them successfully with Gated Transformer-XL, or GTrXL. This network can substitute directly for an LSTM in RL applications.Key insight: A transformer’s attention component models out-of-sequence relationships. Consider a block-stacking task where the first and sixth actions taken are the most important to predicting whether the stack will be in the right order. GTrXL modifies the transformer architecture to allow it to learn sequential relationships early on (say, between the first and second actions, where the first action places the initial block and the second identifies which block needs to be picked up next) before it has learned out-of-sequence relationships.How it works: GTrXL modifies the transformer network (TrXL) as shown in the diagram above. GTrXL replaces the typical transformer’s residual connections with gated connections. This reduces errors that otherwise could flow through the residual connections.GTrXL applies layer normalization to the transformer’s sub-components but not to the gated connections. This allows the network to preserve information, including information derived directly from the input, over many residual connections while maintaining the attention mechanism’s performance.These modifications allow the network to learn from the order of input data while the attention mechanism hasn’t learned to model longer-term relationships. The shorter-term relationships are easier to model early on in training, making the network more stable during training. Results: On DMLab 30, an RL environment that supports puzzle tasks requiring long-term memory, GTrXL outperformed the previous state of the art (MERLIN) averaged across all 30 tasks. It also outperformed an LSTM, the ubiquitous recurrent layer in RL research.Why it matters: LSTMs have been essential to sequence-processing neural networks that work on short-term data. GTrXL give such networks longer-term memory. Longer time horizons eventually may help boost performance in life-long learning and meta-learning.We’re thinking: Since the original paper describing transformer networks was published in 2017, researchers have developed extensions. This work continues to show that, when it comes to transformers, there’s more than meets the eye.", "image_caption": "Comparison between TrXL and GTrXL", "metadata": {"article_id": "issue_15", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-11-2720at2011.00.3120AM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-15/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_15.html"}}
{"id": 60686682001, "type": "news_chunk", "title": "Google AI Explains Itself, Neural Net Fights Bias, AI Demoralizes", "subtitle": "Solar Power Heats Up", "content": "Dear friends, Recently I wrote about major reasons why AI projects fail, such as small data, robustness, and change management. Given that some AI systems don't work, users and customers sometimes rightly wonder whether they should trust an AI system. How can we persuade people to trust an algorithm? Some important techniques are: Explainability. If an AI can explain its decisions, this helps to build trust or identify problems before they can impinge on trust. For instance, the New York State Department of Financial Services is investigating whether the Apple/Goldman Sachs credit card exhibits gender bias in setting credit limits. If the algorithm could explain its decisions, we could determine whether such bias was driving them.Testing. Many of us are willing to take medicinal drugs whose biochemical effects no one fully understands. We trust these drugs because they have passed randomized clinical trials and received FDA approval. Similarly, black-box AI algorithms might gain our trust by undergoing rigorous testing.Boundary conditions. Clearly specifying boundary conditions (where the AI is expected to work) also helps. For instance, machine learning engineers developing systems to read medical images may specify the allowable range of inputs (for instance, X-rays must be this bright, and with a certain resolution) and so we can test against these conditions. Gradual rollout. Rather than having AI make fully automated decisions on Day One, we can start by allowing it merely to assist humans. For example, an AI trained to read X-rays might assist radiologists in making diagnoses rather than replacing doctors outright. Over time, having collected enough data and improved image readers sufficiently, we would come to trust higher and higher levels of automation, perhaps even full automation.Auditing. Third-party audits would build trust that our algorithms have minimal or no gender, race, or other bias, and that they meet certain performance standards.Monitors and alarms. Even after deploying a system, we can make sure we receive alerts if something goes wrong. By designing mechanisms that escalate serious issues, we can ensure that problems are fixed in a timely way. Trust isn’t just about convincing others that our solution works. I use techniques like these because I find it at least as important to convince myself that a solution works, before I ask a customer to rely on it. Keep learning!Andrew Solar-thermal power plants concentrate the sun’s energy using huge arrays of mirrors. AI is helping those arrays stay in focus.What happened: Heliogen, a solar-thermal startup, developed a computer vision setup that tracks hundreds of mirrors at once. The system detects reflectors that go off kilter and adjusts them to concentrate sunlight. The system recently heated a boiler to 1,000 degrees Celsius, a temperature that allows for industrial processes. Serial entrepreneur and Heliogen founder Bill T. Gross delivers his pitch in this video.How it works: A solar-thermal plant's central feature is a tower topped by a boiler. Hundreds, sometimes thousands, of mirrors encircle the tower. By focusing heat on the boiler, they produce steam, which spins a turbine, generating electricity. However, factors like wind, ground subsidence, and natural warping can cause mirrors to drift out of focus, reducing the plant’s efficiency. Heliogen's system calibrates them automatically. Heliogen’s tower contains a plate designed to conduct heat for use in industrial processes like smelting steel and making concrete.Four cameras around the plate monitor the corners of each mirror. If light reflected by a corner is brighter than the center, the system sends a message to servo controllers to adjust the mirror accordingly. Why it matters: 1,000 degrees Celsius is a milestone; most solar-thermal plants reach half that temperature. But the company’s goal is 1,500 degrees. At this temperature, it’s possible to split atmospheric carbon dioxide and water into their constituent molecules of hydrogen and carbon. Heliogen aims to start by producing hydrogen to generate power via fuel cells. Ultimately, it aims to recombine hydrogen and carbon into hydrocarbon fuels — no fossils required.Yes, but: A two-part critique published by the news website CleanTechnica points out that Heliogen’s technology produces a hot spot high above ground, where the heat isn’t immediately useful and is difficult to transport. Moreover, industrial facilities would need to be very nearby, potentially casting shadows over the mirrors. “I think it’s more likely that Heliogen's core machine learning innovation regarding halo focusing will find a completely different niche outside of concentrated solar,” the author concludes.We’re thinking: Heliogen has intriguing technology, a seasoned leader, and a high-profile backer in Bill Gates. It's exciting to see AI helping to make cheaper, cleaner alternatives to highly polluting industrial processes.", "image_caption": "Solar panels", "metadata": {"article_id": "issue_16", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/solar-SMALL.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-16/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_16.html"}}
{"id": 60686682002, "type": "news_chunk", "title": "Google AI Explains Itself, Neural Net Fights Bias, AI Demoralizes", "subtitle": "Bias Fighter", "content": "Sophisticated models trained on biased data can learn discriminatory patterns, which leads to skewed decisions. A new solution aims to prevent neural networks from making decisions based on common biases.What’s new: Ehsan Adeli and a group at Stanford propose Bias-Resilient Neural Network, or BR-Net, an architecture that works with a classifier to minimize the impact of biases that are well understood. In the training data, we can label, say, race and gender (known as bias variables), and BR-Net will learn to prevent spurious correlations between those variables and the model's output classification.Key insight: Biases in data correlate with class labels. If one part of a network learns to predict this correlation, another can learn to minimize the predicted correlation. This adversarial scheme can mitigate bias.How it works: BR-Net comprises three neural networks. The feature extractor finds embeddings of input data. The classifier predicts class labels from the embeddings. The bias predictor predicts the correlation between embeddings and bias variables. Once labels for bias variables have been added to the data, training proceeds in three steps: First, the system maximizes classification accuracy: The feature extractor and classifier together learn to predict labels.Then it identifies the effects of bias variables: The bias predictor learns the correlation between embeddings and bias variables.Finally, it minimizes the influence of bias: The feature extractor learns to generate embeddings that don’t correlate with the bias variables’ labels.By iterating through these steps, the feature extractor generates embeddings that maximize the classifier’s performance and minimize the biased correlation between embeddings and labels. Results: The researchers used a VGG16 classifier with BR-Net to predict a person’s gender from a photo. They trained the model on the GS-PPB dataset. Because classifiers often perform poorly on darker faces, they labeled skin tone as a bias variable. BR-Net achieved 96.1 percent balanced accuracy (accuracy for each of six skin tones considered equally), an improvement of 2 percent. This indicates more consistent results across different skin colors than a VGG16 trained without BR-Net.Why it matters: Bias in AI is insidious and difficult to prevent. BR-Net offers a solution when sources of bias are known.We're thinking: Machine learning presents hard questions to society: Which biases should we avoid? How can we come to agreement about which to avoid? Who gets to decide in the end? In lieu of answers, the choices are in the hands of ML engineers.", "image_caption": "Information related to Bias-Resilient Neural Network (BR-Net)", "metadata": {"article_id": "issue_16", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/Bias-Resilience-SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-16/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_16.html"}}
{"id": 60686682003, "type": "news_chunk", "title": "Google AI Explains Itself, Neural Net Fights Bias, AI Demoralizes", "subtitle": "Prehistoric Pictures Rediscovered", "content": "Image analysis guided by AI revealed a 2,000-year-old picture dug into the Peruvian desert.What happened: Researchers analyzing aerial imagery shot over Peru found a pattern that looks like a three-horned humanoid holding a staff. The figure is roughly 16 feet across and may have served as a waypoint along an ancient path. Known as geoglyphs, such pictures were created by people who predated the arrival of Columbus by 1500 years. The sprawling patterns are visible only from higher elevations.How it works: Using manual methods, researchers at Yamagata University found more than 100 geoglyphs in satellite photos and other imagery from the region of southeastern Peru called the Nazca Pampa. But they had collected too much data from surrounding areas to search manually. So they teamed with IBM Japan to feed the data into PAIRS Geoscope, a cloud-based deep learning system that analyzes geospatial data. This video describes the project. Training Geoscope to find the images presented several challenges. The geoglyphs range in size from tens of feet to nearly a quarter-mile across. They depict birds, humans, reptiles, and abstract shapes. Some are drawn in thin lines, others are filled-in shapes. The system had to learn not to be fooled by river beds and roads, which can trace superficially similar shapes.The team trained the system on photos, lidar, and GIS data describing confirmed geoglyphs.The model selected more than 500 candidates within a three-square-mile test range. The team reviewed the candidates manually. They chose the most promising one and confirmed it in the field. Behind the news: The people who created the Nazca geoglyphs lived on the arid Peruvian plains, or pampas. They made these shapes by removing the top layer of pebbles to expose lighter-colored clay roughly six inches below. Conquistadors noted the geoglyphs in their travelogues as far back as the 1500s, but it wasn’t until the 1940s that researchers began studying their origin and purpose.Why it matters: Remote sensing techniques have spurred a renaissance in archaeology. They’ve helped uncover Mayan pyramids on Mexico’s Yucatan peninsula and abandoned cities in the Cambodian jungle.We’re thinking: Who wants to team with us to create a massive deeplearning.ai geoglyph to confuse and amuse future generations?", "image_caption": "Ancient geoglyph analysed by AI", "metadata": {"article_id": "issue_16", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/Nazca-SIZED-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-16/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_16.html"}}
{"id": 60686682004, "type": "news_chunk", "title": "Google AI Explains Itself, Neural Net Fights Bias, AI Demoralizes", "subtitle": "Google's AI Explains Itself", "content": "Google's AI platform offers a view into the mind of its machines.What’s new: Explainable AI (xAI) tools show which features exerted the most influence on a model’s decision, so users can evaluate model performance and potentially mitigate biased results.How it works: xAI is available to users of Google's Cloud AI platform and its AutoML models and APIs. The core of xAI is a pair of tools that provide graphs or heat maps (depending on the type of data) that show the relative importance of each attribute in a model’s prediction. The tool called Integrated Gradients is for neural networks and other models whose attributes are mathematically differentiated. It assigns each feature a Shapley value (a concept borrowed from game theory that measures how important each player is in a cooperative outcome) that grades its role in the predicted outcome. It’s appropriate for most neural nets, according to Google.Many ensemble neural nets and trees have non-differentiable attributes. The tool called Sample Shapley explains such cases. It grades each attribute after sampling permutations duplicated across the nets in an ensemble.Google Cloud AI also offers free access to its What-If tool, which compares the performance of two models on the same dataset. Why it matters: The ability to explain how AI models arrive at decisions is becoming a major issue as the technology reaches into high-stakes aspects of life like medicine, finance, and transportation. For instance, self-driving cars would fit more easily into existing regulatory and insurance regimes if they could explain their actions in case of an accident. Yes, but: Explainability techniques are not a silver bullet when it comes to mitigating bias. In an analysis of Google’s tools, Towards Data Science argues that, given the subtlety of hidden correlations that can lurk in datasets, xAI is too superficial to offer meaningful insight into bias, especially in ensemble neural networks. The industry as a whole would be better off formulating standards to ensure that datasets are as unbiased as possible, writes Tirthajyoti Sarkar, who leads machine learning projects at ON Semiconductor.Google itself acknowledges xAI’s limitations. A white paper details the many ways in which human bias can impinge on techniques that aim to explain AI decisions. For example, human analysts might disregard explanations highlighting attributes that trigger their own biases. We’re thinking: As machine learning engineers, we need ways to make sure our models are making good decisions. But we should also keep in mind that explainability has limits. After all, human decisions aren’t always explainable either.", "image_caption": "Information related to Explainable AI (xAI)", "metadata": {"article_id": "issue_16", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/Explainability-SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-16/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_16.html"}}
{"id": 60686682005, "type": "news_chunk", "title": "Google AI Explains Itself, Neural Net Fights Bias, AI Demoralizes", "subtitle": "Bigger Corpora, Better Answers", "content": "Models that summarize documents and answer questions work pretty well with limited source material, but they can slip into incoherence when they draw from a sizeable corpus. Recent work by Facebook AI Research and Université de Lorraine’s computer science research lab addresses this problem.What’s new: Angela Fan and collaborators developed a model for multi-document summarization and question answering. While most previous efforts combine all input documents into one, the authors improved the state of t he art by representing them in a more compact form.Key insight: The combined length of major source documents pertaining to a given topic overwhelms current language models' ability to extract meaning. A knowledge graph squeezes out irrelevant and redundant information, enabling models to work more effectively.How it works: The authors’ method involves three steps: constructing a knowledge graph from source documents, encoding the graph as a sequence of words, and extracting information from the sequence. The model reads a set of source documents and converts each sentence into a (subject, object, relationship) triplet. It transforms each triplet into two nodes corresponding to the subject and object plus an edge between them that represents their relationship. Nodes and edges also capture the number of times a given subject, object, or relationship appears, reducing redundancy.For each word, a word embedding encodes meaning and a position embedding encodes relative position. A graph-weight embedding captures the number of times a node or edge appears and a query-relevance embedding reflects a given source document’s relevance to the latest input query. These embeddings combine to yield the vector representation of the graph.The model flattens the graph by concatenating triplets.At this point, the input is much smaller but still large. A modified attention mechanism finds the most salient parts of the graph and focuses there while generating output text. Results: The authors tested their model on a question answering task based on the dataset called Explain Like I'm Five (ELI5).This dataset contains 270,000 question-answer pairs along with source documents (the top 100 web sources from the CommonCrawl corpus for each question). The graph approach edged out the earlier state of the art on F1 for ROUGE-1 (30 percent versus 28.9 percent). They also compared performance on the WikiSum dataset for multi-document summarization using an article’s title as the input query, the footnotes as source documents, and the first paragraph as the target summary. The graph approach underperformed the previous ROUGE-L state of the art 36.5 percent to 38.8 percent, but the comparison wasn't apples-to-apples. The previous research supplemented the corpus with a web search, while the new work used only CommonCrawl. Why it matters: This research shows that natural language generation based on very large bodies of input text can work well. It also shows that source documents don’t need to be composed of well formed sentences. New ways of representing source documents may well lead to better language generation.We’re thinking: Many search engines produce summaries or answer questions by choosing the most relevant document. The ability to draw on any number of documents could enable such models to deliver a far wider diversity of information, leading to better research tools and ultimately a better-informed public.", "image_caption": "Information about a model for multi-document summarization and question answering", "metadata": {"article_id": "issue_16", "chunk_index": 5, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/Knowledge-Graph-SIZED-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-16/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_16.html"}}
{"id": 60686682006, "type": "news_chunk", "title": "Google AI Explains Itself, Neural Net Fights Bias, AI Demoralizes", "subtitle": "Is AI Making Mastery Obsolete?", "content": "Is there any reason to continue playing games that AI has mastered? Ask the former champions who have been toppled by machines.What happened: In 2016, International Go master Lee Sedol famously lost three out of four matches to DeepMind’s AlphaGo model. The 36-year-old announced his retirement from competition on November 27. “Even if I become the number one, there is an entity that cannot be defeated,” he told South Korean's Yonhap News Agency,Stages of grief: Prior to the tournament, Lee predicted that he would defeat AlphaGo easily. But the model’s inexplicable — and indefatigable — playing style pushed him into fits of shock and disbelief. Afterward, he apologized for his failure to the South Korean public.Reaching acceptance: Garry Kasparov, the former world-champion chess player, went through his own cycle of grief after being defeated by IBM’s DeepBlue in 1997. Although he didn’t retire, Kasparov did accuse IBM’s engineers of cheating. He later retracted the charge, and in 2017 wrote a book arguing that, if humans can overcome their feelings of being threatened by AI, they can learn from it. The book advocates an augmented intelligence in which humans and machines work together to solve problems.The human element: Although AlphaGo won in the 2016 duel, its human opponent still managed to shine. During the fourth match, Sedol made a move so unconventional it defied AlphaGo’s expectation and led to his sole victory.We’re thinking: Lee wasn't defeated by a machine alone. He was beaten by a machine built by humans under the direction of AlphaGo research lead David Silver. Human mastery is obsolete only if you ignore people like Silver and his team.", "image_caption": "AlphaGo playing Go with Lee Sedol", "metadata": {"article_id": "issue_16", "chunk_index": 6, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/Sedol-SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-16/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_16.html"}}
{"id": 43586900001, "type": "news_chunk", "title": "Prompting DALL·E for Fun and Profit, Teaching Language Models New Info, Court Rules Against AI Proctoring and more", "subtitle": "Celebrating Our Community Leaders", "content": "Dear friends, Stable Diffusion, an image generation model that takes a text prompt and produces an image, was released a few weeks ago in a landmark event for AI. While similar programs like DALL·E and Craiyon can be used via API calls or a web user interface, Stable Diffusion can be freely downloaded and run on the user’s hardware. I'm excited by the artwork produced by such programs (Developer Simon Willison posted a fun tweetstorm that highlights some of the creativity they’ve unleashed), but I’m also excited by the ways in which other developers are incorporating it into their own drawing tools. Ironically, Stable Diffusion’s manner of release moves us closer to “open AI” than the way DALL·E was released by the company called OpenAI. Kudos to Emad Mostaque and his Stability AI team, which developed the program. If you want to learn about how diffusion models like Stable Diffusion work, you can find a concise description here.Image generation is still maturing, but it’s a big deal. Many people have the creativity to produce art but lack the drawing skill to do so. As an amateur illustrator (I like to draw pandas to entertain my daughter using the Procreate paint app), my meager skill limits what I can create. But sitting in front of the DALL·E or Stable Diffusion user interface, I can ask her what she wants to see a panda doing and render a picture for her. Artists who have greater skill than I can use image generators to create stunning artworks more efficiently. In fact, an image produced this way recently won an art competition at the Colorado State Fair. The rise of inexpensive smartphone cameras brought an explosion in photography, and while expensive DSLRs still have a role, they now produce a minuscule fraction of all pictures taken. I expect AI-powered image generators to do something similar in art: Ever-improving models and user interfaces will make it much more efficient to generate art using AI than without. I see a future where most art is generated using AI, and novices who have great creativity but little drawing skill will be able to participate.My friend and collaborator Curt Langlotz, addressing the question of whether AI will replace radiologists, said that radiologists who use AI will replace radiologists who don’t. The same will be true here: Artists who use AI will (largely) replace artists who don’t. Imagine the transition in the 1800s from the time when each artist had to source their own minerals to mix shades of paint to when they could purchase ready-mixed paint in a tube. This development made it easier for any artist to paint whatever and whenever they wished. I see a similar transition ahead. What an exciting time!Separately from generating images for human consumption, these algorithms have great potential to generate images for machine consumption. A number of companies have been developing image generation techniques to produce training images for computer vision algorithms. But because of the difficulty of generating realistic images, many have focused on vertical applications that are sufficiently valuable to justify their investment, such as generating road scenes to train self-driving cars or portraits of diverse faces to train face recognition algorithms. Will image generation algorithms reduce the cost of data generation and other machine-to-machine processes? I believe so. It will be interesting to see this space evolve. Keep learning! Since September 2019, DeepLearning.AI’s network of Pie & AI Ambassadors has brought the AI community together at more than 700 events in 61 countries. Read about these leaders, their events, and how they’re turning their local areas into AI hubs. Learn more", "image_caption": "2022 Pie & AI Ambassadors with headshots of each of them", "metadata": {"article_id": "issue_162", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed-1.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-162/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_162.html"}}
{"id": 43586900002, "type": "news_chunk", "title": "Prompting DALL·E for Fun and Profit, Teaching Language Models New Info, Court Rules Against AI Proctoring and more", "subtitle": "Prompting DALL·E for Fun and Profit", "content": "An online marketplace enables people to buy text prompts designed to produce consistent output from the new generation of text-to-image generators. What’s new: PromptBase is a virtual marketplace for bespoke text strings designed as input for programs like DALL·E 2, Midjourney, and Stable Diffusion, The Verge reported. How it works: Buyers can browse PromptBase by specifying the desired system, searching categories such as “jewelry” or “wallpaper,” or typing in keywords. They can click to purchase the prompt via credit card or Google Pay. The site, which launched in June, has 50,000 active monthly users. Sellers upload a prompt, a general description of its output, the target model, and example images. Bracketed portions of the prompt indicate ways the buyer can customize the output.PromptBase assesses the quality of uploaded prompts by running them through the target model and performing a reverse image search to weed out submitted images that weren’t generated from the prompt, founder Ben Stokes told The Batch. The site rejects offensive prompts and those that are too specific and lack real-world utility, such as “Homer Simpson on the beach in watercolor.” Sellers retain all rights to accepted prompts.The price per prompt ranges from $1.99 to $4.99. PromptBase takes 20 percent of the revenue from each transaction. What they’re saying: “Every word in a prompt has a weight associated with it, so trying to work out what works best and where becomes a core asset in the skillset,” prompt engineer Justin Reckling, told The Verge. Behind the News: Designer and illustrator Guy Parsons offers The DALL·E 2 Prompt Book, a compendium of tips for producing effective prompts for text-to-image generators. The book offers several pages of tips including words that describe specific art styles, materials, compositional structures, colors, and emotions, as well as words that can influence photorealistic output such as camera angles, settings, lenses, lighting, film stocks, and so on. Moreover, research published last year investigates the relationship between prompt structure, model parameters, and text-to-image output. The work presents a number of helpful guidelines such as, “Keep the focus on keywords rather than rephrasings.” Why it matters: AI-driven media generators are opening a universe of productivity in imagery, text, and music. Marketplaces for effective prompts can supercharge these already-powerful tools by cutting the time it takes to generate desirable output. They can also serve as training grounds for the emerging discipline of prompt engineering: the craft of addressing generative models in ways that yield precise, repeatable output. We’re thinking: While they may not immediately replace professional illustrators — many generated images require touching up for professional purposes — image generators are becoming a staple tool of artists and graphic designers and seem likely to put many of them out of work. We hope that prompt engineering can provide an alternative livelihood for some.", "image_caption": "Captures from PromptBase", "metadata": {"article_id": "issue_162", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-162/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_162.html"}}
{"id": 43586900003, "type": "news_chunk", "title": "Prompting DALL·E for Fun and Profit, Teaching Language Models New Info, Court Rules Against AI Proctoring and more", "subtitle": "Court Blocks AI-Assisted Proctoring", "content": "A U.S. court ruled against an implementation of AI-powered software designed to catch students who cheat on academic examinations. What’s new: A federal judge determined that Cleveland State University’s use of Honorlock, a system that scans students’ behavior and surroundings for signs of cheating, violates their rights, National Public Radio reported. How it works: Students install Honorlock as a web browser extension and permit access to the computer’s microphone and camera. During a test, the extension uses voice detection and computer vision to issue alerts if it detects tablets, phones, open textbooks, dimmed lighting, faces of people other than the student, talking, phrases like “Hey Siri” or “Okay Google,” the student’s looking down or away from the screen before answering questions or absence from the camera’s view for an extended time, and other signs.Instructors can initiate a 360-degree scan of a student’s room prior to a test. Scans take about a minute to complete. Honorlock stores the recorded video data for a year.If it detects anything amiss, the system alerts a human proctor. The case: In 2021, Cleveland State University student Aaron Ogletree sued the school for subjecting him to a virtual room scan, which he claimed violated his Constitutional protection against unreasonable searches. He complied with the scan but filed suit later. The university argued that a room scan doesn’t constitute a search because it’s limited in scope and conducted to ensure academic integrity. The judge ruled that the university had violated Ogletree’s rights. Behind the News: Scientific investigations of other AI-powered proctoring systems have reached conflicting conclusions about their effectiveness. A 2021 study of a program called Proctorio found that it failed to catch any of 30 students whom the authors instructed to cheat. It also incorrectly flagged non-cheating students as engaging in suspicious activities.A 2020 study by Radford University found that test-takers scored lower when they were monitored by proctoring software than when they weren’t. The authors interpreted this result as evidence thatautomated proctoring discourages cheating. Why it matters: Automated proctoring has value, especially in the era of remote education. Although the ruling against Cleveland State applies only to that school, it raises questions about the legality of such automated room scans nationwide. We’re thinking: While the judge's decision ostensibly affects AI-powered proctor software, many institutions use human proctors who might occasionally request a manual room scan. The underlying question — what proctoring methods are reasonable, ethical, fair, and legal? — is independent of whether machines or humans should do the job.", "image_caption": "Capture of Honorlock, an AI-powered software designed to catch students who cheat on academic examinations", "metadata": {"article_id": "issue_162", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed--1-.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-162/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_162.html"}}
{"id": 43586900004, "type": "news_chunk", "title": "Prompting DALL·E for Fun and Profit, Teaching Language Models New Info, Court Rules Against AI Proctoring and more", "subtitle": "If It Ain’t Broke, Fix It", "content": "Factories are using AI to warn them when equipment is reaching the breaking point. What’s new: Services that monitor machinery to predict imminent failure and provide guidance on necessary upkeep are booming, The Wall Street Journal reported. How it works: Predictive maintenance systems anticipate breakdowns based on historical and real-time data collected from industrial machinery, enabling maintenance personnel to schedule repairs before they incur costly downtime. New York-based Augury developed a system that recognizes sounds made by a variety of gear operating at various levels of distress from brand-new to nearly broken. The company outfits factory machines with wireless audio sensors that transmit data to its cloud-based platform. When the system identifies an issue, it sends a real-time update to the plant’s maintenance team.Over 100 U.S. companies use Augury’s service including Frito-Lay, which installed the sensors at four plants, adding 4,000 hours of manufacturing capacity in the past year.Senseye, a company based in the Netherlands that was acquired by Siemens AG earlier this year, uses data that machines already collect, including pressure, vibration, and torque measurements, to identify looming issues. The company helped aluminum manufacturer Alcoa to cut unplanned downtime by 20 percent. Behind the news: Sales of predictive maintenance services stood at around $4 billion in 2020. The global total is expected to reach $18.6 billion by 2027, expanding at a compound annual growth rate of 24.5 percent, according to the research firm Research and Markets. Why it matters: Supply-chain problems have bedeviled industrial companies since the onset of the Covid-19 pandemic. By predicting when a machine is likely to fail, AI can help them avoid costly outages and enable them to stock up on replacement parts ahead of time. We’re thinking: Predictive maintenance helps reduce costs on an industrial scale, but could it be adapted for households? Imagine if your washing machine could figure out for itself whether that ominous knocking sound during the spin cycle was just a momentary annoyance or truly worrisome.", "image_caption": "App that monitors machinery", "metadata": {"article_id": "issue_162", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed--1--3.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-162/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_162.html"}}
{"id": 43586900005, "type": "news_chunk", "title": "Prompting DALL·E for Fun and Profit, Teaching Language Models New Info, Court Rules Against AI Proctoring and more", "subtitle": "Update Any Language Model", "content": "The ability to update language models is essential to incorporate new information and correct undesirable behaviors. Previous methods are unwieldy and often fail as the amount of new data increases. New work offers a workaround. What’s New: Eric Mitchell and colleagues at Stanford and École Polytechnique Fédérale de Lausanne proposed Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), an add-on system that can adapt trained models with an abundance of new information. Key insight: Say you’ve trained a language model to produce output based on the current Prime Minister of the United Kingdom. You’ll need to retrain the model when the Prime Minister changes. Alternatively you can update the model either by fine-tuning or training a secondary model, known as a model editor, that estimates and applies the change in weights necessary to respond to queries about the Prime Minister accurately without affecting responses to other queries. However, both approaches have problems. Fine-tuning every time information changes is impractical, and both approaches fail beyond around 10 new pieces of data (as the authors demonstrate without proposing an explanation why). Instead of changing model weights, a separate system can store new data and learn to provide output to queries that are relevant to that data. Such a system would handle any amount of new data and work with any model without retraining. How it works: The authors’ system is designed to complement a base model. It consists of three parts. The edit memory stored facts in the form of input-output pairs. The scope classifier determined whether a new input is relevant to facts stored in the edit memory. The counterfactual model generated output for relevant inputs. The base model continued to handle all other queries. The edit memory was a list of new input-output pairs (for example “Who is the UK Prime Minister?” “Boris Johnson”). The scope classifier was a pretrained DistilBERT fine-tuned to estimate the probability that an input was relevant to a given pair in the edit memory. The counterfactual model was a pretrained T5 language model that the authors fine-tuned to generate text based on the current input and an input-output pair.The fine-tuning examples, which took the form of input-output pairs, depended on the task at hand, such as question answering. Fine-tuning examples were labeled either relevant or irrelevant to pairs stored in the edit memory. For instance, given the pair “Who is the UK Prime Minister?” “Boris Johnson,” the query “Where is Boris Johnson the PM?” was relevant, while “Where did Boris Johnson attend university?” was not.At inference, given a new input, the scope classifier determined whether it was relevant to a pair in the edit memory. If so, it passed the most relevant pair, along with the input, to the counterfactual model to generate output. Results: The authors used two metrics, edit success and drawdown, to evaluate SERAC’s ability to update responses from a pretrained T5-large. Edit success measured the correctness of the T5’s responses to inputs relevant to the contents of the edit memory; higher is better (1 being perfect). Drawdown measured the correctness of responses to inputs not relevant to data in edit memory; lower is better (0 being perfect). SERAC outperformed model editors such as Model Editor Networks with Gradient Decomposition (MEND). On question-answering, SERAC achieved 0.986 edit success compared to MEND’s 0.823, and 0.009 drawdown compared to MEND’s 0.187. The authors applied the SERAC system they’d trained on T5-large to other sizes. Its performance barely budged. Moreover, SERAC continued to outperform as the number of new input-output pairs increased. The authors increased the number of simultaneous pairs to 75. Measuring performance as the difference between edit success and drawdown (the worst possible being -1, best being 1), SERAC’s fell only from 0.98 to around 0.90, while MEND’s degraded from 0.64 to around -0.95. Why it matters: This work opens the door to keeping trained language models up to date even as information changes at a rapid clip. Presumably businesses could use it to update information about, say, their products, leadership, numbers of employees, locations, and so on. Developers of conversational models could keep their chatbots abreast of changes in politics, law, and scientific discovery. We’re thinking: A single system that can update any language model opens the tantalizing possibility of a product, updated regularly, that can adapt previously trained models to new information.", "image_caption": "Information related to Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC)", "metadata": {"article_id": "issue_162", "chunk_index": 5, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/09/unnamed--2-.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-162/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_162.html"}}
{"id": 55368587001, "type": "news_chunk", "title": "DALL·E for Video, AI Startup Funding Falls, What the Dark Side of the Moon Looks Like and more", "subtitle": "News", "content": "Dear friends, When I wrote recently about how to build a career in AI, several readers wrote to ask specifically about AI product management: the art and science of designing compelling AI products. I’ll share lessons I’ve learned about this here and in future letters.A key concept in building AI products is iteration. As I’ve explained in past letters, developing a machine learning system is a highly iterative process. First you build something, then run experiments to see how it performs, then analyze the results, which enables you to build a better version based on what you’ve learned. You may go through this loop several times in various phases of development — collecting data, training a model, deploying the system — before you have a finished product.Why is development of machine learning systems so iterative? Because (i) when starting on a project, you almost never know what strange and wonderful things you’ll find in the data, and discoveries along the way will help you to make better decisions on how to improve the model; and (ii) it’s relatively quick and inexpensive to try out different models. Not all projects are iterative. For example, if you’re preparing a medical drug for approval by the U.S. government — an expensive process that can cost tens of millions of dollars and take years — you’d usually want to get the drug formulation and experimental design right the first time, since repeating the process to correct a mistake would be costly in time and money. Or if you’re building a space telescope (such as the wonderful Webb Space Telescope) that’s intended to operate far from Earth with little hope of repair if something goes wrong, you’d think through every detail carefully before you hit the launch button on your rocket. Iterating on projects tends to be beneficial when (i) you face uncertainty or risk, and building or launching something can provide valuable feedback that helps you reduce the uncertainty or risk, and (ii) the cost of each attempt is modest. This is why The Lean Startup, a book that has significantly influenced my thinking, advocates building a minimum viable product (MVP) and launching it quickly. Developing software products often involves uncertainty about how users will react, which creates risk for the success of the product. Making a quick-and-dirty, low-cost implementation helps you to get valuable user feedback before you’ve invested too much in building features that users don’t want. An MVP lets you resolve questions about what users want quickly and inexpensively, so you can make decisions and investments with greater confidence. When building AI products, I often see two major sources of uncertainty, which in turn creates risk: Users. The considerations here are similar to those that apply to building software products. Will they like it? Are the features you’re prioritizing the ones they’ll find most valuable? Is the user interface confusing?Data. Does your dataset have enough examples of each class? Which classes are hardest to detect? What is human-level performance on the task, and what level of AI performance is reasonable to expect? A quick MVP or proof of concept, built at low cost, helps to reduce uncertainty about users and/or data. This enables you to uncover and address hidden issues that may hinder your success. Many product managers are used to thinking through user uncertainty and using iteration to manage risk in that dimension. AI product managers should also consider the data uncertainty and decide on the appropriate pace and nature of iteration to enable the development team to learn the needed lessons about the data and, given the data, what level of AI functionality and performance is possible. Keep learning! Text-to-image generators like DALL·E 2, Midjourney, and Stable Diffusion are winning art contests and worrying artists. A new approach brings the magic of text-to-image generation to video. What's new: Make-A-Video, a system built by Uriel Singer and colleagues at Meta, turns text prompts into high-resolution videos without training on text-video pairs. You can see its output here. Key insight: While billions of text-image pairs are available to train a text-to-image generator, text-video pairs are too scarce to train a video equivalent. A model can learn relationships between words and pictures via pretraining on text-image pairs. Then it can be adapted for video by adding further layers that process image patches across frames and — while keeping the pretrained layers fixed — fine-tuning the new layers on videos, which are plentiful. In this way, a system can generate videos using knowledge it learned from text-image pairs. How it works: The authors pretrained a series of models (one transformer and four U-Net diffusion models) to generate images from text, generate in-between video frames, and boost image resolution. To pretrain the text-to-image models, they used 2.3 billion text-image pairs. After pretraining, they modified some of the models to process sequences of video frames: On top of each pretrained convolutional layer, the authors stacked a 1D convolutional layer that processed a grid of pixels in each frame; and on top of each pretrained attention layer, they stacked a 1D attention layer that, likewise, processed a grid of pixels in each frame. To fine-tune or train the modified models on video, they used 20 million internet videos. Given a piece of text, the pretrained transformer converted it into an embedding.The authors pretrained a diffusion model to take the embeddings and generate a 64x64 image. Then they modified the model as described above and fine-tuned it to generate sequences of 16 frames of 64x64 resolution.They added a second diffusion model. Given a 76-frame video made up of 16 frames, each followed by four masked (blacked-out) frames, it learned to regenerate the masked frames.They added a third diffusion model and pretrained it, given a 64x64 image, to increase the image’s resolution to 256x256. After modifying the model, they fine-tuned it to increase the resolution 76 successive frames to 256x256.Given a 256x256 image, a fourth diffusion model learned to increase its resolution to 768x768. Due to memory restrictions, this model was not modified for video or further trained on videos. At inference, given the 76-frame video, it increased the resolution of each frame without reference to other frames. Results: The authors compared their system’s output to that of the previous state of the art, CogVideo, which takes a similar approach but requires training on text-video pairs. Crowdworkers supplied 300 prompts and judged the output of the author’s system to be of higher quality 77.15 percent of the time and to better fit the text 71.19 percent of the time. Why it matters: Text-to-image generators already transform text into high-quality images, so there’s no need to train a video generator to do the same thing. The authors’ approach enabled their system to learn about things in the world from text-image pairs, and then to learn how those things move from unlabeled videos. We're thinking: The Ng family’s penchant for drawing pandas is about to undergo another revolution!", "image_caption": "Panda on a swing", "metadata": {"article_id": "issue_165", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/VIDEO.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-165/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_165.html"}}
{"id": 55368587002, "type": "news_chunk", "title": "DALL·E for Video, AI Startup Funding Falls, What the Dark Side of the Moon Looks Like and more", "subtitle": "A MESSAGE FROM OUR PARTNER", "content": "Join FourthBrain’s Machine Learning Engineer program for access to live, instructor-led classes and dedicated career services. Our graduates have seen an average salary increase of $27,000! Applications are due by October 10, 2022. The next cohort starts on October 18. Learn more", "image_caption": "FourthBrain 2 year anniversary banner", "metadata": {"article_id": "issue_165", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/FourthBrain-banner-ad--1-.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-165/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_165.html"}}
{"id": 42581309001, "type": "news_chunk", "title": "U.S. Blocks AI Chip Sales to China, Joe Rogan Meets Steve Jobs (Virtually) and more", "subtitle": "News", "content": "Dear friends, Is prompt engineering — the art of writing text prompts to get an AI system to generate the output you want — going to be a dominant user interface for AI? With the rise of text generators such as GPT-3 and Jurassic and image generators such as DALL·E, Midjourney, and Stable Diffusion, which take text input and produce output to match, there has been growing interest in how to craft prompts to get the output you want. For example, when generating an image of a panda, how does adding an adjective such as “beautiful” or a phrase like “trending on artstation” influence the output? The response to a particular prompt can be hard to predict and varies from system to system. So is prompt engineering an important direction for AI, or is it a hack? Here’s how we got to this point: The availability of large amounts of text or text-image data enabled researchers to train text-to-text or text-to-image models.Because of this, our models expect text as input.So many people have started experimenting with more sophisticated prompts. Some people have predicted that prompt engineering jobs would be plentiful in the future. I do believe that text prompts will be an important way to tell machines what we want — after all, they’re a dominant way to tell other humans what we want. But I think that prompt engineering will be only a small piece of the puzzle, and breathless predictions about the rise of professional prompt engineers are missing the full picture. Just as a TV has switches that allow you to precisely control the brightness and contrast of the image — which is more convenient than trying to use language to describe the image quality you want — I look forward to a user interface (UI) that enables us to tell computers what we want in a more intuitive and controllable way. Take speech synthesis (also called text-to-speech). Researchers have developed systems that allow users to specify which part of a sentence should be spoken with what emotion. Virtual knobs allow you to dial up or down the degree of different emotions. This provides fine control over the output that would be difficult to express in language. By examining an output and then fine-tuning the controls, you can iteratively improve the output until you get the effect you want. So, while I expect text prompts to remain an important part of how we communicate with image generators, I look forward to more efficient and understandable ways for us to control their output. For example, could a set of virtual knobs enable you to generate an image that is 30 percent in the style of Studio Ghibli and 70 percent the style of Disney? Drawing sketches is another good way to communicate, and I’m excited by img-to-img UIs that help turn a sketch into a drawing. Likewise, controlling large language models remains an important problem. If you want to generate empathetic, concise, or some other type of prose, is there an easier way than searching (sometimes haphazardly) among different prompts until you chance upon a good one? When I’m just playing with these models, I find prompt engineering a creative and fun activity; but when I’m trying to get to a specific result, I find it frustratingly opaque. Text prompts are good at specifying a loose concept such as “a picture of a panda eating bamboo,” but new UIs will make it easier to get the results we want. And this will help open up generative algorithms to even more applications; say, text editors that can adjust a piece of writing to a specific style, or graphics editors that can make images that look a certain way. Lots of exciting research ahead! I look forward to UIs that complement writing text prompts. Keep learning! New U.S. restrictions on chip sales aim to hamper China’s AI efforts. What’s new: The U.S. government published sweeping limits on sales of processors that involve U.S. designs and technology to Chinese businesses. U.S. officials stated that the restrictions are meant to prevent China from militarizing AI. New rules: The rules block sales of certain processors as well as U.S.-made equipment used to design and manufacture them. This includes high-end graphics processing units (GPUs) and other processors optimized for machine learning. The rules apply to chips capable of processing and interconnection speeds on par with Nvidia’s flagship A100 GPU, which is designed to be used in data centers. (Nvidia supplies 95 percent of China’s AI chips.) The less-capable chips typically found in personal computers and video game consoles are not restricted.The restrictions prohibit sales to Chinese companies of advanced chips produced using U.S.-made software and hardware as well as sales of the equipment itself. This goes for companies anywhere in the world.They also bar U.S. citizens and permanent residents from supporting development or manufacturing of advanced chips without permission from the U.S. government. China’s response: A spokesperson for China’s foreign ministry accused the U.S. of abusing export-control measures to target Chinese firms, stating that it would hinder global cooperation and supply chains.Behind the news: The restrictions initially came to light in September, when Nvidia and AMD independently alerted shareholders that the U.S. had imposed controls on their most advanced products. However, their details became publicly available only last week. They represent a significant escalation of earlier U.S. efforts to thwart China’s ambitions in advanced technology. In May 2020, the U.S. required foreign chipmakers that use U.S. equipment to obtain permission to do business with the Chinese tech giant Huawei.In 2019, the government blocked U.S. firms from selling equipment to Huawei and 114 of its affiliates.In 2015, the country barred Intel from selling high-end chips to the Chinese military. Why it matters: China has announced its ambition to become the global leader in AI by 2030, and this requires access to cutting-edge processing power. The most advanced chips are manufactured in Taiwan and South Korea using chip-fabrication equipment made by U.S. companies, and the leading chip designers and makers of chip-design software reside in the U.S. This gives U.S. authorities a tight grip on other countries’ ability to buy and make chips. China’s effort to build domestic capacity to produce advanced semiconductors — which are hampered by the sheer difficulty and expense of etching features on silicon measured in nanometers — now faces additional hardware, software, business, and talent hurdles. We’re thinking: International cooperation has been essential to recent progress in AI. As barriers rise between the U.S. and China, the AI community must navigate a world where geography will have a much bigger impact on access to ideas and resources.", "image_caption": "Shot of Computer Processor Production Line at Advanced Semiconductor Foundry in Bright Environment", "metadata": {"article_id": "issue_167", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/CHIPS.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-167/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_167.html"}}
{"id": 92451636001, "type": "news_chunk", "title": "Amazon's Surveillance Network, AI That Gets the Facts Right", "subtitle": "Neighborhood Watchers", "content": "Dear friends, I’ve been thinking about AI and ethics. With the techlash and an erosion of trust in technology as a positive force, it’s more important than ever that we make sure the AI community acts ethically. There has been a proliferation of AI ethical principles. This paper surveys 84 such statements. These statements are a great start, but we still need to do better. Take, for example, the OECD’s statement: AI should benefit people and the planet by driving inclusive growth, sustainable development, and well-being. When I ask engineers what effect such statements have on their day-to-day actions, they say, “pretty much none.” It is wonderful that the OECD is thinking about this. But we need more actionable codes of ethics that give more concrete and actionable suggestions. I described earlier struggling with an ethical decision of whether to publicize an AI threat. It’s in situations like that we need better guidelines and processes for decision making. Many existing AI ethics codes come from large corporations and governments. But if we hope that the global AI community will follow a set of guidelines, then this community — including you — needs to have a bigger voice in its development. We need an ethical code written by the AI community, for the AI community. That will also be the best way to make sure it truly reflects our community’s values, and that all of us buy into it and will follow it. Last Friday, deeplearning.ai hosted our first Pie & AI on AI and ethics. Four cities joined us: Hong Kong, Manila, Singapore, and Tokyo. We started with an interactive discussion, and each city came up with three actionable ethics statements, preferably starting with, “An AI engineer should ...” The ideas they presented ranged from seeking diverse perspectives when creating data to staying vigilant about malicious coding. I was heartened to see so many people motivated to debate ethical AI in a thoughtful way. I hope to do more events like this to encourage people to start the conversation within their own communities. This is important, and we need to figure this out. I would love to hear your suggestions. You can email us at [email protected]. Keep learning! Smart doorbell maker Ring has built its business by turning neighborhoods into surveillance networks. Now the company is drawing fire for using private data without informing customers and sharing data with police.What’s new: A three-part exposé in Vice is the latest in a series of media reports charging that Ring, an Amazon subsidiary, mishandles private data in its efforts to promote AI-powered “smart policing.” How it works: Ring’s flagship products are doorbells with integrated camera, microphone, and speaker. The camera’s face and object recognition software alerts users, via a smartphone or connected-home device, whenever someone or something enters its field of view. Users can talk with people at the door remotely through the microphone and speaker. Ring has found success promoting its devices as crime-prevention tools. It bolsters this reputation by advertising its partnerships with more than 600 U.S. police departments and 90 city governments. In return for the implicit endorsement, Ring provides police departments with free devices and access to user data.The company exports user data to its R&D office in Kiev, Ukraine, to train its models. A December 2018 report by The Information documented a lax security culture in Kiev, where employees routinely shared customer video, audio, and personal data. Ring says it uses only videos that users have shared publicly (generally with neighbors and/or police) through an app, and that it has a zero-tolerance policy for privacy violations.Google Nest, Arlo, Wyze, and ADP also offer smart doorbells. Ring, however, is the only one sharing data with police departments. Behind the news: When it was founded in 2012, Ring was called DoorBot and marketed its system as a hassle-free way to see who’s at the door. The following year, founder Jamie Siminoff appeared on the TV game show Shark Tank. While he didn’t sway the show’s celebrity investors, he raised several million dollars soon after, and changed the name to Ring. In recent years, he has pivoted from promoting the doorbell as a way to enhance social life to positioning it as a crime-prevention tool. In 2018, Amazon purchased Ring for $839 million and now markets the product as part of its Alexa smart-home services. Pushback: Troubled by the privacy implications of sharing customer data with police, 36 civil rights groups published an open letter calling for a moratorium on partnerships between Ring and public agencies. The letter also asked Congress to investigate the company. We’re thinking: Knock knock. Who’s there? Stopwatch who? Stopwatching me and respect my privacy!", "image_caption": "Excerpt from Ring commercial", "metadata": {"article_id": "issue_17", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ring2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-17/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_17.html"}}
{"id": 92451636002, "type": "news_chunk", "title": "Amazon's Surveillance Network, AI That Gets the Facts Right", "subtitle": "Keeping the Facts Straight", "content": "Automatically generated text summaries are becoming common in search engines and news websites. But existing summarizers often mix up facts. For instance, a victim’s name might get switched for the perpetrator’s. New research offers a way to evaluate factual consistency between source documents along with a measure to evaluate it.What’s new: Wojciech Kryściński and colleagues at Salesforce Research introduce FactCC, a network that classifies such inconsistencies. They also propose a variant called FactCCX that justifies the classifications by pointing out specific inconsistencies. Key insight: Earlier approaches to checking factual consistency determine whether a single source sentence implies a single generated sentence. But summaries typically draw information from many sentences. FactCC evaluates whether a source document as a whole implies a generated sentence.How it works: The authors identified major causes of factual inconsistency in automated abstractive summaries (that is, summaries that don’t copy phrases directly from the source document). Then they developed programmatic methods to introduce such errors into existing summaries to generate a large training dataset. FactCC is based on a BERT language model fine-tuned on the custom dataset. The researchers created a training dataset by altering sentences from CNN news articles. Transformations included swapping entities, numbers, or pronouns; repeating or removing random words, and negating phrases (“snow is in the forecast” versus “snow is not in the forecast”).Some transformations resulted in sentences whose meaning was consistent with the source, while others resulted in sentences with altered meaning. The authors labeled them accordingly.The development and test sets consisted of sentences from abstractive summaries generated by existing models. Each sentence was labeled depending on whether it was factually consistent with the source.BERT received the source document and a sentence from the generated summary. It predicted a binary classification of consistent or inconsistent. Results: FactCC classified summary sentences with an F1 score of 0.51. By contrast, a BERT model trained on MNLI, a dataset of roughly 400,000 sentence pairs labeled as either concordant or contradictory, achieved an F1 score of 0.08. In a separate task, FactCC ranked pairs of new sentences (one consistent, one not) for consistency with a source. It awarded consistent sentences a higher rank 70 percent of the time, better by 2.2 percent than the best previous model ranking the same dataset.Why it matters: A tide of automatically generated text is surging into mainstream communications. Measuring factual consistency is a first step towards establishing further standards for generated text—indeed, an urgent step as worries intensify over online disinformation.", "image_caption": "Automatically generated text summary", "metadata": {"article_id": "issue_17", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Factual20Consistency.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-17/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_17.html"}}
{"id": 92451636003, "type": "news_chunk", "title": "Amazon's Surveillance Network, AI That Gets the Facts Right", "subtitle": "Predicting the Next Eruption", "content": "AI is providing an early warning system for volcanoes on the verge of blowing their top.What happened: Researchers at the University of Leeds developed a neural net that scans satellite data for indications that the ground near a volcano is swelling—a sign it may be close to erupting.How it works: Satellites carrying certain sensors can track centimeter-scale deformations of Earth’s crust. Seismologists in the 1990s figured out how to manually read this data for signs of underground magma buildups. However, human eyeballs are neither numerous nor sharp enough to monitor data for all of Earth’s 1,400 active volcanoes. Matthew Gaddes, Andy Hooper, and Marco Bagnardi trained their model using a year’s worth of satellite imagery leading up to the 2018 eruption of a volcano in the Galapagos Islands.Data came from a pair of European satellites that passed over the volcano every 12 days.The model differentiates rapid ground-level changes associated with catastrophic eruptions from slower, more routine deformations. Behind the news: Researchers at the University of Bristol developed a similar method to measure deformations in the Earth’s crust using satellite data. However, their model can be fooled by atmospheric distortion that produces similar signals in the data. The Leeds and Bristol groups plan to collaborate in side-by-side tests of their models on a global dataset in the near future. Another group based at Cornell University is attempting to make similar predictions through satellite data of surface temperature anomalies, ash, and gaseous emissions.Why it matters: Approximately 800 million people live within the blast zones of active volcanoes, and millions of sightseers visit their slopes each year. On Monday, New Zealand’s White Island volcano erupted, killing at least five tourists. We’re thinking: If researchers can scale their model up to cover the entire globe, they’ll deserve applause that thunders as loudly as Krakatoa.", "image_caption": "Volcano erupting", "metadata": {"article_id": "issue_17", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/volcano2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-17/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_17.html"}}
{"id": 92451636004, "type": "news_chunk", "title": "Amazon's Surveillance Network, AI That Gets the Facts Right", "subtitle": "Fighting Fakes", "content": "China announced a ban on fake news, targeting deepfakes in particular.What happened: The Cyberspace Administration of China issued new rules restricting online audio and video, especially content created using AI. Deepfakes and other such tech, the administration warned, may “disrupt social order and violate people’s interests, creating political risks and bringing a negative impact to national security and social stability.”What could get you in trouble: The rules take effect January 1, 2020. They target both the creators of fake news and websites that host their content. The rules extend beyond explicitly malicious content. Any videos containing AI-generated characters, objects, or scenes must come with an easy-to-see disclaimer, including relatively benign fakes like those above, created by game developer Allan Xia using the Zao face-swapping app.The new restrictions extend China’s earlier efforts to nip digital fakery in the bud. In 2015, the National People’s Congress passed a law mandating three to seven years in prison for anyone caught spreading fake news.China’s laws also make it easy for the country to track deepfake creators. News and social media platforms operating in the country require users to register with their real names. Behind the news: Computer-generated media are facing government scrutiny in the U.S. as well. California issued its own law barring doctored audio and video earlier this year. The law, however, does not name deepfakes or any specific technology. It targets only political content released within 60 days of an election, and it sunsets in 2023. U.S. legislators are considering federal restrictions on deepfakes, according to The Verge.Why it matters: There’s a legitimate fear that deepfakes will be used for political purposes. To date, however, the technology’s biggest victims have been women whose bodies have been visualized without clothes or whose faces have been pasted onto pornographic scenes.We’re thinking: It’s a good idea for governments to think ahead to potential problems caused by deepfakes in advance of broader calamities. However, efforts to crack down on them may turn into a high-tech game of whack-a-mole as falsified media becomes ever harder to spot.", "image_caption": "Face-swap on Leonardo DiCaprio", "metadata": {"article_id": "issue_17", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/deepfake2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-17/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_17.html"}}
{"id": 92451636005, "type": "news_chunk", "title": "Amazon's Surveillance Network, AI That Gets the Facts Right", "subtitle": "Seeing the World Blindfolded", "content": "In reinforcement learning, if researchers want an agent to have an internal representation of its environment, they’ll build and train a world model that it can refer to. New research shows that world models can emerge from standard training, rather than needing to be built separately.What’s new: Google Brain researchers C. Daniel Freeman, Luke Metz, and David Ha enabled an agent to build a world model by blindfolding it as it learned to accomplish tasks. They call their approach observational dropout.Key insight: Blocking an agent’s observations of the world at random moments forces it to generate its own internal representation to fill in the gaps. The agent learns this representation without being instructed to predict how the environment will change in response to its actions.How it works: At every timestep, the agent acts on either its observation (framed in red in the video above) or its prediction of what it wasn’t able to observe (imagery not framed in red). The agent contains a controller that decides on the most rewarding action. To compute the potential reward of a given action, the agent includes an additional deep net trained using the RL algorithm REINFORCE. Observational dropout blocks the agent from observing the environment according to a user-defined probability. When this happens, the agent predicts an observation.If random blindfolding blocks several observations in a row, the agent uses its most recent prediction to generate the next one.This procedure over many iterations produces a sequence of observations and predictions. The agent learns from this sequence, and its ability to predict blocked observations is tantamount to a world model. Results: Observational dropout solved the task known as Cartpole, in which the model must balance a pole upright on a rolling cart, even when its view of the world was blocked 90 percent of the time. In a more complex Car Racing task, in which a model must navigate a car around a track as fast as possible, the model performed almost equally well whether it was allowed to see its surroundings or blindfolded up to 60 percent of the time.Why it matters: Modeling reality is often part art and part science. World models generated by observational dropout aren’t perfect representations, but they’re sufficient for some tasks. This work could lead to simple-but-effective world models of complex environments that are impractical to model completely.We’re thinking: Technology being imperfect, observational dropout is a fact of life, not just a research technique. A self-driving car or auto-piloted airplane reliant on sensors that drop data points could create a catastrophe. This technique could make high-stakes RL models more robust.", "image_caption": "Observational dropout", "metadata": {"article_id": "issue_17", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize201.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-17/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_17.html"}}
{"id": 92451636006, "type": "news_chunk", "title": "Amazon's Surveillance Network, AI That Gets the Facts Right", "subtitle": "Transparency for Military AI", "content": "A U.S. federal judge ruled that the public must be able to see records from a government-chartered AI advisory group.What’s new: The court decided that the National Security Commission on AI, which guides defense research into AI-powered warfighting technology, must respond to freedom-of-information requests.What the suit says: The National Security Commission on AI was set up by Congress in late 2018, and the Electronic Privacy Information Center has been requesting its records under the Freedom of Information Act for almost as long. Finding its requests ignored repeatedly, EPIC took the commission to court in late September. Lawyers for the AI commission argued that their client was protected from freedom of information laws by exemptions for members of the president’s staff, some of whom sit on the commission.EPIC’s attorneys countered by citing a 1990 ruling that congressionally funded groups, such as the Defense Nuclear Facilities Safety Board, are considered federal agencies, and therefore obligated to share information with the public.The judge sided with EPIC, so the AI commission will have to respond to the nonprofit’s requests for copies of its meeting transcripts, working papers, studies, and agendas. EPIC also asked that the AI commission publish advance notice of its meetings in the federal register. Behind the news: Former Google CEO Eric Schmidt (shown above) chairs the commission. Other members include a mix of private sector and government experts. It released an interim report of its recommendations on November 4.Why it matters: EPIC contends that, without public oversight, the commission could steer the Defense Department towards invasive, biased, or unethical uses of AI.We’re thinking: We look forward to learning more about what this group has been up to. Decisions about deploying AI as a weapon of war should not be made in a back room.", "image_caption": "Eric Schmidt on C-Span2", "metadata": {"article_id": "issue_17", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/foia.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-17/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_17.html"}}
{"id": 46448491001, "type": "news_chunk", "title": "Billboards Are Watching, City Goes Algorithmic, Auto-Translation for Unwritten Language, New Views of 3D Scenes Pronto!", "subtitle": "A MESSAGE FROM FILTERED", "content": "What do you think about coding tests and technical interviews? We’d like to hear the good, the bad, and the ugly. Tell us about your technical interview experiences and earn a chance to win a $100 gift card! Take the survey", "image_caption": "Filtered.ai banner ad for a survey", "metadata": {"article_id": "issue_173", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/11/ezgif-1-f144e58736.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-173/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_173.html"}}
{"id": 6334367001, "type": "news_chunk", "title": "ChatGPT Backlash, Face Recognition to Settle Scores, Deepfakes Versus Customer Service, Segmented Images Without Labeled Data", "subtitle": "A MESSAGE FROM AI FUND", "content": "Building a startup is hard. But with a venture studio as a partner, founders dramatically increase their odds of success. Join us on January 12, 2023, at 2:00 p.m. Pacific Time to learn how venture studios work and how AI Fund sets up entrepreneurs to win. Register here", "image_caption": "AI Fund's webinar \"Venture Studios: The Express Lane for Entrepreneurs to Build Successful Startups\" banner ad", "metadata": {"article_id": "issue_179", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2023/01/1673045257638--1-.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-179/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_179.html"}}
{"id": 1424475001, "type": "news_chunk", "title": "Companies Slipping on AI Goals, Self Training for Better Vision", "subtitle": "Deployment Gap", "content": "Dear friends, I’ve been reflecting on the NeurIPS 2019 conference, which ended on Saturday. It’s always a wonderful event, but this year I found it a bittersweet experience. Bitter because the conference has grown so much that we no longer focus on a handful of ideas. I missed the feeling of a community coming together. I was excited about the progress in self-supervised learning. Others were buzzing about Bayesian networks and causality, federated learning in healthcare applications, or using DL to predict biological sequences such as proteins. These are fascinating areas, but it’s clear the AI community no longer marches to only one beat. The sweet part is that NeurIPS is growing up. As Karen Hao wrote in MIT Technology Review, NeurIPS has matured from a venue with great science, hard partying, and wild dancing into a forum with great science and a focus on using AI for good. The AI community is getting better at diversity, inclusion, and taking responsibility for our actions, though there’s still room to grow. As part of the panel during the climate change workshop, I spoke about the importance of building an actionable ethical code for AI. Ideally written by the AI community, for the AI community. You can hear my remarks on that subject here at 1:15. It was great fun speaking on the panel with Yoshua Bengio, Jeff Dean, Carla Gomes, and Lester Mackey. Thanks to David Rolnick, Priya Donti, Lynn Kaack, and others for organizing the great workshop. Keep learning! More and more companies are developing machine learning models for internal use. But many are still struggling to bridge the gap to practical deployments.What’s new: Many companies haven’t figured out how to realize their AI ambitions, according to a report by Algorithmia, a marketplace for algorithms. Although AI budgets are on the rise, only 22 percent of companies using machine learning have successfully deployed a model, the study found.What the report says: The 2020 State of Enterprise Report is based on a survey of nearly 750 people including machine learning practitioners, managers overseeing machine learning projects, and executives at large tech corporations. More than two-thirds of the subgroup that was asked about budgets reported increased spending on AI between 2018 and 2019 (see the graph above).Nonetheless, 43 percent of respondents cited difficulty scaling machine learning projects to their company’s needs, up 13 percent from last year’s survey.Half of respondents said their company takes between a week and three months to deploy a model. 18 percent said it takes from three months to a year. Why it matters: AI is rapidly expanding into new applications and industries, and research is making tremendous strides. Yet building successful projects is still difficult. This report highlights both the great value of practical experience in the field and the need to establish effective practices and processes around designing, building, and deploying models.We’re thinking: There’s a huge difference between building a Jupyter notebook model in the lab and deploying a production system that generates business value. AI as a field sometimes seems crowded but, in fact, it’s wide open to professionals who know what they’re doing.", "image_caption": "Pie chart with information on AI/ML budgets", "metadata": {"article_id": "issue_18", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/algorithmia220ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-18/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_18.html"}}
{"id": 1424475002, "type": "news_chunk", "title": "Companies Slipping on AI Goals, Self Training for Better Vision", "subtitle": "Self-Training for Sharper Vision", "content": "The previous state-of-the-art image classifier was trained on the ImageNet dataset plus 3.5 billion supplemental images from a different database. A new method achieved higher accuracy with one-tenth as many supplemental examples — and they were unlabeled, to boot.What’s new: Qizhe Xie and a team at Google Brain plus Carnegie Mellon’s Eduard Hovy introduced a method they call Noisy Student, in which a model learns from another model in a teacher-student relationship. Noisy Student achieved better performance on ImageNet. Key insight: In the learning approach known as self-training, a model that’s designated the teacher trains on labeled data and then generates pseudo-labels on unlabeled data. Then a student model trains on both the labeled data and pseudo-labeled data. Noisy Student adds two tweaks: The student network is larger than that of the teacher, and the student’s training data is adulterated with noise.How it works: Both teacher and student use an EfficientNet architecture. The higher-capacity architecture is good for the student, which has more parameters and processes more data than the teacher. The teacher is trained on ImageNet’s training set. It then predicts pseudo labels for 300 million unlabeled images from Google’s private JFT dataset.The student training dataset consists of ImageNet’s training set plus the 130 thousand JFT images with the highest confidence predictions for each pseudo label class.During the student’s training, the algorithm applies data augmentation and also uses dropout and the stochastic depth method to perturb the model weights. These steps nudge the student to generalize beyond its teacher’s ability.The teacher-student training cycle can be repeated, treating each previous student as a new teacher. Results: Noisy Student improved state-of-the-art accuracy on ImageNet as a whole and on specialized subsets. On ImageNet, it increased top-5 accuracy, meaning the true label was in the top five predictions, by 0.2 percent to 98.2 percent. Noisy Student also boosted the top-1 accuracy by 1 percent to 87.4 percent. Furthermore, it matched or exceeded previously established records for ImageNet A,C, and P, which are subsets that have been corrupted or perturbed or are commonly misclassified.Why it matters: These results are another step forward for using unlabeled data to boost image classification accuracy. We’re thinking: Unlabeled examples are far more plentiful than labeled datasets. Techniques like this may be key to enabling learning algorithms to exploit far more data than was possible before.", "image_caption": "Graph related to Noisy Student performance on ImageNet", "metadata": {"article_id": "issue_18", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Noisy20Student20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-18/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_18.html"}}
{"id": 1424475003, "type": "news_chunk", "title": "Companies Slipping on AI Goals, Self Training for Better Vision", "subtitle": "Inside AI’s Muppet Empire", "content": "As language models show increasing power, a parallel trend has received less notice: The vogue for naming models after characters in the children’s TV show Sesame Street.What’s new: In a recent feature article, The Verge gets to the bottom of the Muppetware phenomenon.How to get to Sesame Street: The trend encompasses tech giants including Google, Facebook, Baidu, and China’s prestigious Tsinghua University. It began in 2017 with ELMo (Embeddings from Language Models), a product of the Allen Institute for AI. Its creators liked the name’s whimsical flavor, and the model’s way with words garnered headlines in the tech press.The following year, researchers at Google debuted BERT (Bidirectional Encoder Representations from Transformers), acknowledging a debt to ELMo.Other researchers paid homage by perpetuating what had become an inside joke. Today, the Muppetware family includes Big BIRD, ERNIE, Kermit, Grover, RoBERTA, and Rosita.But what about GPT-2? The article reveals that OpenAI’s extraordinarily loquacious model was almost called Snuffleupagus. The researchers dropped the name because it wasn’t serious enough. Why it matters: Muppet names are fun! But the article points out that they’re also memorable. They brand individual models for other researchers and elevate natural language processing for the broader public. Beyond that, the informal naming convention signifies the AI community’s collaborative attitude.Behind the news: Sesame Street debuted in 1969 aiming to create TV that held children’s attention while also educating them. It was the first children’s show to base its episodes on research.We’re thinking: This edition of The Batch was brought to you by the letters A and I.", "image_caption": "Sesame Street characters together", "metadata": {"article_id": "issue_18", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/muppets.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-18/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_18.html"}}
{"id": 1424475004, "type": "news_chunk", "title": "Companies Slipping on AI Goals, Self Training for Better Vision", "subtitle": "Competition or Cooperation?", "content": "Some politicians view international competition in AI as an arms race. That mindset could lead to escalating conflict, experts said.What’s new: If global powers like the U.S. and China adopt a winner-take-all approach to AI, they will lose out on the benefits of international collaboration, Tim Hwang and Alex Pascal argue in Foreign Policy.The analysis: The arms-race mentality springs primarily from the notion that autonomous weapons will prove to be a trump card in international conflicts, the authors say. This belief encourages nations to keep research and development to themselves, but the total benefit of collaboration is often greater than that of any particular initiative, they say. Both the U.S. and China would benefit from AI that targets issues like climate change, global health, and disaster response.Powerful nations can use cooperative projects to shape global norms and rules for AI.A national agenda that prioritizes AI for warfare is likely to divert funding from non-defense applications.Countries that undertake arms races tend to escalate conflict rather than tamp it down. Behind the news: Scientific partnerships between the U.S. and USSR mitigated tensions during the Cold War era. In 1957, the rival nations agreed to send scientists to collaborate on projects. These cooperative relationships influenced diplomatic discussion and helped ease disagreements over issues like nuclear disarmament.Why it matters: AI’s potential role in warfare is still unclear, and the technology is far from fully developed. The gap creates breathing room for national leaders to establish policies that will mutually benefit their own countries and the world at large. For instance, the National Security Commission on AI advocates that the U.S. engage with China and Russia to control military uses of AI.We’re thinking: Electricity has uses in warfare, yet countries didn’t keep that technology to themselves, and the whole world is better off for it.", "image_caption": "US flag on the left, China flag on the right", "metadata": {"article_id": "issue_18", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Arms20Race20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-18/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_18.html"}}
{"id": 1424475005, "type": "news_chunk", "title": "Companies Slipping on AI Goals, Self Training for Better Vision", "subtitle": "Different Skills From Different Demos", "content": "Reinforcement learning trains models by trial and error. In batch reinforcement learning (BRL), models learn by observing many demonstrations by a variety of actors. For instance, a robot might learn how to fix ingrown toenails by watching hundreds of surgeons perform the procedure. But what if one doctor is handier with a scalpel while another excels at suturing? A new method lets models absorb the best skills from each.What’s new: Ajay Mandlekar and collaborators at Nvidia, Stanford, and the University of Toronto devised a BRL technique that enables models to learn different portions of a task from different examples. This way, the model can gain useful information from inconsistent examples. Implicit Reinforcement without Interaction at Scale (IRIS) achieved state-of-the-art BRL performance in three tasks performed in a virtual environment.Key insight: Learning from demonstrations is a double-edged sword. An agent gets to see how to complete a task, but the scope of its action is limited to the most complete demonstration of a given task. IRIS breaks down tasks into sequences of intermediate subgoals. Then it performs the actions required to accomplish each subgoal. In this way, the agent learns from the best parts of each demonstration and combines them to accomplish the task.How it works: IRIS includes a subgoal selection model that predicts intermediate points on the way to accomplishing an assigned task. These subgoals are defined automatically by the algorithm, and may not correspond to parts of a task as humans would describe them. A controller network tries to replicate the optimal sequence of actions leading to a given subgoal. The subgoal selection model is made up of a conditional variational autoencoder that produces a set of possible subgoals and a value function (trained via a BRL version of Q-learning) that predicts which next subgoal will lead to the highest reward.The controller is a recurrent neural network that decides on the actions required to accomplish the current subgoal. It learns to predict how demonstrations tend to unfold, and to imitate short sequences of actions from specific demonstrations.Once it’s trained, the subgoal selection model determines the next subgoal. The controller takes the requisite actions. Then the subgoal selection model evaluates the current state and computes a new subgoal, and so on. Results: In the Robosuite’s lifting and pick-and-place tasks, previous state-of-the-art BRL approaches couldn’t pick up objects reliably, nor place them elsewhere at all. IRIS learned to pick up objects with over 80 percent success and placed them with 30 percent success.Why it matters: Automatically identifying subgoals has been a holy grail in reinforcement learning, with active research in hierarchical RL and other areas. The method used in this paper applies to relatively simple tasks where things happen in a predictable sequence (such as picking and then placing), but might be a small step in an important direction.We’re thinking: Batch reinforcement learning is useful when a model must be interpretable or safe — after all, a robotic surgeon shouldn’t experiment on living patients — but it hasn’t been terribly effective. IRIS could make it a viable option.", "image_caption": "Information related to Implicit Reinforcement without Interaction at Scale (IRIS)", "metadata": {"article_id": "issue_18", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/IRIS20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-18/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_18.html"}}
{"id": 1424475006, "type": "news_chunk", "title": "Companies Slipping on AI Goals, Self Training for Better Vision", "subtitle": "IP for AI", "content": "The number of patents issued for deep learning has doubled every year since 2013.What’s new: Inventor, engineer, and lawyer Nick Brestoff tracks deep learning patents. He detailed his findings in a blog on InsideBigData and offers advice on how to get patent applications approved.AI on the rise: Breston searches weekly for filings containing keywords including “deep learning,” “deep neural,” or “multi-layer neural.” He found that IBM holds the most deep learning patents (51), followed by Google (39) and Microsoft (28). Between 2013 and 2015, the Patent Office issued just three to four deep learning patents a year.In 2016, that number jumped to 36 and has more than doubled each year since. As of December 3, the agency had issued 361 deep learning patents in 2019.A change in the law helps explain the 2016 jump. That year, a federal court lowered the bar for approval of software patents. Senior privilege: Inventors age 65 and over — even those listed as co-authors — can fast-track patent applications using a loophole called a Petition to Make Special. This trick has allowed Brestoff, who is 71 years old and holds eight patents on deep learning techniques, to complete the process in as little as three months, rather than the usual years-long wait. “There’s a wonderful advantage to having a knowledgeable senior on your innovation team,” he said.We’re thinking: Patents have a bad name in some circles, because of patent trolls and frivolous lawsuits that have destroyed value and slowed down innovation. But we’re also not sure a world with no patents whatsoever would be one with more innovation.", "image_caption": "Chart with number of patents per year", "metadata": {"article_id": "issue_18", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Patents20ASPECT-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-18/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_18.html"}}
{"id": 60892557001, "type": "news_chunk", "title": "Generated Code Makes Overconfident Programmers, China's Autonomous Drone Carrier and more", "subtitle": "A MESSAGE FROM WHYLABS", "content": "Join world-class leaders and companies at WhyLabs’ Robust & Responsible AI Summit! This free, half-day event includes a fireside chat with Andrew Ng. Mark your calendar for January 26, 2023, at 9:15 a.m. Pacific Time and register here", "image_caption": "The Robust & Responsible AI Summit banner ad", "metadata": {"article_id": "issue_180", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2023/01/R2AISummit-Jan26-WhyLabs.jpeg", "source_url": "https://www.deeplearning.ai/the-batch/issue-180/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_180.html"}}
{"id": 68776011001, "type": "news_chunk", "title": "Tesla's Deceptive Demo, Image Generator Pays Artists for Training Data, and more", "subtitle": "A MESSAGE FROM FOURTH BRAIN", "content": "Build a practical action plan to grow your organization using AI! Join FourthBrain’s live, three-day workshop for business leaders and executives between February 27 and March 1, 2023. Register today", "image_caption": "FourthBrain's AI for Business Leaders event banner ad", "metadata": {"article_id": "issue_182", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2023/02/Working-AI--600---338-px---Presentation--169----2-.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-182/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_182.html"}}
{"id": 24856136001, "type": "news_chunk", "title": "Chatbots Gone Wild, Surveillance Takes Hold, Rules for Military AI and more", "subtitle": "A MESSAGE FROM DATAHEROES", "content": "Want to build high-quality machine learning models in less time? Use the DataHeroes library to build a small data subset that’s easier to clean and faster to train your model on. Get VIP access", "image_caption": "DataHeroes library to build high-quality machine learning models banner ad", "metadata": {"article_id": "issue_185", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2023/02/The-Batch-ads-and-exclusive-banners.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-185/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_185.html"}}
{"id": 54517831001, "type": "news_chunk", "title": "China Catches ChatGPT Fever, Top Publishers Embrace Text Generation, and more", "subtitle": "A MESSAGE FROM LANDING AI", "content": "Create and deploy computer vision models with ease using LandingLens. Get started for free today!", "image_caption": "Landing AI's LandingLens banner ad", "metadata": {"article_id": "issue_186", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2023/03/LandingAI-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-186/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_186.html"}}
{"id": 9606601001, "type": "news_chunk", "title": "Voice Clones Go Viral, No Copyright for Generated Images, and more", "subtitle": "A MESSAGE FROM WORKERA", "content": "Andrew Ng talks with Workera CEO Kian Katanforoosh about upskilling in machine learning and how he hires world-class AI teams in the newest episode of Workera’s Skills Baseline podcast. Watch it here", "image_caption": "Screen capture of Workera's podcast episode with Kian Katanforoosh and Andrew Ng", "metadata": {"article_id": "issue_187", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2023/03/The-Batch-ads-and-exclusive-banners--9-.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-187/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_187.html"}}
{"id": 80815242001, "type": "news_chunk", "title": "Biggest AI Stories of 2019: Driverless Cars Stall, Deepfakes...", "subtitle": "Language Models Get Literate", "content": "Earlier language models powered by Word2Vec and GloVe embeddings yielded confused chatbots, grammar tools with middle-school reading comprehension, and not-half-bad translations. The latest generation is so good, some people consider it dangerous.What happened: A new breed of language models wrote news that readers rated as credible as the New York Times and contributed to an article in the New Yorker. Happily, these models didn’t fulfill fears that they would unleash a dark tide of disinformation.Driving the story: In 2019, researchers made a leap in natural language performance. The new models become generally proficient by pretraining on a huge, unlabeled dataset. Then they master a given task or subject matter via fine-tuning on a specialized corpus. While earlier models like ULMFiT (by Jeremy Howard and Sebastian Ruder) and ELMo (from the Allen Institute for AI and University of Washington) demonstrated pretraining’s potential, Google’s BERT was the method’s first breakout success. Released in late 2018, BERT scored so high on the GLUE reading comprehension benchmark that, for the first time, the test’s organizers compared the model’s performance with human baseline scores. In June, a Microsoft derivative called MT-DNN beat the human scores.In mid-February, OpenAI announced GPT-2, a pretrained model it deemed too dangerous to release due to its ability to churn out convincing computer-generated prose. Trained on 40GB of Reddit comments, it didn’t fuel a fake-news apocalypse, but it did contribute to a novel, avant-garde song lyrics, and Game of Thrones fan fiction. The organization finally published the full-blown model in November.In between, a parade of models from Baidu, Carnegie Mellon and Google Brain, Facebook, and elsewhere topped the NLP benchmarks in turn. Many were based on the transformer architecture and took advantage of BERT-style bi-directional coding. Behind the news: In July 2018 — months before BERT came out — DeepMind researcher Sebastian Ruder anticipated pretraining’s impact on natural language processing. Further, he predicted that breakthroughs in NLP would revolutionize AI as a whole. He based his argument on the energizing effect of pretrained vision models circa 2012. Many in the field trace the deep learning explosion to this moment. Where things stand: Despite the year’s innovations, language models still have room to grow: Even GPT-2’s 1.5 trillion parameters often spit out gobbledygook. As for whether the latest models are capable of disrupting democracy with potent disinformation: U.S. election season is coming up fast.", "image_caption": "Illustration of a fireplace with \"Happy holidays\" cards in English, Spanish and French", "metadata": {"article_id": "issue_19", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/language20models.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-19/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_19.html"}}
{"id": 80815242002, "type": "news_chunk", "title": "Biggest AI Stories of 2019: Driverless Cars Stall, Deepfakes...", "subtitle": "Face Recognition Meets Resistance", "content": "An international wave of anti-surveillance sentiment pushed back against the proliferation of face recognition systems.What happened: Activist and watchdog groups in the U.S. and Europe, alarmed by the technology’s potential to infringe on civil liberties, spurred legislation restricting its use. Their efforts built momentum toward national bans on public and private uses of the technology. Driving the story: Several U.S. cities passed anti-face recognition laws as the federal government mulled the issues. The European Union is working on its own restrictions. In May, San Francisco became the first U.S. city to ban face recognition by police and other government officials, followed by the Boston, MA suburb of Somerville. In the coming months, San Francisco’s neighbors Oakland and Berkeley passed similar laws. These laws were spearheaded by the American Civil Liberties Union, which aims to build momentum for national legislation.In Washington, members of the U.S. Congress grilled the Department of Homeland Security over the agency’s plan to use the technology at airports and the border. Legislators in both the Senate and House of Representatives have introduced at least a dozen bills — many with bipartisan support — seeking to restrict uses of face recognition to suppress liberties, deny housing, and generate profit, among other things.European watchdogs pushed to classify face images as biometric data subject to existing privacy regulations. The European Commission is considering legislation targeting “indiscriminate use” of face recognition by private organizations and public agencies. Nonetheless, France in October readied a national identification program based on the technology.China’s use of face recognition prompted opposition in the U.S., where federal trade authorities banned exports of U.S. technology to several Chinese companies. Behind the news: In 2016, the U.S. National Telecommunications and Information Administration published face-recognition guidelines asking companies to be transparent, practice good data management, and allow the public some control over sharing of face data with third parties. Although major vendors of the technology are members of the NTIA, it’s not clear whether they follow these guidelines.Where things stand: In June, Amazon Web Service CEO’s Andy Jassy told Recode, “I wish [Congress would] hurry up. . . . . Otherwise, you’ll have 50 different laws in 50 different states.” He may as well have spoken for the tech industry as a whole: Without legal limits, companies are left guessing how far they can push the technology before they violate public trust — risking blowback if they step over the line.", "image_caption": "Illustration of a reindeer with security cameras pointing at it", "metadata": {"article_id": "issue_19", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/recognition20backlash.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-19/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_19.html"}}
{"id": 80815242003, "type": "news_chunk", "title": "Biggest AI Stories of 2019: Driverless Cars Stall, Deepfakes...", "subtitle": "Driverless Cars Stall", "content": "Makers of self-driving cars predicted a quick race to the finish line, but their vehicles are far from the homestretch.What happened: A few years ago, some car companies promised road-ready autonomous vehicles as early as 2017. At a Wall Street Journal conference in January, though, Waymo CEO John Krafcik disclosed his belief that autonomous vehicles would probably never be able to drive in all conditions. His comment set the tone for a year of automotive retrenchment.Driving the story: A confluence of difficulties prompted several car companies to tap the breaks. Urban driving presents hazards so diverse, and dangerous edge cases so rare, that engineers have yet to figure out how to build models that overcome them. Vehicles that traverse predictable routes, such as automated buses and long haul freight trucks, likely will be first to deployment.The high cost and limited availability of sensors — particularly lidar — have forced companies to manufacture their own or scale back the number they use on each car. Fewer sensors mean less data for training and perception.GM Cruise and Tesla postponed their autonomous taxi deadlines to 2020. The U.S. city of Phoenix gave Waymo and Lyft permission to run autonomous taxis in 2018, but the service is available only to a limited area and a small number of users. In November, Waymo shuttered its Austin self-driving research facility. Behind the news: Cities in China are experimenting with a different approach. Rather than training autonomous vehicles to navigate existing urban settings, they’re retrofitting cities to facilitate the technology. Features include roadside sensors that pass along navigational cues, like lane changes and speed limits.Where things stand: Traditional automakers are focusing on assisted driving features like Ford’s Driver Assist and Mercedes’ Parking Assist. Meanwhile, Waymo continues to work on fully autonomous vehicles, and smaller companies such as May Mobility and Voyage are deploying full autonomy in limited scenarios that they aim to expand over time. In parallel, companies such as TuSimple, Embark, and Starsky are concentrating on fully autonomous interstate trucking.", "image_caption": "Sled on the snow", "metadata": {"article_id": "issue_19", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/driverless20cars.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-19/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_19.html"}}
{"id": 80815242004, "type": "news_chunk", "title": "Biggest AI Stories of 2019: Driverless Cars Stall, Deepfakes...", "subtitle": "Deepfakes Go Mainstream", "content": "Society awakened to the delight, threat, and sheer weirdness of realistic images and other media dreamed up by computers.What happened: So-called deepfakes became both more convincing and easier to make, stoking a surge of fascination and anxiety that shows every sign of intensifying in the coming year.Driving the story: Two years ago, the majority of deepfakes were pixelated and difficult to make. Now they’re slicker than ever and improving at a quick clip. Late 2018 brought stand-out models like BigGAN, which creates images of the classes found in ImageNet, and StyleGAN, which generates variations such as poses, hairstyles, and clothing. In early 2019, researchers also developed a network that makes realistic talking-head models from a single photo, raising the question of whether people actually said the things you watched them say.The technology found positive uses such as making English football star David Beckham appear to deliver an anti-malaria message in nine languages. Chinese tech giant Momo released Zao, an app that maps users’ faces onto characters in scenes from popular movies.Yet deepfakes also showed their dark side. Scammers bilked a UK energy company of hundreds of thousands of dollars using fake audio of the CEO’s voice. The technology was implicated in political scandals in Malaysia and Gabon.A report by Deeptrace Labs, which sells deepfake detection software, found that 96 percent of deepfake videos online were non-consensual porn — mostly faces of female celebrities rendered on computer-generated naked bodies. The reaction: Facebook, beset by a fake video of CEO Mark Zuckerberg appearing to gloat at his power over the social network’s members, announced a $10 million contest to automate deepfake detection. Meanwhile, China enacted restrictions on spreading falsified media. In the U.S., the state of California passed a similar law, while the House of Representatives considers national anti-deepfake legislation. Where things stand: Detecting and controlling deepfakes is shaping up to be a high-tech game of cat and mouse. Although today’s fakes bear telling features, they’ll be indistinguishable from real images within a year, according to USC computer science professor Hao Li.", "image_caption": "Illustration of three identical reindeers", "metadata": {"article_id": "issue_19", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/generative20models-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-19/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_19.html"}}
{"id": 80815242005, "type": "news_chunk", "title": "Biggest AI Stories of 2019: Driverless Cars Stall, Deepfakes...", "subtitle": "Simulation Substitutes for Data", "content": "The future of machine learning may depend less on amassing ground-truth data than simulating the environment in which a model will operate.What happened: Deep learning works like magic with enough high-quality data. When examples are scarce, though, researchers are using simulation to fill the gap. Driving the story: In 2019, models trained in simulated environments accomplished feats more complex and varied than previous work in that area. In reinforcement learning, DeepMind’s AlphaStar achieved Grandmaster status in the complex strategy game StarCraft II — able to beat 99.8 percent of human players — through tens of thousands of virtual years competing in a virtual league. OpenAI Five similarly trained a team of five neural nets to best world champions of Dota 2. But those models learned in a virtual world to act in a virtual world. Other researchers transferred skills learned in simulations to the real world. OpenAI’s Dactyl robot hand spent the simulated equivalent of 13,000 years in virtual reality developing the dexterity required to manipulate a Rubik’s Cube puzzle. Then it applied those skills to a physical cube. It was able to solve the puzzle in 60 percent of tries when unscrambling the colored sides required 15 or fewer twists of the cube. Its success rate dropped to 20 percent when solving the puzzle required more moves.Researchers at CalTech trained a recurrent neural network to differentiate overlapping and simultaneous earthquakes by simulating seismic waves rippling across California and Japan and using the simulations as training data.Amazon’s Aurora self-driving vehicle unit runs hundreds of simulations in parallel to train its models to navigate urban environments. The company is training Alexa’s conversational faculties, delivery drones, robots for its fulfillment centers in a similar way. Where things stand: Simulation environments like Facebook’s AI Habitat, Google’s Behavior Suite for Reinforcement Learning and OpenAI’s Gym offer resources for mastering tasks like optimizing textile production lines, filling in blank spots in 3D imagery, and detecting objects in noisy environments. On the horizon, models could explore molecular simulations to learn how to design drugs with desired outcomes.", "image_caption": "Illustration of a crystal snowball", "metadata": {"article_id": "issue_19", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/RL.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-19/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_19.html"}}
{"id": 80815242006, "type": "news_chunk", "title": "Biggest AI Stories of 2019: Driverless Cars Stall, Deepfakes...", "subtitle": "A Smoldering Conflict Flares", "content": "A year-long Twitter feud breathed fresh life into a decades-old argument over AI’s direction.What happened: Gary Marcus, a New York University professor, author, entrepreneur, and standard bearer of logic-based AI, waged a tireless Twitter campaign to knock deep learning off its pedestal and promote other AI approaches. Driving the story: Marcus’ incessant tweets reignited an old dispute between so-called symbolists, who insist that rule-based algorithms are crucial to cognition, and connectionists, who believe that wiring enough neurons together with the right loss function is the best available path to machine intelligence. Marcus needled AI practitioners to reacquaint itself with the symbolist approach lest connectionism’s limitations precipitate a collapse in funding, or AI winter. The argument prompted sobering assessments of AI’s future and culminated in a live debate on December 23 between Marcus and deep learning pioneer and Université de Montréal professor Yoshua Bengio. The conversation was remarkably civil, and both participants acknowledged the need for collaboration between partisans on both sides. Marcus kicked off his offensive in December 2018 by challenging deep learning proponents over what he termed their “imperialist” attitude. He went on to goad Facebook’s Yann LeCun, a deep learning pioneer, to choose a side: Did he place his faith in pure deep learning, or was there a place for good old-fashioned AI?OpenAI made headlines in October with a hybrid model. Its five-fingered robot hand solved the Rubik’s Cube puzzle through a combination of deep reinforcement learning and Kociemba’s algorithm. While Marcus pointed out that Kociemba, not deep learning, computed the solution, others asserted that the robot could have learned this skill with further training.Microsoft stepped into the breach in December with what it calls neurosymbolic AI, a set of model architectures intended to bridge the gap between neural and symbolic representations.As the year drew to a close, the NeurIPS conference highlighted soul searching in the AI community. “All of the models that we have learned how to train are about passing a test or winning a game with a score, [but] so many things that intelligences do aren’t covered by that rubric at all,” Google researcher Blaise Agüera y Arcas stated in a keynote. Behind the news: Animosity between the symbolists and connectionists dates back more than a half-century. Perceptions, a 1969 broadside against early neural networks, helped trigger the first AI winter. The second, nearly two decades later, came about partly because symbolic AI relied on LISP computers that became obsolete with the advent of personal computers. Neural nets began to gain ground in the 1990s and achieved dominance amid the last decade’s explosion of computing power and data. Where things stand: We look forward to exciting times ahead as connectionists and symbolists put their heads together, or until one faction wipes out the other.", "image_caption": "Illustration of two people playing a snowball fight", "metadata": {"article_id": "issue_19", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/twitter20fight.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-19/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_19.html"}}
{"id": 73320113001, "type": "news_chunk", "title": "Gunning for GPUs, Football for DRL, Navigation for the Blind...", "subtitle": "Size Matters", "content": "Dear friends, We ended a busy week in Colombia hosting a Pie & AI meetup in Medellín. I met hundreds of engineers and business professionals excited to take the next step in their AI careers. I was energized by the enthusiasm the Colombia community had for collaborating and for supporting each other to build up the local AI ecosystem. AI is still young enough that many cities can still become hubs of AI talent, but the city has to make smart investments, and the community has to work hard and keep learning, which Colombia is doing. I hope the future will bring many more global AI hubs. I also drank a lot of coffee on this trip. I don’t know whether it was because the coffee really was fresher or if it was a placebo effect, but Colombian coffee tasted better in Colombia than when I drink it at home! Keep learning, Andrew Silicon Valley startup Cerebras shifted out of stealth mode to unveil its flagship product: an enormous chip designed from the ground up to accelerate neural networks. What’s new: The Cerebras Wafer Scale Engine is aimed at data centers, where the company claims it will perform AI computations 100 to 1,000 times faster than alternatives. The chips will be housed in servers equipped with a special cooling system to dissipate the chip’s heat. They’re scheduled to reach the market next month for an undisclosed price. Why it’s different: Where many chips are measured in millimeters, this monster is 56 times larger than Nvidia’s top-of-the-line GPU and bigger than a standard iPad. It comprises more than 400,000 cores and 18 gigabytes of memory right on the chip. That’s equivalent to 84 GPUs communicating with one another 150 times more efficiently than usual, with an additional boost thanks to the ability to handle sparse linear algebra. How it works: Nvidia’s chip architecture is extraordinarily efficient at performing the predictable, repetitive matrix multiplications required by neural networks. Yet it has practical limitations: It must hold an entire neural network in off-chip memory and communicate with other chips through external interfaces that are far slower than communication on the chip itself. By putting all computing resources on a single piece of silicon, the new chip makes it possible to process neural networks at top speed.For even higher efficiency, it processes sparse networks by pruning unnecessary calculations. Behind the news: Deep learning’s rapid growth has prompted a top-to-bottom redesign of computing systems to accelerate neural network training. Cerebras is a front runner among a plethora of startups working on AI chips.And not only startups: Amazon, Facebook, Google, and Tesla have all designed chips for in-house use.Among traditional chip companies, Nvidia has progressively retooled its GPUs to accelerate deep learning, Intel is rolling out its competing Nervana technology, and Qualcomm has been building inferencing engines into its smartphone chips.Cerebras is the only one to opt for a wafer-scale chip. Soon, it may become the first company to have overcome the considerable technical hurdles to putting a wafer-scale chip into production. Why it matters: If the new hardware works as advertised, it will open virgin territory for neural networks several orders of magnitude bigger than today’s largest models. Larger models have been shown to yield higher accuracy, and the additional headroom may well allow new kinds of models that wouldn’t be practical otherwise. We’re thinking: The advent of Nvidia GPUs two decades ago spurred innovations in model architecture that boosted the practical number of network layers from handfuls to 1,000-plus. Cerebras’ approach portends fresh architectures capable of solving problems that are currently out of reach. We don’t yet know what those models will look like, but we’re eager to find out!", "image_caption": "Wafer Scale chip", "metadata": {"article_id": "issue_2", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_cerebras220sized.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-2/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_2.html"}}
{"id": 73320113002, "type": "news_chunk", "title": "Gunning for GPUs, Football for DRL, Navigation for the Blind...", "subtitle": "Get Your Kicks With DRL", "content": "Researchers typically test deep reinforcement learning algorithms on games from Space Invaders to StarCraft. The Google Brain team in Zurich adds another option: football, also known as soccer.What’s new: Google Research Football allows experiments on a variety of RL techniques in a single environment: self playing, stochastic environment, multi-agent cooperation, and several styles of state representation. Check out the video here.Key insight: Popular games generally are either easy to win or offer rewards that are too sparse. Most don’t allow for cooperative agents or graduated degrees of difficulty that would help the agents learn basic strategies. Google Research Football is designed to solve all these problems in one go, and it’s open source to boot.How it works: Karol Kurach and his team provide a physics-based soccer simulator with full-length, 11-player games at a range of difficulty levels. They also offer short scenarios from simple (single player scoring in an empty net) to complex (team coordination to score from a corner kick). Users can build their own scenarios as well. The game state can be represented in three ways: a vector encapsulating 115 features, a full pixel-wise frame, and a “super mini map” of coordinates and speed of every player as well as the ball.Players can perform 16 actions, including directional movement, passing, dribbling, and shooting.The authors implement three state-of-the-art RL algorithms, two using policy gradients (PPO and IMPALA) and one that uses Q-learning (Ape-X DQN), and report their performance on Google Research Football. Observations: The algorithms supplied quickly solve the easy situations, but they struggle on medium and hard settings even after long periods of training. Performance also depends on the input representation and the number of agents involved.Why it matters: GRF is a challenge even for today’s best RL algorithms. It gives researchers a multi-agent environment where they can work on improving agents by having them compete with one another, and it provides resources for building more capable agents through increasing degrees of difficulty in an environment that resembles the real world. We’re thinking: This might be a good time to take to the virtual field and compete for the football leaderboard, as reinforcement learning begins to take on the world’s most popular sport.", "image_caption": "Google Research Football video", "metadata": {"article_id": "issue_2", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_soccer.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-2/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_2.html"}}
{"id": 73320113003, "type": "news_chunk", "title": "Gunning for GPUs, Football for DRL, Navigation for the Blind...", "subtitle": "In a Galaxy Far, Far Away", "content": "The origin of the brief, high-intensity signals from outer space called fast radio bursts baffles astronomers. Now AI is generating real-time data to help solve the mystery. What’s new: A machine learning model deployed at the Molonglo Radio Telescope in Australia detected five fast radio bursts in unprecedented detail. How it works: The Molonglo telescope uses a standard program to flag incoming electromagnetic waves as fast radio burst candidates. However, the mystery signals share the same frequency band as cell phones, lightning storms, and solar emissions, so the system is prone to false positives. Researcher Wael Farah developed a machine learning model to pick out the most viable candidates. Farah first trained the model on recordings of pulsars. Those signals resemble fast radio bursts, but scientists have many more recordings of them and know enough about them to train the model to differentiate them.The model compares incoming signals against known features of fast radio bursts, such as the rate at which their higher frequencies disperse as they cross the cosmos.The model pared down each day’s fast radio burst candidates from tens of thousands to tens, a manageable number for the telescope’s human staff to verify. Results: Since the model debuted in April, 2018, it has flagged the most energetic fast radio burst and the one with the broadest spectrum, and it has captured the most detailed view of the signals’ rapidly fluctuating voltage. Behind the news: Earlier this year, American scientist Brian Metzger won a $3 million Breakthrough Prize for his work on a theory about the genesis of fast radio bursts — not SOSes from an alien intelligence, sadly, but shock waves produced by young neutron stars with dense magnetic fields. Why it matters: Testing ideas about fast radio bursts requires more, and more detailed, data. Farrah’s model delivers it. We’re thinking: Telescopes collect a crushing torrent of data. With the help of AI, human astronomers might manage to analyze them before the universe’s Big Crunch.", "image_caption": "One of the Fast Radio Bursts showing remarkable structure in time and radio frequency", "metadata": {"article_id": "issue_2", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_radio20waves20sized-1024x576.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-2/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_2.html"}}
{"id": 73320113004, "type": "news_chunk", "title": "Gunning for GPUs, Football for DRL, Navigation for the Blind...", "subtitle": "Second Sight", "content": "Unlike bats, humans can’t see with their ears. Now an app is giving sightless pedestrians the ability to navigate by ear. What’s new: Microsoft’s Artificial Intelligence and Research Laboratory offers a free iPhone app called Soundscape. Unlike earlier efforts that tried to identify objects visually, the app orients pedestrians in space by calling out nearby buildings, businesses, landmarks, and road crossings as the walker approaches.How it works: Essentially a navigation app. But unlike conventional navigation apps that issue directions to a destination (“Turn left here!”), Soundscape narrates points of interest along the way. That helps people who don’t see well to explore like a sighted person can — for example, popping into a bakery that caught their attention on the way to work, or simply taking a random walk. Soundscape interprets a phone’s GPS signals and accelerometer to determine the user’s location, trajectory, and facing direction. It pulls labels from a map to describe the surrounding environment.Then it speaks these descriptions in stereo. If the user passes a store on the left, the narrator’s voice sounds in their left ear. If the user approaches a crosswalk on the right, they’ll hear about it in their right ear.Users can also set homing beacons. Select a destination — say, the entrance ramp to their favorite coffee shop. The app provides a soft, snappy drum beat that increases in volume as you approach the destination. It adds a rhythmic ping when you face the destination directly. Behind the news: Project lead Amos Miller, a developer and product strategist at Microsoft, lost his sight as an adult due to a genetic condition. You can hear an interview with him in this podcast.Why it matters: Several previous apps for visually impaired people attempt to replace human vision with computer vision: Point a camera at an object or person, and the app classifies what it sees. That approach has yet to catch on, leaving the field ripe for fresh approaches. We’re thinking: Soundscape isn’t just for the sight-impaired. It may be worth a try the next time you visit a new city and want to take in the sights without constantly referring to a map.", "image_caption": "Video showing how Soundscape app works", "metadata": {"article_id": "issue_2", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_ezgif.com-resize.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-2/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_2.html"}}
{"id": 73320113005, "type": "news_chunk", "title": "Gunning for GPUs, Football for DRL, Navigation for the Blind...", "subtitle": "Scaling Bayes", "content": "Neural networks are good at making predictions, but they’re not so good at estimating how certain they are. If the training data set is small and many sets of model parameters fit the data well, for instance, the network may not realize this explicitly, leading to overly confident predictions. Bayesian models, on the other hand, theoretically can sample from the posterior distribution of parameters. However, the computational load becomes overwhelming as the number of parameters rises. New research allows Bayesian modeling of uncertainty to be applied even to large networks. What’s new: Researchers at Google Brain built neural networks that integrate a Bayesian backpropagation method known as Stochastic Gradient Markov Chain Monte Carlo, fixing issues with noisy updates and slow convergence that affected earlier work. Their technique, Adaptive Thermostat Monte Carlo (ATMC), is the first based on SG-MCMC that scales to larger data sets such as ImageNet. Key insight: Previous research using SG-MCMC failed to find training procedures that were robust to noise arising from parameter sampling in Bayesian methods. ATMC compensates for these issues by adjusting momentum and noise applied to parameter updates. How it works: Non-Bayesian learning techniques compute the loss from outputs and labels only. Bayesian techniques add a prior distribution on learnable parameters. All methods based on SG-MCMC are derived from a stochastic differential equation that modifies a neural network’s parameter distribution based on the sampled output. ATMC samples learnable parameters from the distribution, and the network backpropagates its errors.Then it modifies the computed gradients to ensure that noisy sampling doesn’t overly influence shifts in the parameter distribution.It makes convergence faster and more stable than prior variations of SG-MCMC by dynamically adjusting momentum and noise added to each parameter update.In addition, the authors provide an adjusted ResNet architecture better suited for Bayesian training. The new model replaces batch normalization with SELU activation and uses a different weight initialization. Results: ATMC is the first SG-MCMC method successfully trained on ImageNet. An ATMC-trained network gains a 1 percent increase over a batch-normalized ResNet in ImageNet top-1 accuracy. Why it matters: Estimating uncertainty can be crucial in applications such as medical imaging and autonomous driving. ATMC confers this capability on neural networks even when learning large, complex data sets such as ImageNet. We’re thinking: Bayesian methods have been studied longer than neural networks, and they still define the state of the art in some tasks. The fusion of Bayesian models and neural networks is still evolving. ATMC suggests that such hybrids could deliver the advantages of both approaches.", "image_caption": "Calibration plot for ImageNet", "metadata": {"article_id": "issue_2", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Screen20Shot202019-08-2720at203.12.1620PM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-2/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_2.html"}}
{"id": 72837151001, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "High Hopes for 2020", "content": "Dear friends, Happy New Year!Every winter holiday, I pursue a learning goal around a new topic. In between visits with family, I end up reading a lot.About a decade ago, my holiday topic was pedagogy — I still remember lugging a heavy suitcase of books through the airport — and this helped the early days of Coursera. Last year, before Nova’s birth, I read a pile of books on child care.This holiday, I’ve been catching up on epigenetics and the emerging science (and sometimes quackery) of anti-aging. I also visited my 101-year-old grandfather. I told him what I was reading, and he said that remaining curious is the key to longevity.If he’s right, then I think many of you will thrive well past 101!Wishing you a wonderful 2020, with lots of curiosity, learning, and love. Keep learning! We enter a new decade with great expectations of prosperity, as machine learning finds its place in traditional industries from manufacturing to the arts. Yet we face important questions about how to use it without causing harm through careless data collection, slipshod system design, or the limits of our ability to see around the next corner. In this special issue of The Batch, some of the brightest lights in AI express their hopes for 2020.", "image_caption": "Andrew Ng with his grandfather", "metadata": {"article_id": "issue_20", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-12-3120at202.29.5920PM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 72837151002, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "Anima Anandkumar: The Power of Simulation", "content": "We’ve had great success with supervised deep learning on labeled data. Now it’s time to explore other ways to learn: training on unlabeled data, lifelong learning, and especially letting models explore a simulated environment before transferring what they learn to the real world. In 2020, I hope to see more research in those areas. High-fidelity simulation lets us train and test algorithms more effectively, leading to more robust and adaptive networks. Models can gain far more experience in the virtual world than is practical in the real world. We can simulate rare events that pose severe challenges but are seldom represented by ground truth. For instance, when we’re driving a car, accidents are rare. You won’t see all the variations even if you drive hundreds of thousands of miles. If we train autonomous cars only on real-world data, they won’t learn how to manage the wide variety of conditions that contribute to accidents. But in a simulation, we can generate variation upon variation, giving the model a data distribution that better reflects real-world possibilities, so it can learn how to stay safe. Lately, simulation has helped achieve impressive results in reinforcement learning, which is extremely data-intensive. But it’s also useful in supervised learning, when researchers may have only small amounts of real-world data. For instance, earthquakes are rare and difficult to measure. But researchers at Caltech’s seismology lab used a simple physical model to create synthetic data representing these events. Trained on synthetic data, their deep learning model achieved state-of-the-art results predicting properties of real-world earthquakes. At Nvidia, we’ve developed powerful simulation platforms like Drive Constellation for autonomous vehicles and Isaac for robotics. These open, scalable environments enable models to act in a photorealistic virtual world, complete with highly accurate physics. I hope that more AI scientists will come to recognize the value of training in simulated environments, as well as other techniques beyond supervised learning. That would make 2020 a year of great progress in AI. Anima Anandkumar is director of machine learning research at Nvidia and a professor of computer science at Caltech.", "image_caption": "Anima Anandkumar", "metadata": {"article_id": "issue_20", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Anima20Anandkumar20SIZED-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 72837151003, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "Oren Etzioni: Tools For Equality", "content": "In 2020, I hope the AI community will grapple with issues of fairness in ways that tangibly and directly benefit disadvantaged populations.We’ve spent a lot of time talking about fairness and transparency in our algorithms, and this is essential work. But developing software tools that have a tangible impact is where the rubber meets the road. AI systems designed to improve people’s lives could help solve some of society’s major challenges.Imagine what it’s like to use a smartphone navigation app in a wheelchair — only to encounter a stairway along the route. Even the best navigation app poses major challenges and risks if users can’t customize the route to avoid insurmountable obstacles. Technology exists to support people with limited mobility, including AccessMap, a project of the University of Washington’s Taskar Center for Accessible Technology. But we could do so much more. Thankfully, we are living in a time when we have the means to do it at our fingertips.Accessibility, education, homelessness, human trafficking — AI could have a major positive impact on people’s quality of life in these areas and others. So far, we’ve only scratched the surface. Let’s dig deep in the coming year. Oren Etzioni is chief executive of the Allen Institute for AI, a professor of computer science at the University of Washington, and a partner at Madrona Venture Group.", "image_caption": "Oren Etzioni", "metadata": {"article_id": "issue_20", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Oren20Etzioni20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 72837151004, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "Chelsea Finn: Robots That Generalize", "content": "Many people in the AI community focus on achieving flashy results, like building an agent that can win at Go or Jeopardy. This kind of work is impressive in terms of complexity. But it’s easy to forget another important axis of intelligence: generalization, the ability to handle a variety of tasks or operate in a range of situations. In 2020, I hope to see progress on building models that generalize. My work involves using reinforcement learning to train robots that reason about how their actions will affect their environment. For example, I’d like to train a robot to perform a variety of tasks with a variety of objects, such as packing items into a box or sweeping trash into a dustpan. This can be hard to accomplish using RL. In supervised learning, training an image recognizer on ImageNet’s 14 million pictures tends to result in a certain degree of generalization. In reinforcement learning, a model learns by interacting with a virtual environment and collecting data as it goes. To build the level of general skill we’re accustomed to seeing in models trained on ImageNet, we need to collect an ImageNet-size dataset for each new model. That’s not practical. If we want systems trained by reinforcement learning to generalize, we need to design agents that can learn from offline datasets, not unlike ImageNet, as they explore an environment. And we need these pre-existing datasets to grow over time to reflect changes in the world, just as ImageNet has grown from its original 1 million images. This is starting to happen. For example, robots can figure out how to use new objects as tools by learning from a dataset of their own interactions plus demonstrations performed by humans guiding a robot’s arm. We’re figuring out how to take advantage of data from other institutions. For instance, we collected a dataset of robots interacting with objects from seven different robot platforms across four institutions. It’s exciting to see critical mass developing around generalization in reinforcement learning. If we can master these challenges, our robots will be a step closer to behaving intelligently in the real world, rather than doing intelligent-looking things in the lab. Chelsea Finn is an assistant professor of computer science and electrical engineering at Stanford.", "image_caption": "Chelsea Finn", "metadata": {"article_id": "issue_20", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chelsea20Finn20SIZED-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 72837151005, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "Yann LeCun: Learning From Observation", "content": "How is it that many people learn to drive a car fairly safely in 20 hours of practice, while current imitation learning algorithms take hundreds of thousands of hours, and reinforcement learning algorithms take millions of hours? Clearly we’re missing something big. It appears that humans learn efficiently because we build a model of the world in our head. Human infants can hardly interact with the world, but over the first few months of life they absorb a huge amount of background knowledge by observation. A large part of the brain apparently is devoted to understanding the structure of the world and predicting things we can’t directly observe because they’re in the future or otherwise hidden. This suggests that the way forward in AI is what I call self-supervised learning. It’s similar to supervised learning, but instead of training the system to map data examples to a classification, we mask some examples and ask the machine to predict the missing pieces. For instance, we might mask some frames of a video and train the machine to fill in the blanks based on the remaining frames. This approach has been extremely successful lately in natural language understanding. Models such as BERT, RoBERTa, XLNet, and XLM are trained in a self-supervised manner to predict words missing from a text. Such systems hold records in all the major natural language benchmarks. In 2020, I expect self-supervised methods to learn features of video and images. Could there be a similar revolution in high-dimensional continuous data like video? One critical challenge is dealing with uncertainty. Models like BERT can’t tell if a missing word in a sentence is “cat” or “dog,” but they can produce a probability distribution vector. We don’t have a good model of probability distributions for images or video frames. But recent research is coming so close that we’re likely to find it soon. Suddenly we’ll get really good performance predicting actions in videos with very few training samples, where it wasn’t possible before. That would make the coming year a very exciting time in AI. Yann LeCun is vice president and chief AI scientist at Facebook and a professor of computer science at New York University.", "image_caption": "Yann LeCun", "metadata": {"article_id": "issue_20", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Yann20LeCun20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 72837151006, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "Kai-Fu Lee: AI Everywhere", "content": "Artificial intelligence has moved from the age of discovery to the age of implementation. Among our invested portfolios, primarily in China, we see flourishing applications using AI and automation in banking, finance, transportation, logistics, supermarkets, restaurants, warehouses, factories, schools, and drug discovery. Yet, looking at the overall economy, only a small percentage of businesses is starting to use AI. There is immense room for growth. I believe that AI will be as important as electricity in the history of mankind’s technological advancement. In the next decade or two, AI will penetrate our personal and business lives, delivering higher efficiency and more intelligent experiences. It is time for businesses, institutions, and governments to embrace AI fully and move society forward. I am most excited about the impact of AI on healthcare and education. These two sectors are ready for AI disruption and can deploy AI for good. We invested in a company that uses AI and big data to optimize supply chains, reducing medication shortages for over 150 million people living in rural China. We are also funding drug discovery companies that combine deep learning and generative chemistry to shorten drug discovery time by a factor of three to four. In education, we see companies developing AI solutions to improve English pronunciation, grade exams and homework, and personalize and gamify math learning. This will free teachers from routine tasks and allow them to spend time building more inspirational and stimulating connections with up-and-coming generations of students. I hope to see more bright entrepreneurs and businesses start using AI for good in 2020 and years to come. Kai-Fu Lee is chairman and chief executive of Sinovation Ventures.", "image_caption": "Kai-Fu Lee", "metadata": {"article_id": "issue_20", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Kai-Fu20Lee20SIZED-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 72837151007, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "David Patterson: Faster Training and Inference", "content": "Billions of dollars invested to create novel AI hardware will bear their early fruit in 2020. Google unleashed a financial avalanche with its tensor processing unit in 2017. The past year saw specialized AI processors from Alibaba, Cerebras, Graphcore, Habana, and Intel, with many others in the pipeline. These new chips will find their way slowly into research labs and data centers. I hope the AI community will embrace the best of them, pushing the field toward better models and more valuable applications. How can machine learning engineers know whether a newfangled alternative performs better than the conventional CPU-plus-GPUs combo? Computer architecture is graded on a curve rather than an absolute scale. To account for differing computer sizes, we normalize performance by price, power, or numbers of chips. Competitors select a set of representative programs to serve as a benchmark. Averaging scores across many of these programs is more likely to reflect real performance than scores on any single one. MLPerf is a recent benchmark for machine learning created by representatives from more than 50 companies and nine universities. It includes programs, data sets, and ground rules for testing both inference and training, specifying important details like the accuracy target and valid hyperparameter values. New versions occur every three months (alternating inference and training) to keep up with rapid advances in machine learning. Not every product can win a fair comparison, so some marketing departments may sidestep MLPerf, saying some version of, “Our customers don’t care about the programs in MLPerf.” But don’t be fooled. First, MLPerf welcomes new programs, so if a given workload isn’t in MLPerf, it can be added. Second, competitors check MLPerf results for fairness to ensure apples-to-apples comparisons. Caveat emptor. Ask to see MLPerf scores! David Patterson is a professor of computer science at UC Berkeley.", "image_caption": "David Patterson", "metadata": {"article_id": "issue_20", "chunk_index": 7, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DavidPatterson220ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 72837151008, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "Richard Socher: Boiling the Information Ocean", "content": "Ignorance is a choice in the Internet age. Virtually all of human knowledge is available for the cost of typing a few words into a search box. But managing the deluge of facts, opinions, and perspectives remains a challenge. It can be hard to know what information you’ll find in a lengthy document until you’ve read it, and knowing whether any particular statement is true is very difficult. Automatic summarization can do a lot to solve these problems. This is one of the most important, yet least solved, tasks in natural language processing. In 2020, summarization will take important steps forward, and the improvement will change the way we consume information. The Salesforce Research team recently took a close look at the field and published a paper that evaluates the strengths and weaknesses of current approaches. We found that the datasets used to train summarizers are deeply flawed. The metric used to measure their performance is deeply flawed. Consequently, the resulting models are deeply flawed. We’re working on solutions to these problems. For instance, researchers evaluate summarization performance using the ROUGE score, which measures overlap in words between source documents, automated summaries, and human-written summaries. It turns out that summarizers based on neural networks can make mistakes and still earn high ROUGE scores. A model can confuse the names of a crime’s perpetrator and its victim, for example. ROUGE measures the fact that the names appear in both generated and human-made summaries without taking the error into account. We introduced a model that makes it easy to examine factual consistency between source documents and summaries. We also proposed a metric to evaluate summarizers for factual consistency. Ranking summarizers according to this metric in addition to ROUGE will help researchers develop better models, and that will speed progress in other areas, such as maintaining logical coherence throughout a long summary. This kind of development gives me confidence that 2020 will be a great time for summarization, and for NLP in general. The progress I expect to see in the coming year will help people not only to cope with the ceaseless flood of new information, but also to embrace AI’s great potential to make a better world. Richard Socher is chief scientist at Salesforce.", "image_caption": "Richard Socher", "metadata": {"article_id": "issue_20", "chunk_index": 8, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Richard20Socher20SIZED-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 72837151009, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "Dawn Song: Taking Responsibility for Data", "content": "Datasets are critical to AI and machine learning, and they are becoming a key driver of the economy. Collection of sensitive data is increasing rapidly, covering almost every aspect of people’s lives. In its current form, this data collection puts both individuals and businesses at risk. I hope that 2020 will be the year when we build the foundation for a responsible data economy.Today, users have almost no control over how data they generate are used. All kinds of data are shared and sold, including fine-grained locations, medical prescriptions, gene sequences, and DMV registrations. This activity often puts personal privacy and sometimes even national security at risk. As individuals become more aware of these issues, they are losing trust in the services they use.At the same time, businesses and researchers face numerous challenges in taking advantage of data. First, large scale data breaches continue to plague businesses. Second, with Europe’s General Data Protection Regulation, California’s Consumer Privacy Act, and similar laws, it is becoming more difficult and expensive for businesses to comply with privacy regulations. Third, valuable data are siloed, impeding technical progress. For example, easier use of medical data across institutions for machine learning could lead to improvements in healthcare for everyone.Changing this broken system into a responsible data economy requires creating new technologies, regulations, and business models. These should aim to provide trustworthy protection and control to data owners (both individuals and businesses) through secure computation, the ability to audit, and machine learning that maintains data privacy. Secure computation can be provided by secure hardware (such as Intel SGX and Keystone Enclave) and cryptographic techniques. Those computations can be made auditable by tying encrypted storage and computation to a distributed ledger.Greater challenges remain on the machine learning side. In 2020, we can expand on current efforts in differentially private data analytics and machine learning, building scalable systems for practical deployment with large, heterogeneous datasets. Further research and deployment of federated learning also will be important for certain use cases. Finally, advances in robust learning from limited and noisy data could help enable a long tail of ML use cases without compromising privacy.We are building parts of this vision at Oasis Labs, but there is much more to be done. I hope this year that technologists, businesses, regulators, and the AI community will join us in building the foundation for a truly responsible data economy. Dawn Song is chief executive and co-founder of Oasis Labs and a professor of computer science and electrical engineering at UC Berkeley.", "image_caption": "Dawn Song", "metadata": {"article_id": "issue_20", "chunk_index": 9, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Dawn20Song20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 72837151010, "type": "news_chunk", "title": "Hopes for AI in 2020: Yann LeCun, Kai-Fu Lee, Anima Anandkumar", "subtitle": "Zhi-Hua Zhou: Fresh Methods, Clear Guidelines", "content": "I have three hopes for 2020: Hope that advanced machine learning techniques beyond deep neural networks can emerge. Neural networks have been studied and applied by many researchers, engineers, and practitioners for a long time. Other machine learning techniques offer relatively unexplored spaces for technical innovation.Hope that AI can come into more fields and bring more positive changes to people’s everyday lives.Hope for more thinking and discussion about what AI researchers, engineers, and practitioners must do to prevent wrong developments or misuses of AI techniques. Zhi-Hua Zhou is a professor of computer science and artificial intelligence at Nanjing University.", "image_caption": "Zhi-Hua Zhou", "metadata": {"article_id": "issue_20", "chunk_index": 10, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Zhi-Hua20Zhou20SIZED-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-20/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_20.html"}}
{"id": 96134972001, "type": "news_chunk", "title": "Facebook Takes on Deepfakes, Google AI Battles Cancer, AI Grows", "subtitle": "ImageNet Gets a Makeover", "content": "Dear friends, Many accomplished students and newly minted AI engineers ask me: How can I advance my career? Companies in many industries are building AI teams, but it may not be obvious how to join one of them. Different companies organize their teams differently and use different terms to describe the same job. Even more confusing, job titles don’t correspond directly with common AI tasks like modeling and data engineering. What positions are responsible for which tasks? What skills are recruiters looking for? Which opportunities are right for you? Workera, a deeplearning.ai affiliate, interviewed over 100 leaders in machine learning and data science to answer these questions. They summarized their findings in a report called “AI Career Pathways: Put Yourself on the Right Track.” “AI Career Pathways” is designed to guide aspiring AI engineers in finding jobs and building a career. The table above shows Workera’s key findings about AI roles and the tasks they perform. You’ll find more insights like this in the free PDF. I invite you to read Workera’s report and compare its findings with your own experience, talents, and skills. This will help you understand how AI teams work, what role might fit you best, and which skills you can develop to position yourself for a particular role. You can download it here. Keep learning! Computer scientists are struggling to purge bias from one of AI’s most important datasets.What’s new: ImageNet’s 14 million photos are a go-to collection for training computer-vision systems, yet their descriptive labels have been rife with derogatory and stereotyped attitudes toward race, gender, and sex. Researchers replaced a slew of biased labels and are working on further upgrades, according to Wired. (To be clear, the ImageNet Challenge training set is a subset of 1.2 million images and 1,000 classes.)How it works: Scientists at Princeton and Stanford, including Fei-Fei Li, who built the first version of ImageNet a decade ago, are updating both the dataset and its website. ImageNet’s labels were based on WordNet, a 1980s-era database of word relations. ImageNet’s compilers took WordNet as it was, despite changes in social standards since it was compiled. To weed out slurs and other offensive labels, the Princeton-Stanford team combed through the 2,832 descriptions in ImageNet’s <person> category. They cut nearly 60 percent of <person> labels.ImageNet’s original army of freelance labelers also often tagged photos with subjective labels. A person standing in a doorway, for instance, might be labelled host. To clean up the data, the Princeton-Stanford researchers rated words on how easy they were to visualize. They removed low-scoring words in the <person> subtree, eliminating nearly 90 percent of the remaining labels.The researchers are working to address general lack of diversity in ImageNet labels. First, they labeled people featured in ImageNet according to perceived sex, skin color, and age. Correlating these demographic identifiers with image labels like programmer or nurse, the researchers found the labels were badly skewed toward particular groups. They propose automatically balancing the diversity of images in each category. The number of images tagged both female and nurse, for instance, would be reduced until it matched those tagged male and nurse.A website update will add a button to report offensive images or labels. The researchers are developing a protocol for responding to reported issues. Behind the news: Late last year, a web app called ImageNet Roulette briefly enabled the public to experience the dataset’s biases firsthand. Users could upload images, and an ImageNet-trained model would classify any faces. The app went viral after users posted on social media selfies tagging them as criminals or racial and gender stereotypes. Why it matters: ImageNet can be used to pretrain vision models for sensitive applications like vetting job applicants and fighting crime. It is well established that biases in training data can be amplified when a model encounters real-world conditions.We’re thinking: Bias in AI has been widely discussed for years. It’s surprising that these issues in ImageNet only now are becoming widely recognized —a sign that greater education in bias should be a priority for the AI community. If such biases exist even in ImageNet, they surely exist in many more datasets.", "image_caption": "ImageNet face recognition labels on a picture", "metadata": {"article_id": "issue_21", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ImageNet20Bias20ASPECT202.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-21/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_21.html"}}
{"id": 96134972002, "type": "news_chunk", "title": "Facebook Takes on Deepfakes, Google AI Battles Cancer, AI Grows", "subtitle": "Language Modeling on One GPU", "content": "The latest large, pretrained language models rely on trendy layers based on transformer networks. New research shows that these newfangled layers may not be necessary.What’s new: Networks such as BERT and ERNIE take advantage of multi-headed attention layers to outcompete LSTM language models. But training these layers requires lots of compute on enormous GPU clusters. Stephen Merity of d⁄dx Times Labs struck a blow for garage AI with Single Headed Attention RNN (SHA-RNN), which nearly matched state-of-the-art performance after training on a single GPU for less than 24 hours. As he puts it in a tartly worded paper, “Take that, Sesame Street.”Key insight: The author set out to find a high-performance language model suitable for his personal computer. He used a single attention head out of skepticism that multiple heads are worth their computational cost. Simplifying the transformer’s feed-forward network enabled him to run the model on a single GPU.How it works: SHA-RNN is built on an LSTM to represent more explicitly the sequential nature of text. The model reads an input text sequence token by token and predicts the next token, usually a word or root of a word. The LSTM’s memory component stores important learned features.The LSTM’s output layer feeds the single-headed attention layer, which models relationships between tokens across the sequence.The attention layer’s output feeds a so-called boom layer. This layer replaces the transformer’s usual two feed-forward layers with a single feed-forward layer plus a summing layer to maintain vector length. Results: Merity tested SHA-RNN by compressing the enwik8 dataset. More accurate language models use fewer bits to represent a sequence because they know, to some extent, which words will occur. SHA-RNN achieved 1.068 bits per character compared to 0.99 by Sparse Transformer — slightly less accurate, but in half as many parameters.Yes, but: An LSTM is a good choice for sequential language-prediction tasks like enwik8. In non-sequential tasks such as fill-in-the-blanks, multi-headed attention is a better choice. A version of Transformer-XL that has even fewer parameters than SHA-RNN performed better on the compression task.Why it matters: SHA-RNN isn’t an out-and-out replacement for transformer-based networks. But it shows that LSTMs remain relevant and useful in language modeling. And if you’re looking for a way to get people to read your research, the author’s style offers pointers: This paper is a very entertaining read!We’re thinking: Researchers like to focus on optimizing state-of-the-art methods, and media hype frequently chases the latest leaderboard topper. Yet foundational algorithms remain valuable in a variety of contexts.", "image_caption": "Single Headed Attention RNN (SHA-RNN)", "metadata": {"article_id": "issue_21", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/SHA-RNN20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-21/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_21.html"}}
{"id": 96134972003, "type": "news_chunk", "title": "Facebook Takes on Deepfakes, Google AI Battles Cancer, AI Grows", "subtitle": "Facebook vs Deepfakes", "content": "Facebook announced a ban on deepfake videos, on the heels of a crackdown on counterfeit profiles that used AI-generated faces. What’s new: Facebook declared this week that it will remove deepfake videos it deems deliberately misleading. In December, the company took down hundreds of profiles that included AI-generated portraits of nonexistent people.How it worked: Facebook’s security team determined that 610 Facebook accounts, 89 pages, 156 groups, and 72 Instagram accounts related to a pro-Donald Trump, anti-Chinese government website were fakes. The accounts used AI-generated faces more extensively than experts had seen before, according to CNN. Security researchers spotted the deepfakes based on improbable biology — an oddly-angled neck, mismatched skin tones — or muddled background imagery, as illustrated above. They also looked for telltale asymmetries in features such as glasses and earrings.Researchers did not determine the sources of the images. Several websites distribute deepfake portraits, and they are becoming easier to generate from scratch. Deepfaked faces are even being used to populate dating apps.The accounts were connected to the Beauty of Life Group, which is linked to the publisher Epoch Media Group, according to the fact-check website Snopes.Collectively, the pages had 55 million followers and spent $9.5 million on ads. Behind the news: Facebook along with Amazon, Microsoft, and the Partnership on AI are running a Deepfake Challenge to spur development of technology to detect such images. Meanwhile, Google has contributed a collection of deepfakes to the FaceForensics benchmark.Why it matters: Disinformation spread by social media played a role in recent elections from the UK’s Brexit referendum to contests in the U.S. and Philippines. It can look more credible when it’s distributed by a manufactured persona. While faces copied from, say, stock-photo databases can be discovered, deepfaked faces are more difficult to invalidate. That makes deepfakes especially pernicious in this context.We’re thinking: It’s good to see Facebook taking proactive steps to purge generated media in what promises to be a long, uphill battle.", "image_caption": "AI-generated face", "metadata": {"article_id": "issue_21", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Facebook20Deepfakes.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-21/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_21.html"}}
{"id": 96134972004, "type": "news_chunk", "title": "Facebook Takes on Deepfakes, Google AI Battles Cancer, AI Grows", "subtitle": "Cancer in the Crosshairs", "content": "Computer vision has potential to spot cancer earlier and more accurately than human experts. A new system surpassed human accuracy in trials, but critics aren’t convinced.What’s new: A computer vision model for diagnosing breast cancer outperformed radiologists in the U.S. and UK, according to a study published in Nature. The announcement, however, met with skepticism from some experts. How it works: Researchers at Google Health, DeepMind, and other organizations trained a model on 76,000 X-ray images from one U.S. clinic and 30,000 from two UK screening centers. Each image came with data from a follow up visit at least a year later, when doctors either confirmed or ruled out a tumor. The researchers graded the model’s accuracy against average diagnostic accuracy in each country’s health care system. A single radiologist had checked U.S. mammograms. Compared with the radiologist, the model produced 9.4 percent fewer false negatives and 5.7 percent fewer false positives.In the UK, two human radiologists typically screened each mammogram. Compared to that more rigorous system, the model produced 2.7 percent fewer false negatives and 1.2 fewer false positives.The researchers also recruited six U.S. radiologists to analyze 500 of the images. The model outperformed that panel, particularly with respect to more invasive cancers. But it also missed a tumor that all six radiologists found. Yes, but: The study faced criticism that the dataset, model, and procedural details were not available to researchers aiming to reproduce its results. Moreover, experts said the images used in the new study didn’t adequately represent the at-risk population, according to the Advisory Board, a healthcare consultancy. Incidence of breast cancer in the sample dataset was higher than average, and the images weren’t annotated with the patients’ genetic heritage — which could skew the results, because some ethnic groups are at greater risk of developing tumors. Behind the news: Google’s study overshadowed earlier results from NYU, where researchers trained a similar model to detect cancer in mammograms. Their model scored highly on images that had been verified independently, and it matched the performance of a panel of 12 radiologists. The researchers also found that a hybrid model — which averaged a human radiologist’s decision with the model’s prediction — outperformed either one separately.Why it matters: Worldwide, breast cancer accounts for 12 percent of all cancer cases. The disease has been on the rise since 2008, with confirmed cases increasing by 20 percent and mortality by 14 percent. Meanwhile, the UK suffers a shortage of trained radiologists. Effective AI-driven detection could save countless lives.We’re thinking: Google and NYU are both making strides in computer vision for medical diagnosis, though clearly Google has a much larger PR team. We urge reporters to cover a diverse range of AI projects.", "image_caption": "Breast cancer screening", "metadata": {"article_id": "issue_21", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Google20Breast20Cancer20ASPECT202.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-21/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_21.html"}}
{"id": 96134972005, "type": "news_chunk", "title": "Facebook Takes on Deepfakes, Google AI Battles Cancer, AI Grows", "subtitle": "Tracking AI’s Global Growth", "content": "Which countries are ahead in AI? Many, in one way or another, and not always the ones you might expect. What’s new: The Stanford Institute for Human-Centered Artificial Intelligence published its 2019 Artificial Intelligence Index, detailing when, where, and how AI is on the rise. The authors also launched a Global AI Vibrancy Tool, making it easy to compare countries on a number of metrics.What it says: The report, guided by professor and entrepreneur Yoav Shoham, compiled data from all along the AI pipeline: college enrollment, journal citations, patent filings, conference attendance, job listings, and more. Some highlights: AI hiring is growing fastest in Australia, Brazil, Canada, and Singapore. The percentage of the U.S. workforce performing some sort of AI-related task grew from 0.26 percent to 1.32 percent.Argentina, Canada, Iran, and several European countries have relatively high proportions of women in the field.Total private investment in AI approached $40 billion in the U.S. last year, well ahead of runner-up China. Global private investment topped $70 billion, with startups contributing around half of that.Since 1998, the number of peer-reviewed AI research papers has more than quadrupled, and now accounts for 9 percent of published conference papers. Chinese authors published most of them. Nonetheless, U.S. papers are cited 40 percent more often than the global average. Behind the news: The AI Index is a product of the 100 Year Study on AI. Founded in 2014, the project tracks AI’s impact on jobs, education, national security, human psychology, ethics, law, privacy, and democracy.We’re thinking: William Gibson said it best: “The future is already here, it’s just not very evenly distributed.”", "image_caption": "Excerpt from 2019 Artificial Intelligence Index", "metadata": {"article_id": "issue_21", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI20Index20ASPECT202.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-21/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_21.html"}}
{"id": 96134972006, "type": "news_chunk", "title": "Facebook Takes on Deepfakes, Google AI Battles Cancer, AI Grows", "subtitle": "Easy on the Eyes", "content": "Researchers aiming to increase accuracy in object detection generally enlarge the network, but that approach also boosts computational cost. A novel architecture sets a new state of the art in accuracy while cutting the compute cycles required.What’s new: Mingxing Tan, Ruoming Pang, and Quoc Le at Google Brain modified existing feature pyramid networks to create the lightweight Bi-Directional Feature Pyramid Network. BiFPN is the cornerstone of a new object detection architecture called EfficientDet.Key insight: A typical feature pyramid network includes a pretrained image processing network that extracts features of various sizes and combines the information. Some break large features into smaller ones, while others connect smaller features to identify larger ones. BiFPN improves accuracy by using both techniques and increases efficiency by reducing the number of connections.How it works: An EfficientDet network includes an EfficientNet to extract features, BiFPNs, and classifiers to identify bounding boxes and class labels. BiFPNs create both top-down and bottom-up connections between differently sized features.Each BiFPN can also function as an additional layer, so the output of one can feed another. Stacking BiFPNs in this way makes it easier for the network to learn.The BiFPNs apply a learnable weight to features of different sizes. The weighting enables them to avoid focusing disproportionately on the larger features.The researchers remove network nodes that have only one input, eliminating connections that have little impact on the output. Results: On the COCO object detection benchmark, the largest EfficientDet network tested topped 51 percent mean average precision, which measures the accuracy of bounding boxes. That score beat the previous state of the art by 0.3 percent, yet EfficientDet had only a quarter the parameters and required 1/13 the calculations of the previous state of the art.Why it matters: Object detection continues to advance, driven by a steady stream of new innovations. EfficientDet represents two steps forward: an improvement in both accuracy and efficiency.We’re thinking: Google’s AmoebaNet image classifier, which was designed by a computer, usually outperforms human-designed models. Yet humans crafted the record-setting EfficientDet architecture. Flesh-and-blood engineers still excel at crafting neural networks — for now.", "image_caption": "EfficientDet explained", "metadata": {"article_id": "issue_21", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/EfficientDet20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-21/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_21.html"}}
{"id": 81595019001, "type": "news_chunk", "title": "AI Steals CES, Hollywood Predicts Blockbusters, Washington...", "subtitle": "AI Steals CES", "content": "Artificial intelligence was everywhere at the biggest, buzziest consumer-technology showcase in the U.S. What’s new: AI ruled the convention floor at the annual Consumer Electronics Show in Las Vegas, as numerous media outlets proclaimed. As usual, many products on display were half-baked concepts or solutions looking for problems. (Imagine collecting the training dataset for this cat litter box with scat recognition. On second thought, don’t.) Among the highlights, some were already here, some almost ready, and others still in the lab.Here: Products currently on the market represent the intersection of practical machine learning and mass-market applications. Comma Two upgrades newer cars with semi-autonomous driving capabilities including accelerating, braking, and lane keeping.OrCam’s Hear pairs with bluetooth hearing aids to help users pick out individual voices in noisy settings.Canon’s Photo Culling plug-in to Adobe’s Lightroom image processing service removes badly focussed and red-eyed photos from image libraries. Near: Many of the show’s coolest reveals are either coming soon — if all goes well — or available only to deep-pocketed customers. Samsung showed off the prototype Ballie, a rolling, spherical personal assistant that follows users around like a puppy, and SelfieType, which turns any surface into a virtual keyboard.Agricultural kingpin John Deere presented See and Spray, a coming-soon tractor attachment that uses computer vision to target individual weeds with herbicide (raising the question: Is Big Ag an up-and-coming consumer niche?).LG, Samsung, Sony, and just about every other consumer-tech giant debuted AI-enabled 8K televisions that intelligently multiply the number of on-screen pixels on screens bigger than 75 inches. You can buy one today and enjoy better big-screen picture quality, but nobody is making content for them yet. On the horizon: NEON, a Samsung-backed startup, showed impressively lifelike video imagery of virtual people (shown above). These avatars are meant to be conversational assistants, giving advice on personal fitness, health, or finance, eventually employed in service roles. Their bodies and expressions are based on captured human gestures, with customizable features like eye gaze and eyebrow motion. We’re thinking: CES 2020 was enthralled by AI. Purveyors of consumer tech must take care, though, to deliver on their promises or risk bringing on hype fatigue.", "image_caption": "Lifelike video imagery of virtual people made by NEON", "metadata": {"article_id": "issue_22", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CES2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-22/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_22.html"}}
{"id": 81595019002, "type": "news_chunk", "title": "AI Steals CES, Hollywood Predicts Blockbusters, Washington...", "subtitle": "Neural Networks Study Math", "content": "In tasks that involve generating natural language, neural networks often map an input sequence of words to an output sequence of words. Facebook researchers used a similar technique on sequences of mathematical symbols, training a model to map math problems to math solutions.What’s new: Guillaume Lample and Francois Charton built a sequence-to-sequence model that solves integrals and ordinary differential equations.Key insight: To apply machine translation to math, an equation must be represented as a sequence of characters that capture its semantics. A mathematical expression represented as a tree — with operators as internal nodes and operands as leaves — maps unambiguously to a sequence. For example, the image above shows the tree for 2 + 3*(5+2). The corresponding sequence is [+ 2 * 3 + 5 2].How it works: The authors used existing math software to generate datasets consisting of (problem, solution) pairs for integrals and ordinary differential equations. For each type of problem, they trained a separate transformer model to predict solutions. For function integration, the authors generated three datasets by differentiating a proposed solution, integrating a proposed problem (using SymPy), and integration by parts.Similarly, they generated datasets for first- and second-order ordinary differential equations starting with randomly generated functions.The models presented their results using a beam search with beam sizes [1, 10, 50]. This allowed them to consider a greater variety of possible solutions before making a final decision.Since solutions to problems of these types are easy to verify, the model was able to validate its output. In many cases, all solutions in the beam were equivalent. Results: The transformer model beat Mathematica, Matlab, and Maple on integration for the dataset generated by differentiating the solution (98.4 percent accuracy with beam size 1 compared to 84 percent for Mathematica, the best of those three math apps). It also beat the math software on differential equations with beam sizes 10 and 50. The model solved integration problems in the test set that SymPy couldn’t, showing that it generalized beyond the program used to generate its training dataset.Why it matters: Transformer networks can solve problems that dedicated commercial math programs can’t. That said, their solutions may not be 100 percent accurate.We’re thinking: Beating Mathematica is a remarkable result. Assuming the data distributions for training and test represented the most common problems in integrals and ordinary differential equations, this approach could open a vast frontier to state-of-the-art machine learning.", "image_caption": "Math equations represented as trees", "metadata": {"article_id": "issue_22", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Symbolic20Math20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-22/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_22.html"}}
{"id": 81595019003, "type": "news_chunk", "title": "AI Steals CES, Hollywood Predicts Blockbusters, Washington...", "subtitle": "Here Be Dragons", "content": "AI is contributing to paintings, music, and now a whimsical fantasy video.What’s new: The Squire is an amateur romp through a snowy realm of knights in armor and damsels in distress. The script was composed by AI Dungeon 2, an interactive text-adventure game based on the GPT-2 language model.How it works: Filmmakers Josh Johr and Dominick Todero began like any other AI Dungeon 2 player: By choosing a character (squire, mage, and so on) and setting (forest, dungeon). The program generated an internal context statement and fed it to the text engine, which responded: “You are Jake, a squire living in the kingdom of Larion...” It asked them periodically for input and generated text to advance the story. When the program declared, “GAME OVER!,” they set about planning the production. Unlike earlier text-adventure games, AI Dungeon 2 has no rules for how players can respond to prompts and no pre-programmed outcomes — everything is generated on the fly. The game’s creator, Nick Walton, fine-tuned the 1.5 billion-parameter version of GPT-2 on 30 megabytes of text scraped from an online choose-your-own-adventure forum.The game is prone to unexpected and occasionally nonsensical twists, such as when roadside bandits hand their sword to the befuddled Squire simply because he asks them to. But it anchors each session in its setting and narrative by feeding GPT-2 the previous 10 prompts, along with the original context statement, each time the player enters a new prompt, Walton told Towards Data Science.Inspired by the model CTRL, he imposed a penalty whenever the system reused previously generated words. That keeps the text generator from repeating itself, which can be a problem with the latest language models. We’re thinking: In the early days of generated music, some listeners enjoyed the jarring notes that computers often came up with. We’re still in the early days of generated narrative, but the results, for all their screwball turns, can be delightful.", "image_caption": "Excerpt from The Squire, an AI written short film", "metadata": {"article_id": "issue_22", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-22/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_22.html"}}
{"id": 81595019004, "type": "news_chunk", "title": "AI Steals CES, Hollywood Predicts Blockbusters, Washington...", "subtitle": "Forecasting Blockbusters", "content": "Could a black box become Hollywood’s crystal ball?What’s new: Warner Bros. is using an AI-powered tool that predicts a movie’s box-office success, according to Hollywood Reporter.How it works: Cinelytic promotes its software as a project-management platform to help movie execs make decisions throughout a film’s lifecycle. The company says it’s looking not to automate decision making but to make human managers more effective. The model draws on historical data including financial performance of a slew of films in various geographic markets along with their stars, genres, and other key information.Users input details of the film they’re considering, and the tool predicts foreign and domestic box office sales plus DVD/Blu-ray, cable, and broadcast revenue. By toggling parameters such as release date or key talent, execs can see how the changes might impact the numbers.Beside Warner Bros., the company’s clients include Ingenious Media (Avatar), Productivity Media, and STX. Behind the news: Hollywood honchos have been experimenting with AI to help them home in on blockbusters and award winners for a few years. A growing number of companies are after a piece of the action. Scriptbook, a Belgian company, predicts whether a movie will turn a profit by analyzing its script. The company said it has numerous Hollywood clients.The Israeli company Vault predicts a movie’s success among various demographic groups by analyzing how trailers perform online.20th Century Fox published its own research on a machine learning model that analyzes audience reaction to scenes and objects in a trailer. Why it matters: Movies can cost hundreds of millions of dollars to make, so producers are eager for any insight that can return their investment at the box office. Predictive systems could be especially helpful around film festivals, when executives often have to jump into fast-moving bidding wars.We’re thinking: This kind of approach lends itself to many industries. We look forward to one for publishing AI newsletters.", "image_caption": "Screen capture of Cinelytic searching for historical data of Nicole Kidman movies' performance", "metadata": {"article_id": "issue_22", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Cinelytic20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-22/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_22.html"}}
{"id": 81595019005, "type": "news_chunk", "title": "AI Steals CES, Hollywood Predicts Blockbusters, Washington...", "subtitle": "Two-Way Winner", "content": "AlphaGo Zero demonstrates superhuman performance playing Go, chess, and shogi. Models like R2D2 do the same playing classic Atari titles. A new approach to deep reinforcement learning is the first to achieve state-of-the-art results playing both board and video games.What’s new: DeepMind researchers Julian Schrittwieser, Ioannis Antonoglou, and Thomas Hubert adapted techniques from AlphaGo Zero to develop MuZero. While AlphaGo Zero requires knowledge of game rules, MuZero does not.Key insight: Board games like Go or chess have two players, and the only outcomes are win or lose. Video games may have only one player and offer immediate rewards. MuZero mastered these diverse conditions by learning a world model and employing AlphaGo Zero-style search.How it works: At each step in the game, MuZero considers the immediate outcome of a given move and the probability of winning if it is made. It analyzes potential consequences through a series of components. A state-representation submodel extracts information about the current game state and uses it to form a simplified description of that state.Based on the simplified state description, the value-and-policy submodel predicts the optimal move to make and the expected reward for making it.Similarly, the dynamics-and-reward submodel predicts the next game state and the immediate reward for taking a particular action.At each timestep, the value-and-policy module searches potential outcomes multiple steps ahead, and the dynamics-and-reward submodel produces many future samples. Then MuZero performs the action likely to yield the best overall rewards and value. Results: MuZero matched AlphaZero’s performance in chess, shogi, and Go with slightly less computation at each timestep. In Atari games, MuZero beat the previous state-of-the-art median score across 57 titles by 5 percent in one-tenth of the training time.Why it matters: Previous models either perform precise planning (best for board games) or learn complicated dynamics (best for video games). MuZero shows that a single model can do both.We’re thinking: Stellar performance in games attracts lots of attention, but making the translation to significant impact on real-world tasks has been a challenge. MuZero addresses some of the weaknesses of previous algorithms — a step toward making a difference beyond games.", "image_caption": "Maze action video game Pac-Man", "metadata": {"article_id": "issue_22", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Atari2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-22/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_22.html"}}
{"id": 81595019006, "type": "news_chunk", "title": "AI Steals CES, Hollywood Predicts Blockbusters, Washington...", "subtitle": "White House: “Go Easy on AI”", "content": "The Trump administration announced a hands-off policy for regulating artificial intelligence. What’s new: The White House unveiled an executive order instructing federal agencies to minimize rule-making related to AI businesses.What it says: The order guides regulators to craft rules that protect civil liberties, consider scientific research, and encourage public input while avoiding rules that might hinder innovation or weigh on company finances. The directive asks agencies to avoid making rules whenever possible, and instead establish voluntary standards or issue non-binding statements. When regs are unavoidable or already exist, agencies should shield innovators via pilot programs or waivers.Federal agencies are encouraged to “use their authority to address inconsistent, burdensome, and duplicative state laws that prevent the emergence of a national market.” This could add fuel to legal battles over state laws like Illinois’ BIPA, which is at the center of a class-action lawsuit charging that Facebook’s face recognition models violate civil liberties.The guidelines will take effect after a 60-day public comment period. Behind the news: Six members of the G7 group of the world’s leading economies have been working to build consensus on a set of international AI guidelines since 2018. The U.S. is the only member to reject them. Before the executive order was issued, US Chief Technology Officer Michael Kratsios told Wired that the guidance was part of a broader effort to inject free market principles into international AI standards, as opposed to top-down direction.Why it matters: Clear regulations allow companies to innovate without worrying that the government might suddenly change the rules. They also provide boundaries for generally accepted uses of new technology. We’re thinking: We welcome government actions that help AI reach the market while protecting citizens. Further policies that would help the field grow include investing in education and welcoming the immigration of talented scientists and engineers.", "image_caption": "White House network", "metadata": {"article_id": "issue_22", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/White20House20220ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-22/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_22.html"}}
{"id": 84721724001, "type": "news_chunk", "title": "Problematic White House AI Policy, Parked Cruise Robotaxis, and more", "subtitle": "A MESSAGE FROM LANDING AI", "content": "Learn how to identify and scope vision applications, choose a project type and model, apply data-centric AI, and develop an MLOps pipeline in “Building Computer Vision Applications” with Andrew Ng. Join us on Monday, November 6, 2023, at 10 a.m. Pacific Time. Register here", "image_caption": "Landing AI's upcoming livestream with Andrew Ng promotional banner", "metadata": {"article_id": "issue_221", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2023/11/unnamed--68-.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-221/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_221.html"}}
{"id": 3813006001, "type": "news_chunk", "title": "Algorithm Designs Living Machines, AI Interviews Job Applicants", "subtitle": "Virtual Creatures Come to Life", "content": "Dear friends, Last week brought reports that the European Union is considering a three- to five-year moratorium on face recognition in public places. Face recognition is a problematic technology with significant potential for misuse, and I celebrate the EU’s effort to protect human rights and civil society. But the blunt instrument of a long moratorium is a terrible idea. Five years is an eternity in AI, and implementing this proposal would all but guarantee that EU teams fall behind their colleagues in the U.S., China, and other nations. Contrary to popular belief, face recognition is not a solved problem. Although many teams have achieved good performance on face recognition benchmarks such as LFW, the technology still has a long way to go. Open source software makes it easy to recognize faces from a front-facing still image, but a number of hard problems remain to be solved, including multi-camera tracking, re-identification (when someone exits the frame and then re-enters), robustness to occasional camera outages, and automatic multi-camera calibration. Such capabilities will advance significantly in the next few years. Countries that have the foundation to develop this technology will pull ahead of those that don’t. It would be ironic if the EU, having slowed its own work on face recognition, were to end up having to license it from American and Chinese companies. The Universal Declaration of Human Rights remains one of the most inspirational documents I have ever read. I won’t pretend that forming good regulations is easy; it is hard because it entails hard tradeoffs. We must make sure that privacy-respecting societies don’t fall behind in technology development precisely because of those laudable values. Instead of hobbling them, we must enable them to leap ahead in a way that propagates those values. Keep learning! When artificial intelligence meets biology, even the simplest life forms can be mind-blowing.What happened: Researchers at Tufts and the University of Vermont programmed an evolutionary algorithm to design virtual organisms with specific capabilities. Then they implemented the designs using animal cells to produce living machines, as illustrated in this video.How it works: The algorithm designed organisms to meet one of four behavioral goals: locomotion, object manipulation, object transportation, and collective behavior. For each goal, the algorithm started with randomly assembled virtual organisms. Then it replaced those that performed poorly with mutated copies of better-performing versions, and so on for 100 trials.The virtual organisms consisted of two building blocks: Elements that contract and those that passively hold the structure together.The researchers built the most successful virtual organisms using cells harvested from frogs. In these biological versions — globs of tissue around 1 millimeter wide — pumping heart cells substituted for contracting elements and skin cells replaced structural ones.The team set these tiny Frankensteins loose in petri dishes and monitored how closely the copies replicated the behaviors of their virtual progenitors. The biological versions usually required a few iterations before they performed as expected. Why it matters: The authors envision a “scalable pipeline for creating functional novel life forms.” They believe their approach could yield bugs that perform a variety of tasks, like digesting spilled oil or gathering ocean-borne plastic particles. They could also deliver medicine, identify cancer, or clear away arterial plaque. We’re thinking: We humbly request an army of biobots designed to scrub bathrooms.", "image_caption": "Programmable organism at 4x speed", "metadata": {"article_id": "issue_23", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/frogs.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-23/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_23.html"}}
{"id": 3813006002, "type": "news_chunk", "title": "Algorithm Designs Living Machines, AI Interviews Job Applicants", "subtitle": "Better Than Backprop", "content": "End-to-end backpropagation and labeled data are the peanut butter and chocolate of deep learning. However, recent work suggests that neither is necessary to train effective neural networks to represent complex data. What’s new: Sindy Löwe, Peter O’Connor, and Bastiaan Veeling propose Greedy InfoMax (GIM), an unsupervised method for learning to extract features that trains only one layer at a time.Key insight: The information bottleneck theory (IB) suggests that neural networks work by concentrating information like a data-compression algorithm. In data compression, the amount of information retained is measured in mutual information (MI) between original and compressed versions. IB says that neural nets maximize MI between each layer’s input and output. Thus GIM reframes learning as a self-supervised compression problem. Unlike earlier MI-based approaches, it optimizes each layer separately.How it works: GIM works on modular networks, in which each layer learns to extract features from its input and passes its output to the next available layer, and so on down to the final layer. GIM doesn’t require labels, but if they’re available, a linear classification model can learn from GIM’s compressed output in a supervised manner. GIM uses the previous layer’s output as the next layer’s input to train each layer independently. This differs from the usual backpropagation in which all layers learn at once.The researchers devised a task that teaches layers to extract features that maximize MI. Given a subsequence of input data that has been compressed according to the current weights, the layer predicts the next element in the compressed sequence, choosing from a random selection drawn from the input including the correct choice. High success demonstrates that the layer is able to compress the input.The process effectively removes redundancy between nearby regions of the input. For example, a recording of a song’s chorus may repeat several times, so it’s possible to represent the recording without capturing the repetitions. Results: The researchers pitted Greedy InfoMax against contrastive predictive coding. In image classification, GIM beat CPC by 1.4 percent, achieving 81.9 percent accuracy. In a voice identification task, GIM underperformed CPC by 0.2 percent, scoring 99.4 percent accuracy. GIM’s scores are state-of-the-art for models based on mutual information.Why it matters: Backprop requires storing forward prediction, backward gradients, and weights for an entire network simultaneously. InfoMax handles each layer individually, making it possible to accommodate much larger models in limited memory. Behind the news: Layerwise training or pre-training has been around for at least a decade. For example, stacked autoencoders use reconstruction error as an alternative unsupervised mechanism to control intelligent data compression. Many past approaches are more focused on pre-training and assume that, once each layer has been trained individually, they will be trained together with a supervised task.We’re thinking: Many machine learning applications use a large pretrained network as an initial feature extractor and then apply transfer learning. By maximizing MI between layers, this approach could use more data to train and build still larger networks.", "image_caption": "Information related to Greedy InfoMax (GIM)", "metadata": {"article_id": "issue_23", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/InfoMax20ASPECT201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-23/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_23.html"}}
{"id": 3813006003, "type": "news_chunk", "title": "Algorithm Designs Living Machines, AI Interviews Job Applicants", "subtitle": "Steal Your Face", "content": "What if you could identify just about anyone from a photo? A controversial startup is making this possible.What happened: Hundreds of U.S. law enforcement agencies are using a face ID service that matches photos against a database of billions of images, the New York Times reported. How it works: Clearview AI scraped photos from Facebook and other social media sites, employment sites, mugshot archives, news sites, and message boards. The company’s promotional materials say it holds over 3 billion images, a repository far bigger than law-enforcement databases, as shown in the image above. The company trained a neural network to convert faces into geometric vectors representing the distance between a person’s eyebrows, the angle of the cheekbones, and so on.The network compares such vectors in a submitted photo with those in the database and returns matching photos along with the URLs they came from. Frequently these are on social-media pages, making it possible to connect a name to the face.More than 600 U.S. law enforcement agencies have licensed the application, which has been used to investigate crimes from shoplifting to murder. The company also contracts with corporate customers. Behind the news: Clearview AI was founded in 2016 by an Australian programmer with backing from tech investor Peter Thiel. The company has raised $7 million, according to the funding tracker Pitchbook. Yes, but: The New York Times outlines a number of concerns. Scraping photos violates terms of service for most social media companies, including Facebook, Instagram, and Twitter.Some experts worry the service invites misuse. “Imagine a rogue law enforcement officer who wants to stalk potential romantic partners,” one expert said.Clearview AI doesn’t report an error rate. The model could make false matches, putting innocent people in jeopardy. We’re thinking: We need regulations that balance development and deployment of useful technologies against their potential for abuse and harm. Face identification vendors should be required to report performance metrics, and police departments should be required to use models that pass federally established guidelines and perform background checks of personnel who have access to the technology.", "image_caption": "Chart with amount of photos that can be searched with different sources", "metadata": {"article_id": "issue_23", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Clearview20220ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-23/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_23.html"}}
{"id": 3813006004, "type": "news_chunk", "title": "Algorithm Designs Living Machines, AI Interviews Job Applicants", "subtitle": "HR’s Robot Helper", "content": "For some college graduates, landing a first job means making a good impression on a chatbot.What’s new: University guidance counselors around the U.S. are preparing students for interviews with AI-powered screening algorithms, according to CNN.How it works: Companies like Yobs and HireVue filter candidates for hundreds of corporate customers. Applicants submit videos of themselves answering pre-determined questions. The software then rates their language skills as well as non-verbal elements like tone, pitch, and emotional tenor. HireVue also evaluates body language and facial expressions. Acing an interview with an algorithm requires updating age-old social skills, like making eye contact with a laptop camera and making sure the computer’s speakers hear your upbeat, confident tone of voice.Software company Big Interview is developing an AI-scoring system to help prepare students for interviews with bots. Yobs offers a similar service. Yes, but: Training job hunters to look at the camera and project confidence is a good idea whether they’re talking to a bot or a human being. But critics question whether current AI is capable of reliably matching verbal or body language with traits that make for a good hire. Princeton University computer science professor Arvind Narayanan called AI applicant-screening programs “elaborate random number generators” in a talk last year.Why it matters: Millions of college graduates enter the global job market every year. Good AI could help hiring managers pluck the most qualified candidates from a deluge of resumes. Bad AI could knock many great applicants out of the running.We’re thinking: AI screening systems still need to prove themselves effective and reasonably bias-free. Meanwhile, we welcome tools that can improve, at scale, job opportunities for deserving individuals who otherwise might not hear from a recruiter.", "image_caption": "Excerpt of a video showing how HireVue works", "metadata": {"article_id": "issue_23", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize201-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-23/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_23.html"}}
{"id": 3813006005, "type": "news_chunk", "title": "Algorithm Designs Living Machines, AI Interviews Job Applicants", "subtitle": "Upgrading Softmax", "content": "Softmax commonly computes probabilities in a classifier’s output layer. But softmax isn’t always accurate in complex tasks — say, in a natural-language task, when the length of word vectors is much smaller than the number of words in the vocabulary. A new function renders more accurate predictions with lower computational cost than earlier alternatives.What’s new: Zhilin Yang, Thang Luong, and Ruslan Salakhutdinov at Carnegie Mellon University, with Quoc Le at Google Brain, developed an efficient solution to the so-called softmax bottleneck: Mixtape.Key insight: A previous proposal, Mixture of Softmaxes, (MoS) is a weighted sum of multiple softmaxes, and thus slow to train. Mixtape reformulates MoS as a single softmax of weighted sums. With a clever way of calculating the weights, that rearrangement avoids the bottleneck with much speedier execution.How it works: Mixtape’s weighted sum depends on the word it is evaluating — a not-so-obvious way to formulate the problem. The weights must be generated efficiently to avoid losing the computational advantage over MoS. Mixtape calculates weights for the weighted sum using a sigmoid tree decomposition. The sigmoid tree is a binary tree in which each node is a sigmoid. The tree’s leaves provide the weights. This is more efficient than using a softmax to calculate weights.Some of the weights are shared among infrequent output classes, which further boosts efficiency.This sharing does create potential for a bottleneck, but far less, and with less inaccuracy, than softmax. Results: The researchers compared transformer-based models with output layers employing Mixtape, MoS-15, or softmax. The tasks included recreating a text sample and translating a sentence from English to German or French. On text generation, MoS-15 (which entails 15 softmax calculations) and Mixtape improved perplexity — a measure of the model’s predictive certainty — by around 3, achieving a score of 56. MoS-15 slightly outperformed Mixtape. However, Mixtape required only slightly more training time than softmax, whereas MoS-15 required twice as long. Why it matters: Much research has focused on extracting meaningful features of input, but features are less useful if the output layer can’t classify them properly. Mixtape should allow models to take better advantage of features they extract without sacrificing AWS credits.We’re thinking: Mixtape can do better than softmax with only a little more training time. We may see Mixtape overtake softmax in some applications.", "image_caption": "Graph related to Mixture of Softmaxes (MoS)", "metadata": {"article_id": "issue_23", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Mixtape20ASPECT201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-23/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_23.html"}}
{"id": 3813006006, "type": "news_chunk", "title": "Algorithm Designs Living Machines, AI Interviews Job Applicants", "subtitle": "Bad Recommendations", "content": "YouTube is a great place to learn about new ideas — including some that have been thoroughly discredited.What’s new: YouTube’s recommendation algorithm is helping spread misinformation about climate change, according to research by Avaaz, a self-funded activist group.What they found: The researchers aimed to learn which videos YouTube was likely to feature in its “Up next” recommendations for videos resulting from three searches: “climate change,” “global warming,” and the more skeptical phrase “climate manipulation.” Working between August and December, they entered the search terms into a YouTube service that lists related videos. Then they used a data visualization tool to find the 100 most likely recommendations. The researchers watched the videos and flagged as “misinformation” those that contradict scientific consensus according to the Intergovernmental Panel on Climate Change, U.S. government agencies, and peer-reviewed research.For videos returned by searches on “climate change” and “global warming,” the percentage of recommendations containing misinformation were 8 and 16 percent respectively. For videos returned by a search on “climate misinformation,” the number was 21 percent.Ads by organizations like the World Wildlife Federation as well as major advertisers like L’Oreal and Warner Bros. often accompany videos that contradict scientific findings.The report’s proposals include giving advertisers the ability to stop their ads from running alongside misleading videos, limiting algorithmic recommendation of such videos, and making YouTube’s internal data on recommendations available to independent researchers. The response: YouTube defended its recommendation software and questioned the study’s methodology. It pointed out that it displays a link to Wikipedia’s “Global Warming” page under many climate-related videos.Behind the news: In June, YouTube overhauled its algorithms to give users more control over recommendations. Those changes cut the time viewers spent watching such content by 70 percent. The move followed earlier efforts to block videos espousing miracle cures or conspiracy theories.Why it matters: YouTube’s recommendations are a potent force for spreading information (and misinformation). They were credited with driving around 70 percent of the site’s viewing time in 2018.We’re thinking: It’s great to see YouTube and other companies working to reduce misinformation. But the AI community’s work is far from done. We need incentive mechanisms that don’t just reward numbers of views, but shift incentives toward distributing factual information and rational perspective to the extent they can be determined fairly.", "image_caption": "Chart with top 100 related videos for YouTube search on \"global warming\"", "metadata": {"article_id": "issue_23", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/youtube203.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-23/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_23.html"}}
{"id": 45472666001, "type": "news_chunk", "title": "Fighting Coronavirus, Hunting Drug Dealers, Regulating AI", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, I just finished reading BJ Fogg’s new book, Tiny Habits: The Small Changes That Change Everything. Fogg explains that the best way to build a new habit is to start small and succeed, rather than starting too big and giving up. For example, rather than trying to exercise for 30 minutes a day, he recommends aspiring to do just one push-up, and doing it consistently. This approach may be helpful to those of you who want to spend more time studying. If you hold yourself accountable for watching, say, 10 seconds of an educational video every day — and you do so consistently — the habit of studying daily will grow naturally. Even if you learn nothing in that 10 seconds, you’re establishing the habit of studying a little every day. On some days, maybe you’ll end up studying for an hour. Over the years, I have found a few resources for developing personal productivity that I love. My top picks include Getting Things Done by David Allen, the classic The 7 Habits of Highly Effective People by Stephen R. Covey, and Learning How to Learn Barbara Oakley (I recommend the Coursera course). I’m tempted to add Tiny Habits to this list. Keep learning! Google, Facebook, and Amazon aren’t the only places to work on cutting-edge AI products. Archis Joglekar parlayed his study of nuclear physics into a job building models at Noble.ai, where he helps other scientists speed up R&D. Read more", "image_caption": "Woman doing a push-up", "metadata": {"article_id": "issue_24", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20120ASPECG.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-24/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_24.html"}}
{"id": 45472666002, "type": "news_chunk", "title": "Fighting Coronavirus, Hunting Drug Dealers, Regulating AI", "subtitle": "Stopping Coronavirus", "content": "A company that analyzes online information to predict epidemics spotted the upsurge in coronavirus at least a week ahead of public-health authorities.What’s new: Canadian startup BlueDot alerted customers to the outbreak in the Chinese city of Wuhan on New Year’s Eve, Wired reported. The U.S. Centers for Disease Control and Prevention issued its warning on January 6, and the World Health Organization followed suit three days later. The respiratory illness as of this writing has infected more than 6,000 people and killed more than 130, mostly in China. How it works: Founded in 2014, BlueDot aims to stop the spread of infectious diseases by giving healthcare workers early warning, so they can identify and treat people who become infected. The company’s natural language processing model ingests 100,000 articles in 65 languages daily to track more than 100 infectious diseases. It ignores social media but scans news reports, government information, blogs, and forums related to human, plant, and animal diseases, as well as travel ticketing and local weather data.Human analysts vet the model’s predictions. They issue reports to customers in business, government, and nongovernmental organizations, ultimately reaching healthcare facilities and public health officials in a dozen countries. Behind the news: In 2008, Google undertook a similar effort to forecast influenza outbreaks based on search terms entered by users. In initial research, Google Flu Trends tracked the number of cases two weeks faster than the CDC. However, it dramatically underestimated the peak of the 2013 flu season and was shuttered soon afterward. Subsequent analysis concluded that the algorithm overfit seasonal search terms unrelated to flu. Why it matters: Rapid detection of new diseases is crucial to avoid global pandemics. Virulent diseases often can be contained if they’re caught early enough, but every hour compounds the number of people exposed and thus the number of cases. An epidemic can quickly overwhelm healthcare systems, leaving people even more exposed.We’re thinking: It’s hard to know how well today’s techniques will play out tomorrow. But the ability to catch potential pandemics before they explode is too valuable not to try.", "image_caption": "Three people wearing face masks on a plane", "metadata": {"article_id": "issue_24", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Coronavirus20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-24/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_24.html"}}
{"id": 45472666003, "type": "news_chunk", "title": "Fighting Coronavirus, Hunting Drug Dealers, Regulating AI", "subtitle": "Helpful Neighbors", "content": "School teachers may not like to hear this, but sometimes you get the best answer by peeking at your neighbor’s paper. A new language model framework peeks at the training data for context when making a prediction.What’s new: Facebook AI and Stanford researchers led by Urvashi Khandelwal enhanced language models that predict the next word in an incomplete sentence by enabling them to search for potential answers in the training data. They call their algorithm kNN-LM.Key insight: It’s much easier for a model to identify two sentence fragments that have similar meanings than it is to complete them. kNN-LM takes advantage of the easier task to improve performance on the harder one. Given a sentence fragment and asked to predict the next words, it searches the training set for sentences similar to that sentence fragment and uses what it finds to help predict the missing words. For example, the model might match a target starting, “Dickens is the author of ___,” with the training sentence, “Dickens wrote Oliver Twist.” The model then knows that “Oliver Twist” may be appropriate to add to the target.How it works: The authors offer a pretrained model, vector representations of training sentences, and an algorithm for combining information when analyzing a test sentence. Their approach works with any pretrained neural language model, but they used transformer networks in most experiments. kNN-LM starts by generating vector representations of every sequence in the training set. Then it searches these vectors for the k-nearest neighbor vector representations of the new input sequence. The closer a training sequence’s vector is to the input’s vector, the more heavily it weights the training sequence’s next token.The neural language model also directly predicts the next token for the input.Then it factors both the k-nearest neighbors prediction and language model’s prediction into a final decision. A hyperparameter controls how heavily it considers each one. Results: Tested on a dataset of Wikipedia articles, kNN-LM achieved a score of 15.79 for perplexity, a measure of predictive accuracy, more than 10 percent better than the previous state-of-the-art model.Why it matters: Language models likely won’t interpret technical terms found in, say, the NuerIPS proceedings, if they’re trained on Wikipedia. kNN-LM lets them find less related words in the training data, potentially improving generalization to obscure subject matter.We’re thinking: A key step for winning computer vision competitions like ImageNet has been to train multiple models and ensemble (or average) them. This confers perhaps a 1 percent boost in performance, but it’s impractical for most applications because of the computational expense. kNN-LM appears to require a significant computational expense as well, and we look forward to researchers diving deeper into the computational implications.", "image_caption": "Information related to the kNN-LM algorithm", "metadata": {"article_id": "issue_24", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/k20Nearest20Neighbors20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-24/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_24.html"}}
{"id": 45472666004, "type": "news_chunk", "title": "Fighting Coronavirus, Hunting Drug Dealers, Regulating AI", "subtitle": "The Right Problem for Your Solution", "content": "A new tool connects novice programmers with projects that match their experience and interests.What’s new: Github’s Good First Issues tool uses deep learning to find easy-to-fix issues among the collaborative software development platform’s multitude of open source projects.How it works: Github, which is owned by Microsoft, enables developers to collaborate freely worldwide. Participants often flag bugs to be fixed or features to be implemented, but beginners may have trouble figuring out which are appropriate to their skill level. Github trained a deep learning model on a dataset of issues labeled with designations like “beginner friendly,” “low-hanging fruit,” and “easy bug fix.” The metadata also noted whether issues were closed by someone who had not previously contributed to the repository and how many lines of code were involved.The model assigns a probability score to new issues based on how likely they are to be easily fixed. Users can browse by problem type or project. If they’ve been sufficiently active on Github, they can receive a list of open issues suited to their previous contributions. Behind the news: An earlier version used traditional computing to query a list of 300 beginner-friendly labels. However, it surfaced only about 40 percent of relevant issues, the company said. Why it matters: Github is a focal point of software development and the heart of the open source movement. Helping people figure out where they can have the most impact can only make it more productive.We’re thinking: What a great tool for aspiring developers! When jumping into AI (or, indeed, most disciplines), it’s better to start small and succeed than to start too big and fail.", "image_caption": "GitHub logo illustration", "metadata": {"article_id": "issue_24", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Github20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-24/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_24.html"}}
{"id": 45472666005, "type": "news_chunk", "title": "Fighting Coronavirus, Hunting Drug Dealers, Regulating AI", "subtitle": "Hunting Online Drug Dealers", "content": "Can machine learning help address the scourge of opioid addiction?What’s new: A public health researcher developed a neural network that spots sellers of opioids on social media, Recode reported.How it works: The model built by University of California professor Tim K. Mackey sifts through Instagram profiles to find those that offer drugs. In past research, Mackey identified Instagram as a popular platform for dealers. His team collected posts mentioning opioids and manually confirmed 12,857 that offered opioids for sale.The researchers used half the data to train a model to identify language that correlated with drug advertisements. They used the other half to validate the trained model.The neural net found 1,228 ads posted by 267 unique users. It achieved F1 scores of 95 percent for precision and accuracy, outperforming models based on random forests, decision trees, and support vector machines.The U.S. Department of Health and Human Services has contracted with Mackey to expand his method to cover Reddit, Tumblr, and YouTube. He’s also building a commercial platform for law-enforcement agencies to monitor social media streams in real time. Yes, but: Online opioid sales represent only a small fraction of the total, RAND Corporation drug policy expert Bryce Pardo told Recode. Mackey’s tool spots small-scale dealers, but it can’t do much to bring down the cartels responsible for much of the supply, he said. Why it matters: Shutting down suppliers could save lives. Two million Americans are addicted to opioids, and 130 people die from overdoses every day, according to the National Institute on Drug Abuse. We’re thinking: Will dealers resort to adversarial examples to thwart such automatic detection algorithms? Unfortunately, there’s plenty of financial incentive to advertise illegal opioids on social networks.", "image_caption": "Capture of an Instagram post related to drug dealing", "metadata": {"article_id": "issue_24", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Opioids220ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-24/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_24.html"}}
{"id": 45472666006, "type": "news_chunk", "title": "Fighting Coronavirus, Hunting Drug Dealers, Regulating AI", "subtitle": "Text to Speech in Parallel", "content": "A new system marks a step forward in converting text to speech: It’s fast at inference, reduces word errors, and provides some control over the speed and inflection of generated speech.What’s new: Yi Ren, Yangjun Ruan, and their co-authors at Zhejiang University and Microsoft propose FastSpeech, a text-to-speech system that processes text sequences in parallel rather than piece by piece.Key insight: Previous models predict phonemes, or units of sound, sequentially. This so-called autoregressive approach lets the model base each phoneme on those that came before, so the output can flow like natural speech. But it also limits how fast the model can generate output. Instead, FastSpeech uses a duration predictor that determines the length of each phoneme. Knowing durations ahead of time allows the model to generate phoneme representations independently, yielding much faster operation while maintaining the flow.How it works: Neural text-to-speech models typically generate a mel-spectrogram that represents the frequency spectrum of spoken words. FastSpeech generates mel-spectrograms using a variant on the transformer network known as a feed-forward transformer network (abbreviated FFT, but not to be confused with a fast Fourier transform). The model starts by splitting words into the phonemes they represent. A trainable embedding layer transforms the phonemes into vectors.The first of two FFTs applies attention to find relationships between the phonemes and generate a preliminary mel-spectrogram.The duration predictor (trained by a separate pretrained autoregressive text-to-speech model) determines the length of any given phoneme in spoken form. A length regulator adjusts the FFT’s output to match the predicted durations.A second FFT sharpens details of the mel-spectrogram, and a linear layer readies it for final output.The WaveGlow speech synthesizer produces speech from the final mel-spectrogram. Results: Using the LJSpeech dataset for training and evaluation, FastSpeech was 270 times faster at generating mel-spectrograms than a transformer-based autoregressive system, and 38 times faster at generating speech output, with audio quality nearly as good. The generated speech was free of repetitions and omissions.Why it matters: LSTMs and other autoregressive models have boosted accuracy in generating text and speech. This work highlights an important trend toward research into faster alternatives that don’t sacrifice output quality.We’re thinking: In the long run, end-to-end systems that synthesize the output audio directly are likely to prevail. Until then, approaches like FastSpeech still have an important role.", "image_caption": "Information related to FastSpeech, a text-to-speech system", "metadata": {"article_id": "issue_24", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FastSpeech20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-24/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_24.html"}}
{"id": 45472666007, "type": "news_chunk", "title": "Fighting Coronavirus, Hunting Drug Dealers, Regulating AI", "subtitle": "Limits on AI Job Interviews", "content": "As employers turn to AI to evaluate job applicants, a U.S. state imposed limits on how such tools can be used.What’s new: The Illinois legislature passed the AI Video Act, which gives candidates a measure of control over how hiring managers collect and store video interviews.How it works: The latest generation of video screening tools typically requires applicants to record themselves answering pre-determined questions. A model analyzes their verbal performance and body language to evaluate how well they fit the bill. The law requires employers to notify candidates that AI may be used in their interview. They must explain what the technology does and how it works.Applicants can opt out of such interviews, and employers must provide an alternative that doesn’t involve AI. Yes, but: The Illinois law does not define artificial intelligence, lawyers at Reed Smith LLC pointed out in Technology Law Dispatch. Such lack of precision could lead to disputes if candidates believe they’re being evaluated by AI while the company disagrees. Another ambiguity: The law doesn’t specify consequences for violations. Why it matters: The move in Illinois is a concrete step amid a rising chorus of calls to rein in AI. Last week, Alphabet’s Sundar Pichai spoke out in favor of regulation, and IBM proposed guidelines for reducing algorithmic bias. The EU will vote on limits for automated decision making in February.We’re thinking: The new law may soon run up against the Trump Administration’s recent mandate to keep barriers to innovation in AI low. Nonetheless, it’s important to strike a balance between supporting technology development and protecting the public.", "image_caption": "Person on an online job interview", "metadata": {"article_id": "issue_24", "chunk_index": 7, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Illinois.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-24/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_24.html"}}
{"id": 41494719001, "type": "news_chunk", "title": "Robot Warehouse Workers, Cities Under Surveillance, Chatbot...", "subtitle": "Packing Robots Get a Grip", "content": "Dear friends, Many of us apply labels to ourselves that shape our identity. Some say, “I’m a sports fan,” and this attitude motivates behaviors such as cheering for the home team. Others identify themselves as introverts, extroverts, vegetarians, gamers, athletes, scientists, and/or engineers. Each label implies its own set of habits and activities. I think it’s time for more of us to identify ourselves as life-long learners. To me, a life-long learner: Aspires to keep learning new thingsSeeks knowledge or skill beyond what would be immediately usefulInvests time, energy, and money to learn new thingsShares knowledge to help other lifelong learners This is the best way to keep growing over your entire lifetime. I’ve seen numerous people proactively learn about new technologies or gain skills in everything from product management to personal health, and develop as individuals as a result. They seem happier, and I’m sure they contribute more to their communities. Every weekend I spend several hours reading or taking online courses. This learning helps me do my work better, but I enjoy it so much that I’d do it even if it didn’t affect my work at all. The world is changing faster than ever, driven by technological change. So humanity needs a lot more lifelong learners to make sure we keep up. I hope you’ll join me in proudly telling others, “I’m a lifelong learner!” Keep learning,Andrew Robots are moving into a job that traditionally required the human touch.What’s new: A commercial warehouse that ships electrical supplies deployed AI-driven robotic arms from Covariant, a high-profile Silicon Valley robotics firm. Trained using a hybrid of imitation and reinforcement learning, the new machines are far better than earlier bots at sorting items into boxes.How it works: Robots have been picking objects off conveyor belts for years, but they generally handle only identical items. Covariant’s approach, which uses a single neural network for all objects, enables an arm equipped with a camera and suction gripper to manipulate around 10,000 different items (and counting). The system can share skills with other arms, including those made by other companies. Training starts with attempts at few-shot adaptation. In many cases, the robot can learn from a limited number of attempts, the company told IEEE Spectrum.For more intensive training, an engineer wearing virtual reality gear uses hand-tracking hardware to control the arm in a simulated environment. The model learns to mimic the motion.The model stores basic movements, then hones them using reinforcement learning in a variety of simulated situations.The team then uses behavioral cloning to transfer the robot’s learned skills into the real world. Behind the news: Co-founded by UC Berkeley AI professor Pieter Abbeel (watch our interview with him here), Covariant has raised $27 million from backers including deep learning pioneers Yann LeCun and Geoffrey Hinton as well as Google AI chief Jeff Dean.Why it matters: More than half of warehouse logistics companies could face labor shortages in the next five years, thanks to the job’s tedium and low wages. Market analysts expect automatons to pick up the slack.We’re thinking: Will robots figure out how to ship a RAM stick without a cubic meter of styrofoam peanuts in a box the size of a washtub?", "image_caption": "Packing robot", "metadata": {"article_id": "issue_25", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Robots.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-25/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_25.html"}}
{"id": 41494719002, "type": "news_chunk", "title": "Robot Warehouse Workers, Cities Under Surveillance, Chatbot...", "subtitle": "Bot Comic", "content": "Androids may not dream of electric sheep, but some crack jokes about horses and cows.What’s new: Meena, a 2.6-billion parameter chatbot developed by Google Brain, showed impressive conversational ability, discussing a variety of topics. In one exchange, it unexpectedly sprinkled in some barnyard humor: It commented that “horses go to Hayvard,” not Harvard. The phrase didn’t appear in the training data, but the word Hayvard did appear once, a pun after a mention of horses, according to a company spokesperson. The bot followed up with another farm-animal pun, noting that its interlocutor tried “to steer [the conversation] elsewhere.” The first-ever AI-generated dad joke?How it works: Google engineer Daniel De Freitas Adiwardana told us how, in training on 341 gigabytes of public social media conversation, Meena might have developed a sense of humor. Straight from the horse’s mouth, as it were: “At a high level, when the model is training, it’s required to try to predict a lot of sequences of words, all at the same time. So it’s forced to come up with strategies that solve all these prediction problems at once.“In the beginning of the convergence, when the perplexity is still high, these greedy choices lead to doing things like repeating common words, like ‘the the the the.’ It lowers its loss function value that way.“Over time, it learns new ways to cheat that lower the loss even further, like repeating what the other person said (‘do you like pizza?’ > ‘Do you like pizza?’). Then it makes a twist on that, using repetitions (‘do you like pizza?’ > ‘I like pizza, I like pizza’), contradictions (‘do you like pizza?’ > ‘I like pizza, I don’t like pizza’), and/or added conjunctions (‘do you like pizza?’ > ‘I like pizza, but I don’t like pizza’).“Eventually it gets to something that is still cheating, in a sense, but much more sensible, like (‘do you like pizza?’ > ‘I like pizza, but I try not to eat it everyday’). Maybe no one said that sentence exactly, but it sort of looks like something a lot of people said.“The empirical and hand-wavy moral of the story is that, as it gets harder to make learning progress, the cheating gets more sophisticated. One of possibly many views is that the cow and horse jokes are just a pretty sophisticated form of cheating, which many people would start to feel comfortable calling generalization.” Behind the news: Efforts to give AI a sense of humor have met with limited success. One of the most impressive is a jokebot that writes captions for cartoons in The New Yorker. That model separates the normal aspects of a picture (for instance, a salesman showing off a new car) from oddball elements (the car has cat-like legs), then writes a caption based on the contrast. But even its best efforts (“just listen to that baby purr”) are more intriguing — because they were written by a computer — than side-splitting.Why it matters: Humor affects us in deep and subtle ways. A Tina Fey-level bot may be out of reach, but a funny bone would be a valuable feature in an empathetic AI.We’re thinking: We’d like to see a football game between Hayvard and Dartmooth.", "image_caption": "Capture of a chatbot telling jokes developed by Google Brain", "metadata": {"article_id": "issue_25", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Joke20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-25/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_25.html"}}
{"id": 41494719003, "type": "news_chunk", "title": "Robot Warehouse Workers, Cities Under Surveillance, Chatbot...", "subtitle": "AI Tackles OCD", "content": "A drug designed by AI has been approved for testing in humans.What’s new: A UK startup focused on automated drug discovery teamed up with a Japanese pharmaceutical company to produce a new medicine for obsessive compulsive disorder. The compound, known as DSP1181, is designed to take effect more quickly and last longer than existing treatments. Japanese authorities cleared it for a clinical trial.How it works: Exscientia’s drug-discovery platform can start with a biological target known to influence a particular medical condition. In this case, the target was a tiny cellular structure that, when stimulated, releases the hormone serotonin.The platform drew on databases of DNA sequences, protein structures, and drug actions to generate molecules likely to stimulate the serotonin-producing machinery.The model also scoured scientific literature, patent databases, and studies of genetic toxicology to gauge the candidates’ likely impact.Exscentia’s system likely shaved a few months off the usual discovery process, wrote Derek Lowe, a chemist at Novartis Institutes for BioMedical Research, in a blog post for Science. Why it matters: Pharmaceutical companies invest upward of $2.6 billion to develop a single drug, and it can take three to six years to find a compound that’s viable for testing in humans— with no guarantee that it will prove safe and effective. Automating even small parts of the process can save big money. That’s one reason why Exscientia is one of nearly 200 companies worldwide using AI to find new drugs.We’re thinking: AI is no magic bullet for drug discovery. But cutting the enormous cost of development would enable pharma companies to study more molecules and potentially to bring more medicines to market.", "image_caption": "Process to create an automated drug", "metadata": {"article_id": "issue_25", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Drugs20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-25/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_25.html"}}
{"id": 41494719004, "type": "news_chunk", "title": "Robot Warehouse Workers, Cities Under Surveillance, Chatbot...", "subtitle": "Protein Shapes Revealed", "content": "A protein’s biological function depends largely on its three-dimensional shape, but deducing its shape from its sequence of amino acids has been a longstanding problem. Researchers at DeepMind reveal how they used deep learning to solve the puzzle.What’s new: Andrew Senior and colleagues released long-awaited details about AlphaFold, a protein-folding model that wowed experts in a high-profile competition in late 2018. The paper is behind a paywall. This video offers some details.Key insight: Research has shown that protein shapes are determined by the proximity of essential portions, or residues, of amino acids. The researchers found likely shapes by optimizing over possible structures that keep residues close to one another. Earlier methods predict whether residues are in contact with one another. AlphaFold predicts the distances and angles between residues, making the optimization easier.How it works: AlphaFold extracts features from an input protein sequence, predicts relationships between residues, and uses those predictions to find the protein’s likely shape. The feature extractor compares the input sequence with sequences in a protein database. It represents relationships between amino-acid pairs based on the similarities it finds.The features feed a CNN trained on a dataset of 3D protein structures, which predicts the distribution of distances and angles between residues.The model infers the protein’s physical stability based on the distances and angles. The physical stability equation is differentiable, so the predicted structure can be optimized by gradient descent. The most stable structure is the final output. Results: At the 2018 CASP13 conference, AlphaFold predicted 24 out of 43 previously unknown protein shapes with high accuracy. The next-best model achieved 14 predictions of similar accuracy.Why it matters: The ability to determine protein structures could have wide-ranging impacts on drug discovery, countering neurodegenerative diseases, and more. Stay tuned for further progress when CASP14 convenes in April.We’re thinking: Hard problems don’t always offer enough training data to train an end-to-end neural network. In this case, combining a physical model with neural networks led to significant progress. This design pattern holds promise in many other domains from climate change to robot dynamics.", "image_caption": "Data related to AlphaFold, a protein-folding model", "metadata": {"article_id": "issue_25", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Proteins20320ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-25/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_25.html"}}
{"id": 41494719005, "type": "news_chunk", "title": "Robot Warehouse Workers, Cities Under Surveillance, Chatbot...", "subtitle": "Nowhere to Hide", "content": "Real-time face recognition has become standard operating procedure for cops in a few cities, in both authoritarian and democratic countries.What’s new: After years of trials, police departments in Moscow and London are using face recognition to scan the streets for suspected criminals.How it works: Systems in both cities connect to pre-existing closed-circuit television networks. Enforcers in Moscow aim to deploy the tech city-wide, according to The Verge. So far, though, they’re using only a fraction of the city’s tens of thousands of cameras. London plans a more limited rollout in popular shopping and tourist areas. Moscow paid NTechLabs, a homegrown company, $3.2 million to license its technology. The company maintains a watch list of suspects and notifies authorities if it finds a match.Prior to serving the law enforcement market, NTechLabs offered a consumer app for matching pictures of people to their social media profile. Its FindFace app made headlines in 2016 when internet trolls used it to dox sex workers.London’s Metropolitan Police licenses face recognition tech from NEC. It runs cameras for five to six hours at a time as it tries to match watch lists of suspected violent criminals and child sex traffickers. Why it matters: Face recognition technology is becoming routine for police forces around the globe. It has been used to catch a murderer in Chongqing, helped stop street crime in New York City, and figured in 30 percent of solved cases in one small U.S. city.Yes, but: Independent researchers evaluating recent trials in London found that the system misidentified 81 percent of suspects it flagged. The police department contests those numbers, saying its own studies show only one in 1,000 false positives.We’re thinking: Law enforcement agencies worldwide need thoughtfully designed and clearly worded regulatory guidance so they can use these tools without overstepping civil liberties.", "image_caption": "Security camera next to the Big Ben in London", "metadata": {"article_id": "issue_25", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/London20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-25/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_25.html"}}
{"id": 41494719006, "type": "news_chunk", "title": "Robot Warehouse Workers, Cities Under Surveillance, Chatbot...", "subtitle": "Old Tools for New Synths", "content": "Neural audio synthesizers like WaveRNN or GANSynth produce impressive sounds, but they require large, data-hungry neural networks. A new code library beefs up the neural music studio with efficient sound modules based on traditional synthesizer designs.What’s new: Jesse Engel and colleagues at Google Brain introduced Differentiable Digital Signal Processing (DDSP), a set of digital signal processing tools that integrate with neural networks to boost their performance.Key insight: Traditional synthesizers incorporate powerful sound-generation and -processing tools, but their controls are often limited to sliders and switches that don’t take full advantage of their abilities. A neural network can learn to manipulate such tools more dynamically, potentially producing more realistic renditions of existing instruments as well as novel sounds.How it works: DDSP offers tools such as oscillators (which generate sound), filters (which modify tone color), envelopes (which shape the sound over time), and reverberators (which mimic sound waves that reflect off walls). Most are implemented as layers that can be inserted into neural networks without affecting backprop training, so a network can learn to control them. The researchers use DDSP to emulate the Spectral Modeling Synthesizer (SMS), a 1990s-vintage digital synth. Once it has been trained, their SMS emulator can mimic input sounds. Also, parts of an SMS network trained on, for instance, violins can be swapped with those of one trained on, say, guitars to reinterpret a violin recording using a guitar sound.They re-created the SMS architecture as an autoencoder with additional components. The autoencoder’s encoder maps input sounds to low-dimensional vectors. The decoder’s output drives DDSP’s oscillator and filter, which in turn feed a reverberator to produce the final output. Results: The SMS emulator showed that DDSP can make for a high-quality neural sound generator. Compared to WaveRNN, it scored better for L1 loudness loss, a measure of the difference between audio input and synthesized output (.07 compared to .10). It also had a better L1 loss of fundamental frequency, which measures the accuracy of the synthesized waveform relative to the input (.02 versus 1.0). And it has one tenth as many parameters!Why it matters: Audio synthesis is one of several applications migrating from digital signal processing tech to deep learning. Machine learning engineers need not leave the older technology behind — they can build DSP functions into their neural networks.We’re thinking: The SMS demo is preliminary, but it points toward next-generation audio models that combine deep learning with more intuitive structures and controls.", "image_caption": "Information related to Differentiable Digital Signal Processing (DDSP)", "metadata": {"article_id": "issue_25", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DDSP20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-25/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_25.html"}}
{"id": 2792133001, "type": "news_chunk", "title": "Hotter Dating Profiles, Pandas in Love, Compute for Coronavirus", "subtitle": "Phantom Menace", "content": "Dear friends,A student once asked me, “Can an AI ever love?”Since the early days of AI, people have wondered whether AI can ever be conscious or feel emotions. Even though an artificial general intelligence may be centuries away, these are important questions. But I consider them philosophical questions rather than scientific questions. That’s because love, consciousness, and feeling are not observable. Whether an AI can diagnose X-ray images at 95 percent accuracy is a scientific question; whether a chatbot can convince (or “fool”) an observer into thinking that it has feelings is a scientific question. But whether it can feel is a question best left to philosophers and their debates. Or to the Tin Man, the robot character in The Wizard of Oz who longs for a heart only to learn that he had one all along. Even if we can’t be sure that an AI will ever love you, I hope you love AI, and also that you have a happy Valentine’s Day!Love,Andrew Some self-driving cars can’t tell the difference between a person in the roadway and an image projected on the street.What’s new: A team led by researchers at Israel’s Ben-Gurion University of the Negev used projectors to trick semiautonomous vehicles into detecting people, road signs, and lane markings that didn’t exist.How it works: The researchers projected images of a body (Elon Musk’s, to be precise) on a street, a speed-limit sign on a tree, and fake lane markings on a road. A Tesla on autopilot and a Renault equipped with Intel Mobileye’s assistive driving system — which rely on sensors like cameras and radars rather than three-dimensional lidar — responded by swerving, stopping, or slowing (as you can see in the lower left-hand corner of the clip above). The paper proposes three convolutional neural networks to determine whether an object is real or illusory. One CNN checks whether the object’s surface texture is realistic, flagging, say, a stop sign projected on bricks.Another checks the object’s brightness to assess whether it reflects ambient, rather than projected, light.The third evaluates whether the object makes sense in context. A stop sign projected on a freeway overpass, for instance, would not.The team validated each model independently, then combined them. The ensemble caught 97.6 percent of phantom objects but mislabelled 2 percent of real objects. Behind the news: A variety of adversarial attacks have flummoxed self-driving cars. A 2018 study fooled them using specially designed stickers and posters. Another team achieved similar results using optical illusions.Why it matters: A mischief maker with an image projector could turn automotive features designed for safety into weapons of mass collision.The companies respond: Both manufacturers dismissed the study, telling the authors: “There was no exploit, no vulnerability, no flaw, and nothing of interest: the road sign recognition system saw an image of a street sign, and this is good enough.” — Mobileye“We cannot provide any comment on the sort of behavior you would experience after doing manual modifications to the internal configuration [by enabling an experimental stop sign recognition feature].” — Tesla We’re thinking: The notion that someone might cause real-world damage with a projector may seem far-fetched, but the possibility is too grave to ignore. Makers of self-driving systemsshould take it seriously.", "image_caption": "Autonomous vehicle detecting images projected on the street", "metadata": {"article_id": "issue_26", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Phantom.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-26/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_26.html"}}
{"id": 2792133002, "type": "news_chunk", "title": "Hotter Dating Profiles, Pandas in Love, Compute for Coronavirus", "subtitle": "That Swipe-Right Look", "content": "In an online dating profile, the photo that highlights your physical beauty may not be the one that makes you look smart or honest — also important traits in a significant other. A new neural network helps pick the most appealing shots.What’s new: Agastya Kalra and Ben Peterson run a business called Photofeeler that helps customers choose portraits for dating and other purposes. Their model Photofeeler-D3 rates perceived looks, intelligence, and trustworthiness in photos. You can watch a video demo here.Key insight: Individuals have biases when it comes to rating photos. Some consistently give higher scores than average, while others may consistently give more random scores. By taking into account individual raters’ biases, a model can predict more accurately how a group would judge a photo. How it works: Photofeeler-D3 scores the beauty, intelligence, and trustworthiness of a person in a photo on a scale of 1 to 10. The network was trained on more than 10 million ratings of over 1 million photos submitted by customers through the company website. Photofeeler-D3 learned each rater’s bias (that is, whether the person’s ratings tend to be extreme or middling) based on their rankings of photos in the training dataset. The model represents this individual bias as a vector.A convolutional neural network using the xception architecture learned to predict a score for each trait. (The score wasn’t used.) After training, the CNN used its knowledge to generate vector representations of input images.The model samples a random rater from the training dataset. An additional feed-forward layer predicts that rater’s scores using the bias vector and photo vector.Then it averages its predictions of 200 random raters to simulate an assessment by the general public. Results: Tested on a dataset of face shots scored for attractiveness, Photofeeler’s good-looks rating achieved 81 percent correlation compared to the previous state of the art, 53 percent. On the researchers’ own dataset, the model achieved 80 percent correlation for beauty, intelligence, and trustworthiness.Why it matters: Crowdsourced datasets inherit the biases of the people who contributed to them. Such biases add noise to the training process. But Photofeeler’s voter modeling turns raters’ bias into a benefit: Individuals tend to be consistent in the way they respond to other peoples’ looks, so combining individuals yields a more accurate result than estimating mean ratings while ignoring their source. We’re thinking: We’d rather live in a world where a link to your Github repo gets you the most dates.", "image_caption": "Heart shape made with two hands", "metadata": {"article_id": "issue_26", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Photofeeler20220ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-26/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_26.html"}}
{"id": 2792133003, "type": "news_chunk", "title": "Hotter Dating Profiles, Pandas in Love, Compute for Coronavirus", "subtitle": "Tools For a Pandemic", "content": "Chinese tech giants have opened their AI platforms to scientists fighting coronavirus.What’s new: Alibaba Cloud and Baidu are offering a powerful weapon to life-science researchers working to stop the spread of the illness officially known as Covid-19: free access to their computing horsepower and tools.How it works: The companies have a variety of resources to support tasks like gene sequencing, protein screening, and drug development. Alibaba Cloud is providing access to its computing network as well as AI-driven drug discovery tools, datasets from earlier efforts to find drugs to combat SARS and MERS, and libraries of compounds. Researchers can apply by emailing Alibaba Cloud.In collaboration with Beijing’s Global Health Drug Discovery Institute, Alibaba Cloud is developing a platform to aggregate and distribute coronavirus-related information.Baidu offers LinearFold, a tool developed in collaboration with Oregon State University and the University of Rochester that rapidly sequences RNA. Behind the news: Scientists worldwide are turning to AI to help control the outbreak. An effort led by Harvard Medical School is tracking the disease by mining social media posts. UK researchers used AI to explore properties of an existing drug that could be useful for treating the virus. Meanwhile, a U.S. company is using an algorithm to design molecules that could halt the bug’s ability to replicate.Why it matters: The virus had infected nearly 45,000 people and killed more than 1,100 at press time.We’re thinking: Donating compute, tools, and data to scientists fighting infectious diseases is a great idea. We hope other tech companies will pitch in.", "image_caption": "Covid-19 illustration", "metadata": {"article_id": "issue_26", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Corona2052020ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-26/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_26.html"}}
{"id": 2792133004, "type": "news_chunk", "title": "Hotter Dating Profiles, Pandas in Love, Compute for Coronavirus", "subtitle": "Fighting Fakes", "content": "A supergroup of machine learning models flags manipulated photos.What’s new: Jigsaw, a tech incubator owned by Alphabet, released a system that detects digitally altered images. The organization is testing it with a dozen media partners including Rappler in the Philippines, Animal Politico in Mexico, and Code for Africa.What’s in it: Assembler is an ensemble of six algorithms, each developed by a different team to spot a particular type of manipulation. Jigsaw put them together and trained them on a dataset from the U.S. National Institute of Standards and Technology’s Media Forensics Challenge. Jigsaw contributed a component that identifies deepfakes generated by StyleGAN.The University Federico II of Naples supplied two models, one that spots image splices and another that finds repeated patches of pixels, indicating that parts of an image were repeatedly copied and pasted.UC Berkeley developed a model that scans pixels for clues that they were produced by different cameras, an indication of image splicing.The University of Maryland’s contribution uses color values as a baseline to look for differences in contrast and other artifacts left by image editing software. Why it matters: Fake images can deepen political divides, empower scammers, and distort history. In the U.S., members of Congress have tried to discredit former President Obama with fake pictures purporting to show him shaking hands with Iranian president Hassan Rouhani.Scammers used doctored images of the recent Australian bushfires to solicit donations for nonexistent relief funds.Disinformation is known to have influenced politics in Brazil, Kenya, the Philippines, and at least a dozen other democracies. We’re thinking: Unfortunately, the next move for determined fakers may be to use adversarial attacks to fool this ensemble. But journalists working to keep future elections fair will need every tool they can get.", "image_caption": "Detection of a digitally altered image of a frog holding a violin", "metadata": {"article_id": "issue_26", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Jigsaw202.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-26/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_26.html"}}
{"id": 2792133005, "type": "news_chunk", "title": "Hotter Dating Profiles, Pandas in Love, Compute for Coronavirus", "subtitle": "What Love Sounds Like", "content": "Female giant pandas are fertile for only 24 to 36 hours a year: Valentine’s Day on steroids. A new neural network alerts human keepers when a panda couple mates.What’s new: Panda breeders are struggling to lift the creatures’ global population, and tracking success in mating helps maintain their numbers. WeiRan Yan of Sichuan University, with researchers from Chengdu Research Base of Giant Panda Breeding and Sichuan Academy of Giant Panda, developed CGANet, a speech recognition network that flags consummated unions based on panda vocalizations.Key insight: Prior work discovered the relationship between panda calls and mating success. A preliminary model used hand-crafted features to recognize calls meaning, “Wow, honey, you were great!” CGANet uses features extracted through deep learning.How it works: The researchers trained CGANet on recordings of pandas during mating season labeled for mating success. The model divides each recorded call into pieces and computes a frequency representation of each piece.It uses convolutional, recurrent, and attention layers in turn to find patterns that predict mating success in different aspects of the pieces and their interrelationships.It computes the probability of mating success for each piece, then sums the probabilities to generate a prediction for the call as a whole. Results: CGANet’s predictions were 89.9 percent accurate, a new state of the art compared with the earlier model’s 84.5 percent. CGANet also substantially improved AUC (area under curve, a measure of true versus false positives).Why it matters: Tracking a panda’s love life once required obtaining its hormones — a difficult and time-consuming feat. CGANet allows real-time, non-invasive prediction so keepers can give the less popular pandas another chance while they’re still fertile.We’re thinking: For pandas, a happy Valentine’s Day is essential to perpetuate the species. Tools like CGANet could help save these unique creatures from extinction.", "image_caption": "Two pandas eating", "metadata": {"article_id": "issue_26", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Pandas20220ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-26/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_26.html"}}
{"id": 2792133006, "type": "news_chunk", "title": "Hotter Dating Profiles, Pandas in Love, Compute for Coronavirus", "subtitle": "Extreme Weather Warning", "content": "Severe heat waves and cold snaps are especially hard to forecast because atmospheric perturbations can have effects that are difficult to compute. Neural networks show promise where typical methods have stumbled.What’s new: Researchers at Rice University used a capsule neural network — a variation on a convolutional neural network — to forecast regional temperature extremes based on far fewer variables than usual.How it works: Good historical observations date back only to 1979 and don’t include enough extreme-weather examples to train a neural network. So the researchers trained their model on simulated data from the National Center for Atmospheric Research’s Large Ensemble Community Project (LENS). Starting with 85 years’ worth of LENS data covering North America, the researchers labeled atmospheric patterns preceding extreme temperature swings by three days.Trained on atmospheric pressure at 5 kilometers, the model predicted cold spells five days out with 45 percent accuracy and heat spells (which are influenced more by local conditions) five days out with 40 percent accuracy.Retrained on both atmospheric pressure and surface temperature, the model’s five-day accuracy shot up to 76 percent for both winter and summer extremes. The next step: By adding further variables like soil moisture and ocean surface temperature, the researchers believe they can extend their model’s accuracy beyond 10 days. That would help meteorologists spot regional temperature extremes well ahead of time. Then they would use conventional methods to home in on local effects.Why it matters: Extreme temperatures are disruptive at best, deadly at worst. Advance warning would help farmers save crops, first responders save lives, and ordinary people stay safe.Behind the news: Most weather forecasting is based on crunching dozens of variables according to math formulas. In its reliance on matching historical patterns, this study’s technique — indeed, any deep learning approach to weather prediction — is a throwback to earlier methods. For instance, the U.S. military used temperature and atmospheric pressure maps to predict the weather before the U.S. invasion of Normandy in 1944. We’re thinking: Who says talking about the weather is boring?", "image_caption": "Heat map of Europe", "metadata": {"article_id": "issue_26", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Weather20420ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-26/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_26.html"}}
{"id": 32376394001, "type": "news_chunk", "title": "Chatbots Sue Telemarketers, Neural Nets See Around Corners", "subtitle": "Glimpse My Ride", "content": "Dear friends, Nearly a decade ago, I got excited by self-taught learning and unsupervised feature learning — ways to learn features from unlabeled data that afterward can be used in a supervised task. These ideas contributed only marginally to practical performance back then, but I’m pleased to see their resurgence and real traction in self-supervised learning. Many of you know the story of how the increasing scale of computation and data, coupled with innovation in algorithms, drove the rise of deep learning. Recent progress in self-supervised learning also appears to be powered by greater computational and data scale — we can now train large neural networks on much larger unlabeled datasets — together with new algorithms like contrastive predictive coding. Today feels very much like the early, heady days a decade-plus ago, when we saw neural networks start to work in practical settings. The number of exciting research directions seems larger than ever! Keep learning, Police in the U.S. routinely use AI to track cars with little accountability to the public.What happened: Documents obtained by Wired revealed just how intensively police in Los Angeles, California, have been using automatic license plate readers. Officials queried databases of captured plate numbers hundreds of thousands of times in 2016 alone, records show.How it works: The Los Angeles Police Department, county sheriff, and other local agencies rely on TBird, a license plate tracking system from data-mining company Palantir. Detectives can search for full or partial numbers. The system maps the locations of vehicles with matching plates, annotated with previous searches and the time each image was captured.The system acts as a virtual dragnet, alerting nearby officers whenever cameras spot a flagged plate.It also lists all plates that appeared in the vicinity of a crime, along with each vehicle’s color, make, and style, thanks to machine vision from Intrinsics.The LAPD shares its license plate records with those of other nearby police departments as well as private cameras located in malls, universities, transit centers, and airports. Behind the news: A 2013 survey by the U.S. Dept. of Justice found that many urban police departments use automatic license plate readers.The LAPD was among the first to do so starting in 2009.Why it matters: License plate readers help solve serious crimes. Wired describes a case in which the LAPD used TBird to search for vehicles spotted near the place where a murdered gang member’s body was found. The plates led them to a rival gang member who eventually was convicted for the homicide.We’re thinking: Digital tools are becoming important in fighting crime, but it shouldn’t take a reporter’s public records request to find out how police are using them. We support regulations that require public agencies to disclose their use of surveillance technology, as well as rigorous logging and auditing to prevent misuse.", "image_caption": "Automatic license plate reader", "metadata": {"article_id": "issue_27", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/License20plate204.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-27/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_27.html"}}
{"id": 32376394002, "type": "news_chunk", "title": "Chatbots Sue Telemarketers, Neural Nets See Around Corners", "subtitle": "Periscope Vision", "content": "Wouldn’t it be great to see around corners? Deep learning researchers are working on it.What’s new: Stanford researcher Christopher Metzler and colleagues at Princeton, Southern Methodist University, and Rice University developed deep-inverse correlography, a technique that interprets reflected light to reveal objects outside the line of sight. The technique can capture submillimeter details from one meter away, making it possible to, say, read a license plate around a corner.Key insight: Light bouncing off objects retains information about their shape even as it ricochets off walls. The researchers modeled the distortions likely to occur under such conditions and generated a dataset accordingly, enabling them to train a neural network to extract images of objects from their diminished, diffuse, noisy reflections.How it works: The experimental setup included an off-the-shelf laser and camera, a rough wall (called a virtual detector), and a U-Net convolutional neural network trained to reconstruct an image from its reflections. To train the U-Net, the researchers generated over 1 million images in pairs, one a simple curve (the team deemed natural images infeasible), the other a simulation of the corresponding reflections.The researchers shined the laser at a wall. Bouncing off the wall, the light struck an object around the corner. The light caromed off the object, back to the wall, and into the camera.The U-Net accepted the camera’s output and disentangled interference patterns in the light waves to reconstruct an image. Results: The researchers tested the system by spying hidden letters and numbers 1 centimeter tall. Given the current state of non-line-of-sight vision, quantitative results weren’t practical since the camera inevitably fails to capture an unknown amount of detail). Qualitatively, however, the researchers deemed their system’s output a substantial improvement over the previous state of the art. Moreover, the U-Net is thousands of times faster.Yes, but: Having been trained on simple generated images, the system perceived only simple outlines. Moreover, the simulation on which the model was trained may not correspond to real-world situations closely enough to be generally useful.Why it matters: Researchers saw around corners.We’re thinking: The current implementation likely is far from practical applications. But it is a reminder that AI can tease out information from subtle cues that are imperceptible to humans. Here’s another example.", "image_caption": "Results of a technique that interprets reflected light to reveal objects outside the line of sight", "metadata": {"article_id": "issue_27", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Corners20220ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-27/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_27.html"}}
{"id": 32376394003, "type": "news_chunk", "title": "Chatbots Sue Telemarketers, Neural Nets See Around Corners", "subtitle": "Robocallers vs Robolawyer", "content": "A digital attorney is helping consumers take telemarketers and phone scammers to court.What’s new: DoNotPay makes an app billed as the world’s first robot lawyer. Its latest offering, Robo Revenge, automates the process of suing intrusive robocallers.How it works: U.S. law entitles consumers who have added their phone number to the National Do Not Call Registry to sue violators for $3,000 on average. For $3 a month, Robo Revenge makes it easy: The system generates a special credit card number that users can give to spam callers. When a telemarketer processes the card, it uses the transaction information to determine the company’s legal identity.After the call, users can open a chat session to enter information they’ve gathered.Then the system draws on the chat log, transaction data, and local, state, and federal laws to file a lawsuit automatically. Behind the news: Joshua Browder founded DoNotPay in 2016 to help people fight parking tickets. Since then, he has added tools that cancel unwanted subscriptions, navigate customer service labyrinths, and sue airlines for cancelled flights. Browder in 2018 told Vice that DoNotPay wins about 50 percent of cases, earning clients $7,000 per successful lawsuit on average.Why it matters: The average American receives 18 telemarketing calls a month — even though the Do Not Call Registry contains 240 million numbers, enough to cover around 70 percent of the U.S. population. Spam callers might not be so aggressive if their marks were likely to sue.We’re thinking: We’re not fans of making society even more litigious. But we could be persuaded to make an exception for scofflaw telespammers.", "image_caption": "Screen capture of robot lawyer Robo Revenge in action", "metadata": {"article_id": "issue_27", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Robo20Revenge20220ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-27/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_27.html"}}
{"id": 32376394004, "type": "news_chunk", "title": "Chatbots Sue Telemarketers, Neural Nets See Around Corners", "subtitle": "Meeting of the Minds", "content": "Geoffrey Hinton, Yoshua Bengio, and Yann LeCun presented their latest thinking about deep learning’s limitations and how to overcome them.What’s new: The deep-learning pioneers discussed how to improve machine learning, perception, and reasoning at the Association for the Advancement of Artificial Intelligence conference in New York.What they said: Deep learning needs better ways to understand the world, they said. Each is working toward that goal from a different angle: Bengio, a professor at the Université de Montréal, observed that deep learning’s results are analogous to the type of human thought — which cognitive scientists call system 1 thinking — that is habitual and occurs below the surface of consciousness. He aims to develop systems capable of the more attentive system 2 thinking that enables people to respond effectively to novel situations. He described a consciousness prior, made up of high-level variables with a high probability of being true, that would enable AI agents to track abstract changes in the world. That way, understanding, say, whether a person is wearing glasses would be a matter of one bit rather than many pixels.Hinton, who divides his time between Google Brain and the University of Toronto, began by noting the limitations of convolutional neural networks when it comes to understanding three-dimensional objects. The latest version of his stacked-capsule autoencoder is designed to overcome that issue. The model learns to represent objects independently of their orientation in space, so it can recognize objects despite variations in point of view, lighting, or noise.Facebook AI chief LeCun noted that, while supervised learning has accomplished amazing things, it requires tremendous amounts of labeled data. Self-supervised learning, in which a network learns by filling in blanks in input data, opens new vistas. This technique has had great success in language models, but it has yet to work well in visual tasks. To bridge the gap, LeCun is betting on energy-based models that measure compatibility between an observation (such as a segment of video) and the desired prediction (such as the next frame). Behind the news: The Association for Computing Machinery awarded Bengio, Hinton, and LeCun the 2018 A. M. Turing Award for their work. The association credits the trio’s accomplishments, including breakthroughs in backpropagation, computer vision, and natural language processing, with reinvigorating AI.Words of wisdom: Asked by the moderator what students of machine learning should read, Hinton offered the counterintuitive observation that “reading rots the mind.” He recommended that practitioners figure out how they would solve a given problem, and only then read about how others solved it.We’re thinking: We apologize for rotting your mind.", "image_caption": "Association for the Advancement of Artificial Intelligence conference in New York", "metadata": {"article_id": "issue_27", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Godfathers201.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-27/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_27.html"}}
{"id": 32376394005, "type": "news_chunk", "title": "Chatbots Sue Telemarketers, Neural Nets See Around Corners", "subtitle": "Trading Faces", "content": "AI’s ability to transfer a person’s face from a source photo onto someone in a target photo doesn’t work so well when the target face is partially obscured by, say, eyeglasses, a veil, or a hand. A new technique handles such occlusions.What’s new: Lingzhi Li at Peking University and collaborators at Microsoft Research propose FaceShifter, a face-swapping system that reproduces accurately both a source face and elements that block the target.Key insight: It’s easier to reproduce occlusions if you’ve scoped them out ahead of time. FaceShifter takes an extra step to identify occlusions before it renders the final image.How it works: The system has two major components. Adaptive Embedding Integration Network (AEI-Net) spots occlusions and generates a preliminary swap. Heuristic Error Acknowledging Refinement Network (HEAR-Net) then refines the swap. AEI-Net identifies troublesome occlusions by using the target image as both source and target. The difference between its input and output highlights any occlusions it failed to reproduce.AEI-Net extracts face features from a source image. It learns to extract non-face features from the target image in multiple resolutions, so it can capture larger and smaller shapes.AEI-Net’s generator combines these features into an intermediate representation, using attention to focus on the most relevant features.HEAR-Net uses the occlusion-only and intermediate images to generate a final image. It’s trained to strike a balance between maintaining the source face’s distinctiveness, minimizing changes in AEI-Net’s output, and reproducing images accurately when the source and target are the same. Results: The researchers used a pretrained face classifier to evaluate how well FaceShifter maintained the distinctiveness of the source face compared with DeepFakes and FaceSwap. FaceShifter achieved 97.38 percent accuracy versus 82.41 percent, the next-best score. It also outscored the other models in human evaluations of realism, identity, and attributes like pose, face expression, lighting.Why it matters: FaceShifter introduces a novel method to check its own work. Although the researchers focused on swapping faces, a similar approach could be used to tackle challenges like combating adversarial examples.We’re thinking: Better face swapping one day could transform entertainment by, say, grafting new stars — or even audience members — into movies. But it’s also potentially another tool in the arsenal of deepfakers who aim to deceive.", "image_caption": "Different examples of FaceShifter working on peoples' portraits", "metadata": {"article_id": "issue_27", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FaceShifter20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-27/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_27.html"}}
{"id": 32376394006, "type": "news_chunk", "title": "Chatbots Sue Telemarketers, Neural Nets See Around Corners", "subtitle": "Business Pushes the Envelope", "content": "The business world continues to shape deep learning’s future.What’s new: Commerce is pushing AI toward more efficient consumption of data, energy, and labor, according to a report on trends in machine learning from market analyst CB Insights.What they think: The report draws on a variety of sources including records of mergers and acquisitions, investment tallies, and patent filings. Among its conclusions: Consumers are increasingly concerned about data security. One solution may be federated learning, the report says. Tencent’s WeBank is developing this approach to run credit checks without removing consumers’ data from their devices. Similarly, Nvidia’s Clara allows hospitals to share diagnostic models trained on patient data without compromising the data itself.AI’s success so far has relied on big data, but uses in which large quantities of labeled data are hard to come by require special techniques. One solution to this small data problem is to synthesize training examples, such as faux MRIs that accurately portray rare diseases. Another is transfer learning, in which a model trained on an ample dataset is fine-tuned on a much smaller one.Businesses can have a tough time finding the right models for their needs, given the shortage of AI specialists and the variety of neural network architectures to choose from. One increasingly popular solution: AI tools that automate the design of neural networks, such as Google’s AutoML.Demand for AI in smartphones, laptops, and the like is pushing consumer electronics companies toward higher-efficiency models. That helps explain Apple’s purchase of edge-computing startup Xnor.ai in January. We’re thinking: It’s great to see today’s research findings find their way into tomorrow’s commercial applications. The road from the AI lab to marketplace gets busier all the time.", "image_caption": "Capture of the report on trends in machine learning from market analyst CB Insights", "metadata": {"article_id": "issue_27", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CB20Insights20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-27/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_27.html"}}
{"id": 75101127001, "type": "news_chunk", "title": "Political Deepfakes, Tree-Dodging Drones, Faster Brain Surgery", "subtitle": "OpenAI Under Fire", "content": "Dear friends, I chatted recently with MIT researcher Lex Fridman on his Artificial Intelligence podcast, where we discussed our experiences teaching deep learning. It was the most fun I’ve had in an interview lately, and you can watch the video here. Lex asked me what machine learning concepts students struggle with most. While I don’t think that any particular concept is especially difficult, studying deep learning is a lot like studying math. No particular math concept — addition, subtraction, and so on — is harder than others, but it’s hard to understand division if you don’t already understand multiplication. Similarly, deep learning involves many concepts, such as LSTMs with Attention, that build on other concepts, like LSTMs, which in turn build on RNNs. If you’re taking a course on deep learning and struggling with an advanced concept like how ResNets work, you might want to review earlier concepts like how a basic ConvNet works. As deep learning matures, our community builds new ideas on top of old ones. This is great for progress, but unfortunately it also creates longer “prerequisite chains” for learning the material. Putting in extra effort to master the basics will help you when you get to more advanced topics. Keep learning! An icon of idealism in AI stands accused of letting its ambition eclipse its principles.What’s new: Founded in 2015 to develop artificial general intelligence for the good of humankind, OpenAI swapped its ideals for cash, according to MIT Technology Review.The critique: OpenAI began as a nonprofit committed to sharing its research, code, and patents. Despite a $1 billion initial commitment from Elon Musk, Peter Thiel, and others, the organization soon found it needed a lot more money to keep pace with corporate rivals like Google’s DeepMind. The pursuit of funding led it ever farther afield of its founding principles as it sought to attract further funding and talent, writes reporter Karen Hao. In early 2019, OpenAI set up a for-profit arm. The organization limited investors to a 100-fold return, which critics called a ploy to promote expectations of exaggerated returns.Soon after, the organization accepted a $1 billion investment from Microsoft. In return, OpenAI said Microsoft was its “preferred partner” for commercializing its research.Critics accuse the company of overstating its accomplishments and, in the case of the GPT-2 language model, overdramatizing them by withholding code so it wouldn’t be misused.Even as it seeks publicity, OpenAI has grown secretive. The company provided full access to early research such as 2016’s Gym reinforcement learning environment (pictured above). Recently, though, it has kept some projects quiet, making employees sign non-disclosure agreements and forbidding them from talking to the press. Behind the news: Musk seconded the critique, adding that he doesn’t trust the company’s leadership to develop safe AI. The Tesla chief resigned from OpenAI’s board last year saying that Tesla’s autonomous driving research posed a conflict of interest.Why it matters: OpenAI aimed to counterbalance corporate AI, promising a public-spirited approach to developing the technology. As the cost of basic research rises, that mission becomes increasingly important — and difficult to maintain.We’re thinking: OpenAI’s team has been responsible for several important breakthroughs. We would be happy to see its employees and investors enjoy a great financial return. At the same time, sharing knowledge is crucial for developing beneficial applications, and exaggerated claims contribute to unrealistic expectations that can lead to public backlash. We hope that all AI organizations will support openness in research and keep hype to a minimum.", "image_caption": "Rendering of simulated environment", "metadata": {"article_id": "issue_28", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/OpenAI.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-28/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_28.html"}}
{"id": 75101127002, "type": "news_chunk", "title": "Political Deepfakes, Tree-Dodging Drones, Faster Brain Surgery", "subtitle": "Surgical Speed-Up", "content": "Every second counts when a patient’s skull is open in the operating room. A new technique based on deep learning can shorten some brain surgeries.What’s new: During brain cancer operations, surgeons must stop in mid-operation for up to a half hour while a pathologist analyzes the tumor tissue. Led by neurosurgeon Todd Hollon, researchers at the University of Michigan and elsewhere developed a test powered by deep learning that diagnoses tumor samples in only a few minutes. (The paper is behind a paywall.)Key insight: The authors trained a convolutional neural network (CNN) to diagnose tumor samples based on a rapid digital imaging technique known as stimulated Raman histology (SRH).How it works: Previous approaches require transporting tumor tissue to a lab, running assays, and analyzing the results. The new test takes place within the operating room: A Raman spectroscope produces two SRH images that measure different properties of the sample, and a CNN classifies the images. The researchers fine-tuned the pretrained inception-resnet-v2 architecture on images from 415 patients. They trained the network to recognize 13 cancer types that account for around 90 percent of observed brain tumors.A preprocessing algorithm derives from each image a set of overlapping, high-resolution patches. This procedure creates a uniform, CNN-friendly image size; boosts the number of training samples; and eases parallel processing.The CNN predicts the tumor type of each patch, and the model chooses the diagnosis predicted most frequently in the patches. Results: The researchers measured the CNN’s performance in a clinical trial (the first trial of a deep learning application in the operating room, they said). They evaluated tumor samples using the CNN as well as chemical tests and compared the results with clinical diagnoses. The CNN was 94.6 percent accurate, 0.7 percent better than the next-best method.Why it matters: Chemical tests not only incur the risk of interrupting surgery, they also need to be interpreted by a pathologist who may not be readily available. The CNN renders a diagnosis directly, potentially increasing the number of facilities where such operations can be performed.We’re thinking: Deep learning isn’t brain surgery. But brain surgery eventually might be deep learning.", "image_caption": "Information related to a test powered by deep learning that diagnoses tumor samples in only a few minutes", "metadata": {"article_id": "issue_28", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Raman.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-28/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_28.html"}}
{"id": 75101127003, "type": "news_chunk", "title": "Political Deepfakes, Tree-Dodging Drones, Faster Brain Surgery", "subtitle": ". . . And I Approve of This Deepfake", "content": "Deepfake tech reared its digitally altered head in Indian politics, but with a twist: Altered images of a politician were produced by his own campaign.What happened: On the eve of a legislative assembly election, the local branch of India’s national ruling party, the Bharatiya Janata Party, released a video of its local leader criticizing the opposition in a language he does not speak, according to Vice. How it works: The BJP hired a local company, Ideaz Factory, to feature Delhi BJP president Manoj Tiwar in a video aimed at voters who speak Haryanvi, a Hindi dialect spoken by many of the city’s migrant workers. Tiwari recorded a clip in which he accuses his opponents, in Hindi, of breaking campaign promises. A voice actor translated his words into Haryanvi, and Ideaz Factory used a GAN to transfer the actor’s mouth and jaw motions onto Tiwari’s face. Experts at the Rochester Institute of Technology quoted by Vice believe the company based its model on Nvidia’s vid2vid.The party released the altered video to thousands of WhatsApp groups, reaching as many as 15 million people. Then it commissioned a second deepfake of Tiwari speaking English.The BJP later called the effort a “test,” not an official part of its social media campaign. Encouraged by what it characterized as positive reactions from viewers, Ideaz Factory hopes to apply its approach to upcoming elections in India and the U.S. Why it matters: Deepfakery’s political debut in India was relatively benign: It helped the ruling party reach speakers of a minority language, possibly fooling them into thinking Tiwar spoke their tongue. Nonetheless, experts worry that politicians could use the technology to supercharge propaganda, and malefactors could use it to spur acts of violence.We’re thinking: We urge teams that create deepfakes to clearly disclose their work and avoid misleading viewers. As it happened, the fakery didn’t help much: The BJP won only eight of 70 parliamentary seats in the Delhi election.", "image_caption": "Example of a deepfake", "metadata": {"article_id": "issue_28", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize204.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-28/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_28.html"}}
{"id": 75101127004, "type": "news_chunk", "title": "Political Deepfakes, Tree-Dodging Drones, Faster Brain Surgery", "subtitle": "Imitation Learning in the Wild", "content": "Faster than a speeding skateboard! Able to dodge tall trees while chasing a dirt bike! It’s ... an upgrade in the making from an innovative drone maker.What’s new: Skydio makes drones that follow and film extreme sports enthusiasts as they skate, cycle, and scramble through all types of terrain. The company used imitation learning to develop a prototype autopilot model that avoids obstacles even while tracking targets at high speed.How it works (and sometimes doesn’t): Six fisheye cameras give the drone a 360-degree view. Separate models map the surroundings, lock onto the target, predict the target’s path, and plan the drone’s trajectory. But the rule-based autopilot software has trouble picking out details like small tree branches and telephone wires from a distance. At high speeds, the drone sometimes has to dodge at the last moment lest it wind up like a speeder-riding stormtrooper in Return of the Jedi.The next step: The researchers aim to build an end-to-end neural network capable of flying faster while avoiding obstacles more effectively than the current autopilot. They began by training a standard imitation learning algorithm on a large corpus of real-world data generated by the current system. But the algorithm didn’t generalize well to obstacles like tree branches. The researchers believe that’s because the open sky often provides a variety of obstacle-free flight paths, so even a good path often yields little useful learning.So they switched the learning signal from the distance between the learner’s and the autopilot’s actions to the autopilot’s reward function. They also raised the penalty for deviating from its decisions when flying through crowded environments. This forced the model to learn that trees in the distance would mean dangerous branches up close.The weighted penalty system trained the drone to follow a target effectively through lightly wooded terrain based on only three hours of training data. Why it matters: The new software is still in development, and Skydio has not announced a release date. But the faster its drones can fly through crowded terrain without mishaps, the more its customers will be able to pull off gnarly stunts without losing their robot sidekick to a tree branch, and the more cool videos we’ll get to watch on YouTube.We’re thinking: Now, if only Skydio could help the Empire’s stormtroopers improve their aim with blasters.", "image_caption": "Drone following a person riding an ATV", "metadata": {"article_id": "issue_28", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize205.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-28/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_28.html"}}
{"id": 75101127005, "type": "news_chunk", "title": "Political Deepfakes, Tree-Dodging Drones, Faster Brain Surgery", "subtitle": "Deep Learning for Object Tracking", "content": "AI is good at tracking objects in two dimensions. A new model processes video from a camera with a depth sensor to predict how objects move through space.What’s new: Led by Chen Wang, researchers from Stanford, Shanghai Jiao Tong University, and Nvidia built a system that tracks objects fast enough for a robot to react in real time: 6D-Pose Anchor-based Category-level Keypoint-tracker (6-PACK). Why 6D? Because three-dimensional objects in motion have six degrees of freedom: three for linear motion and three for rotation. You can see the system in action in this video.Key insight: Rather than tracking absolution location, 6-PACK tracks an object’s change in position from video frame to video frame. Knowing its position in one frame makes it easier to find in the next.How it works: The network’s training data is labeled with a six-dimensional vector that represents changes in an object’s location and orientation between frames. From that information, it learns to extract keypoints such as edges and corners, calculate changes in their positions, and extrapolate a new position. Objects in the training data are labeled with a category such as bowl or mug. The researchers identify an object’s center in the first frame.The model uses that information to generate a cloud of points representing the object.Based on the center and point cloud, the network generates a user-defined number of keypoints, essentially a 3D bounding box.In each successive frame, the model uses the previous keypoint locations to estimate the center roughly. An attention layer learns to find the center more precisely. Then the network updates the point cloud, and from there, the keypoints. Results: Tested on a dataset of real-world videos, 6-PACK predicted object position and rotation within 5cm and 5 degrees in 33.3 percent of cases, versus the previous state of the art of 17 percent.Why it matters: The ability to track objects as they move and rotate is essential to progress in robotics, both to manipulate things and to navigate around them.We’re thinking: Object tracking algorithms and visual keypoints have a long history stretching beyond the 1960-vintage Kalman filter. Deep learning has come to dominate object recognition, and it’s good to see progress in tasks like tracking and optical flow.", "image_caption": "Information and images related to 6D-Pose Anchor-based Category-level Keypoint-tracker (6-PACK)", "metadata": {"article_id": "issue_28", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/6-PACK.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-28/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_28.html"}}
{"id": 75101127006, "type": "news_chunk", "title": "Political Deepfakes, Tree-Dodging Drones, Faster Brain Surgery", "subtitle": "Poultry in Motion", "content": "A top meat packer is counting its chickens with AI.What’s new: Tyson Foods is using computer vision to track drumsticks, breasts, and thighs as they move through its processing plants, the Wall Street Journal reports.How it works: Workers in slaughterhouses typically count the packages of poultry parts bound for market, then use hand signals to communicate the totals to another worker who enters the numbers into a computer. Tyson is replacing them with cameras that feed neural networks. The company has installed the system in three U.S. factories and plans to roll it out in its four other supermarket packaging factories by the end of the year. The camera system identifies cuts of meat along with inventory codes, while a scale weighs them.Workers double-check some of the system’s findings.Tyson says the system is 20 percent more accurate than the human-only method. Behind the news: AI and robotics are coming home to roost in the poultry industry and beyond. The Tibot Spoutnic (shown above) is a Roomba-like robot that roams commercial chicken pens so the birds lay eggs in their nests rather than on the floor.Cargill is developing machine learning algorithms that monitor poultry farms for clucks indicating the birds are distressed or ill.A precision deboning bot from Georgia Tech uses computer vision to slice up chickens more efficiently than human butchers.AI is helping to manage other food animals as well: There’s face recognition for cows, activity trackers for pigs, and movement tracking to optimize feeding schedules for fish farms. Why it matters: Food companies are looking to AI to drive down costs amid uncertain market conditions. Tyson’s profits took a hit last year. A recent industry analysis warns that the vagaries of feed prices, geopolitics, and avian flu make this year precarious as well. We’re thinking: Consumers around the world are eating poultry. Any company hoping to meet demand had better not chicken out from the latest technology.", "image_caption": "The Tibot Spoutnic", "metadata": {"article_id": "issue_28", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize206.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-28/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_28.html"}}
{"id": 90645625001, "type": "news_chunk", "title": "Standing Up for Ethical AI, Efficient Transformers, Up-Rezzing", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, In addition to creating tremendous value, AI is creating tremendous concentrations of power. Our community is wrestling with what constitutes fair use of that power. The Markup published an article criticizing car insurance giant Allstate for price discrimination — charging different fees to different customers — based not only on their risk but also on their predicted willingness to pay. Is this behavior okay? Digital technology enables online comparison shopping, which shifts pricing power toward consumers. But it also enables companies to create unique products for individual customers — say, a ride from point A to point B at a particular time, or a health insurance plan tailored to the customer’s personal history — and AI can help optimize prices to maximize profit for vendors. That can lead to both better products and worse price transparency. If an online store sells the same hammer to different people for different prices, customers eventually will notice. That helps keep this form of price discrimination in check. But the temptation for sellers is still there. In 2016, Uber revealed that customers pay higher prices when their phone battery is low. (The company said it didn’t take advantage of this phenomenon.) I wonder sometimes if I should comparison-shop more frequently than I do. Less because I’m anxious to save a few dollars on one purchase, but because I want to train vendors’ AI systems to think I’m sensitive to price and thus to offer me lower prices. In college, my Economics 101 professor taught about supply and demand, and how our economy creates surpluses for both producers and consumers. But AI is prompting us to revisit old economic theories — along with our sense of what’s fair. These are hard questions. I hope we can work on them together to give the world great products and services at even better prices. Keep learning! After a decade in wireless communications, Cherif was ready for a change. Online courses, textbooks, and meetups helped him build his skills and land a Machine Learning Engineer role at Postmates. Learn how he overcame obstacles, aced job interviews, and started applying ML in the real world in the latest installment of our “Breaking Into AI” series. Read more", "image_caption": "Economic surplus", "metadata": {"article_id": "issue_29", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20220ASPECT201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-29/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_29.html"}}
{"id": 90645625002, "type": "news_chunk", "title": "Standing Up for Ethical AI, Efficient Transformers, Up-Rezzing", "subtitle": "News", "content": "While deep learning is taking us into the future, it’s also opening windows into the past.What’s new: Machine learning-savvy Redditor Denis Shiryaev brought 100-year-old silent film footage of New York City into the 21st century by automatically sharpening the picture, boosting the frame rate, and adding color.How he did it: Shiryaev obtained eight minutes of footage shot by a Swedish film maker in 1911. He spent five days running the movie through a gauntlet of neural nets. Shiryaev used Enhanced Super-Resolution Generative Adversarial Networks to compute additional pixels, boosting resolution to 4K (approximately 4,000 pixels horizontally).He used Depth-Aware Video Frame Interpolation to generate in-between frames, raising the frame rate to 60 per second. Film shot in the early 1900s typically had much lower frame rates, making them look sped-up and jerky.He applied DeOldify to colorize the imagery. He noted on Reddit that he isn’t completely sold on that process, because the colors aren’t historically accurate.The sound effects were digital audio clips. Several commenters said they recognized horse whinnies from the video game Age of Empires II. Behind the news: Shiryaev has used these procedures to update footage of Moscow in 1896, an iconic film from the same year that shows a French train pulling into station and Apollo 16 astronauts driving their moon buggy across the lunar surface in 1972.Why it matters: Shiryaev’s work brings these pieces of the past to life, overcoming the poor image quality, jerky movements, and lack of colors that diminish so much historic film. Similar treatment no doubt would perk up careworn Hollywood classics as well.We’re thinking: We can’t wait to up-res old home videos. It’s about time our parents’ 1980s hairstyles were revealed in high def.", "image_caption": "Film from 1911 colored", "metadata": {"article_id": "issue_29", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize208.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-29/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_29.html"}}
{"id": 48162265001, "type": "news_chunk", "title": "Taming Dangerous AI, Microscopes Spot Tumors, NLP Grades...", "subtitle": "Points Paint the Picture", "content": "Dear friends, I traveled to Taiwan last week, where I met many CEOs interested in AI transformation of traditional companies. I also visited Taiwan AI Labs which, similar to OpenAI, started as a nonprofit AI research institute. Funded by government and private financing, Taiwan AI Labs works on smart city, healthcare, and other projects; for example, using computer vision to estimate traffic flow. Ethan Tu, the lab’s leader, tells me it focuses on practical and socially important projects, including ones that are hard to fund commercially, and openly publishes all its work. I also several professors on sabbatical there. They told me that the lab gives them more engineering resources for AI than they can generally find in a university. I’m glad to see different nations experiment with new ways to organize AI research and development. I hope more countries will fund nonprofit AI research labs. Shout out also to National Taiwan University, Taiwan Ministry of Science and Technology, and Taiwania Capital for helping organize my trip! Keep learning, Creating a virtual representation of a scene using traditional polygons and texture maps involves several complex operations, and even neural-network approaches have required manual preprocessing. Researchers from the Samsung AI Center and Skolkovo Institute of Science and Technology propose a new deep-learning pipeline that visualizes scenes with far less fuss. What’s new: Aliev et al.’s Neural Point Based Graphics technique rapidly produces realistic images in an end-to-end process. It does particularly well with thin objects that are hard to model using a polygonal mesh, such as shoe laces and bicycle tires. You can see it in action here.Key insight: There’s no need to model surfaces to represent a scene. Point clouds and corresponding images together contain enough information for a neural network to generate realistic images. Moreover, neural networks can fill in missing information such as parts of objects hidden from view, which simplifies scene modeling. How it works: The system starts with a point cloud representing a scene, an image of the scene, camera parameters including viewing angle, and a randomly initialized vector representation of each point that encodes shape and surface properties. Using traditional graphics libraries and algorithms, it pixelizes a scene’s point cloud and vectors into a multi-channel raw image.A rendering network based on the U-Net architecture takes the raw image as input. It learns simultaneously to improve the vectors and generate a final RGB image by minimizing the difference between generated and ground-truth images.Once trained, the system can accept a new camera position to generate corresponding viewpoints from a given pixel cloud and learned vectors. Results: The researchers compared photographic input and generated images from a variety of data sets, including consumer cameras, across several scene-capture techniques, including traditional and deep learning methods. Their system scored highest on a number of measures of image similarity. While its rendering of synthetic scenes isn’t as realistic as that achieved by state-of-the-art ray tracing methods, it produces good-looking images roughly 2,000 times faster. Why it matters: Neural Point-Based Graphics is a distinct step forward for end-to-end scene capture. By demonstrating that point clouds and images — which can come from a smartphone — together can represent scenes in realistic detail, this research opens the door for refinements that could ultimately compete with the best current methods in a much simpler pipeline.We’re thinking: Just as neural networks have replaced rule-based systems in computer vision and language applications, they’re on track to have a similar impact in graphics. Given its simplicity and speed, this approach could facilitate real-time applications such as video games, virtual reality, and augmented reality.", "image_caption": "Neural Point Based Graphics technique producing a realistic image", "metadata": {"article_id": "issue_3", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_pointcloud.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-3/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_3.html"}}
{"id": 48162265002, "type": "news_chunk", "title": "Taming Dangerous AI, Microscopes Spot Tumors, NLP Grades...", "subtitle": "Smart Students, Dumb Algorithms", "content": "A growing number of companies that sell standardized tests are using natural language processing to assess writing skills. Critics contend that these language models don’t make the grade. What happened: An investigation by Motherboard found that several programs designed to grade English-language essays show bias against minorities and some students who speak English as a second language. Some models gave high scores to computer-generated essays that contained big words but little meaning.What they found: Models trained on human-graded papers learn to correlate patterns such as vocabulary, spelling, sentence length, and subject-verb agreement with higher or lower scores. Some experts say the models amplify the biases of human graders. In 2018, the publishers of the E-Rater — software used by the GRE, TOEFL, and many states — found that their model gave students from mainland China a 1.3 point lift (on a scale of 0 to 6). It seems that Chinese students, while scoring low on grammar and mechanics, tend to write long sentences and use sophisticated vocabulary.The same study found that E-Rater docked African-American students by .81 points, on average, due to biases in grammar, writing style, and organization.Motherboard used BABEL to generate two essays of magniloquent gibberish. Both received two 4-out-of-6 scores from the GRE’s online ScoreItNow! practice tool. Behind the news: At least 21 U.S. states use NLP to grade essays on standardized tests for public schools. Of those, 18 also employ human graders to check a small percentage of papers randomly.Why it matters: Standardized tests help determine access to education and jobs for millions of Americans every year. Inappropriate use of NLP could be robbing them of life-changing opportunities. We’re thinking: The company behind E-Rater is the only one that publishes studies on its grading model’s shortcomings and what it’s doing to fix them. Colleges and school boards should lead the charge in demanding that other test providers do the same.", "image_caption": "Question from an exam", "metadata": {"article_id": "issue_3", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Manatee20Question.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-3/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_3.html"}}
{"id": 48162265003, "type": "news_chunk", "title": "Taming Dangerous AI, Microscopes Spot Tumors, NLP Grades...", "subtitle": "How to Share Dangerous AI", "content": "OpenAI raised eyebrows in February when it announced — and withheld — the full version of its groundbreaking language model, GPT-2. Six months later, the company has re-examined the decision. What happened: The for-profit research organization issued a follow-up report that details the results of GPT-2’s “staged release.” Fearing that the full version would be used to generate convincing misinformation, OpenAI initially released a limited version (124 million parameters). That release was followed by larger versions culminating, so far, in a 774 million-parameter model made available along with the report. What the report says: Releasing the model in stages while allowing certain partners full access helped advance an understanding of both benign and malignant uses, the organization says. OpenAI remains convinced that staged release is “likely to be a key foundation of responsible publication of AI.” Research by OpenAI’s partners confirms that GPT-2’s output can be as credible as New York Times articles and very difficult to detect.The report found no malicious uses of available GPT-2 versions.OpenAI is aware of five other groups that replicated the complete model. Some of them also opted for a staged release.It’s working with several universities to study the social and policy implications of larger GPT-2 models.OpenAI plans to release the complete version (1.5 billion parameters) in coming months, barring adverse consequences of smaller releases. Behind the news: OpenAI’s decision to withhold the complete GPT-2 rankled many in the AI community; without the code and detailed training information, it’s impossible to replicate the results. However, the organization’s reticence didn’t stop a pair of Brown University graduate students, neither of whom had a background in language modeling, from replicating GPT-2 in August. We’re thinking: The AI community thrives on shared information. Yet the potential for powerful AI models to wreak havoc on the general welfare suggests some sort of gatekeeping mechanism is in order. Staged release may be just the device that white hats need to stay one step ahead of malefactors.", "image_caption": "OpenAI's GPT-2 results", "metadata": {"article_id": "issue_3", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_GPT20Resized-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-3/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_3.html"}}
{"id": 48162265004, "type": "news_chunk", "title": "Taming Dangerous AI, Microscopes Spot Tumors, NLP Grades...", "subtitle": "Seeing Cancer", "content": "Microscopes outfitted with AI-driven augmented reality could improve the accuracy of cancer diagnoses. What’s happened: Google Health developed an attachment for analog microscopes that outlines signs of breast and prostate cancer in real time. How it works: A computer-vision system spots cancer in a cell slide, while augmented-reality tech superimposes the AI’s prediction over the slide at around 27 frames per second. The developers combined the Inception V3 image classifier with a fully convolutional neural network, which allowed the system to recognize tumorous patterns much faster.A camera captures a head-on view of the slide and projects it, overlaid with the AI prediction, into the microscope eyepiece. Behind the news: Pathologists use microscopes to measure tumor size relative to nearby lymph nodes and to count the number of cells nearing or undergoing mitosis. That information tells them how aggressively a patient’s cancer is spreading.Why it matters: Interpreting cell slides is subjective, and one pathologist’s understanding can differ greatly from another’s. Patients in locations where trained pathologists are scarce tend to suffer most from this inconsistency. AI-enhanced tools could help make diagnoses more reliable. We’re thinking: AI is a natural complement to digital microscopes, but analog microscopes are far more common. This technology promises to upgrade those tools at a fraction of the cost of replacing them.", "image_caption": "Hardware components of the Augmented Reality Microscope (ARM)", "metadata": {"article_id": "issue_3", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Screen20Shot202019-09-0320at203.25.3120PM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-3/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_3.html"}}
{"id": 48162265005, "type": "news_chunk", "title": "Taming Dangerous AI, Microscopes Spot Tumors, NLP Grades...", "subtitle": "Honey, I Shrunk the Network!", "content": "Deep learning models can be unwieldy and often impractical to run on smaller devices without major modification. Researchers at Facebook AI Research found a way to compress neural networks with minimal sacrifice in accuracy. What’s new: Building on earlier work, the researchers coaxed networks to learn smaller layer representations. Rather than storing weights directly, the technique uses approximate values that can stand in for groups of weights. Key insight: The researchers modified an existing data-compression method, product quantization, to learn viable weight approximations. How it works: By representing groups of similar weights with a single value, the network can store only that value and pointers to it. This reduces the amount of storage needed for weights in a given layer. The network learns an optimal set of values for groups of weights, or subvectors, in a layer by minimizing the difference between layer outputs of the original and compressed networks. For fully connected layers, the authors group the weights into subvectors. (They propose a similar but more involved process for convolutional layers.)They pick a random subset of subvectors as starting values, then iteratively improve the values, layer by layer, to minimize the difference between the compressed and original neural network.Then they optimize the compressed network representation against multiple layers at a time, starting with the first two and ultimately encompassing the entire network. Results: The researchers achieve best top-1 accuracy on ImageNet for model sizes of 5MB and 10MB. (They achieve competitive accuracy for 1MB models.) They also show that their quantization method is superior to previous methods for ResNet-18. Why it matters: Typically, researchers establish the best model for a given task, and follow-up studies find new architectures that deliver similar performance using less memory. This work offers a way to compress an existing architecture, potentially taking any model from groundbreaking results in the lab to widespread distribution in the field with minimal degradation in performance. Yes, but: The authors demonstrate their method on architectures with fully connected layers and CNNs only. Further research will be required to find its limits, and also to optimize the compressed results for compute speed.We’re thinking: The ability to compress top-performing models could put state-of-the-art AI in the palm of your hand and eventually in your pacemaker.", "image_caption": "Illustration of Facebook AI Research method to compress neural networks", "metadata": {"article_id": "issue_3", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_quantized20networks.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-3/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_3.html"}}
{"id": 48162265006, "type": "news_chunk", "title": "Taming Dangerous AI, Microscopes Spot Tumors, NLP Grades...", "subtitle": "AI in the Real World", "content": "Theoretical advances can be thrilling, but the excitement can drown out all the ways AI is actually being put to use. DeepIndex provides an up-to-date, well organized, cheeky guide to practical applications culled from news reports. What it is: DeepIndex.org lists over 630 examples, organized into 19 categories and ranked according to how well they work. Categories include Gaming, Finance, Agriculture, Education, and Security, plus a catch-all for miscellaneous models like the one Apple used for its Animoji feature.DeepIndex creator Chris Yiu ranks the effectiveness of each application: Crushing It, Capable, or Getting There. The ranking reflects factors like product status, academic publications, and case studies. Our favorites: DeepIndex is a treasure trove of bold efforts and unlikely concepts. Yiu’s personal favorite is a model that “fixes Warner Bros.’ terrible attempts to digitally remove Henry Cavill’s mustache in [the Hollywood blockbuster] Justice League.” That’s a fun use case, no doubt, but we found others more compelling: Fraugster, an AI-powered fraud prevention tool, calms some of our fears of getting swept up in the next data breach.A chatbot called DoNotPay has overturned hundreds of thousands of parking tickets in the UK.The Agriculture section includes a harvest of models capable of picking strawberries, sorting cucumbers, and spraying pesticides.Orbital Atlas is helping the U.S. Air Force (and presumably the incipient Space Force) navigate the increasingly cluttered space surrounding planet Earth.A machine learning algorithm called Warblr that matches tweets, chirps, and warbles to the bird species that sang them.And for metalheads, Dadabots generates an endless stream of death metal music. Now that’s crushing it.", "image_caption": "DeepIndex.org list with ways AI is being put to use", "metadata": {"article_id": "issue_3", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Deep20Index20Resized-2.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-3/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_3.html"}}
{"id": 50645224001, "type": "news_chunk", "title": "Mind-Controlled Robot Hand, Fashions by GAN, Face Recognition", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends,The Covid-19 pandemic is a tragedy that demands urgent and humane response. It’s also pushing us toward new ways of gathering and sharing information — and that may be a faint silver lining that might grow brighter over time.Many important conferences are being canceled. Just as the rise of online video brought a new generation of online education, I believe the rise of livestreaming and videoconferencing will bring a new generation of online conferences. For many years, attendees at top conferences have asked themselves: Why do we travel to one location, when it means: Significant costIncreased carbon emissionsLimitations on attendance due to venue sizeLimitations imposed by the host country’s visa policies Just as MOOCs today are a lot more than video, online conferences will be much richer than livestreamed video. Perhaps we’ll have regional chat rooms where attendees in the same country can share local resources even while they listen to a keynote. Or we will generate live transcripts through automatic speech recognition that attendees can tag with live commentary. Up- and downvoting one another’s questions will be routine, and some answers will be crowdsourced. I don’t expect online conferences to replace in-person events, which still have an important role. Rather, they’ll complement them. With more team members (including many in my organizations) working from home, the time is ripe to experiment with these ideas and move toward lower costs, smaller carbon footprints, democratized access, and stronger communities. If you have thoughts, let us know at [email protected]. Wash your hands, stay safe, and keep learning!Andrew We want to make sure we’re giving you the most useful newsletter in AI. Please answer a few questions to let us know what you’d like to see more (or less) of. Take the brief survey", "image_caption": "Welcome slide for the Global Interactive AI Conference", "metadata": {"article_id": "issue_30", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter20ASPECT204.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-30/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_30.html"}}
{"id": 50645224002, "type": "news_chunk", "title": "Mind-Controlled Robot Hand, Fashions by GAN, Face Recognition", "subtitle": "AI Gets a Grip", "content": "Amputees can control a robotic hand with their thoughts — plus machine learning.What’s new: University of Michigan researchers developed a system that uses signals from an amputee’s nervous system to control a prosthetic hand.How it works: The researchers grafted bits of muscle onto the severed nerve bundles at the ends of amputees’ forearms, then implanted electrodes into the muscle. They amplified and recorded the electric signals transmitted to the nerves when the recipients thought about, say, making a fist, pointing a finger, or rotating a thumb. Then they trained a pair of models to match the signals with the corresponding hand motions. A naive Bayes classifier learned to associate nerve signal patterns with common hand shapes.The researchers asked the subjects to mimic a virtual thumb as it made back-and-forth and side-to-side motions on a computer screen. A Kalman filter took in the electrical signals and the position and velocity of the avatar and learned to control the digit.Once trained, the software enabled the subjects to pick up and move objects and play Rock, Paper, Scissors. Behind the news: Other research groups are using similar methods to control robotic prostheses. Some promising approaches: Epineural electrodes wrap around nerves like a cuff to track signals from the brain.Intraneural electrodes tap into nerves using needles, so researchers can target brain signals more precisely.Targeted muscle reinnervation re-routes nerves from a severed limb into a nearby muscle. Sensors attached to the skin pick up the signals and transmit them to a prosthesis. Why it matters: Nearly two million Americans have lost a limb, along with millions more worldwide. More responsive prostheses could dramatically improve their quality of life.We’re thinking: Will they train robotic hands to do COVID-19-safe, palms-together namaste greetings?", "image_caption": "Robotic hand controlled by an amputee taking a can", "metadata": {"article_id": "issue_30", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Prosthetics1-hand20only2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-30/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_30.html"}}
{"id": 50645224003, "type": "news_chunk", "title": "Mind-Controlled Robot Hand, Fashions by GAN, Face Recognition", "subtitle": "Less Labels, More Learning", "content": "In small data settings where labels are scarce, semi-supervised learning can train models by using a small number of labeled examples and a larger set of unlabeled examples. A new method outperforms earlier techniques.What’s new: Kihyuk Sohn, David Berthelot, and colleagues at Google Research introduced FixMatch, which marries two semi-supervised techniques.Key insight: The technique known as pseudo labeling uses a trained model’s most confident predictions on unlabeled examples for subsequent supervised training. Consistency regularization penalizes a model if its predictions on two versions of the same data point — say, distorted variations on the same image — are dissimilar. Using these techniques in sequence enables a model to generalize insights gained from unlabeled data.How it works: FixMatch learns from labeled and unlabeled data simultaneously. It learns from a small set of labeled images in typical supervised fashion. It learns from unlabeled images as follows: FixMatch modifies unlabeled examples with a simple horizontal or vertical translation, horizontal flip, or other basic translation. The model classifies these weakly augmented images. If its confidence exceeds a user-defined threshold, the predicted class becomes a pseudo label.FixMatch generates strongly augmented versions of the pseudo-labeled images by applying either RandAugment (which samples image augmentations randomly from a predefined set) or CTAugment (which learns an augmentation strategy as the model trains). Then it applies Cutout, which removes portions randomly.The new model learns to classify the strongly augmented images consistently with the pseudo labels of the images they’re based on. Results: FixMatch achieved state-of-the-art performance for semi-supervised learning on several benchmarks devised by the researchers. (They removed labels from popular image datasets to create training sets with between four and 400 labels per class.) An alternative semi-supervised approach performed slightly better on some benchmarks, though it’s not obvious under what circumstances it would be the better choice.Why it matters: Google Research has been pushing the envelope of semi-supervised learning for image classification with a series of better and better algorithms. FixMatch outperforms its predecessors in the majority of comparisons, and its simplicity is appealing.We’re thinking: Small data techniques promises to open the door to many new applications of AI, and we welcome any progress in this area.", "image_caption": "FixMatch example", "metadata": {"article_id": "issue_30", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FixMatch20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-30/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_30.html"}}
{"id": 50645224004, "type": "news_chunk", "title": "Mind-Controlled Robot Hand, Fashions by GAN, Face Recognition", "subtitle": "Secret Identity", "content": "Hoping to keep surveillance capitalists from capitalizing on your face? Safeguard your selfies with a digital countermeasure.What’s new: Researchers at the University of Chicago and Fudan University devised a program that subtly alters portrait photos to confuse face recognition models without distorting the image to the human eye.How it works: Named after the Guy Fawkes mask beloved by privacy advocates, Fawkes cloaks faces by imposing patterns that, to machines, look like someone else. Fawkes compares a portrait photo to another person’s picture, using a feature extractor to find the areas that differ most. Then it generates a perturbation pattern and uses it to alter individual pixels.A penalty system balances the perturbations against a measure of user-perceived image distortion to make sure the effect is invisible to humans.An additional algorithm stress-tests Fawkes’ cloaks to make sure they fool models that use different feature extractors. Results: The researchers uploaded 50 cloaked photos of the same person to face recognition services from Amazon, Megvii, and Microsoft, which trained on the data. All three failed to identify the person in 32 uncloaked validation images — a 100 percent success rate. However, Fawkes had a hard time fooling models that were already familiar with a given face, having already trained on many uncloaked images. The models developed amnesia, though, after ingesting a fake social media account that exclusively contained cloaked (and renamed) photos.Yes, but: Fawkes isn’t fool-proof. The researchers were able to build models that saw through the system’s output. In fact, one such model cut its effectiveness to 65 percent.Models trained on photos of a single person in which 15 percent of the pictures were uncloaked were able to identify the person more than half of the time. Why it matters: We need ways, whether legal or technical, to enable people to protect their privacy. The U.S. startup Clearview.ai made headlines in January when the New York Times reported that its surveillance system, trained on billions of photos scraped from social media sites without permission, was widely used by law enforcement agencies and private businesses. We’re thinking: If this method takes off, face recognition providers likely will find ways to defeat it. It’s difficult to make images that humans can recognize but computers can’t.", "image_caption": "Examples of original and cloaked portrait photos", "metadata": {"article_id": "issue_30", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Fawkes20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-30/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_30.html"}}
{"id": 50645224005, "type": "news_chunk", "title": "Mind-Controlled Robot Hand, Fashions by GAN, Face Recognition", "subtitle": "Clothes Make the GAN", "content": "Fashion models recently sashayed down Paris Fashion Week catwalks in outfits designed by deep neural nets.What’s new: Swedish design firm Acne Studios based its 2020 fall/winter men’s line on output from a generative adversarial network.How it works: AI artist Robbie Barrat trained a GAN to generate images of high-fashion outfits by feeding it thousands of pictures of models wearing Acne’s past work. The results were often surreal (though not much more so than Acne’s usual designs). Acne’s creative director chose some favorites and adapted them for the real world. He used big patches of sewn-on cloth to recreate the generated images’ bold color swatches, for instance, and unraveled fabric to mimic their glitchy textures.Many jackets in the generated images featured large, curved openings at the waist, apparently because the network conflated pockets with hemlines. Behind the news: Barrat, a 20-year-old machine learning engineer, worked on Nvidia’s self-driving car program and Stanford’s Khatri Lab biomedical research team. He has used AI to generate landscapes and nudes as well as lyrics in the style of Kanye West. In 2017, a trio of French art students used one of his models to create an artwork that sold for nearly half a million dollars.Why it matters: In the fashion industry, AI is the new black. Indian fast-fashion company Myntra uses a model called Ratatouille to design t-shirts and kurtas (a long-sleeved, collarless shirt popular in south Asia).Christopher Wylie, who coded (and subsequently blew the whistle on) Cambridge Analytica’s effort to glean political insights from Facebook posts, is using his data-mining savvy to target customers for clothing giant H&M. Yes, but: Acne’s GAN-driven outfits didn’t wow all the critics in Paris. Vogue’s Luke Leitch wrote: “So the good news is that, on the evidence of this highly original Acne menswear collection, clothes design is not a human profession under threat from AI anytime soon.”We’re thinking: If we didn’t already know, we never would have guessed that these clothes were designed by a GAN. We’re not sure whether that’s a testament to the designers’ genius or our hopeless fashion sense.", "image_caption": "Male models during the Paris Fashion Week", "metadata": {"article_id": "issue_30", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Fashion2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-30/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_30.html"}}
{"id": 50645224006, "type": "news_chunk", "title": "Mind-Controlled Robot Hand, Fashions by GAN, Face Recognition", "subtitle": "Guest Speaker", "content": "Deepfake videos in which one person appears to speak another’s words have appeared in entertainment, advertising, and politics. New research ups the ante for an application that enables new forms of both creative expression and misinformation.What’s new: Linsen Song with researchers at China’s National Laboratory of Pattern Recognition, SenseTime Research, and Nanyang Technological University produced a model that makes a person on video appear to speak words from a separate audio recording with unprecedented realism. You can see the results in this video.Key insight: Most people’s mouths move similarly when pronouncing the same words. The model first predicts facial expressions from the audio recording. Then it maps those predictions onto the target speaker’s face.How it works: This approach works with any target video and source audio, synthesizes new motions, and maps them to a model of the target’s face frame by frame. The audio-to-expression network learns from talking-head videos to predict facial motions from spoken words.A portion of the network learns to remove personal quirks from the recorded voices, creating a sort of universal speaking voice. That way, individual vocal idiosyncrasies don’t bias the predicted mouth movements.Software associated with the FaceWarehouse database of facial expression models extracts features of the target speaker’s face, such as head pose and positions of lips, nose, and eyes. The model generates a 3D mesh combining predicted mouth movements from the source audio with the target face.In each target video frame, U-net architecture replaces the original mouth with a reconstruction based on the FaceWarehouse meshes. Results: To test the model’s effectiveness quantitatively, the researchers evaluated its ability to resynthesize mouth movements from their original audio tracks in a video dataset. The model reduced the error in expression (average distance between landmark features) to 0.65 from a baseline of 0.84. In a qualitative study, viewers judged generated videos to have been real 65.8 percent of the time — a high score considering that they identified real videos as real 77.2 percent of the time.Why it matters: Putting new words in a talking head’s mouth is getting easier. While previous approaches often impose prohibitive requirements for training, this method requires only a few minutes of video and audio data. Meanwhile, the results are becoming more realistic, lending urgency to the need for robust detection methods and clear rules governing their distribution.We’re thinking: Let’s get this out of the way: We never said it!", "image_caption": "Method overview of model that makes a person on video appear to speak words from a separate audio recording", "metadata": {"article_id": "issue_30", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Deepfakes.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-30/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_30.html"}}
{"id": 50645224007, "type": "news_chunk", "title": "Mind-Controlled Robot Hand, Fashions by GAN, Face Recognition", "subtitle": "Machine Learning Churning", "content": "Many of this year’s hottest AI companies are taking the spotlight from last year’s darlings.What’s new: CB Insights, which analyzes early-stage companies, published its annual list of the 100 “most promising” startups in AI.Highlights: Startups in the AI 100 have raised $7.4 billion collectively. Most are headquartered in the U.S., but others are based in 13 countries including Canada, China, and the UK. Around 80 percent of the list is new this year. Entire categories turned over, including not only AI strongholds like Cybersecurity and Transportation but also Food & Agriculture, Media & Entertainment, and Retail & Warehousing.Several of the survivors are in the Healthcare sector, including Atomwise, Butterfly, Owking, Paige.ai, and Viz.ai.Healthcare has the largest number of companies, 13 in all. Retail is second with nine, which is roughly double last year’s tally.The list includes 10 unicorns, or companies valued at more than $1 billion, down from 15 last year.Among the most richly funded are U.S. autonomous vehicle developer Aurora ($693 million), UK AI-chip designer Graphcore ($536 million), and Lemonade, an American insurance company that uses AI to find fraudulent claims ($480 million).For the first time, the list highlights AI startups making products and services that address a variety of industries. Such “cross-industry tech” includes model development, computer vision, natural language processing, business intelligence, cybersecurity, and sales. Methodology: CB Insights chooses the AI 100 based on a basket of metrics, some of them indirect or subjective, such as the “sentiment” of news coverage. It scores a company’s potential to succeed using a proprietary system based on funding, the overall health of its industry, and its “momentum.”Why it matters: AI is a hot industry, but not yet a stable one.We’re thinking: Don’t let the churn scare you. If you join a startup that doesn’t make it, as long as you keep learning, you’ll be in a better position to choose another that won’t repeat the same mistakes — or to start your own.", "image_caption": "Some results from CB Insights' annual list of the 100 most promising startups in AI", "metadata": {"article_id": "issue_30", "chunk_index": 7, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI10020ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-30/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_30.html"}}
{"id": 91010198001, "type": "news_chunk", "title": "Claude 4 Advances Code Gen, How DeepSeek Built V3 For $5.6m, Google I/O Roundup, and more...", "subtitle": "A MESSAGE FROM DEEPLEARNING.AI AND SNOWFLAKE", "content": "We’re featured partners of Snowflake’s Dev Day 2025, a full day for AI and data practitioners to explore cutting-edge demos, make valuable contacts, and hear from top voices in the field (including Andrew Ng). See you on June 5! Register here Anthropic continued its tradition of building AI models that raise the bar in coding tasks. What’s new: Anthropic launched Claude 4 Sonnet 4 and Claude Opus 4, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit. Input/output: Text, images, PDF files in (up to 200,000 tokens); text out (Claude Sonnet 4 up to 64,000 tokens, Claude Opus 4 up to 32,000 tokens)Features: Parallel tool use including computer use, selectable reasoning mode with visible reasoning tokens, multilingual (15 languages)Performance: Ranked Number One in LMSys WebDev Arena, state-of-the-art on SWE-bench and Terminal-benchAvailability/price: Anthropic API, Amazon Bedrock, Google Cloud Vertex AI. Claude Sonnet 4 $3/$15 per million input/output tokens, Claude Opus 4 $15/$75 per million input/output tokensUndisclosed: Parameter counts, specific training methods and datasets How it works: The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to be helpful, honest, and harmless according to human and AI feedback. The models make reasoning tokens visible within limits. For especially lengthy chains of thought, an unspecified smaller model summarizes reasoning tokens.Given local file access, Claude Opus 4 can create and manipulate files to store information. For instance, prompted to maintain a knowledge base while playing a Pokémon video game, the model produced a guide to the game that offered advice such as, “If stuck, try OPPOSITE approach” and “Change Y-coordinate when horizontal movement fails.” Results: Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests. On SWE-bench Verified, which tests the model’s ability to solve software issues from GitHub, Claude Opus 4 succeeded 72.5 percent of the time, and Claude Sonnet 4 succeeded 72.7 percent of the time. The next best model, OpenAI o3, succeeded 70.3 percent of the time.Terminal-bench evaluates how well models work with the benchmark’s built-in agentic framework to perform tasks on a computer terminal. Claude Opus 4 succeeded 39.2 percent of the time and Claude Sonnet 4 succeeded 33.5 percent of the time, whereas the closest competitor, OpenAI GPT 4.1, succeeded 30.3 percent of the time. Using Claude Code as the agentic framework, Claude Opus 4 succeeded 43.2 percent of the time and Claude Sonnet 4 succeeded 35.5 percent of the time. Why it matters: The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including a Tetris clone built in one shot and a seven-hour stint refactoring Rakutan’s open-source code base. We’re thinking: Prompting expert @elder_plinius published a text file that is purported to be Claude 4’s system prompt and includes some material that does not appear in Anthropic’s own publication of the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning.", "image_caption": "AI model performance comparison chart: Claude Opus 4, Sonnet 4, Sonnet 3.7, OpenAI o3, GPT-4.1, and Gemini 2.5 Pro.", "metadata": {"article_id": "issue_303", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-303/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_303.html"}}
{"id": 50026824001, "type": "news_chunk", "title": "AI Vs Coronavirus, Workers Prepare for Job Losses, Quantum...", "subtitle": "News", "content": "Dear friends, The unfolding Covid-19 crisis calls for individuals and organizations to step up and contribute to the common good. I believe that the tech community has an important role to play in slowing the progress of the virus and shortening the time it takes society to recover. Tech businesses can offer free or reduced-cost services, as well as extra support, to healthcare providers. I’m seeing a lot of unfulfilled needs in healthcare systems that communication and visualization tools might address. I’m providing IT support to doctor friends. Many of us can help with this.Individuals and organizations alike can combat fake news by calling out inaccurate and ill-informed perspectives and passing along accurate, timely information. Keeping digital channels free of misinformation and open for rapid dissemination of important news is critical.It’s especially important to encourage the free flow of information among researchers, healthcare systems, and epidemiologists, including data that can feed analytics or AI systems.Help others wherever you can, especially people in greater need. In my neighborhood, I’ve been gratified to see people volunteering on a local messaging app (Nextdoor) to shop for groceries or help out the elderly. We all need to pull together and lend a hand wherever we can.And of course, I hope you will take care of yourselves and your family. Machine learning thrives on data, but information about the novel coronavirus and the illness it produces has been either thin or hard to access. Now researchers are pooling resources to share everything we do know.What’s new: The White House and researchers from top U.S. AI and health institutions launched CORD-19, a free, machine-readable dataset of nearly 30,000 scholarly articles on the coronavirus. Kaggle is hosting a competition for text- and data-mining tools that sift this mountain of information for valuable insights. Promising directions: Lack of data so far has limited AI’s usefulness in combating this outbreak, but stronger data-collection efforts could prove decisive in the next, according to MIT Technology Review. Author Will Douglas Heaven describes three areas to focus on: Prediction: Health surveillance companies spotted Covid-19 in late December by parsing news reports, social media, and official statements, but predicting how the epidemic will spread is harder. AI companies could do more if they were allowed access to patient records, but that would require working through thorny privacy issues. The U.S. recently finalized new rules for giving patients more control over their health data. What’s missing is an option for patients to share their data securely with researchers.Diagnosis: A number of tools analyze scans of patients’ lungs to detect coronavirus infections. These technologies can’t see the virus itself, however, only the damage it has caused — and by the time such damage is visible, the illness may have progressed too far to be treated easily. Small data techniques might do better, pending further research.Treatment: AI could accelerate discovery of new drugs and vaccines, though that will take time. DeepMind used its AlphaFold model to predict protein structures associated with the virus. If they’re verified, the information could aid efforts to develop treatments. Generative algorithms can model millions of molecules and sift through them to find potentially useful ones. More data on the disease’s evolution could accelerate that effort. Behind the news: AI spotted the disease early, but humans still beat it to the punch. At least one Chinese doctor posted his concerns about what came to be known as Covid-19 on a WeChat group before AI health monitors issued their alerts. He later died of the virus.Why it matters: AI has great potential to combat epidemics, and hopeful news reports bring attention to and support for the field. The community must work diligently while taking care not to encourage wildly inflated expectations and false hopes.We’re thinking: Covid-19 isn’t the first pandemic, and sadly it won’t be the last. The AI community’s efforts to fight this virus will prove critical when the next one emerges. And there’s plenty we can do outside the medical sphere: Machine learning can help manage critical resources, coordinate responses, and optimize logistics. At this moment of international crisis, we face a common foe that is bigger than any of us, and we’re gratified to see so many AI developers eager to pitch in.", "image_caption": "Animated symbol of Covid-19 virus structure", "metadata": {"article_id": "issue_31", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Covid.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-31/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_31.html"}}
{"id": 47765000001, "type": "news_chunk", "title": "Tracking China's Covid-19 Revival, A Robot Star is Born...", "subtitle": "Satellite Data Hints at China Upswing", "content": "Dear friends, When I was younger, I was not a fan of working from home. Too many distractions! So I worked a lot in coffee shops. They turned out to be convenient places to talk to strangers and ask for feedback about products I was working on, including early MOOC prototypes. Now much of the world is undergoing a remote work experiment. My teams and I are working from home. There have been positives and negatives. I love running into colleagues in our #virtualcoffeechat slack channel, especially people I don’t see so often around the office. I love reducing my carbon footprint and not having to commute, and I love getting to see Nova during my lunch break. (She’s learning to walk, and her unstable toddling is simultaneously cute and terrifying.) On the flip side, I miss seeing everyone in 3D. I miss the serendipitous discussions, and I miss being able to gather in the break room to chat and partake in the babka, gulab jamun, chicharron, and durian candy that teammates sometimes bring to share. Even though Covid-19 is a painful challenge, there is a silver lining in this shift in how we work. People in the tech industry are fortunate that a lot of work can be done remotely, and many companies are now learning how to do this well. Once this pandemic is over, I believe that many remote roles will open up. It will be easier for an aspiring AI engineer who lives in Dallas to get a job in Silicon Valley — without having to move. A recruiter who lives in Buenos Aires will have a better chance of being hired by a company in Montreal. A front-end engineer in Sydney might work for an employer in Tokyo. No matter where you live, more jobs will be coming to you in the future. Stay safe and keep learning! Neural networks revealed both how hard Covid-19 has hit the Chinese economy, and hopeful signs that a renaissance may be underway.What’s new: Researchers at WeBank, a Chinese financial institution, analyzed satellite imagery, GPS signals, and social media to get a multifaceted view of the pandemic’s impact.What they found: The team compared data collected before, during, and after the peak of China’s containment efforts. It focused on three data sources: Satellite images: The researchers adapted SolarNet, an image recognition model that maps solar panels based on infrared satellite photos, to look for heat signatures from steel mills. Then they correlated the results with steel output. In late January, at the height of the outbreak, China’s steel production had dropped to 30 percent of capacity, they found. By February 9, it had recovered to 76 percent.GPS: Using a different model, the researchers analyzed anonymized GPS signals collected in 2019 from millions of phones to determine whether they belonged to commuters. They used the results to estimate economic activity. Then they analyzed signals surrounding the quarantines to assess the impact. Comparing the datasets, they found that the country’s economic activity had fallen to 20 percent of normal by February 9. A month later, however, it had rebounded to 72 percent. Extrapolating the findings, they predict that China’s economy will recover fully by March 26.Social media: The researchers used natural language processing to scan social media posts to determine whether people were working from homes or offices. According to that analysis, the number of telecommuters ballooned by more than sixfold between January 1 and mid-March. Behind the news: China dramatically cut its coronavirus transmission rate by imposing strict measures to limit social interaction including a quarantine of 50 million people in the province that includes Wuhan, the disease’s epicenter.Why it matters: Before Covid-19 rocked China’s economy, J.P. Morgan had estimated that the country’s GDP would grow nearly 6 percent by the end of 2020. Now the U.S. investment bank predicts a meager 1 percent growth. China’s isn’t the only economy in trouble: The bank’s analysts warn of a global recession.We’re thinking: China’s economic revival would be great news for the rest of the world. But full recovery isn’t likely until its Western trading partners come back up to speed.", "image_caption": "Heat treatment on metal", "metadata": {"article_id": "issue_32", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/China20Econ202.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-32/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_32.html"}}
{"id": 47765000002, "type": "news_chunk", "title": "Tracking China's Covid-19 Revival, A Robot Star is Born...", "subtitle": "Silent Snacking", "content": "As working from home becomes the new normal, AI may protect you from the sound of coworkers munching while they chat.What’s new: No more smacking lips and rustling chip bags! Microsoft’s online collaboration platform Teams announced a feature that removes extraneous sounds from videoconferences. How it works: The Teams team trained neural networks to recognize and filter out non-speech noises using datasets they built for the 2020 Deep Noise Suppression Challenge. The researchers curated 500 hours worth of 30 second clips from a repository of public-domain audiobooks.They combined half of this set with 60,000 annotated clips from YouTube videos. Those files represented 150 different classes of noise including hours of crinkling and chewing.A recurrent neural net learned the difference between voices overlaid with noise and their clean counterparts.Microsoft expects to make the feature available later this year. Behind the news: People across the globe are hunkering down for a long virus season. Zoom added more than 2 million monthly active users in January and February, more than in all of 2019. Microsoft Teams’ daily user count shot up from 13 million to 44 million between July 2019 and March 2020. Slack, the other big telecommuting program, hasn’t published monthly average user numbers since October, when the tally was 12 million.Why it matters: Nobody wants to listen to your mukbang during working hours.We’re thinking: Next, can we get a feature that filters out intrusive toddlers?", "image_caption": "Conference on Microsoft Teams with a person eating a chip bag", "metadata": {"article_id": "issue_32", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Snack1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-32/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_32.html"}}
{"id": 47765000003, "type": "news_chunk", "title": "Tracking China's Covid-19 Revival, A Robot Star is Born...", "subtitle": "Deep Learning Finds New Antibiotic", "content": "Chemists typically develop new antibiotics by testing close chemical relatives of tried-and-true compounds like penicillin. That approach becomes less effective, though, as dangerous bacteria evolve resistance to those very chemical structures. Instead, researchers enlisted neural networks.What’s new: Jonathan Stokes and colleagues at MIT, Harvard, and McMaster University built an ensemble model that predicts molecules that are structurally unrelated to known antibiotics, harmless to humans, and deadly to E. coli, a common bacterium that served as a proxy microorganism. The model spotted a previously unrecognized antibiotic that proved effective at killing a variety of germs.Key insight: Neural networks can stand in for petri dishes to zero in on promising molecules. An initial simulation reduced an enormous number of molecules to a few thousand solid possibilities, of which the model selected a couple dozen for testing in a wet lab.How it works: The researchers used an ensemble of 20 graph neural networks (GNNs) to evaluate molecules’ ability to inhibit E. coli, and another ensemble of five GNNs to evaluate toxicity. They used a standard measure to evaluate chemical structure. Then they tested the most promising compounds on mice. Each GNN examines molecules atom by atom. The Chemprop architecture learns a vector for each atom based on its atomic number, mass, other properties along with vectors of the atoms it’s bound to.The GNN graphs collapse into vectors that describe the molecule as a whole.Fully connected layers predict either E. coli inhibition based on labels from an FDA library or toxicity based on a dataset of qualitative evaluations of approved drugs. Results: The researchers examined more than 107 million compounds to produce a ranked list. Empirical tests on the top-ranked 3,260 chemicals yielded 51 that were effective. Of those, 23 had low predicted toxicity and structures distinct from known antibiotics. In mouse experiments, Halicin, a known diabetes treatment, proved effective as a broad-spectrum antibiotic.Why it matters: Alexander Fleming’s discovery of penicillin in 1928 revolutionized medicine. Now that transformation is at risk as bugs evolve resistance to that drug and its successors. Discovery of new antibiotics has been hampered by lack of a way to narrow the list of possibilities for lab tests. This method offers a way to vet candidates quickly and efficiently. We’re thinking: Antibiotic-resistant bugs are responsible for 2.8 million infections and 35,000 deaths annually in the U.S. alone. Crank up those GNNs!", "image_caption": "Data related to model that predicts molecules that are structurally unrelated to known antibiotics", "metadata": {"article_id": "issue_32", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Antibiotic20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-32/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_32.html"}}
{"id": 47765000004, "type": "news_chunk", "title": "Tracking China's Covid-19 Revival, A Robot Star is Born...", "subtitle": "Algorithm and Blues", "content": "Bored with your Spotify playlists? Let this robot singer/songwriter take you on a trip “Into Your Mind.”What’s Goin’ On: A music-composing, marimba-playing robot named Shimon has learned to write and sing its own lyrics, IEEE Spectrum reports. Shimon performs its debut single with a flesh-and-blood backup band in this video. An album is scheduled to drop on Spotify on April 10. (A)I Write the Songs: Two of the robot’s creators, Georgia Tech professor Gil Weinberg and grad student Richard Savery, treat the machine as though it were a human collaborator. Shimon’s language model was trained on 50,000 lyrics from jazz, rock, and hip hop songs. Given a keyword, it generates thousands of phrases that are related to the word itself, its synonyms, or its antonyms. It picks a song’s worth of phrases, emphasizing rhymes and thematic similarity.After it has completed the lyrics, the humans weave melodies it generates with their own ideas to produce music that suits the words.Shimon’s voice is based on a model developed by the University of Pomeu Fabra and trained on human voices in a wide variety of pitches.Weinberg aims to combine Shimon’s musical improvisations with its lyrical skills to transform the machine into a freestyle rapper. Robot Rock: Shimon’s music-video debut showcases several other innovations created by Weinberg and his colleagues, who have been working on this project for nearly a decade. The robot’s vaguely serpentine head automatically moves to the rhythm while its mouth and eyebrows emote along with the lyrics.The system originally was designed to improvise on marimba in response to notes played by human musicians. New hardware enables its arms to play more precisely in time and with greater control over loud and soft accents.Shimon’s human bandmate Jason Barnes lost one hand in a 2012 accident. He performs on drums using a robotic prosthesis that holds two sticks. Electrical activity from Barnes’ muscle controls one stick, while the second stick moves autonomously in response to the music. Unchained Melody: Machine learning is challenging timeless assumptions about human creativity. Shimon’s output is reminiscent of progressive rock masters. Could it conquer the pop charts by working with the producers of Taylor Swift, Beyonce, or Drake?Wish You Were Here: The Covid-19 pandemic forced Weinberg to postpone plans to take Shimon on tour. Will his next project be building robot fans?", "image_caption": "Excerpts from promotional video for music-composing robot named Shimon", "metadata": {"article_id": "issue_32", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Shimon2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-32/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_32.html"}}
{"id": 47765000005, "type": "news_chunk", "title": "Tracking China's Covid-19 Revival, A Robot Star is Born...", "subtitle": "Rightsizing Neural Nets", "content": "How much data do we want? More! How large should the model be? Bigger! How much more and how much bigger? New research estimates the impact of dataset and model sizes on neural network performance.What’s new: Jonathan Rosenfeld and colleagues at MIT, York University, Harvard, Neural Magic, and Tel Aviv University introduced an equation — a so-called error landscape — that predicts how much data and how large a model it takes to generalize well.Key insight: The researchers made some assumptions; for instance, models without training should be as accurate as a random guess. They combined these assumptions with experimental observations to create an equation that works for a variety of architectures, model sizes, data types, and dataset sizes.How it works: The researchers trained various state-of-the-art vision and language models on a number of benchmark datasets. Considering 30 combinations of architecture and dataset, they observed three effects when varying data and model size: For a fixed amount of data, increasing model size initially boosted performance on novel data, though effect leveled off. The researchers observed a similar trend as they increased the amount of training data. The effect of boosting both model and dataset size was approximately the same as the combined impact of changing each one individually.An equation captures these observations. It describes the error as a function of model and data size, forming a 3D surface or error landscape.The equation contains variables dependent on the task. Natural language processing, for instance, often requires more data than image processing. A simple regression can determine their values for a target dataset. Results: After fitting dataset-specific variables to the validation dataset, the researchers compared the predicted model error against the true error on the test set. The predictions were within 1 percent of the true error, on average.Why it matters: It turns out that the impact on accuracy of model and dataset size is predictable. How nice to have an alternative to trial and error!Yes, but: When varying network sizes, the researchers focused mainly on scaling width while holding the rest of the architecture constant. Neural network “size” can’t be captured in a single number, and we look forward to future work that considers this nuance.We’re thinking: Learning theory offers some predictions about how algorithmic performance should scale, but we’re glad to see empirically derived rules of thumb.", "image_caption": "Graphs related to ImageNet error landscape", "metadata": {"article_id": "issue_32", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Error20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-32/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_32.html"}}
{"id": 47765000006, "type": "news_chunk", "title": "Tracking China's Covid-19 Revival, A Robot Star is Born...", "subtitle": "AI’s Gender Imbalance", "content": "Women continue to be severely underrepresented in AI.What’s new: A meta-analysis of research conducted by Synced Review for Women’s History Month found that female participation in various aspects of AI typically hovers between 10 and 20 percent.What they found: Much of the research included in the analysis was based on numbers generated by rules-based software that categorizes names according to gender. Synced Review, which is based in China, said it didn’t examine studies of Chinese companies or institutions because Chinese names don’t correlate as tightly with gender as names in other languages. The 2019 AI Index produced by Stanford University’s Human-Centered AI Institute reported that females made up 20 percent of faculty members in academic AI departments. That number isn’t likely to rise soon; 20 percent of new faculty hires and 20 percent of AI-related PhD recipients are female.A 2018 study by Wired and Element AI, an enterprise software company, tallied men and women who contributed to the major AI conferences NeurIPS, ICLR, and ICML. Twelve percent were women. In a 2019 review of 21 conferences by Element AI, the percentage rose to 18 percent.A 2018 Wired analysis of AI researchers at Google and Facebook estimated that 10 percent of Google’s machine learning workforce and 15 percent of Facebook’s AI researchers were women. (Both companies later said the report had understated the true number, but they didn’t provide further information.)Researchers at Nesta, a UK research foundation, analyzed AI research papers on Arxiv. Women accounted for 12 percent of authors in 2015, less than in 2009. Behind the news: Women have a prominent place in AI’s history, going all the way back to Ida Rhodes, who in the 1960s laid the groundwork for natural language processing. The percentage of American women with computer science degrees, however, peaked in the mid-1980s at around 35 percent, and since has declined to under 20 percent.Why it matters: It’s important that people building the future represent diverse groups to make sure that anyone can participate and that the products we build encompass a variety of perspectives.We’re thinking: Each one of us can help promote diversity. Leaders can make an effort to interview, hire, and mentor underrepresented groups, and everyone can help make the workplace inclusive.", "image_caption": "Women in AI in academia and industry chart", "metadata": {"article_id": "issue_32", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Gender20ASPECT201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-32/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_32.html"}}
{"id": 27808096001, "type": "news_chunk", "title": "AI-Against-Coronavirus Datasets, Voice Cloning for the Masses", "subtitle": "Chatbots Disagree on Covid-19", "content": "Chatbots designed to recognize Covid-19 symptoms dispense alarmingly inconsistent recommendations.What’s new: Given the same symptoms, eight high-profile medical bots responded with divergent, often conflicting advice, according to STAT News. Conflicting information: Reporters Casey Ross and Erin Brodwin discussed Covid-19 symptoms such as coughing, fever, and shortness of breath with conversational systems offered by government agencies, hospitals, and tech companies. The CDC’s Coronavirus Self-Checker told the reporters that they had at least one Covid-19 symptom and recommended they isolate themselves and contact a healthcare provider within 24 hours. A tool from Providence St. Joseph Health told the reporters they might have Covid-19 and suggested they call a physician or 911.Buoy Health, a web-based medical service, suggested the symptoms might be a common cold and didn’t recommend special precautions. Google sister company Verily determined that the reporters’ complaints did not warrant further testing.In a similar test of popular smart-speakers platforms, Recode found that voice assistants from Amazon, Apple, and Google often answered questions about Covid-19 with information that was overly general, outdated, or lacking context. Behind the news: A study from Stanford University suggests that symptom checkers built for Covid-19 are flawed partly because the disease’s early signs are similar to those of the common cold or garden-variety influenza. A 2015 study that found that online symptom checkers for a range of conditions often reach faulty conclusions.Yes, but: Screening tools don’t need to be perfect to add a lot of value. They are statistical tools intended to prioritize quickly and inexpensively which cases should be escalated for deeper examination. Why it matters: The significant disagreement among these tools means there’s a lot of room for improvement, and bad advice is clearly dangerous in a situation like this. Still, AI-based screening could play a helpful role in this pandemic, especially considering how many countries are short on test kits. It could ease the burden on hospital staff and testing centers, which risk becoming overwhelmed as the pandemic spreads. We’re thinking: Human doctors’ recommendations aren’t always consistent.", "image_caption": "Chatbot asking for Covid-19 symptoms", "metadata": {"article_id": "issue_33", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chatbot.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-33/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_33.html"}}
{"id": 27808096002, "type": "news_chunk", "title": "AI-Against-Coronavirus Datasets, Voice Cloning for the Masses", "subtitle": "Optimize Your Training Parameters", "content": "Last week we reported on a formula to determine model width and dataset size for optimal performance. A new paper contributes equations that optimize some training parameters.What’s new: Jared Kaplan and Sam McCandlish led researchers at Johns Hopkins and OpenAI to derive equations that describe the effects of parameter count, training corpus size, batch size, and training time on language model performance, plus their own ways to find the best model and dataset sizes.Key insight: The researchers devised a set of equations that approximate the effects of different combinations of two variables. It’s easier to reason about 2D graphs than to visualize an n-dimensional surface. Findings: Three findings stand out. First, as many researchers have suspected, transformers outperform LSTMs when trained to convergence. Second, where data and compute are limited, it’s more efficient to train a large model in fewer training steps than to train a smaller model to convergence. Third, some researchers have conjectured that exceeding a so-called critical batch size degrades performance. The researchers offer a way to find optimal batch sizes. How it works: The researchers trained many model shapes and sizes on various subsets of a proprietary dataset of Reddit posts and linked articles. They measured performance of every combination during training to track the impact of design choices on performance. They derived a slightly different formula for loss as a function of model and data size than recent MIT research, but they found a similar relationship: Increasing either parameter count or dataset size improves performance to a point, but then the gains level off. They established a similar relationship between model size and number of training steps.The equation that evaluates loss for a given parameter count and numbers of training steps revealed a lower boundary on the number of training steps necessary for early stopping to prevent overfitting. As you might expect, the number of training steps before overfitting rises with dataset size.Optimal batch size depends on the model’s loss, they found, not parameter count or dataset size. Optimal batch size rises as the loss decreases. Why it matters: Most machine learning practitioners don’t have the seemingly infinite computational resources that some large companies do. These insights should help them use resources more effectively.We’re thinking: Natural language processing is notoriously compute-hungry. The ability to balance processing power against performance could not only save money but reduce environmental impacts.", "image_caption": "Data and graphs related to equations that optimize some training parameters.", "metadata": {"article_id": "issue_33", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Scaling20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-33/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_33.html"}}
{"id": 27808096003, "type": "news_chunk", "title": "AI-Against-Coronavirus Datasets, Voice Cloning for the Masses", "subtitle": "Where Are the Live Bombs?", "content": "Unexploded munitions from past wars continue to kill and maim thousands of people every year. Computer vision is helping researchers figure out where these dormant weapons are likely to be.What’s new: Data scientists at Ohio State University combined computer vision with military records to identify areas in Cambodia where bombs dropped by U.S. planes during its war on neighboring Vietnam remain unexploded.How it works: The U.S. Air Force kept records of how many bombs it dropped in each air raid, but no one knows how many failed to detonate. The researchers built a tool that counts craters — evidence of bombs that did explode — and then subtracted that number from the total number dropped. The difference enabled them to estimate how many bombs still litter the countryside. The researchers hand-labeled 49 craters in satellite imagery, along with 108 other circular objects (such as trees and rice silos) within a 1.5 square kilometer training area. By flipping and rotating the crater images, they increased the training dataset to 1,256 total labeled images.The craters can be hard to spot, blurred by decades of erosion and overgrowth and often filled with water. So the researchers developed a two-step model that first identifies potential craters and then culls false positives.Step one picked out everything that was circular enough to look vaguely like a crater. It identified 1,229 candidates in the validation area of 9.2 square kilometers (the image with more blue dots in the animation above).In step two, the model compared the candidates’ color, shape, size, and texture with craters in the labeled data. Using a decision tree, it correctly identified 152 of 177 verified craters in the area (the image with fewer blue dots). The results: The researchers used their model to sweep a 100 square kilometer area near the Vietnamese border that had been slammed with 3,205 bombs during the war. Using multiple runs of their model, the researchers found between 1,405 to 1,618 craters, which suggests that up to half of all the bombs dropped in this area are still waiting to be found.Behind the news: In 1969, U.S. President Richard Nixon secretly ordered the Air Force to begin bombing Cambodia to disrupt Viet Cong supply lines. The campaign left between 50,000 and 150,000 dead. Since then, unexploded bombs and landmines have killed or maimed at least 60,000 people in Cambodia.Why it matters: Bombs from past wars pose a present danger to people all over the world. But finding them is expensive and labor-intensive. Models that map high concentrations of unexploded ordnance could help organizations working on the problem direct their resources more efficiently.We’re thinking: It’s heartening to see the technology of the future applied to problems created in the past.", "image_caption": "Map seen with computer vision", "metadata": {"article_id": "issue_33", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Bombs2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-33/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_33.html"}}
{"id": 27808096004, "type": "news_chunk", "title": "AI-Against-Coronavirus Datasets, Voice Cloning for the Masses", "subtitle": "Voice Cloning for the Masses", "content": "Are you secretly yearning to have a My Little Pony character voice your next online presentation? A new web app can make your dreams come true.What’s new: 15.ai translates short text messages into the voices of popular cartoon and video game characters.How it works: The model’s anonymous developer began the project in 2018 while an undergrad at MIT. In an email to The Batch, the coder declined to disclose details about how the model works but said it was inspired by the 2019 paper that pioneered transfer learning for text-to-speech models. 15.ai’s author didn’t disclose how its model differs from the implementation in the paper but said they stumbled on a technique that makes it possible to learn new voices with less than 15 minutes of training data.The current roster of voices includes My Little Pony’s Princess Celestia and Team Fortress 2’s Soldier. Characters from the video game Fallout: New Vegas and animated series Steven Universe and Rick and Morty are in the works. Behind the news: DeepMind made a significant advance in neural audio synthesis in 2016 with WaveNet. That work demonstrated that a neural net trained on multiple examples of similar speech or music could create passable facsimiles. Other teams continue to make advances, for example, in real-time systems. Why it matters: Voice cloning could be enormously productive. In Hollywood, it could revolutionize the use of virtual actors. In cartoons and audiobooks, it could enable voice actors to participate in many more productions. In online education, kids might pay more attention to lessons delivered by the voices of favorite personalities. And how many YouTube how-to video producers would love to have a synthetic Morgan Freeman narrate their scripts?Yes, but: Synthesizing a human actor’s voice without consent is arguably unethical and possibly illegal. And this technology will be catnip for deepfakers, who could scrape recordings from social networks to impersonate private individuals.We’re thinking: Anyone who wants to synthesize Andrew’s voice has more than 15 minutes of deeplearning.ai courseware to train on.", "image_caption": "15.ai screen capture with a character from My Little Pony", "metadata": {"article_id": "issue_33", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Voices20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-33/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_33.html"}}
{"id": 27808096005, "type": "news_chunk", "title": "AI-Against-Coronavirus Datasets, Voice Cloning for the Masses", "subtitle": "Seeing the See-Through", "content": "Glass bottles and crystal bowls bend light in strange ways. Image processing networks often struggle to separate the boundaries of transparent objects from the background that shows through them. A new method sees such items more accurately.What’s new: Shreeyak Sajjan and researchers at Synthesis.ai, Google, and Columbia University premiered a state-of-the-art model for identifying transparent objects. They call it ClearGrasp, a reference to its intended use in robotics.Key insight: Faced with a transparent object, RGB-D cameras, which sense color and depth per pixel, can get confused: They take some depth measurements off the object’s surface, others straight through the object. ClearGrasp recognizes such noisy measurements and uses them to predict an object’s shape. Once it knows the object’s shape and how far away one point is, it can infer how far away every point is.How it works: ClearGrasp incorporates a trio of Deeplabv3+ models with the DRN-D-54 architecture. ClearGrasp’s training dataset included 18,000 simulated and 22,000 real images. To make the real images, the researchers photographed transparent objects, yielding depth measurements that encoded distorted light passing through them. Then they painted the objects and photographed them again to obtain accurate depth measurements.The first Deeplabv3+ model removes depth measurements associated with transparent objects, retaining data on opaque objects, which presumably is accurate. The second extracts approximate object boundaries. The third generates improved depth measurements.ClearGrasp combines the three outputs to get accurate depth measurements of both foreground and background. Results: Fed real-life data captured by the researchers, ClearGrasp improved the previous state of the art’s root mean squared error of corrected depth measurements from 0.054 to 0.038. A robotic arm using ClearGrasp picked up transparent objects 72 percent of the time, a big step up from 12 percent using unprocessed images.Why it matters: Machine learning has proven to be adept at noise reduction in various domains. ClearGrasp takes special care to modify only the depth measurements that are distorted.We’re thinking: ClearGrasp could prevent your robot assistant from having to clean up broken glass all day.", "image_caption": "Robotic hand identifying transparent objects", "metadata": {"article_id": "issue_33", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ClearGrasp.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-33/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_33.html"}}
{"id": 27808096006, "type": "news_chunk", "title": "AI-Against-Coronavirus Datasets, Voice Cloning for the Masses", "subtitle": "Selling Shovels to Data Miners", "content": "When the world is panning for machine learning gold, it pays to help them dig through the data.What’s new: Machine learning entrepreneurs can make their mark (and their fortune) building services that help other companies develop, deploy, and monitor AI, venture capitalist Rob Toews argues in Forbes.How it works: Toews points to Scale.AI, a startup that labels data, as one of a new generation of companies capitalizing on the AI industry’s demand for ancillary services. In August, the four-year-old company raised $100 million at a valuation of more than $1 billion. And labeling isn’t the only area of machine learning ripe for entrepreneurship. Synthetic data: Applied Intuition, Parallel Domain, and Cognate specialize in making synthetic data for autonomous driving and other applications where real-world training data is often scarce.Optimization: Gradio and Alectio help AI developers curate data to improve training efficiency. SigOpt offers a platform that guides companies through model specification from choosing an architecture to determining the number of training epochs.End-to-end management: Amazon’s SageMaker offers tools that help manage custom models throughout their lifecycle. Microsoft Azure Machine Learning Studio is geared toward data analysis. Google recently released Cloud AI Platform to get in on the action. Behind the news: Companies like Adobe and Capital One are spending hundreds of millions on cloud computing. This is driving demand for services that help them handle their cloud resources more efficiently. Among the beneficiaries are companies like Alation, Collibra, and Starburst Data that help catalog, query, and manage machine learning data, writes investor Matt Turck.Why it matters: Toews believes there are billions of dollars to be made by companies that provide machine learning services. Such services will also nurture new AI applications and accelerate their adoption across a variety of industries.We’re thinking: These companies aren’t only promising businesses. By taking on tasks like data procurement, model optimization, and lifecycle management, they could free engineers to focus on building products that fulfill deep learning’s potential.", "image_caption": "Good and bad examples of labeling images with pictures of birds", "metadata": {"article_id": "issue_33", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Labels20ASPECT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-33/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_33.html"}}
{"id": 49680725001, "type": "news_chunk", "title": "Antiviral Resources, Robot Superstars, AI for Scientists, 3D Data", "subtitle": "News", "content": "AI experts convened to discuss how to combat the coronavirus crisis.What’s new: An online conference hosted by Stanford University’s Institute for Human-Centered AI explored how machine learning is being deployed to address this pandemic — and prepare for the next one. You can watch the video here.The agenda: Nearly two dozen experts in fields as diverse as machine learning and public health delivered updates on topics from containing the pandemic to finding treatments. A few notable presentations from the six-hour conference: Epidemiology: Given the many people who fall ill but don’t enter a hospital, not to mention those who never show symptoms, it can be maddeningly difficult to track how the disease spreads and how many it kills. Lucy Li of the Chan Zuckerberg BioHub used a branching model to estimate how many people have been infected. Each time a virus infects a host, it mutates slightly. Analyzing viral DNA extracted from each known patient, the model uses the rate of mutation to interpolate how many other people the virus passed through along the way. According to Li’s estimates, 87 percent of all Chinese cases and 95 percent of total cases have gone undetected.Social Distancing: Stanford pediatrician C. Jason Wang described how Taiwan’s Central Epidemic Command Center tracked individual Covid-19 cases and enforced social distancing rules. Activated in response to the epidemic, the data analytics hub uses GPS, health insurance records, and immigration data to track infections and alert individuals who may have been exposed.Treatment: Creating new drugs from scratch takes a lot of time. So Stefano Rensi and colleagues in Stanford’s Department of Bioengineering searched for existing compounds to fight Covid-19. They used natural language processing to sift the medical literature for clues about how the novel coronavirus delivers its payload to a cell’s nucleus. Then they used a model that predicts protein structure to look for proteins that might inhibit this process. They found 15 known drug candidates that contain this protein, and they’re conducting their own trials to gauge their effectiveness. Behind the news: Infectious disease expert Dr. Michele Barry explained that machine learning was critical to keeping infection rates low in Singapore, South Korea, and Taiwan. All three countries deployed the technology to encourage social distancing, move medical supplies where they were needed most, and keep the public informed.Why it matters: Machine learning engineers and disease specialists have a lot to learn from one another. Conferences like this can bring them together and build lasting alliances that may result in tools for fighting future outbreaks.We’re thinking: If you’re itching to join the fight against Covid-19, you can find a list of tools, datasets, and information just above the news in this issue of The Batch. Also: Stay at home and wash your hands!", "image_caption": "Screen capture of online conference called Covid-19 and AI", "metadata": {"article_id": "issue_34", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Conf20SLOW.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-34/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_34.html"}}
{"id": 22896698001, "type": "news_chunk", "title": "AI For Medicine Special! Eric Topol’s Planetary Health System", "subtitle": "Smarter Care, Healthier Lives", "content": "Dear friends, This week’s issue of The Batch is all about medical applications of AI. Amid the current pandemic, the marriage of AI and medicine is more urgent than ever. My father is a practicing doctor, and I grew up seeing firsthand how the right care can save lives and reunite families. I’ve been privileged to participate in projects that applied deep learning to diagnosing chest X-rays, assisting with mental health, and interpreting electrocardiograms. Despite significant research progress, there’s still a long way to go. Jumping into AI for medicine now is like jumping into AI for computer vision back in 2012. For those who are ready to make the leap, deeplearning.ai is proud to introduce the AI for Medicine Specialization. This new series of courses will teach you the machine learning techniques you need to build a wide range of medical applications. If you’re new to deep learning, start with the Deep Learning Specialization. But if you’ve completed the DLS, or if you have a working knowledge of deep learning and convolutional networks as well as intermediate Python skills, the AI For Medicine Specialization will unlock many opportunities to help solve important problems. The world needs more AI people working on medicine. I hope you’ll consider being one of them. Keep learning! We stand at the threshold of a new era in medicine. We can collect detailed data about individuals continuously throughout their lives. With deep learning, we can correlate background, actions, and outcomes to find paths to optimal health. Some day we may do this globally, so everyone on Earth receives health care appropriately tailored to their unique biology and circumstances. In this special issue of The Batch, we look at how AI is having an impact in medical diagnosis, prognosis, treatment, and data extraction. We hope you’ll join the medical AI revolution and help create a healthier world.", "image_caption": "Neural network with the Caduceus sign", "metadata": {"article_id": "issue_35", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/5-Andrews-letter_3201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-35/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_35.html"}}
{"id": 22896698002, "type": "news_chunk", "title": "AI For Medicine Special! Eric Topol’s Planetary Health System", "subtitle": "A Visionary Doctor Prescribes AI", "content": "Eric Topol is one of the world’s leading advocates for AI in medicine. He believes the technology can not only liberate physicians from the growing burden of clerical work, but also synthesize layers of patient data — behavioral, genomic, microbiomic, and so on — into truly personalized healthcare. A cardiologist and geneticist at Scripps Research Institute in Southern California, he is the author of Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again. Below he shares his insights into the fusion of AI and medicine and advice for machine learning engineers who want to get involved.The Batch: Let’s start with the topic on everyone’s mind: Where do you see AI’s greatest potential in addressing the Covid-19 pandemic?Topol: One thing that’s been overlooked is the ability to develop and validate algorithms for at-home monitoring. We don’t want everyone who has Covid-19 symptoms to go to the hospital. On the other hand, some people who catch Covid-19 have sudden demise, and it’s hard to predict. If we could tell who’s safe to monitor at home, that would be great help in managing this epidemic around the world.The Batch: You’re concerned with the depersonalization of doctor-patient relationships. How can AI help?Topol: Four words: the gift of time. Clinicians spend too much of their time being data clerks. There shouldn’t be any need for a screen and a keyboard to see a patient. Entering notes into the medical record should be done by AI.The Batch: Researchers have had experimental success interpreting medical images. Yet these innovations haven’t had much impact on clinical practice. What’s the holdup?Topol: The medical community feels threatened that the machines will encroach on their lives. Also, some companies working on things like this have proprietary algorithms and don’t publish their data, so there’s a lack of transparency. They get their FDA clearance based on retrospective studies and use the same data over and over, because there aren’t many large, annotated medical datasets. We need prospective studies based on real-world patients in multiple real-world clinical settings. And we need more randomized trials — there have been only six or seven of those.The Batch: If you could collect any data you wanted for everyone in the world, what would it be, and for what AI task?Topol: That’s easy: We need a planetary health system. We’d have multilevel data for every person, and each person would teach the rest of their species about preventing and managing illnesses using nearest neighbor analysis and other tools of AI. It’s possible now, but it requires an international commitment. I wrote about this with my colleague Kai-Fu Lee in an article called “It Takes a Planet.” The Batch: How can we build a planetary health system that protects data privacy and security?Topol: The tools are in front of us now. We can use federated and homomorphic computing. No country has to hand their data over. The algorithms can be used at the locale.The Batch: Much of the AI community is deeply concerned about making sure the technology is used ethically. What should AI practitioners keep in mind in that regard?Topol: Anything that exacerbates the very significant health inequalities that exist today is not acceptable. Human bias that finds its way into algorithms is a significant ethical concern that needs extensive review and scrutiny. And that’s not all. Algorithms in medicine need to be under constant surveillance because if an algorithm is hacked, it could hurt a lot of people.The Batch: What advice would you give machine learning engineers who want to make a positive impact in medicine?Topol: We’re still in the early phase. We need more interdisciplinary or transdisciplinary efforts between clinicians and AI practitioners. We need more large, annotated datasets, or to use self-supervised learning that preempts the need for them. We need to go to a higher validation plane, however we get there. Then we’ll be able to take advantage of this extraordinary opportunity to transform medicine and return the human essence that has been largely lost.", "image_caption": "Eric Topol illustration", "metadata": {"article_id": "issue_35", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/7-interview-with-Eric-Topol201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-35/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_35.html"}}
{"id": 22896698003, "type": "news_chunk", "title": "AI For Medicine Special! Eric Topol’s Planetary Health System", "subtitle": "Diagnosis: The Telltale Heart", "content": "The wearable revolution is helping doctors figure out what’s troubling your ticker — thanks to deep learning.The problem: Arrhythmias, a range of conditions in which the heart beats too fast, too slow, or erratically, can cause heart attack or stroke. But they don’t necessarily happen when a doctor is listening.The solution: Wearable devices from iRhythm constantly monitor a patient’s heartbeat and transmit the measurements to a neural network for analysis.How it works: The iRhythm Zio AT is an electrocardiogram monitor about the size of a breath-mint box with two wings of peel-and-stick medical tape that fasten onto the skin over a patient’s heart. Electrodes in the monitor track each heartbeat while a separate wireless transmitter sends the data to iRhythm. The system collects up to two weeks worth of continuous heartbeat data. If patients feel their heart begin to beat irregularly, they can push a button on the monitor to send a 90-second snippet to iRhythm’s headquarters immediately.A neural network analyzes the data. Trained on readings from 53,000 iRhythm Zio wearers, it classifies 12 different patterns: 10 arrhythmias, a normal heartbeat, and a heartbeat distorted by other bodily noises.An iRhythm technician reviews the neural network’s analysis and posts it to the patient’s electronic health record for physicians to see. Status: The United States Food and Drug Administration approved iRhythm’s Zio AT in 2018, and the system is on the market. The company recently partnered with Verily and Apple to develop further products.Behind the news: A 2019 review of 14 studies that compared AI with human clinicians found that deep learning models were roughly as good as human professionals at diagnosing signs of disease in medical imagery. The authors noted, however, that the studies tend to suffer from poor controls, inconsistent metrics for measuring success, and lack of independent validation. No comparable assessment of non-image AI diagnostics exists, but the fact that Apple is integrating arrhythmia detection into its smartwatch suggests that the field is maturing.Why it matters: Arrhythmias occur sporadically enough that spotting them requires many days of data. “You’ll never catch one by running an electrocardiogram in the office,” according to Dr. Mauricio Arruda of Cleveland’s University Hospitals Harrington Heart & Vascular Institute. By combining long-term observations with short-turnaround assessment, AI enables cardiologists to intervene with precise, timely, and potentially life-saving treatments.We’re thinking: Just the thought of AI saving somebody from a stroke makes our hearts skip a beat. To learn how you can use AI to diagnose illnesses, check out Course 1 of the AI for Medicine Specialization from deeplearning.ai.", "image_caption": "Illustration of doctor seeing a patient", "metadata": {"article_id": "issue_35", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1-diagnosis.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-35/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_35.html"}}
{"id": 22896698004, "type": "news_chunk", "title": "AI For Medicine Special! Eric Topol’s Planetary Health System", "subtitle": "Prognosis: Early Warning for Sepsis", "content": "An AI-driven alarm system helps rescue patients before infections become fatal.The problem: Machine learning can spot patterns in electronic health data indicating where a patient’s condition is headed that may be too subtle for doctors and nurses to catch. Sepsis, for instance, is a response to infection that inflames a patient’s organs, killing some 270,000 Americans each year. The ability to catch it early can save lives.The solution: Sepsis Watch is a deep learning model that spots signs of sepsis up to five hours before it becomes dangerous. This crucial window allows clinicians to intervene.How it works: The system integrates vital signs, test results, and medical histories of emergency-room patients, assessing their risk of septic shock on a scale of 0 to 100 percent. If the risk reaches 60 percent, the system alerts nurses in the hospital’s rapid response team. It also publishes an hourly list of each patient’s septic risk score. Researchers from Duke University, Harvard, and Google trained the model on a dataset of 50,000 patient records from the Duke hospital system.They evaluated the model at Duke and later expanded to two other community hospitals. All three continue to use it.The researchers designed Sepsis Watch with input from the hospital’s rapid response nurses. The collaboration, they say, made staff more likely to use the app. Status: Duke physician and data scientist Mark Sendak and colleagues conducted a clinical trial between November 2018 and July 2019. Sepsis Watch significantly improved sepsis response times, Sendak told The Batch. The team plans to publish the results in the near future. Last July, Duke University licensed the software to Cohere Med, an AI healthcare startup.Behind the news: Suchi Saria, a machine learning expert at Johns Hopkins University, was a pioneer in the use of reinforcement learning to identify sepsis treatment strategies back in 2018. Duke’s Sendak helped evaluate models for other kinds of clinical decision support in a recent survey in the European Medical Journal. The authors’ picks included early-warning systems for cardiac arrest, surgical complications, pneumonia, and kidney disease.Why it matters: As little as three hours of warning can give caregivers time to begin tests and medications that dramatically improve a sepsis victim’s odds of survival.We’re thinking: Building a great model is Step 1. Deployment is Step 2. Collaborating with hospital staff is a sharp way to promote Step 3, utilization. Learn how to build your own prognostic models in Course 2 of the AI For Medicine Specialization.", "image_caption": "Illustration of a patient in a hospital bed", "metadata": {"article_id": "issue_35", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/2-prognosis.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-35/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_35.html"}}
{"id": 22896698005, "type": "news_chunk", "title": "AI For Medicine Special! Eric Topol’s Planetary Health System", "subtitle": "Treatment: The Elusive Molecule", "content": "Will deep learning discover new medicines? Startups — and big-pharma partners — are betting on it.The problem: In theory, there’s a pharmacological cure for just about any ailment. In practice, discovering those therapies takes years and billions of dollars.The solution: Deep learning, with its ability to discern patterns amid noise, could speed up drug discovery considerably. In a dramatic test, Insilico used an algorithm to sift through petabytes of biochemical data to find potential drugs in 21 days.How it works: Based in Rockville, Maryland, Insilico used its Generative Tensorial Reinforcement Learning, or GENTRL, to create digital representations of molecules with properties that inhibit an enzyme linked to several types of cancer, atherosclerosis, and fibrosis. To make sure the model steered clear of established intellectual property, the researchers fed it a database of 17,000 patented compounds.The model produced 30,000 candidates, which the researchers whittled down to 848 using a mix of computational and AI methods.They selected 40 at random to examine more closely. They sent six of the most promising to WuXi AppTec, a pharmaceutical contract manufacturer in Shanghai, to synthesize. One of the molecules did indeed inhibit the enzyme in mice. Status: Insilico’s enzyme inhibitor was only a proof of concept. However, it attracted partnerships with GlaxoSmithKline, Jiangsu Chia Tai Fenghai Pharmaceutical, and Pfizer.Behind the news: Drug discovery is an attractive target for AI startups, given the abundance of biochemical data and desperation of pharmaceutical giants to cut costs. But success still seems hit-or-miss. Only one AI-designed drug — made by Exscientia — has progressed to human trials. Verseon has been working on the problem for nearly two decades without creating a marketable product. And, crucially, no one has found a reliable way to accelerate clinical trials, the most expensive and time-consuming part of drug development.Why it matters: The average successful drug costs $2.5 billion dollars to bring to market, according to a 2016 study. Cutting even a fraction of that cost could allow companies to channel resources towards more and different drugs, potentially providing the public with more cures in less time.We’re thinking: Finding a molecule that becomes a viable drug is like hunting for a single, specific plankton in the Pacific Ocean. Good thing machine learning engineers relish searching for tiny patterns in massive pools of data. Use deep learning to estimate treatment effects for individual patients in Course 3 of our AI for Medicine Specialization.", "image_caption": "Illustration of a syring with red liquid inside", "metadata": {"article_id": "issue_35", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/3-treatment_2.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-35/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_35.html"}}
{"id": 22896698006, "type": "news_chunk", "title": "AI For Medicine Special! Eric Topol’s Planetary Health System", "subtitle": "Data: From Patient to Health Record", "content": "Doctors are overwhelmed by clerical work. Healthcare-savvy voice assistants are picking up the slack.The problem: Doctors generate lots of vital information while examining a patient. Properly recorded, it becomes data that informs treatment — but entering it properly is a time-consuming task that drains docs’ attention and finances.The solution: Voice assistants can serve as clinical stenographers. Suki is one of several apps on the market that transcribe doctors’ observations and instructions and insert them into a patient’s electronic health record.How it works: Saying “Suki, the patient is running a fever and has fluid in their lungs,” inserts a note in the patient’s record. “Suki, show me the patient’s prescriptions,” retrieves that information. “Suki, I examined the patient,” enters the full description of a normal exam, ready for customization to the particular case. The model also adds diagnostic codes for tests and procedures, which aid in billing. Suki uses off-the-shelf voice recognition from Google and other vendors, augmented by the company’s own deep learning models. These models were trained on public datasets of speech plus a proprietary corpus of 250,000 anonymized patient-doctor interactions to capture the nuances of medical jargon. The engineers added background noises and conversation to make the models more robust.The engineers built several natural language task models that incorporate custom word embeddings, text classification, and entity recognition. These were trained on a combination of anonymized proprietary patient notes and public repositories of medical and clinical text. They retrain these models periodically using updated data.The company cites internal research showing that doctors who use Suki spend 70 percent less time doing clerical work. The system complies with U.S. regulations that protect sensitive personal information. Status: Suki, which integrates with several popular electronic health records, is deployed in the health network Ascension, Unified Women’s Health Care, and more than 90 small-to-midsize practices. As of July, the software operated in seven specialties including internal medicine, OB-GYN, and pediatrics. The company is working on new features for smarter billing ordering items like prescriptions and tests.Behind the news: Suki has plenty of competition. Rivals include Saykara, Nuance, M*Modal, and Notable.Why it matters: Doctors are drowning in paperwork, and voice-assistant technology can help them come up for air. A 2016 study estimates that doctors spend between 37 and 49 percent of their working hours on clerical tasks. All that paperwork contributes to the high level of burnout and depression in the profession, according to a 2019 study.We’re thinking: If you notice an improvement in your physician’s bedside manner, you might want to thank a robot. Build a natural language tool to extract data from medical information in Course 3 of the AI for Medicine Specialization from deeplearning.ai.", "image_caption": "Illustration of doctor sheets and a pencil", "metadata": {"article_id": "issue_35", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/4-knowledge-extraction201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-35/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_35.html"}}
{"id": 71444793001, "type": "news_chunk", "title": "AI Versus Covid Wake-Up Call, Straight Poop from a Smart Toilet", "subtitle": "News", "content": "Dear friends, I spoke on Tuesday at Coursera’s annual conference. It was the company’s most well-attended conference yet, and the first to be held online. Higher education is in for turbulent times. With campuses shut down indefinitely and many professors teaching digitally for the first time, schools are challenged to deliver high-quality education while both students and teachers remain at home. At the same time, more people than ever — of all ages, all over the world — are more interested than ever in taking courses online. In his keynote, Coursera CEO Jeff Maggioncalda spoke of the importance of social justice. Even as Covid-19 heightens social inequities, within this crisis lies an opportunity for educators to serve learners around the world. It’s also an opportunity to rebuild society’s trust in science, reason, and each other. When the pandemic is over, hundreds of millions of learners around the world will have picked up the habit of learning online. The momentum could drive a new golden age of learning, and it’s not too early to start preparing. Let’s make sure we keep working to democratize access to education and make our society more fair and equitable. Stay safe and keep learning! Machine learning engineers need tools and data to help fight the Covid-19 pandemic. Here are some that crossed our radar screen last week. CoViz: With new research being published daily, it can be difficult to keep track of everything known about Covid-19. The Allen Institute for AI offers CoViz, an interactive network that visualizes relationships among concepts present in the COVID-19 Open Research Dataset. You can use it to explore relationships between relevant proteins, genes, cells, diseases, and chemicals to make sure you’re up to date.Keystone Policy Intervention Dataset: In addition to medical interventions, policy interventions like social distancing have played a key role in battling Covid-19. To help researchers evaluate them, Keystone Strategy, in association with Stanford’s Susan Athey and Harvard’s Marco Iansiti, compiled a dataset that documents non-pharmaceutical interventions implemented by various local and national governments.ICU Beds: One challenge of the novel coronavirus is the strain it puts on health care systems. The shortage of personal protective equipment has been well documented, and recently Kaiser Health News documented the availability of ICU beds in the U.S. The data, which records the number of ICU beds per county along with population and demographics, is available for download. The corpus could be used to explore, for instance, the sensitivity of Covid-19 fatality rate to ICU resources, or the value of policy measures such as social distancing in resource-constrained counties.", "image_caption": "Neural networks over a world map and the title \"AI Versus Covid-19\" on a corner", "metadata": {"article_id": "issue_36", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Resources20ASPECT20REPLACEMENT.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-36/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_36.html"}}
{"id": 71444793002, "type": "news_chunk", "title": "AI Versus Covid Wake-Up Call, Straight Poop from a Smart Toilet", "subtitle": "News", "content": "Researchers have rushed out a battery of AI-powered tools to combat the coronavirus, but an assessment of dozens of models is a wake-up call for machine learning engineers.What’s new: Many models built to spot Covid-19 infection, predict the likelihood of hospitalization, or forecast outcomes are built on flawed science, according to a survey published in the British Medical Journal.What they found: A group of clinicians, scientists, and engineers led by Laure Winants, an epidemiologist at Maastricht University in the Netherlands, found that biased data compromised all of the 31 models analyzed. Nearly a dozen models used patient data that did not represent populations of people infected by the virus.Most models trained to detect Covid-19 infection in CT scans were trained on poorly annotated data. Many of the researchers who built them neglected to benchmark their work against established machine learning methods.Many models designed to predict patient outcomes were trained only on data from patients who had died or recovered. These models didn’t learn from patients who remained symptomatic by the end of the study period, yielding prognoses that were either overly optimistic or overly dire. Results: In a commentary that accompanied the survey, BMJ’s editors declared the models so “uniformly poor” that “none can be recommended for clinical use.”The path forward: The authors recommend that machine learning researchers adopt the 22-point TRIPOD checklist as a standard for developing predictive medical AI. Developed by an international consortium of physicians and data scientists, the checklist is designed to help engineers report their work clearly and reduces risk of developing models with biased data.Why it matters: Patients and health care systems alike need more accurate and faster diagnoses and prognoses. The AI community is used to publishing preliminary results to accelerate progress, but the health care community tends to wait for rigorous peer review to avoid causing harm.We’re thinking: Given how fast the Covid-19 situation is evolving, sharing results early and often is a good thing. But the AI community also needs new mechanisms to make sure preliminary models don’t cause harm.", "image_caption": "Information related to machine learning and Covid-19", "metadata": {"article_id": "issue_36", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Covid-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-36/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_36.html"}}
{"id": 90851973001, "type": "news_chunk", "title": "Tesla Parts the Curtain, Detecting Dangerous Bugs, Mapping...", "subtitle": "News", "content": "Recognizing stop signs, with their bold color scheme and distinctive shape, ought to be easy for computer vision — but it turns out to be a tricky problem. Tesla pulled back the curtain on what it takes to train its self-driving software to perform this task and others.What’s new: Tesla AI chief Andrej Karpathy describes in a video presentation how the electric car maker is moving toward fully autonomous vehicles. Shot at February’s ScaledML Conference, the video was posted on YouTube last week.Not just a big red hexagon: Stop signs take a surprising variety of forms and appearances, and that can make them hard to identify. Rather than an oversized icon on a pole, they’re often waved by construction workers, hanging off school buses, or paired with other signs. Karpathy describes how his team trained the company’s Autopilot system to detect a particularly vexing case: stop signs partially obscured by foliage. Engineers understood that AutoPilot was having trouble recognizing occluded stop signs because, among other things, the bounding boxes around them flickered.Using images from the existing dataset, they trained a model to detect occluded stop signs. They sent this model to the fleet with instructions to send back similar images. This gave them tens of thousands of new examples.They used the new examples to improve the model’s accuracy. Then they deployed it to HydraNet, the software that fuses outputs from AutoPilot’s 48 neural networks into a unified, labeled field of view. Behind the news: Tesla is the only major autonomous driving company that doesn’t use lidar as its primary sensor. Instead, it relies on computer vision with help from radar and ultrasonic sensors. Cameras are relatively cheap, so Tesla can afford to install its self-driving sensor package into every car that comes off the line, even though self-driving software is still in the works. It’s also easier to label pictures than point clouds. The downside: Cameras need a lot of training to sense the world in three dimensions, which lidar units do right out of the box. Unstoppable: Responding to Karpathy’s presentation on Twitter, Google Brain researcher David Ha (@hardmaru) created stop sign doodles using sketch-rnn, an image generator he and colleague David Eck trained on crude hand-drawn sketches. Generate your own doodled dataset here.", "image_caption": "Self-driving software working", "metadata": {"article_id": "issue_37", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize209.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-37/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_37.html"}}
{"id": 84822535001, "type": "news_chunk", "title": "Pandemic Triage, Asia's AI Advantage, Detecting Deepfakes", "subtitle": "News", "content": "A new generative model croons like Elvis and raps like Eminem. It might even make you think you’re listening to a lost demo by the Beatles.What’s new: OpenAI released Jukebox, a deep learning system that has generated thousands of songs in styles from country to metal and soul. It even mimics the voices of greats like Frank Sinatra.How it works: Jukebox generates music by drawing from a database of 1.2 million songs. Where some other AI-powered systems use symbolic generators to create tunes, Jukebox uses audio recordings, which capture more of music’s subtleties. In working with raw audio, the biggest bottleneck is its sheer size and complexity, the authors write. They used vector quantized variational autoencoders, or VQ-VAEs, to compress the training set to a lower-dimensional space. Then they trained the model to generate audio in this compressed space. Transformers create successively higher-resolution versions of a new song. Finally, a decoder turns that output into audio.The researchers paired each song with metadata including its artist, lyrics, and genre. That helps guide the model as it generates made-to-order music in any designated style.The model made cohesive music, but it struggled to produce coherent lyrics. To overcome this, researchers added existing lyrics into the conditioning information. It also had a hard time associating chunks of words with musically appropriate passages, so the researchers used open source tools to manually align words with the music windows in which they appear.The model requires upward of nine hours of processing to render one minute of audio. Results: OpenAI released over 7,000 songs composed by Jukebox. Many have poor audio quality and garbled lyrics, but there are more than a few gems. Have a listen — our favorites include the Sinatra-esque “Hot Tub Christmas,” with lyrics co-written by OpenAI engineers and a natural language model, and a country-fied ode to deep learning.Behind the news: AI engineers have been synthesizing music for some time, but lately the results have been sounding a lot more like human compositions and performances. In 2016, Sony’s Flow Machine, trained on 13,000 pieces of sheet music, composed a pop song reminiscent of Revolver-era Beatles.The production company AIVA sells AI-generated background music for video games, patriotic infomercials, and tech company keynotes.Last April, OpenAI released MuseNet, a music generator that predicts a sequence of notes in response to a cue. Why it matters: Jukebox’s ability to match lyrics and voices to the music it generates can be uncanny. It could herald a new way for human musicians to produce new work. As a percentage of all music consumed, computer generated music is poised to grow.We’re thinking: Human artists already produce a huge volume of music — more than any one person can listen to. But we’re particularly excited about the opportunity for customization. What if you could have robo-Beyonce sing a customized tune for your home movie, or robo-Elton John sing you a song celebrating your birthday?", "image_caption": "Series of images related to Jukebox, a deep learning system by OpenAI", "metadata": {"article_id": "issue_38", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2011-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-38/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_38.html"}}
{"id": 17042778001, "type": "news_chunk", "title": "Covid Mask Detection, Brain To Text Translation, AI Chooses Tax", "subtitle": "News", "content": "Cameras that detect face masks are helping French authorities to evaluate citizens’ adherence to government mandates intended to fight Covid-19.What’s new: Starting this week, everyone riding public transportation in France is required to wear a face mask. Paris and Cannes are using computer vision to count people who comply.How it works: Datakalab, a French AI startup, is installing chips in existing CCTV cameras that run an object recognition model. The model is trained to distinguish masked faces from unmasked ones. Paris is testing the cameras at the busy Chatelet-Les Halles metro station. Cannes has installed them on buses and in public markets.The software counts mask wearers every 15 minutes and transmits aggregate statistics to the authorities. The company says the system is meant to help authorities determine where to step up efforts to promote mask-wearingDatakalab provides similar technology for use in retailing. Those systems note customers’ age, gender, how long they stay in certain areas, and whether they’re smiling. Behind the news: AI is being used widely to monitor compliance with rules designed to combat the spread of Covid-19. The Indian state of Punjab is using drones from Skylark Laboratories to enforce social distancing and curfew regulations.Hospitality companies have deployed cameras from Wobot Intelligence to ensure that employees are washing their hands for at least 20 seconds. Yes, but: France’s privacy commission warns that mask detection technology may violate European rules that limit personal data collection. Datakalab counters that its systems neither identify individuals nor store data. In any case, 94 percent of French citizens support wearing masks in public according to a recent poll. (France continues to outlaw burqas and other religious face coverings under a 2011 law.)Why it matters: As France and other countries begin to lift rules that keep people physically apart, wearing masks is critical to limiting coronavirus transmission.We’re thinking: Covid-19 surveillance is a double-edged sword: helpful in containing the pandemic but worrisome in other contexts. Governments and businesses must use it appropriately and only while the need persists.", "image_caption": "Face recognition system identifying a person wearing a mask", "metadata": {"article_id": "issue_39", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize2010.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-39/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_39.html"}}
{"id": 30442938001, "type": "news_chunk", "title": "Autonomous Nuclear Weapons?!, Fighting Deepfakes, Recognizing...", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, When it comes to artificial intelligence, one of the biggest mistakes large companies make is thinking tactically rather than strategically. What’s the difference? Some taxi companies thought they had the internet revolution “covered” because they built a website. Then ride-sharing startups disrupted the industry with internet-connected mobile apps that transformed the ride-hailing experience. Similarly, some companies’ response to AI starts and ends with tactically building a few small projects. But the strategic question is: How will AI transform your industry’s core business, and how will that change what it takes for your company to thrive? I spoke about this at TechCrunch’s conference on Thursday, and Fortune published a nice summary of my remarks. It’s not too late for traditional companies to develop a strategic plan to take advantage of AI. The technology is only beginning to find its way into applications outside of software development. But for many companies, it will be critical to act quickly. AI transformation should start with concrete projects, but it cannot end there. I hope more CEOs learn about AI and think strategically about it. Keep learning! Omoju Miller’s journey from comp-sci undergrad to GitHub was anything but straightforward. Learn about her day-to-day as a senior machine learning engineer in the latest installment of our Working AI series. Read more", "image_caption": "Photo of Omoju Miller, senior ML engineer at Github", "metadata": {"article_id": "issue_4", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TheBatch-WorkingAIOmoju.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-4/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_4.html"}}
{"id": 30442938002, "type": "news_chunk", "title": "Autonomous Nuclear Weapons?!, Fighting Deepfakes, Recognizing...", "subtitle": "Can AI Wage War? Should It?", "content": "The U.S. military is developing a new generation of automated weaponry. Some people are calling for automated generals as well. What happened: A pair of defense experts argue in War on the Rocks, an online publication covering national security, that the Pentagon should replace the human chain of command over nuclear defense with machines. The time available to respond to incoming warheads has dwindled from hours during the Cold War to roughly 6 minutes today. The change makes automated command-and-control a necessity, they say. Their analysis added urgency to feature stories on military AI published last week in The Atlantic and The Economist. Behind the news: The Department of Defense’s 2019 budget calls for nearly $1 billion in AI spending. Almost one-third of the money will fund the Joint Artificial Intelligence Center dedicated to establishing and scaling up AI throughout the military. The remainder of the department’s AI budget will support initiatives led by individual branches. Among those efforts: The Air Force is developing SkyBorg, an AI copilot for F-16s to help with navigation, radar awareness, and target recognition. The system will also pilot Valkyrie drones (pictured above) to serve as autonomous wingmen.The Marine Corps is building self-driving assault boats to deliver troops to a beach, then support the landing via autonomous .50 caliber machine guns.The Navy is testing Sea Hunter, an autonomous ship intended eventually to destroy enemy submarines without human intervention. The controversy: The debate over automated warfare follows trench lines similar to those of the earlier (and ongoing) argument over nuclear weapons. Pro-AI military experts fear that developments like hypersonic missiles — capable of traveling up to 20 times the speed of sound — could decapitate the U.S. military before it has a chance to react. They argue that AI-driven warfare is imperative, if only for effective defenses.But critics such as The Bulletin of the Atomic Scientists warn that hackers, faulty code, or wayward machines could handicap AI-driven weapons. Moreover, non-nuclear autonomous weaponry could violate ethical and legal codes in the Geneva Conventions. We’re thinking: Autonomous weapons are terrifying enough. Autonomous nuclear weapons verge on the unthinkable. We strongly support the United Nations’ effort to establish a ban on autonomous weapons as a complement to nuclear disarmament. That said, AI has potential nonlethal uses like mine removal and search and rescue. It will take vigorous, well informed argument to arrive at military uses of AI that improve conditions for humanity as a whole. It’s critical that the AI community play an active role in the discussion.", "image_caption": "Automated aircraft overflying a field", "metadata": {"article_id": "issue_4", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/XQ-58a.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-4/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_4.html"}}
{"id": 30442938003, "type": "news_chunk", "title": "Autonomous Nuclear Weapons?!, Fighting Deepfakes, Recognizing...", "subtitle": "What Language Models Know", "content": "Watson set a high bar for language understanding in 2011, when it famously whipped human competitors in the televised trivia game show Jeopardy! IBM’s special-purpose AI required around $1 billion and a squadron of engineers. New research suggests that today’s best language models can accomplish similar tasks right off the shelf. What’s new: Researchers at Facebook AI Research and University College London pitted top-shelf language models against task-specific networks in a Jeopardy!-like challenge they call Language Model Analysis (LAMA). Their LAMA data set provides a large corpus of sentences, each missing a key fact.Key Insight: The latest language models are pretrained to address a variety of downstream tasks. In learning language representations, they retain knowledge that can be used to complete statements lacking key words.How it works: LAMA builds its incomplete sentences based on facts drawn from Google-RE (facts from Wikipedia), T-REx (facts aligned with Wikipedia text), ConceptNet (a semantic network), and SQuAD (questions and answers). LAMA requires models to fill in a missing subject or object. For example, “The theory of relativity was developed by ___.”The researchers evaluated off-the-shelf versions of BERT, ELMo, and Transformer-XL without further training. Results: BERT-Large filled in the blanks most accurately overall, and it was best at completing statements based on Google-RE and ConceptNet. It proved only half as accurate as task-specific models on LAMA’s SQuAD portion, which contains more complicated sentences. Similarly, BERT’s performance suffers when T-REx facts contain multiple subjects or blanks.Why it matters: The Allen institute last week reported using BERT to score better than 90 percent on the multiple-choice questions in the New York Regents science test for the eighth grade. That system included additional task-specific models and retrieved external information to complete tasks. This research suggests that BERT as-is would score well on the Regents test. Takeaway: Large, pretrained language models can glean and recall nearly as much information — from some data sets, at least — as specially designed question answering models. This knowledge can allow them to accomplish various language tasks, including fill-in-the-blank, without special preparation.", "image_caption": "Graph related to Language Model Analysis (LAMA)", "metadata": {"article_id": "issue_4", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/questionanswer.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-4/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_4.html"}}
{"id": 30442938004, "type": "news_chunk", "title": "Autonomous Nuclear Weapons?!, Fighting Deepfakes, Recognizing...", "subtitle": "Chimp Recognition", "content": "AI is capable of picking faces out of the crowd — even if that crowd is squabbling over bananas in a jungle. What’s new: Researchers at the University of Oxford developed a face recognition app that identifies individual chimpanzees in footage shot in the wilds of Guinea. The work could give wildlife conservation efforts a powerful new tool. How it works: The group adapted the VGG-M convolutional neural network architecture. They trained the model on roughly 50 hours of footage representing 23 individuals over 14 years. The model identified apes as they aged.It was able to recognize individuals regardless of low light, poor image quality, and facing away from the camera.The researchers pitted their model against a human trained to recognize chimps. The human sorted 42 percent of the images correctly. The model’s accuracy was 84 percent. Behind the news: Zoologists have embraced image recognition for conservation efforts. The technology is counting giraffes in Africa and tracking wolverines in the Pacific Northwest. An innovative application called WildBook that trawls YouTube for wildlife videos has been used to catalog whale shark migrations. Why it matters: Chimpanzees, like humans, are highly social animals. The ability to track individuals enabled the researchers to map the tribe’s structure. The model generalized well to other primate species in preliminary tests. The researchers suggest that their approach could be used with other animals where a sufficient video record exists. We’re thinking: Applications like this could help cash-strapped conservation efforts to focus on translating data into action, and reduce the need for invasive, labor-intensive methods like tagging animals with RFID.", "image_caption": "Face recognition app identifying individual chimpanzees in footage shot in the wilds of Guinea", "metadata": {"article_id": "issue_4", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chimpanzees20faces.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-4/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_4.html"}}
{"id": 30442938005, "type": "news_chunk", "title": "Autonomous Nuclear Weapons?!, Fighting Deepfakes, Recognizing...", "subtitle": "Facing Down Deepfakes", "content": "Deepfakes threaten to undermine law and order, perhaps democracy itself. A coalition of tech companies, nonprofits, and academics joined forces to counter potential adverse impacts. What’s new: The Deepfake Detection Challenge aims to provide a data set of custom-built deepfakes. Funded by a $10 million grant from Facebook, it also promises a prize for developing tools that spot computer-generated pictures.The details: Facebook is producing videos with actors who have consented to having their features altered by deepfake technology. A working session at the International Conference on Computer Vision in October will perform quality control.Facebook plans to offer access on a limited basis, with full release to follow at the NeurIPS conference in December.A competition to identify deepfakes in the dataset will run until spring 2020, with the winner to be awarded an unspecified prize.Other partners include Cornell Tech, Microsoft, MIT, the Partnership on AI, UC Berkeley, University at Albany-SUNY, University of Maryland College Park, University of Oxford, and WITNESS. Behind the news: Activists goaded Facebook to action in June, when they released a synthesized video of Mark Zuckerberg rhapsodizing over his control of billions of peoples’ data.Why it matters: Deepfakes often are portrayed as a potential vector for political disinformation. But, as Vice and Wired point out, the clear and present danger is harassment of individuals, particularly women, activists, and journalists. We’re thinking: The fact that deepfakes are created by adversaries means the data set — and resulting filters — will need to evolve as the fakers adapt to detection algorithms. Can you spot fakes? Test your personal deepfake radar via this online guessing game.", "image_caption": "Original vs Deepfake example", "metadata": {"article_id": "issue_4", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FB20Deepfake20challenge.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-4/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_4.html"}}
{"id": 30442938006, "type": "news_chunk", "title": "Autonomous Nuclear Weapons?!, Fighting Deepfakes, Recognizing...", "subtitle": "Leveling the Playing Field", "content": "Deep reinforcement learning has given machines apparent hegemony in vintage Atari games, but their scores have been hard to compare — with one another or with human performance — because there are no rules governing what machines can and can’t do to win. Researchers aim to change that. What’s new: Most AI research demonstrating superhuman performance in Atari games applies widely varying limits on gameplay, such as how frequently buttons can be pressed. Researchers from MINES ParisTech and Valeo offer a standardized setup: Standardized Atari Benchmark for Reinforcement Learning (Saber). They use it to achieve a new state of the art in around 60 games from Pong to Montezuma’s Revenge.Key Insight: Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde noticed that the reported human world-record scores average 1,000 times higher than the “expert human player” scores given in the first major deep reinforcement learning paper published in late 2013. Analyzing the settings used in deep learning publications since, the team pinpointed seven potential causes for reported variations in performance. How it works: The authors propose a set of guidelines designed to match human capabilities. Their benchmark includes a new metric for evaluating models, since the previous human benchmark misrepresents human capabilities. Saber removes limitations on gaming time — it takes time for human players to rack up a world record! — rather than the few minutes many researchers allow.The benchmark specifies that models can receive only the game screen as input, no further information allowed. For example, they must be able to use all buttons even if some don’t function.The benchmark ranks models on a normalized scale in which 0 represents a score obtained by pressing buttons randomly and 1 is the human world record. Results: The researchers tested a state-of-the-art model, Rainbow-IQN, and achieved an average of only 31% of the best human scores. The model achieved superhuman scores in four of 58 games. Why it matters: Training reinforcement learning models is so laborious that researchers often don’t bother to reproduce previous results to see how their own stack up. Saber finally provides a consistent basis for comparison.We’re thinking: Deep reinforcement learning research is exciting, but a lack of standardized benchmarks has kept the state of the art in a state of ambiguity. Saber signals a new and promising maturity.", "image_caption": "Arcade game", "metadata": {"article_id": "issue_4", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Atari20Cropped.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-4/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_4.html"}}
{"id": 30442938007, "type": "news_chunk", "title": "Autonomous Nuclear Weapons?!, Fighting Deepfakes, Recognizing...", "subtitle": "Automation’s Frontier: Fast Food", "content": "Quick-service restaurants are experiencing record-high employee turnover, while labor advocates are pushing for higher wages. Some experts say these forces are propelling the fast food industry toward full automation. Who’s already automating: The move to put fast food under machine control is already in high gear: McDonalds announced on Tuesday its acquisition of Apprente, a company that develops voice-driven conversational agents. The 34-year-old fast-food pioneer has tested automated ordering kiosks since 2003 and recently allocated $1 billion to upgrade the technology.In China, Yum! Brands, owner of KFC, Taco Bell, and Pizza Hut, says 50 percent of transactions take place via app or kiosk.Zume Pizza of California uses robots to form dough, spread sauce (pictured above), and bake the resulting pies. Humans place toppings.At Spyce in Boston, customers order and pay by kiosk, and a machine mixes their grain-based meals. Human prep cooks par-bake rice, chop veggies, and reduce sauces. Behind the news: Humans are opting out for the quick-service business. In July, the CEO of Panera Bread told CNBC’s @Work conference that his company experienced nearly 100 percent annual employee turnover — and this number was low for the industry. Turnover in the Accommodations and Restaurants category (which includes traditional restaurants as well as hotels) has climbed nearly 15 percent over the last decade, according to the U.S. Bureau of Labor Statistics. Why it matters: Fast food is shaping up to be a leading edge of an automation wave that could be squeezing lower-skilled, lower-wage employees out of the economy. A 2017 report by the National Council on Compensation Insurance found that, while automation historically replaces human labor, the jobs that remain tend to be higher skilled and better compensated. We’re thinking: Apps and kiosks are clearly capable of replacing fast-food customer service. Back-of-the-house work like assembling burritos and stacking sandwiches requires more dexterity. While those positions likely persist longer, it may be cold comfort to find yourself automated out of a job five years from now rather than one.", "image_caption": "Machine putting sauce on pizza dough", "metadata": {"article_id": "issue_4", "chunk_index": 7, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker201.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-4/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_4.html"}}
{"id": 57890247001, "type": "news_chunk", "title": "COVID-19 Infects AI, Learning from Small Data, Generated Music", "subtitle": "News", "content": "Dear friends, I recently received an email from one of you who lives far from the major AI hubs, saying, “I feel like I’m all alone.” I want to tell you all: Even if it sometimes feels like you’re facing the challenges of work and life in isolation, you are not alone! I am here for you, and all of us are in this together. Most software engineers are working from home and connecting digitally with colleagues, mentors, and friends. In this time of social distancing, the AI community has the potential to come out even stronger and more tightly knit. There are many ways to regain a feeling of connection with your peers. I invite you to join our virtual Pie & AI events. Read and reply on a Coursera forum, or discuss your ideas on Reddit or Twitter. Send a message to a favorite researcher asking questions about their work. Poke around open source projects to see what you can contribute. Some of my teams are split across the U.S. and Colombia. Ironically, sheltering in place has brought them closer, because now it’s exactly as convenient for a U.S. team member to communicate with one in Columbia as one in the U.S. The playing field has leveled. Let’s all keep finding ways to connect and help each other through this time. Keep learning! The pandemic has radically altered online shopping behavior, throwing a wrench into many AI systems.What’s new: AI inventory trackers, recommendation algorithms, and fraud detection systems trained on pre-pandemic consumer behavior have been flummoxed by the wildly different ways people now browse, binge, and buy, according to MIT Technology Review.What’s happening: Companies are scrambling to retrain machine learning systems for the new normal. Amazon’s recommender typically promotes items the company itself can ship. With its distribution network under strain, the algorithm seems to be promoting products from sellers who handle their own shipments.Featurespace, which provides fraud detection technology for financial and insurance companies, revamped its behavior models to account for surges in demand for things like power tools and gardening supplies. Such spikes used to trigger alerts. Now they’re business as usual.AI consulting firm Pactera Edge says an upswing in bulk orders broke a client’s predictive inventory systems. Another client found its public-sentiment analysis software distorted by all the gloomy news.Phrasee, a company that generates ad copy using natural language processing, tweaked its algorithm to avoid phrases that could spark panic (“going viral”), raise anxiety (“stock up!”), or promote risky behavior (“party wear”). Behind the news: E-commerce is one of the pandemic’s few beneficiaries: Growth in online sales in April tripled over the same month last year, according to an analysis by electronic payments processor ACI Worldwide.Why it matters: Beyond its terrible human toll, Covid-19 is making yesterday’s data obsolete. The AI community must find ways to build resilient systems that can adjust as conditions change.We’re thinking: In our letter of April 29, we pointed out that AI often suffers from a gap between proofs of concept and practical deployments because machine learning systems aren’t good at generalizing when the underlying data distribution changes. Covid-19 is bringing about such changes on a grand scale, and our systems are showing their brittleness. The AI community needs better tools, processes, and frameworks for dealing with this issue.", "image_caption": "People's fingers forming a triangle", "metadata": {"article_id": "issue_40", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Andrews20Letter.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-40/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_40.html"}}
{"id": 20111826001, "type": "news_chunk", "title": "AI's New Supercomputer, GANs as Simulators, Giant Chatbot...", "subtitle": "News", "content": "Dear friends, I’m proud to announce that we held the 100th Pie & AI last Friday. Pie & AI is our meetup series that brings together members of the AI community worldwide for education, conversation, and a slice of pie. Pie & AI kicked off in Seattle last year shortly after Pi Day (March 14, or 3.14). Since then, we’ve hosted events in over 68 cities in 38 countries. Friday’s event was streamed from Azerbaijan. With social distancing keeping us apart physically, it’s more important than ever for AI to have a strong online community. So we’ve doubled down on making Pie & AI a virtual meetup. No matter where you are, you can attend any of our events, learn from experts, and chat with peers even if they’re thousands of miles away. I would like to say a special thank you to Pie & AI’s 60 event ambassadors. These extraordinary people organize events locally, share resources and tips, and sometimes speak about how AI applies to local businesses and problems. I am grateful and inspired by your dedication to sharing your knowledge and enthusiasm. If Pie & AI has answered your questions, helped you grow, or inspired you, please let us know on Twitter using #PieandAI. You can check out upcoming events here. Keep learning! Generative adversarial networks don’t just produce pretty pictures. They can build world models, too.What’s new: A GAN generated a fully functional replica of the classic video game Pac-Man. Researchers from Nvidia, MIT, the University of Toronto, and Vector Institute developed GameGAN to celebrate the original Pac-Man’s 40th anniversary. The company plans to release the code in a few months.How it works: GameGAN learned to reproduce the game by watching it in action for 50,000 hours. During gameplay, the system synthesizes the action frame by frame using three neural networks. An LSTM-style network learned how user actions change the game’s state. For example, pressing the system’s joystick equivalent upward moves the Pac-Man character forward one space.A network inspired by neural Turing machines allows the system to store information about previously generated frames. In a maze game, retracing your steps should look familiar, and that would be difficult without memory.Based on the memory, updated game state, and latest user action, GameGAN’s generator produces the next frame. Behind the news: While Nvidia is the first to use a generative adversarial network to reproduce a video game, other researchers have used machine learning for this purpose. An earlier model from Georgia Tech learns approximate representations of classic titles to create new games.The Metacreation Lab at Simon Fraser University is working on models that generate new levels for existing games.Researchers from Queen Mary University trained a neural network to duplicate a video game’s underlying mechanics by observing pixels. Yes, but: Compared to the original arcade game, Pac-Man’s GAN-driven twin requires orders of magnitude more computation to run.Why it matters: Autonomous systems such as self-driving cars and robots are often trained in elaborate simulators. Nvidia hopes that GAN-based sims can save time and money.We’re thinking: Fifty thousand hours is an awful lot of Pac-Man — or anything else! Simulation makes it possible to amass training data that would be virtually impossible to collect in the real world. It’s also a crutch that leads researchers to develop algorithms that work well in simulated environments but are hard to generalize to real-world conditions. Until better small-data algorithms emerge, GAN-based simulation looks like an exciting new direction.", "image_caption": "Replica of the video game Pac-Man generated by a GAN", "metadata": {"article_id": "issue_41", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Pacman3.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-41/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_41.html"}}
{"id": 18569141001, "type": "news_chunk", "title": "Facebook’s Unruly Algorithm, AI That Does the Dishes, New Life", "subtitle": "Facebook Likes Extreme Content", "content": "Dear friends, Like many of you, I’m deeply saddened by the events of the past week. I’m horrified by the senseless violence perpetrated against Black communities and appalled by the persistent racial injustice of our society. It’s long past time to right these terrible wrongs. The tragic deaths of George Floyd, Ahmaud Arbery, Breonna Taylor, Sean Reed, and innumerable others remind us that life is precious, and that we have much more work to do to build an inclusive society. Minority voices are often marginalized, and that creates a responsibility for the rest of us to keep our ears and minds open, and add our voices to theirs when the occasion calls. The AI community itself has a diversity problem. The number of Black people in the field is vanishingly small. A narrow perspective can lead to severely flawed work if we overlook factors like skin color when we collect and annotate datasets or validate results. Without diverse teams, instead of building AI systems that help a cross section of people, we open doors for some while locking out others. Lack of diversity in the AI community has another effect: It reinforces the belief, often unconscious, that certain people can’t make important contributions to the field. We need to fight this sort of bias as well. If you are Black and working in AI, we would like to know about your experiences in the field. If you have Black colleagues whom you admire, please let us know about them as well. We hope to share some of your stories. Please write to us at [email protected]. Maybe I’m naive, but the protests this time do feel different, and I’m cautiously optimistic that this may be the time when we finally make a huge dent in racism. As members of the AI community, let us join this movement, condemn racism everywhere we see it, and settle for nothing less than a fair and inclusive world. Keep learning! Facebook’s leadership has thwarted changes in its algorithms aimed at making the site less polarizing, according to the Wall Street Journal.What’s new: The social network’s own researchers determined that its AI software promotes divisive content. But the company’s management rejected or weakened proposed reforms, concerned that such changes might cut into profits or give the appearance of muzzling conservatives. Fizzled reforms: Facebook’s recommender system promotes posts from its most active users: those who do the most commenting, sharing, and liking. Internal investigations conducted between 2016 and 2018 showed that such so-called superusers disproportionately spread misinformation, much of it politically divisive. Internal committees proposed ways to address the issue, but the company ultimately made changes that blunted their potential impact. One proposal called for lowering recommendation scores for content posted by superusers on the far right or far left of the political spectrum. Content from moderates would receive higher scores.The company accepted the approach but lowered the penalties applied to extremist posts by 80 percent.Facebook also nixed the building of a classification system for polarizing content and quashed plans to suppress political clickbait. Behind the news: Conservatives in the U.S. have long accused social media platforms of left-wing bias, a charge to which Facebook has been particularly sensitive. In 2018, lawmakers grilled Facebook CEO Mark Zuckerberg over accusations that the platform marginalized conservatives.Last week, Twitter put warning labels on tweets by Donald Trump that it deemed misleading or inciting violence. The president responded with an executive order that would strip social media companies of legal protections from liability for content posted by users.Facebook publishes similarly inflammatory posts by the president without challenge. Some Facebook employees protested that stance with a virtual walkout on Monday. Facebook’s response: “We’ve built a robust integrity team, strengthened our policies and practices to limit harmful content, and used research to understand our platform’s impact on society so we continue to improve,” the company said in a statement.Why it matters: The algorithms that govern popular social media platforms have an outsized influence on political discourse worldwide, contributing to polarization, unrest, and hate crimes. Divisive rhetoric distributed by Facebook has been linked to violence in Sri Lanka, Myanmar, and India.We’re thinking: Social media is a double-edged sword. It has been helpful for quickly disseminating (mostly accurate) information about concerns like Covid-19. But what brings people together can also drive them apart. The AI community has a responsibility to craft algorithms that support a just society even as they promote business.", "image_caption": "Angry emoji over dozens of Facebook like buttons", "metadata": {"article_id": "issue_42", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/FACEBOOK.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-42/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_42.html"}}
{"id": 18569141002, "type": "news_chunk", "title": "Facebook’s Unruly Algorithm, AI That Does the Dishes, New Life", "subtitle": "When Models Take Shortcuts", "content": "Neuroscientists once thought they could train rats to navigate mazes by color. It turns out that rats don’t perceive colors at all. Instead, they rely on the distinct odors of different colors of paint. New work finds that neural networks are especially prone to this sort of misalignment between training goals and learning.What’s new: Robert Geirhos, Jörn-Henrik Jacobsen, and Claudios Michaelis led a study of neural network hiccups conducted by the University of Tübingen, Max Planck Research School for Intelligent Systems, and the University of Toronto. They argue that many of deep learning’s shortcomings reveal shortcut learning.Key insight: Shortcuts are pathways to solving a problem that result in good performance on standard benchmarks but don’t require understanding of the problem and therefore don’t transfer well to real-world situations.How it works: The authors identify apparent causes of shortcut learning in neural networks, circumstances that tend to encourage it, and techniques available to discourage it. Dataset bias can cause models to focus on spurious correlations rather than valid relationships. For instance, cows often stand in pastures, so black, white, and green textures can indicate their presence — but a lawn is not an identifying mark of cattle. Models have a hard time learning true bovine characteristics when their training data offers this simpler approach.Training data may be free of spurious correlations and still fail to represent the task at hand. For example, cats have fur while elephants have wrinkled skin, so an animal classifier may wind up becoming a texture detector instead.To address such issues, the authors propose training and testing on out-of-distribution, augmented, and adversarial examples. If a model incorrectly recognizes a test sample that has been altered to change, say, the color of grass from green to brown, it’s likely the model relied on shortcuts.In the animal classification tasks described above, domain experts can make sure the training set depicts animals in a variety of scenes and breeds such as hairless cats that exhibit a range of textures. Why it matters: The authors shed light on an issue that has troubled machine learning engineers for decades and highlight the lack of robustness of current algorithms. Addressing these issues will be key to scaling up practical neural network deployments. We’re thinking: Humans also use shortcuts; we’ve all memorized formulas by rote instead of fully understanding them. Our misbehaving models may be more like us than we’d like to admit.", "image_caption": "Data and information related to shortcut learning", "metadata": {"article_id": "issue_42", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/SHORTCUT.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-42/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_42.html"}}
{"id": 18569141003, "type": "news_chunk", "title": "Facebook’s Unruly Algorithm, AI That Does the Dishes, New Life", "subtitle": "AI Does the Dishes", "content": "A pioneer in dishwashing robots is reaching into commercial kitchens.What’s new: Dishcraft Robotics uses machines equipped with computer vision to scrub dirties for corporate food services and, soon, restaurants.How it works: Every morning, Dishcraft’s biodiesel-fueled trucks deliver clean dishes and utensils to corporate clients near its Silicon Valley hub. At the day’s end, the trucks retrieve them. Back at headquarters, workers load racks of dirty dishes and cutlery into an automated washing machine. The system classifies each item and tailors its cleaning cycle accordingly, a company rep told The Batch.A pose estimation model helps suction-powered robotic arms pass items between scrubbing and rinsing stations, as seen above.Another model inspects items for cleanliness. The company says its sensors can detect particles too small for humans to see.A recent $20 million investment will fund the company’s expansion into reusable takeout containers. Customers will drop off soiled plasticware at set locations, and the company will clean and redistribute it to its restaurant partners. Behind the news: Other robotics companies are also aiming to disrupt the kitchen. Last year, Toyota Research Institute showed off a mechanical prototype that organizes dishes and silverware in a household dishwasher.Robotics startup Moley built a pair of AI-guided arms capable of cooking everything from soups to macarons. The company plans to release a consumer model this year. Why it matters: Dishcraft estimates its system saves clients as much as 1.6 gallons of water per meal. Its plan to clean reusable to-go containers could keep tons of waste out of landfills. We’re thinking: Such machines also could mean fewer bodies in food-service kitchens — a plus in the Covid era but not so much for human staff who may find themselves out of a job.", "image_caption": "Dishwashing robot working", "metadata": {"article_id": "issue_42", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Dishes2-optimized.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-42/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_42.html"}}
{"id": 18569141004, "type": "news_chunk", "title": "Facebook’s Unruly Algorithm, AI That Does the Dishes, New Life", "subtitle": "Cars Idled, AV Makers Keep Rolling", "content": "The pandemic has forced self-driving car companies off the road. Now they’re moving forward by refining their mountains of training data.What’s new: Self-driving cars typically collect real-world training data with two human operators onboard, but Covid-19 makes that unsafe at any speed. Instead, several companies are squeezing more value out of work they’ve already done, according to MIT Technology Review. What they’re doing: Makers of autonomous vehicles are relabeling old data and fine-tuning simulations. Drivers at the autonomous truck company Embark are sifting through four years of past driving records, flagging noteworthy events and annotating how vehicles should react.Pittsburgh-based Aurora Innovation reassigned vehicle operators to scan its data for unusual situations that can be converted into simulated training scenarios.Scale AI, a data-labeling firm, is adding detail to its datasets. It’s also developing a tool that predicts the intentions of drivers and pedestrians by tracking their gaze.GM’s Cruise is updating its simulations. For instance, the company is improving the way it scores vehicle responses to uncommon occurrences such as encounters with ambulances. Behind the news: With little income, $1.6 million in average monthly overhead, and increasingly tight funding, autonomous vehicle companies are making tough choices. Lyft, Kodiak Robotics, and Ike have laid off employees, while Zoox is looking for a buyer.Why it matters: Data can be a renewable resource: By adding new labels and sharpening old ones, AI teams can imbue old datasets with new life. Using refurbished datasets to improve simulations compounds the effect.We’re thinking: Development of self-driving cars had moved into the slow lane even before the pandemic. It’s better to keep making incremental progress than none at all.", "image_caption": "Self-driving car from the inside", "metadata": {"article_id": "issue_42", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Drive1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-42/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_42.html"}}
{"id": 18569141005, "type": "news_chunk", "title": "Facebook’s Unruly Algorithm, AI That Does the Dishes, New Life", "subtitle": "Another Look at YOLO", "content": "The latest update of the acclaimed real-time object detector You Only Look Once is more accurate than ever.What’s new: Alexey Bochovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao at Taiwan’s Institute of Information Science Academia Sinica offer YOLOv4 — the first version not to include the architecture’s original creators.Key insight: Rapid inference is YOLO’s claim to fame. The authors prioritized newer techniques that improve accuracy without impinging on speed (their so-called “bag of freebies”). In addition, they made improvements that boost accuracy at a minimal cost to speed (the “bag of specials”). All told, these tweaks enable the new version to outperform both its predecessor and high-accuracy competitors running at real-time frame rates.How it works: YOLO, as well as most object detectors since, tack a model that predicts bounding boxes and classes onto a pre-trained ImageNet feature extractor. Techniques under the heading “bag of freebies” boost accuracy by adding computation during training. These include alternate bounding box loss functions, data augmentation, and decreasing the model’s confidence for ambiguous classes.The authors introduce new data augmentation techniques such as Mosaic, which mixes elements drawn from four training images to place objects in novel contexts.“Bag of specials” techniques include the choice of activation function: ReLU variants are marginally slower, but they can yield better accuracy.The authors accommodate users with limited hardware resources by choosing techniques that allow training on a single, reasonably affordable GPU. Results: The authors pitted YOLOv4 against other object detectors that process at least 30 frames per second, using the COCO image dataset. YOLOv4 achieved 0.435 average precision (AP), running at 62 frames per second (FPS). It achieved 0.41 AP at its maximum rate of 96 FPS. The previous state of the art, EfficientDet, achieved 0.43 AP running at nearly 42 FPS and 0.333 AP at its top speed of 62 FPS.Why it matters: YOLOv4 locates and classifies objects faster than measurements of human performance. While it’s not as accurate as slower networks such as EfficientDet, the new version boosts accuracy without sacrificing speed.We’re thinking: You only look once . . . twice . . . thrice . . . four times and counting!", "image_caption": "Data related to YOLOv4", "metadata": {"article_id": "issue_42", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/YOLO.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-42/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_42.html"}}
{"id": 18569141006, "type": "news_chunk", "title": "Facebook’s Unruly Algorithm, AI That Does the Dishes, New Life", "subtitle": "Goodbye Tourists, Hello Labelers", "content": "Covid-19 has cost many workers their livelihood, but it has provided a lucky few on the lowest rungs of Africa’s machine learning industry with luxury suites.What’s new: Samasource, a data labeling company headquartered in San Francisco, California, is housing its East African workforce in hotels and resorts so they can continue to work while maintaining social distance, Wired reports.How it works: The pandemic prompted strict lockdowns in Kenya and Uganda, where Samasource employs some 2,000 workers. Many live in communities with no internet connectivity. So the company put up its workforce in four internet-equipped hotels that were vacant amid the coronavirus-driven collapse of tourism. Over half the company’s workforce in East Africa agreed to the arrangement. Employees each get a suite where they must remain throughout the workday. Housekeepers handle their laundry and nurses check their temperature daily.Wired profiled data-labeler Mary Akol (pictured in one of the photos above), one of 140 employees staying at the four-star Ole Sereni hotel, which overlooks Nairobi National Park.Workers there are allowed to leave their rooms at sunset to watch wildlife like rhinos, zebras, and giraffes from a terrace. They also engage in socially distanced group exercise. Akol has been teaching her co-workers salsa dancing — sans partners, of course. Behind the news: Several companies are providing jobs that help feed both the AI industry’s hunger for data and underserved communities. U.S.- and India-based iMerit has an all-female center in Kolkata that employs nearly 500 Muslim women who label computer vision data for companies like eBay, Microsoft, and TripAdvisor.Based in New York, Daivergent hires people on the autism spectrum to label data and helps neurodivergent people find tech jobs. Why it matters: Socially conscious outsourcing increases the tech industry’s talent pool by providing decent jobs to people who, because of geography, gender, race, or other factors, otherwise might be locked out.We’re thinking: The grocery industry’s Fair Trade labels help consumers distinguish between socially responsible employers and their wage-slashing competitors. A similar measure for AI would foster both growth and diversity.", "image_caption": "Series of pictures of hotels and resorts located in African countries", "metadata": {"article_id": "issue_42", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AFRICA.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-42/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_42.html"}}
{"id": 94531998001, "type": "news_chunk", "title": "AI's Progress Problem, Recognizing Masked Faces, Mapping...", "subtitle": "News", "content": "Dear friends, Last week, I wrote about the diversity problem in AI and why we need to fix it. I asked you to tell us about your experiences as a Black person in AI or share the names of Black colleagues you admire. Thank you to everyone who responded. It was heart-warming to hear from so many of you. Many of you shared your frustration with the lack of mentors who understand your challenges, the alienation of being the only Black face at professional meetings, and the struggle to overcome economic and social inequalities. Black women, especially, wrote about the difficulties of building a career in AI. Some of you described your efforts to support Black people in science and technology and provide tech resources to underserved communities. Thank you for sharing with us your dreams and also your disappointments. We will feature some of your stories in our Working AI blog series. Please stay tuned. One thing I love about the AI community is that many of us set the highest ideals for ourselves and our community — things like fairness, equity, and justice. Sometimes these ideals are so high, we may never fully live up to them, but we keep aspiring and keep trying. These days, I know it feels like society is falling far shorter of these ideals than we would like, but that’s why it’s more important than ever that we keep aspiring and keep trying. It will be a long road to vanquish racism, but working together, I believe we will get there. Keep learning! Vendors of face recognition are updating their tech as people don masks to protect against Covid-19. Police are bound to take notice.What’s new: Companies that provide computer vision systems, including at least one that supplies law enforcement agencies, are training models to recognize obscured faces, according to USA Today. Worldwide protests in support of civil rights for Black people have energized police interest in the technology while reigniting concerns about potential violations of civil liberties.What’s happening: With people’s noses, mouths, and chins obscured by masks, companies are retraining face recognition models to identify people based only on their upper faces. Some claim to have solved the problem. Rank One Computing, which provides face recognition systems to 25 U.S. police forces, recently upgraded its system to identify people by eyes and eyebrows.SAFR, which markets to its technology schools, claims its system recognizes masked faces with 93.5 percent accuracy, but only under perfect conditions.U.K.-based AI firm Facewatch, which targets retail companies, says its models recognize masked individuals.Several municipal and federal law enforcement agencies in the U.S. have collected face imagery from protests held in recent weeks.In March, researchers from Wuhan University released a trio of simulated and real masked-face datasets, including one with 5,000 real-world examples. The following month, U.S.-based startup Workaround published a dataset that contains 1,200 masked selfies scraped from Instagram. Behind the news: Many face recognition models have trouble identifying individuals even without masks, particularly members of minority groups, according to the U.S. National Institute of Standards and Technology. The agency announced plans to test the accuracy of masked face detection but suspended the effort amid the pandemic.Why it matters: Many U.S. law enforcement agencies are using face recognition to identify protesters. The questionable accuracy of these systems — particularly those aimed at masked individuals — could exacerbate the very injustices the current protests aim to highlight.We’re thinking: Face recognition technology cannot achieve its potential for good until the public can trust these systems are accurate and free of bias, both institutional and algorithmic.", "image_caption": "Face recognition system working on people wearing masks", "metadata": {"article_id": "issue_43", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/MASKED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-43/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_43.html"}}
{"id": 72052010001, "type": "news_chunk", "title": "NLP Special Issue! Powerful techniques from Amazon, Apple,", "subtitle": "An Explosion of Words", "content": "Dear friends, I’m thrilled to announce our new Natural Language Processing Specialization! Courses 1 and 2 are available on Coursera. We expect to release Courses 3 and 4 soon. NLP is reshaping daily life. No doubt you’ve found valuable information using web search and the search functions found on countless websites and apps. Anti-spam systems are a critical part of the global email system. How does a smart speaker understand your commands? How does a chatbot generate relevant responses? This specialization will give you the foundation you need to understand such systems and the knowledge to build them yourself. You will implement a sentiment analysis system, build models that translate human languages, and even construct a chatbot. You will master the most important NLP architectures including transformer networks, and you will receive practical, hands-on training to implement techniques like tokenizing text (turning words into features suitable for training neural networks or other machine learning algorithms). The courses are taught by two wonderful instructors: Younes Bensouda Mourri, with whom I’ve had the pleasure of working for many years at Stanford, and Łukasz Kaiser, a member of the Google Brain team whom you might recognize as a co-author of TensorFlow. I invite you to dive into the NLP Specialization and use the skills you gain to do amazing things. Keep learning! Not long ago, language models were confined to narrow topics and foiled by shifts in context. Today, they’re advancing rapidly thanks to innovations in model architecture, training methods, and distributed computing. Neural networks are translating languages, answering questions, summarizing texts, generating articles that can be indistinguishable from those written by reporters at the New York Times, and even popping off an occasional pun. This explosion makes it more important than ever that our models track subtle shades of meaning, grasp narrative logic, and choose words that are free of bias with respect to gender and ethnicity. In this special issue of The Batch, we probe the frontiers of NLP.", "image_caption": "Neural networks with pencils", "metadata": {"article_id": "issue_44", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/2-Intro.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-44/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_44.html"}}
{"id": 72052010002, "type": "news_chunk", "title": "NLP Special Issue! Powerful techniques from Amazon, Apple,", "subtitle": "AI Transformed", "content": "Noam Shazeer helped spark the latest NLP revolution. He developed the multi-headed self-attention mechanism described in “Attention Is All You Need,” the 2017 paper that introduced the transformer network. That architecture became the foundation of a new generation of models that have a much firmer grip on the vagaries of human language. Shazeer’s grandparents fled the Nazi Holocaust to the former Soviet Union, and he was born in Philadelphia in 1976 to a multi-lingual math teacher turned engineer and a full-time mom. He studied math and computer science at Duke University before joining Google in 2000. Below, he discusses the transformer and what it means for the future of deep learning.The Batch: How did you become interested in machine learning?Shazeer: I always liked messing around with the computer and probability was one of my favorite topics. My favorite course in grad school was a seminar where the class collaborated to write a crossword puzzle solver. We got to put together all kinds of different techniques in language processing and probabilities.The Batch: Was that your gateway to NLP?Shazeer: It was a great introduction to the field. They say a picture is worth 1,000 words, but it’s also 1 million times as much data. So language is 1,000 times more information dense. That means it’s a lot easier to do interesting stuff with a given amount of computation. Language modeling feels like the perfect research problem because it’s very simple to define (what’s the next word in the sequence?), there’s a huge amount of training data available, and it’s AI-complete. It’s great working at Google because it’s a language company.The Batch: How did the idea of self-attention evolve?Shazeer: I’d been working with LSTMs, the state-of-the-art language architecture before transformer. There were several frustrating things about them, especially computational problems. Arithmetic is cheap and moving data is expensive on today’s hardware. If you multiply an activation vector by a weight matrix, you spend 99 percent of the time reading the weight matrix from memory. You need to process a whole lot of examples simultaneously to make that worthwhile. Filling up memory with all those activations limits the size of your model and the length of the sequences you can process. Transformers can solve those problems because you process the entire sequence simultaneously. I heard a few of my colleagues in the hallway saying, “Let’s replace LSTMs with attention.” I said, “Heck yeah!”The Batch: The transformer’s arrival was hailed as “NLP’s ImageNet moment.” Were you surprised by its impact?Shazeer: Transformer is a better tool for understanding language. That’s very exciting, and it’s going to affect a lot of applications at Google like translation, search, and accessibility. I’ve been very pleasantly surprised by transfer learning for transformers, which really kicked off with BERT. The fact that you could spend a lot of computation and train a model once, and very cheaply use that to solve all sorts of problems.The Batch: One outcome is an ongoing series of bigger and bigger language models. Where does this lead?Shazeer: According to the papers OpenAI has been publishing, they haven’t seen any signs that the quality improvements plateau as they make the models bigger. So I don’t see any end in sight.The Batch: What about the cost of training these enormous models?Shazeer: At this point, computation costs 10-17 to 10-18 dollars per operation. GPT-3 was trained using 3×1023 operations, which would mean it cost on the order of $1 million to train. The number of operations per word is roughly double the parameter count, so that would be about 300 billion operations per word or roughly 1 millionth of a dollar per word that you analyze or produce. That doesn’t sound very expensive to me. If you buy a paperback book and read it, that costs around one ten-thousandth of a dollar per word. You can still see significant scaling up possible while finding cost-effective applications.The Batch: Where do you find inspiration for new ideas?Shazeer: Mostly building on old ideas. And I often find myself looking at the computational aspects of deep learning and trying to figure out if you could do something more efficiently, or something better equally efficiently. I wasted a lot of time in my first few years in deep learning on things that would never work because fundamentally they weren’t computationally efficient. A lot of the success of deep learning is because it runs many orders of magnitude faster than other techniques. That’s important to understand.The Batch: What’s on the horizon for NLP?Shazeer: It’s hard to predict the future. Translation of low-resource languages is one fun problem, and a very useful one to give way more people the opportunity to understand each other.The Batch: Who is your number-one NLP hero?Shazeer: There have been a massive number of people standing on each other’s shoulders.The Batch: But who stands at the bottom?Shazeer: I don’t know! From here, it looks like turtles all the way down.", "image_caption": "Illustration of Noam Shazeer", "metadata": {"article_id": "issue_44", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/3-QnA.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-44/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_44.html"}}
{"id": 72052010003, "type": "news_chunk", "title": "NLP Special Issue! Powerful techniques from Amazon, Apple,", "subtitle": "Outing Hidden Hatred", "content": "Facebook uses automated systems to block hate speech, but hateful posts can slip through when seemingly benign words and pictures combine to create a nasty message. The social network is tackling this problem by enhancing AI’s ability to recognize context.What’s new: Facebook built a hate speech detector designed to recognize that a statement like, “You are welcome here,” is benign by itself but threatening when accompanied by a picture of a graveyard. The model automatically blocks some hateful speech, but in most cases it flags content for humans to review.Key insight: Facebook extracts separate features from various aspects of a post. Then it melds the features to represent the post as a whole.How it works: The system examines 10 different aspects of each post including text, images, video, comments, and external context from the web. Separate models extract feature vectors from these elements, fuse them, and classify the post as benign or hate speech. The training and test data came from the company’s own Hateful Memes dataset. The researchers trained the system using a self-supervised method, hiding portions of input data and training the model to predict the missing pieces. They fine-tuned the resulting features on a labeled dataset of hateful speech. To extract vectors from text, the researchers used XLM-R, a pre-trained multilingual model trained on 100 languages.They used an object detection network to extract features from images and video. Facebook doesn’t specify the architecture in its production system, but the best baseline model on this dataset used Faster R-CNN.They fused vectors from various inputs using the approach known as early fusion, in which a model learns to combine features into a unified representation. Results: A BERT model achieved 59.2 percent accuracy on a text-only subset of Hateful Memes. The best multimodal classifier released by Facebook, ViLBERT, achieved 63.2 percent accuracy.Free money: If you think you can do better, there’s cash up for grabs in a competition for models that recognize hateful combinations of words and imagery. The contest is set to end in October.Why it matters: The need to stop the viral spread of hatred, fear, and distrust through social media seems to grow only more urgent with the passage of time. Numerous experts have drawn a connection between online hate speech and real-world violence.We’re thinking: What constitutes hate speech is hard for humans to agree on, never mind neural networks. There is a danger in policing speech either way. But there is greater danger in fanning flames of hostility on a global scale. Companies need strong, ethical leadership that can work with stakeholders to define limits on expressions of hatred. Then AI will be key in implementing such standards at scale. Meanwhile, we hope that blocking examples that are easiest to recognize opens room for reasoned debate about the edge cases. Learn how to extract the sentiment from text in Course 1 of the NLP Specialization from deeplearning.ai, available now on Coursera. To master more sophisticated techniques using neural networks and transformers, stay tuned for Courses 3 and 4, coming soon to Coursera.", "image_caption": "Illustration of a broken heart with a smirk in the middle", "metadata": {"article_id": "issue_44", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S1-Facebook.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-44/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_44.html"}}
{"id": 72052010004, "type": "news_chunk", "title": "NLP Special Issue! Powerful techniques from Amazon, Apple,", "subtitle": "What Were We Talking About?", "content": "Conversational agents have a tough job following the zigs and zags of human conversation. They’re getting better at it — thanks to yesterday’s technology.What’s new: Amazon recently improved the Alexa chatbot’s ability to identify the current topic of conversation. The system keeps its responses relevant by tracking the back and forth between itself and the user.Key insight: In conversation, the topic can shift fluidly. The meaning of a word that’s ambiguous in a single conversational exchange, such as “it,” is often clear in light of previous conversational turns. Evaluating several exchanges makes it possible to identify the current topic more accurately.How it works: The system recognizes 12 common topics (like politics, sports, fashion, books, and movies) and 14 intentions (like information request, opinion request, and general chat). The training data came from 100,000 conversations gathered in the 2017 Alexa Prize competition. Human annotators labeled a topic and intention for each statement. Each time a user or Alexa speaks, a 2017-vintage architecture known as a conditional adversarial domain network predicts the current dialog action.A pre-trained network extracts word vectors and passes them as a sequence to a biLSTM, a small, efficient recurrent layer that debuted in 2015.The biLSTM reads through what has already been said, word by word, forward and backward, to extract conversational features.Based on the features and dialog action, the biLSTM predicts the current topic. Results: Amazon evaluated its topic identifier using a test dataset collected alongside the training data. The system exceeded baseline accuracy of 55 percent to achieve 74 percent accuracy when it used context from five conversational exchanges.Why it matters: There’s plenty of life left in older techniques. Given the right data, algorithms from years ago can still do well on modern tasks. We’re thinking: Is it too much to ask that deep learning take its place alongside sports and fashion as one of the 12 topics? To learn about word vectors and how to use them in NLP, check out Courses 1 and 2 of the NLP Specialization from deeplearning.ai, now available on Coursera. Build powerful models using RNNs and LSTMs in the upcoming Course 3.", "image_caption": "Illustration of Amazon Alexa with a question mark inside of a thought bubble", "metadata": {"article_id": "issue_44", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S2-Amazon201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-44/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_44.html"}}
{"id": 72052010005, "type": "news_chunk", "title": "NLP Special Issue! Powerful techniques from Amazon, Apple,", "subtitle": "Choosing Words Carefully", "content": "The words “big” and “large” have similar meanings, but they aren’t always interchangeable: You wouldn’t refer to an older, male sibling as your “large brother” (unless you meant to be cheeky). Choosing among words with similar meanings is critical in language tasks like translation.What’s new: Google used a top language model to develop BLEURT, a way to compare translation models.Background: Machine learning engineers typically evaluate a translation model’s ability to choose the right words by translating a sentence from one language to another and back again. The metric called BLEU quantifies how far the re-translation’s meaning has drifted from that of the original sentence. But BLEU, which scores similarity on a 0-to-1 scale using an n-gram method, often misses nuances. BLEURT does a better job by training a language model to predict the semantic similarity between different sequences of words.Key insight: BERT is a general-purpose, unsupervised language model at the heart of many state-of-the-art systems. Fine-tuned on sentences that humans judge to be similar, it should learn to agree with human notions of similarity.How it works: BLEURT uses BERT to extract feature vectors from an original sentence and its re-translation. A linear layer predicts their similarity. The researchers created a dataset of millions of sentence pairs. Each pair includes a sentence from Wikipedia and a version modified by randomly deleting some words and replacing others with similar ones.The researchers used BLEU and other techniques to estimate the similarity between these pairs.They pre-trained BLEURT to predict those measures of similarity.Then they fine-tuned it on a smaller set of human-annotated data to predict human similarity scores. Results: The authors drew sentences from each of several datasets and created variations on them. BLEURT and BLEU ranked the similarity between each variation and the original, and the authors compared the Kendall Tau correlation, the percentage of pairs assigned the same order minus the percentage of pairs ordered differently, with the human ranking (which is given a score of 1.0). BLEURT achieved a Kendall Tau correlation of 0.338 while BLEU achieved 0.227 — a nice bump, although it leaves plenty of room for improvement.Why it matters: Language models have improved by leaps and bounds in recent years, but they still stumble over context. Better word choices could improve not only automatic translation but the gamut of language tasks including chat, text summarization, sentiment analysis, question answering, and text classification.We’re thinking: BLEU stands for Bilingual Evaluation Understudy. BERT stands for Bidirectional Encoder Representations from Transformers. Does anyone know what BLEURT stands for? Course 1 of the NLP Specialization from deeplearning.ai covers translation basics. Learn how to build a cutting-edge encoder/decoder attention model for translation in the upcoming Course 4, coming soon.", "image_caption": "Illustration of two translators on a scale", "metadata": {"article_id": "issue_44", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S3-Google.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-44/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_44.html"}}
{"id": 72052010006, "type": "news_chunk", "title": "NLP Special Issue! Powerful techniques from Amazon, Apple,", "subtitle": "Gender Bender", "content": "AI learns human biases: In word vector space, “man is to computer programmer as woman is to homemaker,” as one paper put it. New research helps language models unlearn such prejudices.What’s new: Double-Hard Debias improves on a previous algorithm to mitigate gender bias in trained language generators. Tianlu Wang developed the method with researchers at the University of Virginia and Salesforce.Key insight: The earlier Hard Debias works by identifying a masculine-to-feminine dimension in word vectors. Words that don’t have gender-specific meanings and, in popular word embeddings, fall at either end of this axis (such as doctor and nurse) are considered biased. Hard Debias compensates by shrinking the vector’s magnitude in this dimension. However, other work shows the relative frequency of words in various contexts distorts the feature space. For instance, grandfather appears as a genderless verb in legal discussions, where it means “to exempt,” while grandmother doesn’t, and that difference deforms grandfather’s gender dimension. Removing the dimension that encodes such alternative uses should make Hard Debias more effective.How it works: Double-Hard Debias removes this frequency-related dimension before adjusting for gender bias. (It doesn’t affect the processing of inherently gendered words identified by the researchers, such as he and she.) The researchers applied their method to several models that extract word embeddings including the popular GloVe. Double Hard Debias first identifies the most gender-biased words: those whose gender dimension falls farthest from the mean.It finds the dimensions that capture the most variability. These dimensions are most likely to distort the gender axis and therefore candidates for removal.It selects the candidate dimension with the most impact on gender by determining the effect of removing it on the gender-bias dimension of the words identified in the first step.Then it removes the selected frequency dimension from all word vectors.Finally, the original Hard Debias algorithm recalculates the gender dimension of the revised word vectors. Results: The researchers applied Double-Hard Debias and Hard Debias to separate models. They trained the models on two data subsets drawn from the OntoNotes corpus of informal speech. One was made up of biased statements (say, pairing doctor with he). The other comprised anti-biased statements (for instance, pairing doctor with she). Then they asked the models who he and she referred to. The difference in the Hard Debias model’s F1 scores when tested on the biased and unbiased data was 19.7. The difference in the Double Hard Debias model’s F1 scores was 7.7, showing that gender had a far smaller impact on its performance in the task.Why it matters: Bias in machine learning is a serious problem. A medical language model that assumes all doctors are male and all nurses female could make serious mistakes when reading medical reports. Similarly, a legal platform that equates sexual assault victim with female could lead to unjust outcomes. Solutions like this are crucial stopgaps on the way to developing less biased datasets. The model’s authors told The Batch that Double Hard Debias could be applied towards other types of bias, too.We’re thinking: If you’re building an NLP system, often bias won’t affect metrics like relevance or BLEURT results. But it’s important to attend to it anyway, because bias can have a significant unforeseen impact on users. We need the whole AI community to work hard to reduce undesirable biases wherever possible. Learn how to create NLP models using word vectors in Courses 1 and 2 of the NLP Specialization from deeplearning.ai. To use word vectors with deep neural networks, stay tuned for Course 3, available soon on Coursera.", "image_caption": "Illustration of a doctor and a nurse", "metadata": {"article_id": "issue_44", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S4-Salesforce.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-44/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_44.html"}}
{"id": 72052010007, "type": "news_chunk", "title": "NLP Special Issue! Powerful techniques from Amazon, Apple,", "subtitle": "Bigger is Better", "content": "Natural language processing lately has come to resemble an arms race, as the big AI companies build models that encompass ever larger numbers of parameters. Microsoft recently held the record — but not for long.What’s new: In February, Microsoft introduced Turing Natural Language Generation (Turing-NLG), a language model that comprises 17 billion parameters.Key insight: More parameters is better. More training data is better. And more compute is better. For the time being, these factors determine the state of the art in language processing.How it works: Like other recent large language models, Turing-NLG is based on the transformer architecture, which extracts features across long sequences of data without having to examine every element in between. Also like its immediate predecessors, it’s trained on unlabeled data via an unsupervised method, which enables it to absorb information from far more text than supervised models have available. Turing-NLG draws on knowledge stored in its parameter values to answer questions such as: “How many people live in the U.S.?”. It generates responses one word at a time depending on context provided by the preceding words. For example, it would have to generate “There are 328.2 million” before deciding to generate “people.”The researchers fine-tuned the model on multiple text summarization datasets to generate abstractive summaries, or summaries that use novel words rather than phrases drawn from source texts. This enables it to answer questions by summarizing relevant portions of reference data.Like many deep learning models, Turing-NLG is far too big to train on a single GPU. Instead, such models are divided into pieces and distributed to many processors that run in parallel. That approach incurs a cost in processing efficiency, as each chip must move redundant data to and from memory, and for an architecture as big as Turing-NLG, that inefficiency can be crippling. To train their gargantuan model, the researchers used techniques developed by Nvidia for Megatron to distribute the model efficiently, and Microsoft’s own ZeRO to schedule memory resources dynamically. Results: The researchers pitted Turing-NLG against Megatron. Turing-NLG improved state-of-the-art accuracy on the Lambada language understanding benchmark from 66.51 percent to 67.98 percent. It also improved perplexity (lower is better) on the WikiText of verified Wikipedia articles from 10.81 to 10.21.Yes, but: The race to build bigger and better language models doesn’t leave any breathing room even for engineers at the biggest tech powerhouses. Less than four months after Microsoft announced Turing-NLG, OpenAI detailed GPT-3. At 175 billion parameters, it’s roughly 10 times bigger and achieved 76.2 percent accuracy on Lambada.Why it matters: As language models balloon, so do scores on NLP benchmarks. Keep your seatbelts on: Microsoft says its approach to allocating hardware resources can scale past 1 trillion parameters.We’re thinking: The recipe of adding parameters, data, and compute for better performance has a long history. That today’s language models ingest far more text than a human could read in a lifetime reveals both the power of brute-force training and the algorithms’ inefficiency at learning. To learn how to build cutting-edge transformer models, stay tuned for Course 4 of the NLP Specialization from deeplearning.ai, coming soon.", "image_caption": "Talking bubbles inside talking bubbles", "metadata": {"article_id": "issue_44", "chunk_index": 7, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S5-Microsoft.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-44/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_44.html"}}
{"id": 72052010008, "type": "news_chunk", "title": "NLP Special Issue! Powerful techniques from Amazon, Apple,", "subtitle": "Found in Translation", "content": "Language models can’t correct your misspellings or suggest the next word in a text without knowing what language you’re using. For instance, if you type “tac-,” are you aiming for “taco,” a hand-held meal in Spanish, or “taca,” a crown in Turkish? Apple developed a way to head off such cross-lingual confusion.What’s new: It’s fairly easy to identify a language given a few hundred words, but only we-need-to-discuss-our-relationship texts are that long. Apple developed a way to tell, for example, Italian from Turkish based on SMS-length sequences of words.Key insight: Methods for identifying languages in longer text passages take advantage of well studied statistical patterns among words. Detecting languages in a handful of words requires finding analogous patterns among letters.How it works: The system comprises only a lightweight biLSTM and a softmax layer. This architecture requires half the memory of previous methods. A separate model narrows the possibilities by classifying the character set: Do the letters belong to Latin? Cyrillic? Hanzi? For instance, European languages and Turkish use the Latin alphabet, while Japanese and some Chinese languages use Hanzi.The biLSTM considers the order of input characters in both directions to squeeze out as much information as possible.Then it predicts the language based on the features it extracts. Results: The system can spot languages in 50 characters as accurately as methods that require lots of text. Compared with Apple’s previous method based on an n-gram approach, the system improves average class accuracy on Latin scripts from 78.6 percent to 85.7 percent.Why it matters: Mobile devices don’t yet have the horsepower to run a state-of-the-art multilingual language model. Until they do, they’ll need to determine which single-language model to call.We’re thinking: Humans are sending more and more texts that look like this: ????????????. We hope NLP systems don’t go ????. Learn how to build your own LSTM models for natural language processing in Course 3 of the NLP Specialization from deeplearning.ai, coming soon to Coursera.", "image_caption": "Illustration of two people talking with a typo", "metadata": {"article_id": "issue_44", "chunk_index": 8, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/S6-Apple2.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-44/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_44.html"}}
{"id": 64249727001, "type": "news_chunk", "title": "Who Still Sells Face Recognition to Police?, AI's Talent Pipeline", "subtitle": "Tech Giants Face Off With Police", "content": "Dear friends, I was dismayed on Monday to read that the U.S. is suspending the H1-B visa program at least through the end of the year. This effort to discourage immigration can only bring distress to workers from other countries and harm to the U.S. H1-B visas allow U.S. companies to bring in talent from around the world, enriching both their business and the economy. People from many different countries have been central to U.S. innovation in AI (see “Mapping AI’s Talent Pipeline” below). To me, H1-B holders aren’t just “workers.” They are my friends, students, and collaborators, and it pains me to see them facing the stress and uncertainty that comes with sudden, arbitrary shifts in immigration policy. Stanford University sponsored my H1-B visa many years ago, which enabled me to teach and do research there. It feels deeply unfair to deny the same opportunities to the next generation. We should do whatever we can to attract top talent, not turn it away. As a planet, we should be working to empower individuals to do their best work, wherever they may end up doing it. Through education, I remain committed to creating opportunities to learn and grow for as many people as I can. I hope the AI community will continue to transcend national borders and come together to build AI for the betterment of all. Keep learning! Three of the biggest AI vendors pledged to stop providing face recognition services to police — but other companies continue to serve the law-enforcement market.What’s new: Amid protests over police killings of unarmed Black people in the U.S., Amazon imposed a one year moratorium on licensing its Rekognition technology to police departments, and Microsoft announced a similar hiatus. Both said they would re-enter the market if the government imposed limits on police use of the technology. IBM exited the face recognition market altogether.Demand, meet supply: The big AI companies are highly visible, but most law enforcement agencies get the technology from lesser-known firms, the Wall Street Journal reported. Clearview AI has 2,400 police customers in the U.S. and Canada.NEC licenses face recognition to 20 law enforcement agencies.Ayonix, iOmniscient, and Herta Security each serve a handful of U.S. law enforcement agencies.The French company Idemia works with the New York Police Dept., the U.S. State Dept., and the U.S. Transportation Safety Administration as well as the European and Australian governments. Why it matters: Concern over fairness in law enforcement has renewed worries that unfettered use of face recognition leads to miscarriages of justice. Research spearheaded by MIT Media Lab researcher Joy Buolamwini showed that commercially available systems consistently misclassified women and people with darker complexions. A study by the American Civil Liberties Union found that Amazon’s system erroneously matched mugshots with the faces of 28 members of the U.S. Congress. Some police departments have misused the technology in ways that experts say could lead to mistaken arrests.We’re thinking: It’s great to see the big AI providers exercising responsibility. Now we need prudent regulation and auditing mechanisms geared to protect civil rights and support social justice.", "image_caption": "Face recognition system in a supermarket", "metadata": {"article_id": "issue_45", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Facecops.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-45/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_45.html"}}
{"id": 64249727002, "type": "news_chunk", "title": "Who Still Sells Face Recognition to Police?, AI's Talent Pipeline", "subtitle": "Mapping AI’s Talent Pipeline", "content": "China launches most of AI’s top researchers, but the U.S. is their number-one destination.What’s new: U.S.-based research group MacroPolo published the Global AI Talent Tracker. The report traces international trends in education and employment among elite engineers.Findings: The study tracked the locations of 675 high-performing AI practitioners through undergraduate studies, graduate school, and employment. Nearly 30 percent of the AI pros earned their undergraduate degree in China. Some 20 percent earned theirs in the U.S.More than half relocated to another country after their undergraduate education. The U.S. was their favorite destination by far.Over half of Chinese-educated researchers went to the U.S. for graduate school or employment.While the U.S. and China dominated the educational and employment pipeline, a significant number of undergraduates came from India, Europe, and Canada. Behind the news: MacroPolo sought to sample top AI talent by selecting its cohort at random from authors whose papers were accepted to NeurIPS 2019, one of the most prestigious and selective conferences in the field. The report’s conclusions align closely with previous studies that also used conference acceptance to track where AI researchers were educated and employed.Why it matters: Developing AI is a global project. Collaboration and freedom of movement are essential to progress.We’re thinking: The recent U.S. suspension of H1B visas will shatter dreams and disrupt lives. But the MacroPolo data shows that it will also be very damaging to U.S. innovation in AI. At a time when some countries are trying to make immigration harder, the AI community must redouble its effort to make sure that people from all over the world are welcome and able to contribute.", "image_caption": "Excerpts from Global AI Talent Tracker report", "metadata": {"article_id": "issue_45", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Talent6.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-45/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_45.html"}}
{"id": 64249727003, "type": "news_chunk", "title": "Who Still Sells Face Recognition to Police?, AI's Talent Pipeline", "subtitle": "Build Once, Run Anywhere", "content": "From server to smartphone, devices with less processing speed and memory require smaller networks. Instead of building and training separate models to run on a variety of hardware, a new approach trains a single network that can be adapted to any device.What’s new: Han Cai and researchers at MIT developed Once-for-All (OFA). This method trains a single large model and derives subnetworks — subsets of the original model’s weights — that perform well on less powerful processors.Key insight: Typical pruning methods downsize neural networks one at a time by reducing, say, the size and number of convolutional filters and then fine-tuning the smaller model. It’s more efficient to extract and fine-tune a fleet of progressively smaller models in a single process.How it works: OFA extracts subnetworks by varying the parent network’s number of layers, number of filters per layer, filter sizes, and the input resolution. The researchers constrained each of these factors to a predetermined set of values that allow up to 1019 possible subnetworks. OFA trains the original network, then randomly samples a slightly smaller version. Then it fine-tunes both.It repeats this procedure with ever smaller subnetworks until it arrives at the smallest allowable version.OFA randomly samples and evaluates 10,000 subnetworks. The results constitute a dataset that represents model performance at a given size.Using the new dataset, OFA trains another network to predict the accuracy of any subnetwork, so it can select the best network of a given size. Results: The authors compared OFA with a variety of neural architecture search methods suitable for finding models for mobile devices. The popular NASNet-A required 48,000 hours to generate the smallest model, and it would require that time again to generate another one optimized for different constraints. OFA’s baseline model required 1,200 hours to find all models. They also compared OFA to MobileNetV3-Large, the state-of-the-art image recognition network for mobile devices. The OFA model that ran on similar hardware achieved 76.9 percent top-one accuracy on ImageNet compared to MobileNetV3’s 75.2 percent. The most accurate neural search method the researchers considered, FBNet-C, required roughly half as much time as OFA to generate a single, less accurate model, but much more time to generate the second.Why it matters: OFA produces equivalent models of many sizes in slightly more time than it takes to train the original large models. In situations that require deploying a given network to heterogenous devices, this efficiency can translate into big savings in development time and energy consumption.We’re thinking: Smart speakers, watches, thermostats, pacemakers — it’s inevitable that neural networks will run on more and more heterogenous hardware. This work is an early step toward tools to manage such diverse deployments.", "image_caption": "Information related to the Once-for-All (OFA) method", "metadata": {"article_id": "issue_45", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/OFA.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-45/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_45.html"}}
{"id": 64249727004, "type": "news_chunk", "title": "Who Still Sells Face Recognition to Police?, AI's Talent Pipeline", "subtitle": "Baidu Leaves Partnership on AI", "content": "Baidu backed out of a U.S.-led effort to promote ethics in AI, leaving the project without a Chinese presence.What’s new: The Beijing-based search giant withdrew from the Partnership on AI, a consortium of businesses, nonprofits, and research organizations that promotes cooperation on issues like digital privacy and algorithmic bias, Wired reported.Mixed Motivations: Baidu joined the organization in 2018 as its first and only Chinese member. Other members include Amazon, Apple, Facebook, Google, and Microsoft as well as organizations like the U.N. and Human Rights Watch. Baidu explained its decision as a response to the partnership’s high membership fees and its on financial woes. Wired estimates that for-profit companies pay a few hundred thousand dollars for membership. Baidu’s 2019 revenue was $15.4 billion.Experts believe that Baidu’s departure had more to do with rising tensions between the U.S. and China. Behind the news: Much of the Partnership on AI’s work involves research and education. Recent projects include a symposium on bias in machine learning, a study of how organizations can deploy explainable AI, and an interactive blog that explains face recognition to the general public. The group doesn’t lobby politicians directly, but it does aim to make its research relevant to policymakers.Why it matters: The U.S. and China are the world’s top AI powerhouses. Baidu’s exit from the organization will affect its ability to influence AI globally toward fairness and justice.We’re thinking: The AI community spans many nations, and many bonds hold it together. Let’s keep building bridges and working to ensure that AI is used for the common good.", "image_caption": "Partnership in AI, Amazon, Baidu, Google, Facebook, IBM, Microsoft logos", "metadata": {"article_id": "issue_45", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Baidu4.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-45/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_45.html"}}
{"id": 64249727005, "type": "news_chunk", "title": "Who Still Sells Face Recognition to Police?, AI's Talent Pipeline", "subtitle": "Misleading Metrics", "content": "A growing body of literature shows that some steps in AI’s forward march may actually move sideways. A new study questions advances in metric learning.What’s new: Kevin Musgrave and researchers at Cornell Tech and Facebook AI re-examined hallmark results in models that learn to predict task-specific distance metrics, specifically networks designed to quantify the similarity between two images. They found little evidence of progress.Key insight: Researchers have explored metric learning by experimenting with factors like architectures, hyperparameters, and optimizers. But when those factors aren’t held constant, comparisons with earlier approaches can’t be apples-to-apples. Improved results often reflect advances in the surrounding components (such as hyperparameter tuning), not in the metric learning algorithm itself.How it works: Models that assess similarity between images typically extract features and predict a distance between them. The distances may be learned through a metric loss function, while features are often extracted from pre-trained networks. The authors reviewed 12 of the most popular papers on this topic. They point out common causes of invalid comparisons and present a new approach that levels the playing field. Several papers compare a ResNet50 to a GoogLeNet trained using different methods. A ResNet50 is larger and outperforms a GoogLeNet on other image processing tasks, so it’s no surprise the ResNet50 performs better at metric learning.Many researchers don’t use a validation set but still tune hyperparameters. Presumably, they chose hyperparameter values based on the models’ performance on test sets — a big no-no.These two flaws, and a list of smaller mistakes, inspired the authors to propose a consistent test bed for metric learning research. Their benchmark calls for BN-Inception networks, RMSprop optimizers, and cross-validation for hyperparameter search. Results: The authors reproduced and benchmarked many past approaches on the CUB200, Cars 196, and Stanford Online Products datasets. Controlling for confounding variables, their analysis shows that metric learning hasn’t improved since 2006 (as shown in the plots above).Why it matters: Image similarity is a key component of many products (such as image-based search). Knowing what really works is key to helping practitioners build useful applications as well as drive further research.We’re thinking: Metric learning still has a distance to go.", "image_caption": "Excerpt from study about models that learn to predict task-specific distance metrics", "metadata": {"article_id": "issue_45", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/METRIC.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-45/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_45.html"}}
{"id": 65992005001, "type": "news_chunk", "title": "Bias in Plain Sight, Apple's New AI, Amazon's Virtual Fitting...", "subtitle": "News", "content": "Dear friends, We know that biased data leads to biased machine learning. But does the problem go beyond that? A few colleagues asked about this after a heated exchange on Twitter between Yann LeCun and Timnit Gebru (see “Image Resolution in Black and White” below). There are plenty of documented examples of biased data contributing to bad outcomes. But suppose we find purely unbiased data and build an AI system that helps lenders optimize interest rates for payday loans. We’re careful to make sure the data, algorithms, and learned models don’t discriminate unfairly against any disadvantaged or minority group. Our results are unbiased and in the clear, right? Unfortunately, no. Payday loans are quick-turnaround loans often with very high interest rates — in California, a lender can charge 459 percent interest on a $100, 14-day loan. They target low income individuals. In the U.S., they’re used disproportionately by the Black community. Thus even a fair algorithm will hurt this community especially. Beyond biased data, the way we frame problems, choose what to build, and choose where to deploy can add to or subtract from problems of bias and privilege. An “unbiased” AI technology operating in an unfair social system can contribute to biased outcomes. We still have a lot of work ahead to address harmful biases throughout society. Twenty years ago, the AI community was a small group working on an exciting but obscure technology. Today our community is large, worldwide, and rapidly growing, and we contribute to applications at the center of daily life. We have a greater responsibility than ever to educate ourselves not only in the technology but also in its social context. It’s not always easy to foresee the indirect impact of our work. Who would have guessed that a poorly designed software implementation to enable freedom of speech would lead to toxic communications on social media? But with a broader perspective, I hope our community can better understand the impact of our work and make better decisions about how to help society move forward with greater fairness and less bias. Keep learning! A new model designed to sharpen images tends to turn some dark faces white, igniting fresh furor over bias in machine learning.What’s new: Built by researchers at Duke University, Photo Upsampling via Latent Space Exploration (Pulse) generates high-resolution versions of low-resolution images. It sparked controversy when it transformed a pixelated portrait of Barack Obama into a detailed picture of a white man.How it works: Most upsampling models are trained to generate high-res output from low-res input. Pulse creates a series of high-res images progressively optimized to match the low-res source. Pulse uses StyleGAN, a pre-trained generative adversarial network, to generate a new image from the original and an input vector.The system downsamples the generated image to the original’s resolution. Then it compares the two.It modifies the input vector based on the differences and repeats the process 100 times to arrive at its final output.Human judges scored Pulse’s output more realistic than that of four competing models. The computer-based assessment Natural Image Quality Evaluator (NIQE) rated Pulse’s images higher than those in a database of high-res celebrity portraits. The controversy: Twitter user Chicken3gg revealed Pulse’s bias using a downsampled photo of the former U.S. president. That prompted machine learning engineer Robert Osazuwa Ness to try it on blurred images of U.S. Senator Kamala Harris, actress Lucy Liu, and other nonwhite individuals. The system whitewashed them, too, and also interpreted some female faces as male. The incident triggered an online debate that drew in major figures in the AI community.Pulse’s authors blamed racially imbalanced training data. When they trained their system on a more diverse dataset, its accuracy with respect to race ranged from 79 percent to 90 percent.Facebook’s AI chief Yann LeCun echoed the notion that the system’s bias resulted from racially lopsided training data. Timnit Gebru, co-leader of Google’s ethical AI team, shot back that focusing on data alone downplays systemic bias in the machine learning community. As the argument grew heated, LeCun announced his withdrawal from Twitter. Why it matters: Flawed AI leads to real harm. In January, police in Detroit arrested an African-American man for theft after he was misidentified by a face recognition system. Such systems have been shown to misclassify Black people. We’re thinking: Upscaling powered by machine learning is making images sharper on televisions and microscopes. The AI community has a pressing need for tests and audit procedures to ensure that such technology is trustworthy and free of bias.", "image_caption": "Examples of high-resolution versions of low-resolution images.", "metadata": {"article_id": "issue_46", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Obama203.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-46/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_46.html"}}
{"id": 61984344001, "type": "news_chunk", "title": "Biased Datasets, AI for Footballers, Transformers for Object", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, I am appalled by the policy, announced on Monday by U.S. Immigrations and Customs Enforcement (ICE), that international students in the country on an F-1 visa must leave if their school goes fully online to cope with Covid-19. Two weeks ago, I wrote about the suspension of H1-B visas for foreign workers. The policy unveiled this week will deepen the pain of young people who are aiming to contribute to society and further deprive the U.S. of much-needed talent. The new policy, which is being called the #StudentBan on social media, is cruel and capricious. Sometimes an entire family may pool their savings to send someone to study and give them a brighter future. Imagine being halfway to earning a degree and suddenly forced to leave the country amid the pandemic, when your home country may have closed its borders, even to citizens. Students have confided to me their worries about letting down their family or being unable to afford a plane ticket home. University faculty and administrators are scrambling to offer in-person classes, even if it may not be safe or may have little pedagogical benefit, just for the purpose of protecting their international students from deportation. They were already struggling to manage campus shutdowns. This policy delivers another blow at a time when they least can afford it. The U.S. is known worldwide as a great place to receive an education. That’s why I came here many years ago — on an F-1 visa — to attend college. If the U.S. loses this reputation, the whole world will be poorer for it. If my daughter ever studies overseas, I hope that whatever country hosts her will treat her with greater kindness and respect than the U.S. is extending to our international students. Keep learning, Growing up in Mauritania, Adama Diallo was fascinated by the human brain. Now, as an AI developer at the software company Paradigm, he’s using artificial intelligence to map architectural spaces. In this edition of our Working AI series, Adama discusses his projects, advice for learners, and views on social bias in the industry. Learn more", "image_caption": "Adama Closeup 2", "metadata": {"article_id": "issue_47", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Adama20Closeup202.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-47/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_47.html"}}
{"id": 61984344002, "type": "news_chunk", "title": "Biased Datasets, AI for Footballers, Transformers for Object", "subtitle": "News", "content": "MIT withdrew a popular computer vision dataset after researchers found that it was rife with social bias.What’s happening: Researchers found racist, misogynistic, and demeaning labels among the nearly 80 million pictures in Tiny Images, a collection of 32-by-32 pixel color photos. MIT’s Computer Science and Artificial Intelligence Lab removed Tiny Images from its website and requested that users delete their copies as well. What the study found: Researchers at the University College of Dublin and UnifyID, an authentication startup, conducted an “ethical audit” of several large vision datasets, each containing many millions of images. They focused on Tiny Images as an example of how social bias proliferates in machine learning. Psychologists and linguists at Princeton in 1985 compiled a database of word relationships called WordNet. Their work has served as a cornerstone in natural language processing.Scientists at MIT CSAIL compiled Tiny Images in 2006 by searching the internet for images associated with words in WordNet. The database includes racial and gender-based slurs, so Tiny Images collected photos labeled with such terms. What the dataset’s creators said: “Biases, offensive and prejudicial images, and derogatory terminology alienates [sic] an important part of our community — precisely those that we are making efforts to include,” the researchers who built Tiny Images said in a statement. Behind the news: Social bias — in data an d models, in the industry, and in society at large — has emerged as a major issue in the machine learning community. Concerns over bias in AI reignited last week after a generative model called Pulse converted a pixelated picture of Barack Obama, who is Black, into a high-resolution image of a white man.The compilers of ImageNet recently culled labels — also based on WordNet — deemed biased or offensive from the dataset’s person subtree. Why it matters: Social biases encoded in training data become entwined with the foundations of machine learning. WordNet transmitted its derogatory, stereotyped, and inaccurate information to Tiny Images, which may have passed them along to countless real-world applications. We’re thinking: As AI practitioners, we have a responsibility to re-examine the ways we collect and use data. For instance, Cifar-10 and Cifar-100 were derived from TinyImages. We’re not aware of biases in those datasets, but when one dataset’s bias may propagate to another, it’s necessary to track data provenance and address any problems discovered in an upstream data source. Recent proposals set standards for documenting models and datasets to weed out harmful biases before they take root.", "image_caption": "Tiny Images photos and datasets", "metadata": {"article_id": "issue_47", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Dataset202.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-47/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_47.html"}}
{"id": 95985864001, "type": "news_chunk", "title": "Easier Shopping, Smarter Manufacturing, Scarier Monsters,", "subtitle": "Assembly Line AI", "content": "Dear friends, Last week, I wrote about the U.S. Immigration and Customs Enforcement (ICE) policy that would have forced international students to leave the country if their university went fully online to manage the risk of Covid-19. This sudden change in the rules for student visas had students and institutions alike scrambling to figure out ways to comply. Social media erupted in protest as students, parents, teachers, and administrators expressed their concerns. Harvard and MIT sued to block the policy. Attorneys general in at least 18 states brought lawsuits as well. Yesterday, the government rescinded the policy, allowing international students to remain in the U.S. even if they take all their courses online. I am thrilled! ICE’s retreat is an important reminder that our voices can make a difference. I have little doubt that the public outcry helped motivate the universities to sue and the government to backtrack. I believe we all have a responsibility to speak out against injustice — respectfully and with cogent arguments, not “flame wars.” Even if each individual voice is just one among many, collectively we can make a huge impact. Speaking out is especially important for the AI community as we grapple with difficult issues of bias, privacy, surveillance, and disinformation. We need every voice — including yours — to fulfill AI’s promise for the benefit of all people. Keep learning! Computer vision has been learning how to spot manufacturing flaws. The pandemic is accelerating that education.What’s happening: Companies like Instrumental and Elementary are making AI-powered cameras that automate the spotting of damaged or badly assembled products on factory assembly lines, Wired reports. (For the record, deeplearning.ai’s sister company Landing AI is, too.)How it works: Instrumental’s quality-control system first learns to recognize components in their ideal state and then to identify defects. It can spot faulty screws, disfigured circuit boards, and flaws in the protective coating on smartphone screens. Cameras along the assembly line take photos of products in the making. The manufacturer’s engineers review the images and label defects. The labeled data is used to fine-tune the system.Manufacturers often don’t allow outsiders direct access to their equipment, so Instrumental’s engineers typically tweak systems on-site. Amid the pandemic, though, five clients are allowing the company to monitor the assembly line remotely, making it possible to update the computer vision model on the fly. Coming soon: Elementary plans to install robotic cameras in a U.S. Toyota plant. Workers will place a completed part beneath the camera for inspection, then press a button to indicate whether they agree with the robot’s assessment to fine-tune the model.Behind the news: Omron, Cognex, and USS Vision have sold non-neural inspection systems for decades. Neural networks are making their way into the field as engineers develop techniques for learning what flaws look like from small numbers of examples.Why it matters: Earlier automated inspection systems use hand-coded rules to identify specific flaws. Machine learning promises to be more adaptable and quicker to deploy. That could speed up assembly lines and cut manufacturing costs.We’re thinking: The ability to learn from small amounts of data is the key to many applications of deep learning that are still beyond reach. We look forward to continued progress in this area.", "image_caption": "AI-powered camera spotting a damaged product", "metadata": {"article_id": "issue_48", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/QC1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-48/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_48.html"}}
{"id": 95985864002, "type": "news_chunk", "title": "Easier Shopping, Smarter Manufacturing, Scarier Monsters,", "subtitle": "That Online Boutique, But Smarter", "content": "Why search for “a cotton dress shirt with button-down collar, breast pockets, barrel cuffs, scooped hem, and tortoise shell buttons in grey” when a photo and the words “that shirt, but grey” will do the trick? A new network understands the image-text combo. (This is the second of three papers presented by Amazon at the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). We’ll cover the third one next time.)What’s new: Online stores offer all kinds of clothing, but search engines may suggest items of a different color or style than you want. Visiolinguistic Attention Learning, developed by Yanbei Chen with researchers at Queen Mary University of London and Amazon, hones product searches based on text input from shoppers.Key insights: If you can create a picture that approximates the ideal product, you can search for similar images. Generating realistic images is hard, but comparing extracted features is much easier.How it works: VAL learns to modify features extracted from a product image according to text input such as “I want it to have a light floral pattern.” Then it searches for other products with features similar to the modified product features. VAL learned from datasets that provide an image paired with text as input, and a photo of the corresponding product as output.VAL contains a text encoder network and an image encoder network. The image encoder extracts image features at a few levels of detail, for instance shapes and textures.A pair of transformers fuses the text and image features at each level of detail.One transformer is a variation on self-attention transformers. It identifies relationships between image and text features, and adjusts the image features to agree with the text features.The second transformer learns to identify features that are unchanged in the new product and copies them without modification.The element-wise sum of both transformers comprises the desired product’s features. VAL compares them with features extracted from product images in its database and returns the closest matches. Results: The researchers put VAL head-to-head against TIRG, the previous state of the art in image search with text feedback using the Fashion200K dataset of garment photos with text descriptions. VAL achieved 53.8 percent recall of the top 10 recommended products, the fraction of search results that are relevant, compared to TIRG’s 43.7 percent. VAL also outperformed TIRG on the Shoes and FashionIQ datasets.Why it matters: VAL provides a new method for interpreting images and text together, a useful skill in areas where either one alone is ambiguous.We’re thinking: We’ll take the blue shirt!", "image_caption": "Examples of clothes image-text combo search", "metadata": {"article_id": "issue_48", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/VAL.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-48/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_48.html"}}
{"id": 95985864003, "type": "news_chunk", "title": "Easier Shopping, Smarter Manufacturing, Scarier Monsters,", "subtitle": "AI Makes Headlines", "content": "Which headline was written by a computer?A: FIFA to Decide on 2022 World Cup in MarchB: Decision in March on 48-team 2022 World Cup, Says InfantinoWhat’s new: Researchers at Primer, an AI-driven document analysis company, introduced an automatic headline generator. In an appealing twist, some articles that human publishers had tried to tart up with clickbait — for instance, You’ll Never Guess Which U.S. Counties Grew the Fastest — the model gave a sensible, informative headline: MacKenzie County in North Dakota Had Highest Population Growth in Entire U.S.How it works: A headline is a very short document summary. Summarizers come in two flavors. Extractive models use only sentences or phrases from the text itself, building summaries that are closely related to the source but may be narrow or off-point. Abstractive models create new text based on an independent dictionary, synthesizing fresh but potentially confused summaries. Primer developed a hybrid model that generates abstractive headlines using vocabulary found in the document. The authors fine-tuned a Bert Question-Answer model from Hugging Face on 1.5 million news story/headline pairs drawn from sources including the New York Times, BBC, and CBC.The model frames headline generation as a series of question-answer tasks. The question is the beginning of the headline and the answer is the passage that makes up the next part. The model iterates this process sequentially through the document.The researchers also adapted the model to create bullet-point summaries of news articles, financial reports, and even movie plots — though not perfect. For instance, it declared that the character named Maverick in the 1986 Tom Cruise hit Top Gun enters a romantic relationship with his co-pilot Goose, rather than his instructor, per the actual plot. Results: Human evaluators each read 100 news stories and graded two accompanying headlines, one written by a person and the other by the model. The computer-generated headlines scored slightly better overall. The model performed best on short-form journalism but stumbled on longer articles, probably because key information in longer items is more spread out.Behind the News: Earlier headline generation methods mostly use an encoder-decoder to produce abstractive results. Unlike the new model, the encoder-decoder approach can generate any possible headline but risks poor grammar, factual inaccuracy, and general incoherence.Why it matters: Imagine a world without clickbait!We’re wondering: The computer wrote option A. Did you guess correctly?", "image_caption": "Examples and explanation of an automatic headline generation", "metadata": {"article_id": "issue_48", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Headline.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-48/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_48.html"}}
{"id": 95985864004, "type": "news_chunk", "title": "Easier Shopping, Smarter Manufacturing, Scarier Monsters,", "subtitle": "Monsters in Motion", "content": "How do you control a video game that generates a host of unique monsters for every match? With machine learning, naturally.What’s new: The otherworldly creatures in Source of Madness learn how to target players through reinforcement learning, the developers told The Batch.How it works: Players battle an infestation of fiends in a procedurally generated, side-scrolling wasteland. At the start of each level, the game uses non-neural computation to slap together a menagerie of unique monsters, each an assemblage of spidery legs, fireball-spitting tentacles, and bulbous carapaces. The monsters become more powerful as the game progresses.The endless variety of monsters makes traditional game-control techniques impractical. Instead, a feed-forward network trained on a sandbox simulation of the game receives a reward for a monster’s every step toward a player.The reinforcement learning environment comes from Unity, which makes 3D software development tools.The game’s developer, Carry Castle, is still fine-tuning it. The release date hasn’t been set, but you can request a test version here. Behind the news: Most commercial titles use rules-based systems to control non-player characters. But some games have had success experimenting with neural networks. Supreme Commander 2, a war game similar to Starcraft, uses neural networks to decide whether the computer’s land, airborne, and naval units will fight or flee.The racing series Forza trains networks to imitate a human player’s style, such as how they take corners or how quickly they brake. These agents compete against other humans to earn points for the one they mimic. Why it matters: Machine learning is infiltrating games as developers seek to build virtual worlds as variable and surprising as the real one. We’re thinking: To all monsters, we say: keep learning!", "image_caption": "Takes from videogame Source of Madness", "metadata": {"article_id": "issue_48", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Madness204.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-48/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_48.html"}}
{"id": 9179574001, "type": "news_chunk", "title": "Corporate Deepfakes, Robot Chemists, Smart Boutiques, Neural Nets", "subtitle": "News", "content": "Dear friends, I received a copy of Why We Sleep: Unlocking the Power of Sleep and Dreams as a Christmas gift — back in the pre-Covid era — and finished it last weekend. This book by Matthew Walker, director of UC Berkeley’s sleep and neuroimaging lab, is a useful reminder of the importance of sleep for learning and also for physical and mental health. Say you spend a few hours learning something new on Wednesday. Getting a solid night of sleep the same day will help consolidate the new memories and strengthen your long-term retention. If your sleep on Wednesday night is disrupted, your long-term retention will be affected even if you catch up on sleep later in the week. But the story doesn’t end there. Over the next few days, your brain may still be busy consolidating the new learnings. A surprising study showed that even if your sleep is disrupted on Friday — two days later — long-term retention can still be significantly affected. Bottom line: After you spend time studying during the day, I encourage you to get a good night’s sleep. Even better, try to get a good night’s sleep every night. The world is going through turbulent times. With society buffeted by biological, social, and political forces, who has time for sleep?! I try to sleep from midnight to 8 a.m. every day, including weekends. With an 18-month-old daughter who wakes up whenever she wants, and occasional meetings with business partners in Asia or in Europe at odd hours, my sleep schedule is far from perfect. You’re probably incredibly busy as well. Despite everything going on, I make sleep a priority, and I hope you will, too. Keep learning, A mechanical lab assistant could accelerate chemistry research.What’s new: Researchers at the University of Liverpool trained a mobile robot arm to navigate a lab, operate equipment, handle samples, and obtain results far faster than a human scientist. The authors believe their system is the first mobile robot capable of running lab experiments. How it works: In a recent study, the articulated arm on wheels completed 688 experiments, testing various hypotheses to extract hydrogen from water efficiently using chemicals and light. The system navigates using lidar, so it can operate in the dark.The researchers divided the lab into a series of stations devoted to specific procedures. Upon arriving at each station, the arm calibrated its position by tapping the sides of cubes that the scientists had mounted next to each piece of gear.The arm is topped with a gripper for mixing chemical samples and operating laboratory equipment.A Bayesian optimization model uses the results of each experiment to update the next round by adjusting one of 10 variables, such as the chemical mixture. Results: The study discovered chemical formulae that made it easier to separate hydrogen from oxygen in water. More important, it proved that a robot can do such work effectively, speedily, and without interruption. The authors estimate that a human scientist would have taken 1,000 times longer to produce similar results.Why it matters: The authors hope to offer robots for sale within 18 months. The $150,000-plus price tag might be a bargain if the Covid-19 pandemic makes in-person lab experimentation unfeasible.We’re thinking: Most factory automation involves stationary robots positioned along a manufacturing line. Perhaps mobile manipulation — where the arm moves to the object being manipulated — will prove to be more efficient for automating science labs.", "image_caption": "Mobile robot arm lab assistant working", "metadata": {"article_id": "issue_49", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize2014.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-49/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_49.html"}}
{"id": 20682921001, "type": "news_chunk", "title": "Robot Farm Workers, Making Algorithms Rock, Automating Contract", "subtitle": "Agbots Want Jobs Americans Don’t", "content": "Dear friends, Over the weekend, we hosted our first Pie & AI meetup in Kuala Lumpur, Malaysia, in collaboration with the AI Malaysia group, MDEC, and ADAX. The event was part of Malaysia’s AI & Data Week 2019. Several people traveled from neighboring southeast Asian countries to attend! I’m glad to see so many AI communities growing around the world, and I’m excited to bring more exposure to them. If you’d like to partner with us for a Pie & AI event, I hope you’ll drop us a note at [email protected]. Keep learning! Advances in computer vision and robotic dexterity may reach the field just in time to save U.S. agriculture from a looming labor shortage.What happened: CNN Business surveyed the latest crop of AI-powered farmbots, highlighting those capable of picking tender produce, working long hours, and withstanding outdoor conditions.Robot field hands: Harvest bots tend to use two types of computer vision: one to identify ripe fruits or vegetables, the other to guide the picker. Vegebot, a lettuce harvester developed at the University of Cambridge, spots healthy, mature heads of lettuce with 91 percent accuracy and slices them into a basket using a blade powered by compressed air. The prototype harvests a head in 30 seconds, compared to a human’s 10-second average. The inventors say with lighter materials, they could catch up.Agrobot’s strawberry-picking tricycle straddles three rows of plants. It plucks fragile berries using up to 24 mechanical hands, each equipped with a camera that grades the fruit for ripeness.California’s Abundant Robotics built a rugged, all-weather autonomous tractor that vacuums up ripe apples (pictured above). Behind the news: Unauthorized migrants do as much as 70 percent of U.S. harvest work, according to a study by the American Farm Bureau Association. Tighter immigration policies and improving opportunities at home increasingly keep such workers out of the country.Why it matters: The shortage of agricultural workers extends across North America. During harvest season, that means good produce is left to rot in the fields. The situation costs farmers millions in revenue and drives up food prices.Our take: The robots-are-coming-for-your-job narrative often focuses on people put out of work but fails to acknowledge that workers aren’t always available. Between a swelling human population and emerging challenges brought on by climate change, the agriculture industry needs reliable labor more than ever. In some cases, that could be a machine.", "image_caption": "Harvest bot", "metadata": {"article_id": "issue_5", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-video-to-gif201-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-5/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_5.html"}}
{"id": 20682921002, "type": "news_chunk", "title": "Robot Farm Workers, Making Algorithms Rock, Automating Contract", "subtitle": "Speech Recognition With an Accent", "content": "Models that achieve state-of-the-art performance in automatic speech recognition (ASR) often perform poorly on nonstandard speech. New research offers methods to make ASR more useful to users with heavy accents or speech impairment. What’s new: Researchers at Google fine-tuned ASR neural networks on a data set of heavily accented speakers, and separately on a data set of speakers with amyotrophic lateral sclerosis (ALS), which causes slurred speech of varying degree. Their analysis shows marked improvement in model performance. The remaining errors are consistent with those associated with typical speech. Key insight: Fine-tuning a small number of layers closest to the input of an ASR network produces good performance in atypical populations. This contrasts with typical transfer learning scenarios, where test and training data are similar but output labels differ. In those scenarios, learning proceeds by fine-tuning layers closest to the output. How it works: Joel Shor and colleagues used data from the L2-ARCTIC data set for accented speech and ALS speaker data from the ALS Therapy Development Institute. They experimented with two pre-trained neural models, RNN-Transducer (RNN-T) and Listen-Attend-Spell (LAS). The authors fine-tuned both models on the two data sets with relatively modest resources (four GPUs over four hours). They measured test-set performance on varying amounts of new data.They compared the sources of error in the fine-tuned models against models trained on typical speech only. Results: RNN-T achieved lower word error rates than LAS, and both substantially outperformed the Google Cloud ASR model for severe slurring and heavily accented speech. (The three models were closer with respect to mild slurring, though RNN-T held its edge.) Fine-tuning on 15 minutes of speech for accents and 10 minutes for ALS brought 70 to 80 percent of the improvement.Why it matters: The ability to understand and act upon data from atypical users is essential to making the benefits of AI available to all.Takeaway: With reasonable resources and additional data, existing state-of-the-art ASR models can be adapted fairly easily for atypical users. Whether transfer learning can be used to adapt other types of models for broader accessibility is an open question.", "image_caption": "Average Relative WER improvement as a function of the amount of training data", "metadata": {"article_id": "issue_5", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Speech20Recognition.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-5/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_5.html"}}
{"id": 20682921003, "type": "news_chunk", "title": "Robot Farm Workers, Making Algorithms Rock, Automating Contract", "subtitle": "Generative Models Rock", "content": "AI’s creative potential is becoming established in the visual arts. Now musicians are tapping neural networks for funkier grooves, tastier licks, and novel subject matter.What happened: Aaron Ackerson, whom the Chicago Sun-Times called “a cross between Beck and Frank Zappa,” produced his latest release with help from the latest generation of generative AI. MuseNet helped generate the music and GPT-2 suggested lyrics. DeepAI’s Text To Image API synthesized the cover art.Making the music: “Covered in Cold Feet” began its existence as an instrumental fragment scored for violin, piano, and bass guitar. Ackerson fed a two-bar MIDI file into MuseNet, which spat out a few more bars based on his raw material.He tweaked MuseNet’s output to his liking using his digital audio workstation. Then he fed that material back to MuseNet, which expanded by a few bars more, repeating the process until he had a composition he was happy with.“Early in the process, I had MuseNet generate continuations of my simple idea in a lot of different styles, and most did not find their way into the finished song,” Ackerson said in an interview with The Batch. “The one I decided to use followed the main melody with what would later turn into the beginning of the guitar solo.”That solo combusts into righteous shredding near the 1:25 mark, a triumph of manual dexterity as much as AI. Writing the lyrics: The groove reminded Ackerson of the band Phish. So he fed a list of that band’s song titles to Talk to Transformer, an online text-completion app based on the half-size version of GPT-2. “Covered in Cold Feet” was his favorite of its responses to the original list.He repeatedly fed Talk to Transformer the phrase “covered in cold feet again,” and curated the lyrics from its responses.Talk to Transformer doesn’t generate rhymes, so Ackerson added them manually. Behind the music: The artist composed his first AI-assisted song in 2017 using the Botnik Voicebox text generator. He fed the model bass and melody lines from 100 of his favorite songs translated into solfège (a note-naming system that maps the tones in any musical key to the syllables do, re, mi, and so on). The model spat out fresh note pairings, many of which he had never before considered using. The result, “Victory Algorithm,” is a slide guitar-fueled psychobilly foot stomper. We’re thinking: AI skeptics worry that computers, if allowed to do creative work, will erase humanity from art. No worries on that point: Ackerman’s personality comes through loud and clear. We look forward to hearing more from musicians brave enough to let computers expand their creative horizons. (For more on MuseNet, see our interview with project lead Christine Payne.)", "image_caption": "Person playing a musical instrument", "metadata": {"article_id": "issue_5", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Generative20Models.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-5/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_5.html"}}
{"id": 20682921004, "type": "news_chunk", "title": "Robot Farm Workers, Making Algorithms Rock, Automating Contract", "subtitle": "My Chatbot Will Call Your Chatbot", "content": "Companies with large numbers of contractual relationships may leave millions of dollars on the table because it’s not practical to customize each agreement. A new startup offers a chatbot designed to claw back that money.What happened: Pactum, a startup that automates basic vendor and service contracts at immense scale, emerged from stealth with a $1.15 million investment from Estonian tech upstart Jaan Tallinn and his posse of Skype alumni.How it works: Let’s say a prominent computer company develops a new laptop and hires Pactum to cut distribution deals with hundreds of thousands of computer stores around the globe. Pactum’s AI model reviews the computer maker’s existing contracts to establish baseline terms.Then it examines variables such as pricing, schedule, and penalties in search of more favorable arrangements. For instance, it may seek to improve cash flow by asking retailers to pay for orders faster.The AI then initiates negotiations via chatbot.The model automatically updates contract terms as negotiations proceed. Behind the news: Contracts are a hot area for AI. In 2015, Synergist.io and Clause launched automated platforms that mediate contract negotiations. And last year, Dutch information services firm Wolters Kluwer acquired legal AI startups CLM Matrix and Legisway.Why it matters: Standardized contracts can save time and effort spent customizing agreements. But they also bring costs. A 2018 study by KPMG estimated that standard contracts can soak up between 17 and 40 percent of a contract’s expected revenue.The Tallinn Effect: Funding from Jaan Tallinn brings the credibility of a serial entrepreneur who co-founded Skype and Kazaa and invested in DeepMind. It’s also a stamp of approval from a technologist who thinks deeply about AI’s potential for both benefit and harm. Tallinn co-founded the Centre for the Study of Existential Risk and once wrote, “In a situation where we might hand off the control to machines, it’s something that we need to get right.” Apparently he believes Pactum meets that standard.", "image_caption": "Two people shaking hands", "metadata": {"article_id": "issue_5", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-5/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_5.html"}}
{"id": 20682921005, "type": "news_chunk", "title": "Robot Farm Workers, Making Algorithms Rock, Automating Contract", "subtitle": "Working Through Uncertainty", "content": "How to build robots that respond to novel situations? When prior experience is limited, enabling a model to describe its uncertainty can enable it to explore more avenues to success. What’s new: In reinforcement learning, meta-learning describes teaching a model how to complete multiple tasks, including tasks the model hasn’t seen before. One way to approach meta-learning is to divide it into two subproblems: creating a plan based on current surroundings and the task at hand, and taking action to implement the plan. Stanford researchers developed deep learning models that facilitate the planning phase by learning to generate better representations of the task. Key insight: Deep learning has been used to learn vector descriptions of the initial state prior to accomplishing a task and the final state afterward. The new work uses probabilistic descriptions, allowing more flexibility in novel tasks. For example, instead of having to choose between the contradictory descriptions object 1 is on object 2 and object 2 is on object 1, the network updates its confidence in each statement throughout the planning steps. How it works: Previous methods use a neural network model as a classifier to decide state descriptions from potential configurations. Instead, De-An Huang and his colleagues use the model’s confidence in each potential configuration to represent states. This approach produces a probabilistic description of current and final states. For training, the model takes a set of demonstrations of similar tasks plus the actions available to the planning algorithm. For testing, it takes a single demonstration of a novel task, the initial state, and the allowed operations.For both initial and final states, a network is trained to predict the probability that certain configurations are observed. For example, based on an image, learn the probability that object 1 is on top of object 2.The planning algorithm takes the probabilistic descriptions and selects the action most likely to move the initial state closer to the final state. Since the choice is a function of the state descriptions and potential operations, the planning algorithm requires no training. Results: The authors’ approach achieves state-of-the-art meta-learning performance in sorting objects and stacking blocks. When sorting, it matches performance based on human heuristics. When stacking, it outperforms human heuristics plus fixed state descriptions with less than 20 training examples (although the heuristics win with 30 training examples). Yes, but: The researchers achieved these results in tasks with a small number of operations and potential state configurations. Their method likely will struggle with more complex tasks such as the Atari games that made deep reinforcement learning popular. Takeaway: In past models, misjudgments of surroundings and goals tend to accumulate, leading the models far from the intended behavior. Now, they can relax their fixed state descriptions by representing potential points of confusion as probabilities. This will enable them to behave more gracefully even with little past experience to draw on.", "image_caption": "Continuous Planner for One-Shot Imitation Learning", "metadata": {"article_id": "issue_5", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Working20Through20Uncertainty.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-5/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_5.html"}}
{"id": 20682921006, "type": "news_chunk", "title": "Robot Farm Workers, Making Algorithms Rock, Automating Contract", "subtitle": "Easing Cross-Border Collaboration", "content": "Tighter national borders impede progress in AI. So industry leaders are calling for a different kind of immigration reform.What happened: The Partnership on AI published a report calling out the corrosive effect of restrictive immigration policies and suggesting alternatives.What it says: The nonprofit industry consortium’s report calls for countries aiming to grow their AI industry to ease visa requirements for high-skilled tech workers. Among its recommendations: Countries should present visa rules clearly and make the review process transparent, so applicants know they are being judged on skill. Get rid of caps, too.Provide assistance to students and low-income applicants in navigating the visa process. Countries serious about fostering AI should provide students with pathways to citizenship.Update immigration laws to redefine the concept of family to include long-term partnerships and nontraditional marriages.No nation should bar tech workers based on nationality. Moreover, countries should apply patent law, not immigration policy, to protect intellectual property. Why it matters: Strict immigration policies limit opportunities for researchers and practitioners. They also shut out students, small businesses, startups, and small colleges from the broader AI community. In the U.S., tighter borders are cutting into the country’s competitiveness in AI, according to a new study from Georgetown University’s Center for Security and Emerging Technology. CSET recommends eliminating existing U.S. barriers to recruiting and retaining foreign-born AI talent.We’re thinking: Human capital is critical to AI, and the global community has an interest in channeling it toward worthwhile projects. The Partnership on AI’s recommendations offer a solid foundation for international trade groups, such as the Organization for Economic Cooperation and Development, to build policies that accelerate progress by opening the world to workers in emerging tech.", "image_caption": "Passport with multiple stamps", "metadata": {"article_id": "issue_5", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Easing20Cross20Border20Collaboration.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-5/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_5.html"}}
{"id": 65693034001, "type": "news_chunk", "title": "GPT-3 Gone Wild, Covid Tech Roundup, AI for Sushi...", "subtitle": "News", "content": "Dear friends, Over the weekend, my family celebrated my grandfather’s 102nd birthday on Zoom. We dialed in from Hong Kong (my grandfather), the U.S. (myself), the UK, Singapore, and New Zealand. In a normal year, I might not have made it to Hong Kong for the party. But because we now celebrate on Zoom, I was able to attend. For my family, the occasion was a bright spot amid the global tragedy of the pandemic. Many people are wondering when the world will go back to normal. I believe the world one day will become normal again, but the new normal will be very different from the normal of yesteryear. Just as the Covid crisis led me to attend my grandfather’s birthday party, once the virus recedes, our newfound ease with high-quality telecommunications will bring people together virtually for all kinds of purposes. My teams in Colombia now work with my U.S. staff more smoothly than they did pre-Covid — it matters less and less whether my teammates are in Medellin, Colombia, or Palo Alto, California. I look forward to a world where digital communications enable anyone anywhere to receive an education and have access to meaningful job opportunities. I hope all of you will live long, healthy lives like my grandfather. Although we find comfort in the past, it is by actively creating the future that we move forward. It’s up to each of us to constantly envision and create a better future. Keep learning! People granted early access to OpenAI’s latest language model are raving about its way with words — and more.What’s new: Beta testers of GPT-3 are showing off the model’s ability to write business memos, craft blogs, pen tweets, and even generate computer code. You can apply for access to the API via this link. A paid version is expected in about two months. Demo explosion: Yaser Martinez Palenzuela, a data scientist at Deutsche Telekom, compiled a list of demos on Github. Here are a few of our favorites. A venture capitalist at Founders Fund used the system to help write an investment memo and declared himself “truly amazed” by its output.It composed a convincing blog post comparing itself to bitcoin, based only on a headline and one-sentence summary provided by an executive at Zeppelin Solutions, which provides blockchain technology.Entrepreneur Sharif Shameem showed that the model, prompted by descriptions of website features, can generate working code.Product designer Jordan Singer built a GPT-3 interface to a graphics program that renders code for plugins based on brief descriptions.A student at Oregon State University asked the model a series of physics questions meant to test its ability to reason. It responded with many correct answers. Hype alert: OpenAI often has been accused of exaggerating the capabilities of its new technologies. Initially it withheld GPT-2, saying the model was too dangerous to release, and it has threatened to cancel GPT-3 access for anyone who uses the tech maliciously. Yet the company itself warns against overhyping the new model. “It still has serious weaknesses and sometimes makes very silly mistakes,” OpenAI CEO Sam Altman wrote in a tweet.Bigger is better: GPT-3 owes much of its performance to a gargantuan parameter count of 175 billion, which dwarfs GPT-2’s 1.5 billion and exceeds by an order of magnitude recent models from Google (11 billion) and Microsoft (17 billion).Why it matters: Large language models based on the transformer architecture have made natural language processing one of the most exciting areas of machine learning. They’re also raising AI’s public profile. GPT-3 is quickly becoming the technology’s foremost spokesbot.We’re thinking: Sometimes GPT-3 writes like a passable essayist, sometimes like an insightful poet. But after reading the fascinating AI Weirdness blog post in which author Janelle Shane gives the model a question-and-answer workout, it seems a lot like some public figures who pontificate confidently on topics they know little about.", "image_caption": "Information related to OpenAI's model GPT-3", "metadata": {"article_id": "issue_50", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/GPT-3203.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-50/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_50.html"}}
{"id": 31710259001, "type": "news_chunk", "title": "Faster AI Chips, Smarter Prosthetic Legs, Transformers for Image", "subtitle": "News", "content": "Dear friends, I spoke last week at the National Intergovernmental Audit Forum, a meeting attended by U.S. federal, state, and local government auditors. (Apparently some of the organizers had taken AI for Everyone.) Many attendees wanted to know how AI systems can be rolled out in a responsible and accountable way. Consider the banking industry. Many regional banks are under tremendous competitive pressure. How well they assess risk directly affects their bottom line, so they turn to credit scoring systems from AI vendors. But if they don’t have the technical expertise to evaluate such models, a hasty rollout can lead to unintended consequences like unfairly charging higher interest rates on loans to minority groups. For AI systems to enjoy smooth rollouts, we need to (a) make sure our systems perform well and pose minimal risk of unintended consequences and (b) build trust with customers, users, regulators, and the general public that these systems work as intended. These are hard problems. They require not just solving technical issues but also aligning technology with society’s values, and expectations. An important part of the solution is transparency. The open source software movement has taught us that transparency makes software better. And if making source code publicly available means that someone finds an embarrassing security bug, so be it! At least it gets fixed. With the rise of AI, we should similarly welcome third-party assistance, such as allowing independent parties to perform audits according to a well established procedure. That way, we can identify problems and fix them quickly and efficiently. After my presentation, the moderator asked me how auditors can avoid getting into adversarial relationships with AI vendors. Instead, we need to build collaborative relationships. By collaborating, we can help make sure the criteria used to judge our systems is reasonable and well specified. For instance, what are the protected groups we need to make sure our systems aren’t biased against? We can also better avoid “gotcha” situations in which our systems are assessed according to arbitrary, after-the-fact criteria. The AI community has a lot of work to do to ensure that our systems are fair, accountable, and reliable. For example, Credo AI (disclosure: a portfolio company of AI Fund, a sister organization to deeplearning.ai) is building tools that help audit and govern AI systems. Efforts like this can make a difference in designing and deploying AI systems that benefit all people. Keep learning! A prosthetic leg that learns from the user’s motion could help amputees walk more naturally.What’s new: Researchers from the University of Utah designed a robotic leg that uses machine learning to generate a human-like stride. It also helps wearers step over obstacles in a natural way.How it works: Rather than trying to recognize obstacles in the user’s path, the prosthesis relies on cues from the user’s body to tell it when something is in the way. Sensors in the user’s hip feed data a thousand times per second into a processing unit located in the unit’s calf. For instance, the way a user rotates their hip might tell the leg to tuck its knee to avoid tripping over an obstacle. A finite state machine (a logic-based controller) determines when and how to flex the knee based on angles of the ankle and thigh and the weight on the prosthetic foot.A second model called the minimum-jerk planner kicks in when the angle and speed of the artificial limb reach a certain point. It works to minimize sharp, sudden actions.The prosthesis applies reinforcement learning to adjust its motion as the user walks, using smoothness as the cost function. Behind the news: A new generation of AI-powered prosthetics could give amputees more control over robotic limbs. Researchers from the University of Michigan developed an open-source bionic leg that extrapolates knee and ankle movements by analyzing the wearer’s hip muscles, similar to the University of Utah’s method.A pair of Canadian students won Microsoft’s 2018 Imagine Cup with a camera-equipped prosthetic hand that uses computer vision to detect objects it is about to grasp and adjusts its grip accordingly.A mechanical arm from École polytechnique fédérale de Lausanne learns to associate common movements with cues from the user’s muscles. Why it matters: Battery-powered prostheses allow amputees to walk more easily, but they tend to stumble on unfamiliar terrain. This smart leg could provide them with smooth, hazard-free perambulation.We’re thinking: AI is helping people with the most basic human functions as well as the most abstract scientific problems.", "image_caption": "Man with prosthetic leg walking", "metadata": {"article_id": "issue_51", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize2015.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-51/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_51.html"}}
{"id": 50321661001, "type": "news_chunk", "title": "Apple's AI Strategy, Retail Surveillance, Clothes That Fight", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, Earlier this week, I asked a question on social media: What is the most important problem that the AI community should work on? Thousands of you responded. The most frequently mentioned themes included: Climate change and environmental issuesCombating misinformationHealthcare including Covid-19Explainable and ethical AI Thank you to each person who responded. I have been reading and thinking a lot about your answers. Many of the most pressing problems, such as climate change, aren’t intrinsically AI problems. But AI can play an important role, and I’m encouraged that so many of you want to do good in the world. Each of us has a role to play. But we rarely succeed alone. That’s why community matters. To my mind, the defining feature of a community is a shared set of values. The medical community prioritizes patients’ wellbeing. When one doctor meets another, their shared priorities immediately create trust and allow them to work together more effectively, say, consulting on complex cases or building initiatives to help underserved people. The academic community also has a history of collaboration stemming from its shared belief in the value of searching for and disseminating knowledge. So, too, in other fields. We in the AI community may share many aims, but the first step toward being more effective as a community is to converge on a set of values we can all stand behind. I believe that if we do this, we can tackle much bigger problems with much greater success. So what, my fellow deep learners, does the AI community stand for? The task of organizing ourselves to tackle big problems together will come later. But first, we need to define the common ground on which we will stand. Many of us hold a strong belief in lifelong learning, sharing information, and working on projects that make society better off. What else? I have ideas of my own, but I would love to hear yours. Please reply to [email protected] or let me know on LinkedIn, Twitter, or Facebook. None of us can solve even one of these issues single-handedly. But working together, I’m optimistic that we can have a huge impact on all of them. Keep learning! Jasjeet Thind is bringing the convenience of ecommerce to real estate. In this edition of our Working AI series, Zillow’s VP of AI explains how he’s building an all-in-one pipeline for home sales and offers advice to up-and-coming machine learning engineers. Learn more", "image_caption": "WorkingAI 4", "metadata": {"article_id": "issue_52", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/WorkingAI204.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-52/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_52.html"}}
{"id": 50321661002, "type": "news_chunk", "title": "Apple's AI Strategy, Retail Surveillance, Clothes That Fight", "subtitle": "News", "content": "After years of trailing other tech giants in AI, Apple has a new ambition: to become the industry’s leading purveyor of products powered by machine learning.What’s new: In an interview with Ars Technica, the company’s AI chief argues that its pro-privacy, on-device approach is the best way to build such applications.Think different: John Giannandrea, the former head of Google’s AI and search who joined Apple in 2018, outlined the iPhone maker’s effort to infuse the technology into a wide range of products and services. Apple is putting a marketing push behind augmented reality apps and upgrades to its personal digital assistant Siri. It also touts AI features such as managing its devices’ energy consumption based on user habits and fusing successive photos into a single high-quality image.Like Google, Huawei, Qualcomm, and Samsung, Apple designed specialized chips to run AI software on smartphones, tablets, and watches. Its laptops are expected to include the chip later this year.Rather than executing tasks in the cloud, a chip subsystem called the Neural Engine processes most machine learning tasks on Apple devices. Processing data on the device helps preserve user privacy and reduces latency, so the software runs closer to real time, according to Giannandrea.Despite the company’s pro-privacy stance, it does collect and label some anonymized data, Giannandrea said. It also asks users to donate data with prompts like, “Would you like to make Siri better?” Buying in: Apple lists dozens of AI job openings, but it has acquired much of its AI technology by buying other companies. It purchased at least 20 machine learning startups — more than any of its rivals — since buying Siri in 2010, according to venture tracker CB Insights. Why it matters: Apple’s privacy-centric, edge-based approach stands out from much of the industry’s reliance on aggressive data collection and processing in the cloud. The difference could help counteract the longstanding impression that it’s behind other tech giants in AI.We’re thinking: AI’s voracious appetite for data boosts the accuracy of supervised learning systems, but it poses risks to user privacy. Apple’s effort to avoid collecting and exposing user data is refreshing — and raises the stakes for small data techniques that enable systems to learn effectively with fewer examples.", "image_caption": "Different Apple products using augmented reality", "metadata": {"article_id": "issue_52", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-52/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_52.html"}}
{"id": 40229165001, "type": "news_chunk", "title": "Predicting Car Crashes, Profiting From Deepfakes, Piloting Drone", "subtitle": "News", "content": "Dear friends, Last week, I asked what values the AI community stands for. Thank you to everyone who replied! The email responses in aggregate ran to 55 pages of text, and I enjoyed reading all of them. A reader who works for a large company wrote, “A purely commercial objective of work is not my calling and I often find myself dreaming about how to break out of the corporate shackles and contribute the rest of my life to doing something meaningful.” These words struck a chord with me. Many of us have the good fortune to find meaning in our work. But if you don’t currently, I hope the AI community will help you do so. Some other comments stood out to me (lightly edited): “A challenge for all of us working in AI is to reimagine the world with respect to concerns like healthcare, education, justice, and environmental protection.” — Shane Ó Seasnáin, Program Manager, Eindhoven AI Systems Institute, Eindhoven“The foundation of our shared values should be refusal to participate in works that would bring harm, regardless of political pressure and monetary rewards.” — Cecilia Cheung, Member, British Computer SocietyWe stand for “fair treatment for all, establishment of trust throughout society, and decreasing the gap between the haves and have-nots.” — Shira L. Broschat, Professor, Washington State University, PullmanThe community “believes in science, data, and facts.” — Nick Brestoff, Chief Inventor, Intraspexion, Seattle “AI has to be made accessible to as many people as possible.” — Benjamin Freisberg, Data Scientist, Substring, BernThe AI community should “engage and empower the community to contribute to all levels of the conversation.” — Reece Robinson, VP Engineering, Orion Health, AucklandWe ought to “push harder on compassion and squeeze out the cruelty.” — Natalie Smithson, Digital Innovation Copywriter, Warwick These thoughts, and many, many others you sent, are wonderful. But one challenge of pushing on compassion (as in the last comment) is that compassion means different things to different individuals. To one person, it may mean mentoring an underprivileged student. To another, it may mean tuning an algorithm to reduce hate speech in social media. Concepts like compassion, empowerment, and being human are easy to agree on in the absence of specifics, but difficult to define and realize in a concrete way. We all want to be compassionate. But what does that mean in practice? We will reach a common understanding only by considering such abstractions in light of a wide variety of ways they might translate into action. This will require tireless discussion as a community. When we have a chance to talk to one another, let’s take the opportunity to discuss the values we hold in common and what it would mean to stand for them in real life. That way, the next time we feel the urge to take a stand — say, tuning a hyperparameter to reduce hate speech at the cost of revenue — we’re more likely to act in a consistent and principled way. I’m heartened by your responses and encouraged that so many of you are looking for greater meaning and positive impact. I will continue to think about how we can come together as a community and keep the conversation going. Keep learning! AI is helping avert traffic accidents by assessing the risk of car crashes at specific intersections.What’s happening: MicroTraffic, a Canadian video analytics company, predicts the odds that accidents will occur at intersections that traditional methods overlook. More than 40 cities in Canada and the U.S. have used its analyses.How it works: The usual approach to monitoring traffic safety identifies dangerous intersections based on crashes that already have occurred. Considering close calls brings previously unidentified trouble spots to light. MicroTraffic uses computer vision to identify motor vehicles, cyclists, pedestrians, and scooters in traffic-cam videos. Its system flags moments when a vehicle came close to colliding with something. The algorithm grades risk based on speed, angle, and the types of vehicles and other objects involved.The company provides city planners with data that show the rate of near misses at each intersection. The city, in turn, can mitigate risks by changing signal timing, adding signage, or redesigning the flow of traffic.Canadian nonprofit Aviva is funding five cities to install the technology at busy intersections. Behind the news: Commercial and government organizations are working on AI for traffic safety. A Thai company installed face recognition systems inside its cars to detect signs of fatigue in drivers hired to travel on an accident-plagued highway. Affectiva, Bosch, Panasonic, and others have developed similar technology.The Finnish city of Espoo put AI-powered lidar sensors inside a busy tunnel to measure vehicle speed, congestion, and stoppages. Why it matters: Globally, motor vehicles kill 3,700 people each day. AI could help traffic engineers cut that grim tally.We’re thinking: When your AI software crashes, take heart in the thought that AI is reducing crashes elsewhere.", "image_caption": "AI-powered traffic monitoring in an intersection", "metadata": {"article_id": "issue_53", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Microtraffic203.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-53/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_53.html"}}
{"id": 33324692001, "type": "news_chunk", "title": "Students Protest AI-Predicted Exam Scores, Autonomous Fighter Jet", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, I’ve been trying to teach my toddler the alphabet. Despite having some educational experience, when she mispronounces a vowel for the nth time, I can’t help but feel like I’m doing it wrong. I hope that Nova somehow will still grow up to be literate and consider my efforts to have been adequate. Teachers have been instructing young people in languages for centuries, yet our methods strike me as remarkably uneven. I’ve tried many alphabet instruction software apps, a number of them featuring dancing animals and the like. But my favorite tools have turned out to be a word processor, which lets me type words against a plain white canvas for Nova to read, and letter-shaped stickers that I can rearrange on my kitchen wall. I was struck by how often Nova, like a neural network, wound up in local minima. She learned to count out loud from one to five by uttering a sequence of sounds without understanding the concept of numbers, much like a recurrent neural network generates plausible text without understanding the meanings of the words it uses. I fed her the sequence of sounds, and she overfit to it. Watching her generalize (and sometimes fail to generalize) gave me fresh appreciation for the difficulty of learning from a small number of examples and how crafting a training dataset with care — curriculum learning? — can promote learning. Amid the pandemic, schools worldwide find themselves in varying states of chaos, and many parents are juggling their children’s education with working from home. Many of us have insufficient time and energy to do both well. It can feel like a no-win situation. My heart goes out to everyone who is caught in this bind. I think the best thing a parent can do is to keep loving your kids. As long as you do that, it will be more than enough. Educational apps can be great, and I hope the AI community will come up with better ones, but an attentive parent armed with a pack of post-its and a loving touch or smile is all a child really needs to learn the basics. Beyond the education you impart, the relationship you build will widen the channels of learning for a lifetime. Keep learning! U.S. Air Force Flight Commander Ronisha Carter is charting an uncommon flight path in AI. She collaborates with academia and industry to build applications that keep the force’s planes flying efficiently. Her work could also help solve complex logistics and scheduling problems in the civilian world. Read more", "image_caption": "Working AI (The Batch teaser + Working AI interior layout 1)", "metadata": {"article_id": "issue_54", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Working20AI20The20Batch20teaser2020Working20AI20interior20layout201.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-54/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_54.html"}}
{"id": 33324692002, "type": "news_chunk", "title": "Students Protest AI-Predicted Exam Scores, Autonomous Fighter Jet", "subtitle": "News", "content": "The UK government abandoned a plan to use machine learning to assess students for higher education.What’s new: The UK Department of Education discarded grades generated by an algorithm designed to predict performance on the annual Advanced Level qualifications, which had been canceled due to the pandemic. Also known as A Levels, these tests serve as entrance exams for colleges and universities.The algorithm’s predictions were generally lower than teachers’ assessments of their students’ likely performance, sparking days of demonstrations in London.The government ultimately agreed to accept whichever grade was higher. What went wrong: Education officials initially asked teachers to award their students an expected grade based on past performance on practice exams. This resulted in a higher-than-normal share of good grades. The department developed its model in an effort to bring grades into line with their usual distribution. The algorithm predicted A-level grades based primarily on two inputs: students’ past academic ranking within their school and the school’s historical performance relative to others.Forty percent of students across England, Ireland, and Wales received predicted scores lower than those their teachers had estimated, compared to 2.2 percent whose scores improved.Most students whose predicted grade was lower than the teacher’s assessment attended schools that serve primarily poor, non-white communities. Behind the news: The pandemic also induced the International Baccalaureate Organization, a nonprofit foundation that provides two-year high school diplomas, to develop its own grading model. The organization said its model produced a distribution similar to that produced by teachers last year. Nonetheless, over 15,000 parents, students, and teachers are petitioning the foundation to reevaluate its model, which they say predicts unfairly low grades.Why it matters: In many countries, high school exit exams determine whether students can pursue higher education. Flawed grading models can have lifelong consequences.We’re thinking: If an AI algorithm predicted grades that were, on average, more accurate, less biased, and less noisy than those estimated by human teachers, it would be worth considering deploying it. If an AI algorithm unfairly lowered many individuals’ grades, it would seem like a terrible idea to use it. The truth is, we live in a world where AI systems can fit both descriptions simultaneously, leading to strong arguments for and against using it. Whether such systems yield a net benefit is an ethical question that requires vigorous debate. We don’t see an easy answer.", "image_caption": "Protest in the UK and information about grading algorithm", "metadata": {"article_id": "issue_54", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Grades.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-54/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_54.html"}}
{"id": 1909979001, "type": "news_chunk", "title": "Autonomous Air Freight, U.S. National AI Centers, Photorealistic", "subtitle": "News", "content": "Dear friends, Did you ever spend days obsessing over a technical problem? If so, I applaud you. Determined pursuit of solutions to hard problems is an important step toward building deep expertise. I’ve been privileged to have worked with several of today’s AI leaders when they were still students. Every one of them spent days, weeks, and months relentlessly trying out different approaches to a range of problems, coming up with hypotheses and performing experiments to hone their intuition. This gave them a thorough understanding of machine learning. It takes many judgement calls to build an effective AI system. How do you tune a particular hyperparameter? What are the tradeoffs between model size, real-time throughput, and accuracy for an application? What type of data pre-processing will yield the best results? When facing complex questions, engineers with deep expertise will come up with better answers. Lately I’ve been thinking about how to train neural networks on small amounts of data. I try to find quiet time to brainstorm, and sometimes I end up with many pages of handwritten notes. After I’ve obsessed over a problem during the day, before I fall asleep I remind my brain that I want to make progress on it. Then, if I’m lucky, I awaken in the morning with new ideas. The world is complex and becoming more so. We need people, in AI and other disciplines, who will take the time and effort to build deep expertise. When a worthy problem taps you on the shoulder, I encourage you to give it your attention. Give yourself the time you need to explore a solutions, and keep at it. It’s not a weird thing to do. Even if you don’t succeed — as a student, I spent countless hours trying, and failing, to prove P ≠ NP, and I don’t regret a minute of it — the journey will make you better. Keep learning! An aviation startups is using neural networks to put air freight on autopilot.What’s new: Xwing, a California startup, is test-flying an autonomous pilot system aboard cargo aircraft with an eye toward crewless commercial flights in 2022, the Wall Street Journal reported. How it works: A suite of models reads sensor data while the plane is in motion. When the models detect another plane or an obstacle, they funnel the information to a rules-based flight control system, which adjusts course, Xwing CEO Marc Piette told The Batch. The company installed its system aboard a fleet of Cessna Grand Caravans modified with extra sensors and computing power. These propeller-driven planes typically carry around 3,300 pounds of freight over relatively short distances.Sensors mounted on the aircraft include electro-optical and infrared cameras, radar, lidar, and GPS. Some sensors capture annotated data; for example, radar labels other aircraft. This allows automated annotation of camera images, enabling the company to generate large datasets quickly and save on manual annotation.Human pilots sit in the cockpit as emergency backups. Xwing hopes to make the system fully autonomous with oversight by people on the ground, who can take control if necessary. Behind the news: Several companies are racing toward regulatory approval for autonomous freight transport, including Amazon, which this week gained permission to deliver packages using drones. The remaining issues are not technical. Commercial airliners routinely fly on autopilot, and last year a Cessna outfitted with an AI-powered autopilot from Reliable Robotics performed the first autonomous take-off, flight, and landing over an urban area. However, regulations and public concerns have kept human pilots in cockpits. Xwing and its proponents believe that restriction may lift before long, starting with approval for flights over water or uninhabited areas. The company’s reliance on existing aircraft may help expedite the process. Why it matters: Small planes move cargo between outlying areas and central hubs. Autonomous systems could make service faster, more frequent, and less costly.We’re thinking: Air, land, or sea: Where will fully autonomous vehicles first enjoy widespread deployment?", "image_caption": "Autopiloted aircraft", "metadata": {"article_id": "issue_55", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize201-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-55/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_55.html"}}
{"id": 22077620001, "type": "news_chunk", "title": "Data for Defense, Predicting Credit Approvals, More Learning", "subtitle": "DeepLearning.ai Exclusive", "content": "Dear friends, Today we take it for granted that many people know how to read and write. Someday, I hope, it will be just as common that people know how to write code. Several hundred years ago, society didn’t view language literacy as a necessary skill. A small number of people learned to read and write, and everyone else let them do the reading and writing. It took centuries for literacy to spread, and now society is far richer for it. Words enable deep human-to-human communication. Code is the deepest form of human-to-machine communication. As machines become more central to daily life, that communication becomes ever more important. Traditional software engineering — writing programs that explicitly tell a computer sequences of steps to execute — has been the main path to code literacy. But AI, machine learning, and data science offer a new paradigm in which computers extract knowledge from data. This technology offers another pathway to coding — one that strikes me as even more promising. Many Sundays, I buy a slice of pizza from my neighborhood pizza parlor. The gentleman behind the counter may have little reason to learn how to build software applications (beyond personal growth and the pleasure of gaining a new skill). But AI and data science have great value even for a pizza maker. A linear regression model might enable him to better estimate demand so he could optimize the restaurant’s staffing and supply chain. He could better predict sales of Hawaiian pizza — my favorite! — so he could make more Hawaiian pies in advance and reduce the amount of time customers had to wait for them. Uses of AI and data science can be found in almost any situation that produces data, and I believe that a wide variety of professions will find more uses for custom AI applications and data-derived insights than for traditional software engineering. This makes literacy in AI-oriented coding even more valuable than traditional skills. It could enable countless individuals to harness data to make their lives richer. I hope the promise of building basic AI applications, even more than that of building basic traditional applications, encourages more people to learn how to code. If society embraces this new form of literacy as it has the ability to read and write, we will all benefit. Keep learning! As a senior machine learning engineer at Retro Rabbit, a software consultancy, Jade Abbott focuses on solving customer problems. On the side, she develops natural language processing models for African languages. Read more", "image_caption": "Screen Shot 2020-09-02 at 11.35.37 AM", "metadata": {"article_id": "issue_56", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-0220at2011.35.3720AM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-56/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_56.html"}}
{"id": 22077620002, "type": "news_chunk", "title": "Data for Defense, Predicting Credit Approvals, More Learning", "subtitle": "News", "content": "A neural network is helping credit card users continue to shop even when the lender’s credit-approval network goes down.What’s new: Visa developed a deep learning system that analyzes individual cardholders’ behavior in real time to predict whether credit card transactions should be approved or denied. The system can step in when a card issuer — generally a bank that normally would vet such transactions — suffers a network outage that makes it impossible to assess creditworthiness.How it works: If a cardholder’s purchases are blocked, they might switch to another card, costing the bank revenue and possibly a customer. And if a miscreant tries to commit fraud, the bank stands to lose money. So Visa provides a backup system that predicts the decision in case the lender can’t due to software glitches, severe weather, or routine maintenance. The new model is trained on the company’s database of historical transactions. It learns an individual’s normal behavior based on factors like spending history, location, and timing of transactions.In tests, it matched banks’ decisions with 95 percent accuracy. An earlier, rule-based algorithm was half as accurate, according to a report by the Wall Street Journal.Visa plans to make the service available for a fee starting in October. Why it matters: Unlike, say, fraud detection, this model touches cardholders directly to improve the customer experience. It points the way toward public-facing models that personalize banking, credit, and other financial arrangements. Yes, but: Visa declined to share details of its new algorithm with The Batch. Decisions to extend credit can be based on patterns in data that encode social biases, and an algorithm trained on a biased dataset will reflect its biases. For instance, an algorithm may decline transactions requested by a cardholder whose home address is in a neighborhood associated with defaults on loans, and accept those requested by someone with a comparable history of repayment who lives in a wealthier neighborhood. Large financial institutions are aware of this problem, but standards that specify what is and isn’t fair are still in development. We’re thinking:The financial industry’s health depends on trust. That should provide ample incentive to define the fairness of automated systems in lending and other financial services. Efforts such as Singapore’s Principles to Promote Fairness, Ethics, and Transparency are an important step.", "image_caption": "Woman with plenty of shopping bags", "metadata": {"article_id": "issue_56", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Visa.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-56/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_56.html"}}
{"id": 19268658001, "type": "news_chunk", "title": "Training 1 Trillion Parameters, Medical AI Gets a Shot in the Arm", "subtitle": "DeepLearning.AI Exclusive", "content": "Dear friends, I’d like to share a programming tip that I’ve used for years. A large part of programming involves googling for code snippets you need on Stack Overflow and other websites. (Shh. Don’t tell the nondevelopers. ????) But that’s not enough if your goal is to maximize your own learning. When the relevant code snippet is just several lines, rather than copy-pasting them from a web page into my code, I usually retype them myself. The physical practice helps train my brain to internalize the concept and syntax.To gain skill as a programmer, you need to internalize both the concepts and the syntax. When I’m trying to help friends get started on coding, I ask them to type print(“Hello World”). By typing it out, you can be sure you know the command’s syntax, such as whether it requires parentheses ( ), square brackets [ ], and so on. You can’t learn to ride a bicycle by reading a book on the theory of bicycling. You have to do it yourself! Coding is more similar to this type of physical skill than most people realize, and practice makes perfect. When you’re trying to master a programming technique, consider these practices: Read a line of code, then type it out yourself. (Bonus points for doing it without looking at the reference code while typing.)Learn about an algorithm, then try to implement it yourself.Read a research paper and try to replicate the published result.Learn a piece of math or a theorem and try to derive it yourself starting with a blank piece of paper. Many creative artists start by replicating the works of artists who came before; so, too, in coding. By replicating examples of good programming (being mindful of copyright and attribution, of course), your brain masters the ability to create them. This frees you to focus on higher-level tasks so you can rearrange what you’ve learned into new, original works. So next time you’re tempted to copy and paste a few lines of code, I hope you’ll start typing instead. Keep learning! Kennedy Kamande Wangari works as a junior data scientist, organizes Nairobi’s AI community, studies machine learning, and is considering a startup, all while maintaining his personal life. In this edition of Breaking Into AI, he explains how he keeps so many balls in the air. Read more", "image_caption": "Screen Shot 2020-09-14 at 11.34.48 AM", "metadata": {"article_id": "issue_57", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-1420at2011.34.4820AM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-57/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_57.html"}}
{"id": 19268658002, "type": "news_chunk", "title": "Training 1 Trillion Parameters, Medical AI Gets a Shot in the Arm", "subtitle": "News", "content": "An open source library could spawn trillion-parameter neural networks and help small-time developers build big-league models.What’s new: Microsoft upgraded DeepSpeed, a library that accelerates the PyTorch deep learning framework. The revision makes it possible to train models five times larger than the framework previously allowed, using relatively few processors, the company said. How it works: Microsoft debuted DeepSpeed in February, when it used the library to help train the 17 billion-parameter language model Turing-NLG. The new version includes four updates: Three techniques enhance parallelism to use processor resources more efficiently: Data parallelism splits data into smaller batches, model parallelism partitions individual layers, and pipeline parallelism groups layers into stages. Batches, layers, and stages are assigned to so-called worker subroutines for training, making it easier to train extremely large models.ZeRO-Offload efficiently juggles resources available from both conventional processors and graphics chips. The key to this subsystem is the ability to store optimizer states and gradients in CPU, rather than GPU, memory. In tests, a single Nvidia V100 was able to train models with 13 billion parameters without running out of memory — an order of magnitude bigger than PyTorch alone.Sparse Attention uses sparse kernels to process input sequences up to an order of magnitude longer than standard attention allows. In tests, the library enabled Bert and Bert Large models to process such sequences between 1.5 and 3 times faster.1-bit Adam improves upon the existing Adam optimization method by reducing the volume of communications required. Models that used 1-bit Adam trained 3.5 times faster than those trained using Adam. Results: Combining these improvements, DeepSpeed can train a trillion-parameter language model using 800 Nvidia V100 graphics cards, Microsoft said. Without DeepSpeed, the same task would require 4,000 Nvidia A100s, which are up to 2.5 times faster than the V100, crunching for 100 days.Behind the news: Deep learning is spurring a demand for computing power that threatens to put the technology out of many organizations’ reach. A 2018 OpenAI analysis found the amount of computation needed to train large neural networks doubled every three and a half months.A 2019 study from the University of Massachusetts found that high training costs may keep universities and startups from innovating.Semiconductor manufacturing giant Applied Materials estimated that AI’s thirst for processing power could consume 15 percent of electricity worldwide by 2025. Why it matters: AI giants like Microsoft, OpenAI, and Google use enormous amounts of processing firepower to push the state of the art. Smaller organizations could benefit from technology that helps them contribute as well. Moreover, the planet could use a break from AI’s voracious appetite for electricity.We’re thinking: GPT-3 showed that we haven’t hit the limit of model and dataset size as drivers of performance. Innovations like this are important to continue making those drivers more broadly accessible.", "image_caption": "Graphs with data related to Microsoft's library DeepSpeed", "metadata": {"article_id": "issue_57", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2012.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-57/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_57.html"}}
{"id": 7391349001, "type": "news_chunk", "title": "YouTube vs. Conspiracy Theorists, Robots That Think Ahead...", "subtitle": "News", "content": "Dear friends, AI researchers keep coming up with impressive innovations: transformer-based language models, self-supervised learning, deep reinforcement learning, small data. All of these developments hold great promise. But some will continue to improve over time and set new directions for AI, and others will turn out to have less impact. How can you tell which is which? I remember seeing early data, over a decade ago, that indicated deep learning algorithms could scale up to become very useful. Similarly, I remember thinking that sequence-to-sequence models, when they were first presented and not yet working well, set a new direction. In these instances, my instincts turned out to be right. But I’ve been wrong, too. For example, in the mid-2000s, I thought that mobile manipulation would take off faster than it has so far. I’ve thought about how to evaluate whether an exciting idea that doesn’t yet work well is likely to become a winner or whether it’s unlikely to improve much for a long time. Over the past decade, three major drivers of improvement in AI performance have been: Computational scaling: Does running an algorithm on computers 10 or 100 times faster result in better performance?Data scaling: Does feeding an AI system more data improve its performance?Algorithmic improvements: Does the data available still hold a significant amount of information that current algorithms do not extract? I believe these three factors will continue to drive AI performance for years to come. Thus, nascent ideas that can take advantage of them seem more promising to me. If the “only” thing a new algorithm requires to be useful is a 10x improvement in computation speed, you have Nvidia, Intel, and AMD working hard to make that improvement, so it’s a good bet that it will happen. This reasoning leads me to believe that GPT-3 is setting a new direction for building language models and applications. I see a clear path toward scaling computation (by making models cheaper to run or building bigger ones) and algorithmic improvements. At AI Fund (where I’m managing general partner), we’re seeing many entrepreneurs looking to build new companies using GPT-3. On the other hand, I don’t expect quantum computing to have a dramatic impact on AI any time soon. I look forward to quantum AI and I’m glad that many groups are investing in it. But it doesn’t appear to ride any of the three drivers above, and I believe it will take a significant amount of time to become practical for machine learning. Regarding algorithmic improvements, it’s important to note that the information must be in the data for an algorithm to extract it. If someone’s DNA doesn’t contain enough information to determine whether that person will develop diabetes, then no amount of algorithmic work will yield the ability to predict the disease from only the genetic sequence. If humans can perform a task, that’s strong evidence that the data available to humans holds information helpful for completing that task — and that points to the possibility that algorithmic improvements can enable AI to complete it, too. This is why I believe that small data is a promising area: A handful of pictures contains sufficient information for a human to learn to recognize a new object. This offers hope that improved algorithms will be able to extract that information and learn from far fewer examples than are required today. When you hear about an exciting category of emerging AI technology, you might ask yourself whether it can ride on the backs of computational scaling, data scaling, and algorithmic improvement. If so, it’s more likely to make a big impact in the future. We can create immense value if we can get better at recognizing new ideas that, although they may not yet work well today, have potential to become tomorrow’s top performers. Keep learning! A major corporate acquisition could reshape the hardware that makes AI tick.What’s new: U.S. processor giant Nvidia, the world’s leading vendor of the graphics processing units (GPUs) that perform calculations for deep learning, struck a deal to purchase UK chip designer Arm for $40 billion. The transaction faces regulatory approvals and other hurdles, but if it’s completed, it will be the biggest-ever acquisition in the chip industry and one of the biggest technology deals. Deal drivers: Nvidia’s technology undergirds much of the cloud infrastructure for AI workloads, while Arm’s technology drives inference in 95 percent of smartphones. Nvidia said it plans to integrate Arm’s energy-efficient designs with its data center chips.It also aims to use the technology to spur the internet of things, a buzzword for devices like smart thermostats, doorbells, speakers, and industrial equipment that are expected to distribute intelligence throughout buildings and infrastructure.Nvidia CEO Jensen Huang envisions trillions of AI-equipped devices enabling everything from autonomous heavy machinery to walk-through retail checkout.Huang also plans to extend Arm’s licensing practices, which let any company lease its designs, to Nvidia’s GPUs and AI services. Behind the news: Nvidia developed GPUs to process high-resolution video game graphics in 1999. Nearly a decade later researchers realized their potential for training deep learning models. Since then, the company’s value has multiplied tenfold.Why it matters: By combining Arm’s energy efficiency with its growing presence in the cloud, Nvidia chips may be able to drive coming generations of multi-trillion parameter models.Yes, but: Mergers are difficult to pull off, and international tie-ups of this scale especially so. Whether Nvidia can take full advantage of its new possession may remain unclear for a long time. Meanwhile, Arm co-founder Hermann Hauser is urging UK authorities to block the deal on the grounds that it would put Nvidia on the road to monopolizing the chip industry.We’re thinking: Data centers increasingly require both CPUs to process traditional workloads and GPUs to process deep learning (with help from a CPU). Data center operators would appreciate a vendor that can supply CPUs and GPUs that interoperate smoothly. That’s one reason why CPU producers like Intel and AMD are expanding into GPUs, and why Nvidia wants to buy Arm.", "image_caption": "AI chip and graphics processing unit", "metadata": {"article_id": "issue_58", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2015.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-58/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_58.html"}}
{"id": 97650525001, "type": "news_chunk", "title": "GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes", "subtitle": "Reality Reimagined", "content": "Dear friends, This special issue of The Batch celebrates the launch of our new Generative Adversarial Networks Specialization! GANs are among the most exciting technologies to emerge from deep learning. These networks learn in a very different way than typical supervised methods for learning x-to-y mappings. By pitting a discriminator network and a generator network against one another (details below), they produce photorealistic images, medical training data, children’s book illustrations, and other types of output. Earlier today, we held an online panel discussion on “GANs for Good” with Anima Anandkumar, Alexei Efros, Ian Goodfellow, and our course instructor Sharon Zhou. I was struck by the number of new applications GANs are enabling, and the number that are likely to come. Ian explained that GAN-generated training examples for a particular application at Apple are one-fifth as valuable as real examples but cost much less than one-fifth as much to produce. Anima described exciting progress on disentanglement and how the ability to isolate objects in images is making it easier to control image generation (“add a pair of glasses to this face”). Alexei talked about the impact GANs are having on art through tools like Artbreeder. All the speakers talked about alternatives to reading research papers to keep up with the exploding literature. If you missed the live discussion, you can watch a video of the entire event here. We’re still in the early days of practical GAN applications, but I believe they will: Transform photo editing and make it easier to add or subtract elements such as background objects, trees, buildings, and cloudsGenerate special effects for media and entertainment that previously were prohibitively expensiveContribute to creative products from industrial design to fine artAugment datasets in small data problems in fields from autonomous driving to manufacturing As an emerging technology, GANs have numerous untapped applications. This is a moment to dream up new ideas, because no one else may be working on them yet. I hope this technology will spark your hunger to learn more and invent new applications that will make life better for people all over the world. Keep learning! Generative adversarial networks, or GANs, are said to give computers the gift of imagination. Competition between a discriminator network, which learns to classify the system’s output as generated or real, and a generator network, which learns to produce output that fools the discriminator, produces fantasy images of uncanny realism. First proposed in 2014, the architecture has been adopted by researchers to extend training datasets with synthetic examples and by businesses to create customized imagery for ads, entertainment, and personal services. But it has a dark side: GANs make it easy and convincing for jilted lovers to graft an ex’s face onto another person’s body, or politicians to misrepresent themselves as able to speak an ethnic minority’s language to win their votes. And it tends to tilt the training data distribution, favoring common examples while ignoring outliers. Researchers are improving the technology at breakneck pace, and developing ways to thwart, or at least detect, egregious uses. We have yet to see the best — and the worst — that GANs have to offer.", "image_caption": "Online panel discussion on “GANs for Good” with Andrew Ng, Anima Anandkumar, Alexei Efros, Ian Goodfellow, and Sharon Zhou", "metadata": {"article_id": "issue_59", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker201-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-59/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_59.html"}}
{"id": 97650525002, "type": "news_chunk", "title": "GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes", "subtitle": "A Man, A Plan, A GAN", "content": "Brilliant ideas strike at unlikely moments. Ian Goodfellow conceived generative adversarial networks while spitballing programming techniques with friends at a bar. Goodfellow, who views himself as “someone who works on the core technology, not the applications,” started at Stanford as a premed before switching to computer science and studying machine learning with Andrew Ng. “I realized that would be a faster path to impact more things than working on specific medical applications one at a time,” he recalls. From there, he earned a PhD in machine learning at Université de Montréal, interned at the seminal robotics lab Willow Garage, and held positions at OpenAI and Google Research. Last year, he joined Apple as director of machine learning in the Special Projects Group, which develops technologies that aren’t part of products on the market. His work at Apple is top-secret. The Batch: How did you come up with the idea for two networks that battle each other?Goodfellow: I’d been thinking about how to use something like a discriminator network as a way to score a speech synthesis contest, but I didn’t do it because it would have been too easy to cheat by overfitting to a particular discriminator. Some of my friends wanted to train a generator network using a technique that would have taken gigabytes of data per image, even for the tiny images we studied with generative models in 2014. We were discussing the problem one night at a bar, and they asked me how to write a program that efficiently manages gigabytes of data per image on a GPU that, back then, had about 1.5GB RAM. I said, that’s not a programming problem. It’s an algorithm design problem. Then I realized that a discriminator network could help a generator produce images if it were part of the learning process. I went home that night and started coding the first GAN.The Batch: How long did it take?Goodfellow: By copying and pasting bits and pieces of earlier papers, I got the first GAN to produce MNIST images in only an hour of work or so. MNIST is such a small dataset that, even back then, you could train on it very quickly. I think it trained for tens of minutes before it could produce recognizable handwritten digits.The Batch: What did it feel like to see the first face?Goodfellow: That wasn’t as much of a revolutionary moment as people might expect. My colleague Bing Xu modeled face images from the Toronto Face Database, which were only 90 pixels square and grayscale. Because the faces were always centered and looking straight at the camera, even very simple algorithms like PCA could make pretty good faces. The main thing we were surprised by were the images it made of CIFAR10, where there’s a lot of variability. They looked like crap. But we had been looking at crap from generative models for years, and we could tell that this was an exciting, new kind of crap.The Batch: Has anything surprised you about the way this work has played out?Goodfellow: In the first GAN paper, we included a list of things that might happen in future work. A lot of them did. The one big category of things I didn’t anticipate was domain-to-domain translation. GANs like CycleGAN from Berkeley. You can take a picture of a horse and have it transformed into a zebra without training on matched pairs of horse and zebra images. That’s very powerful because it can be easy to passively collect data in each domain, but it’s time-consuming and expensive to get data that matches up across domains.The Batch: What are you most hopeful about in GAN research?Goodfellow: I’d like to see more use of GANs in the physical world, specifically for medicine. I’d like to see the community move toward more traditional science applications, where you have to get your hands dirty in the lab. That can lead to more things that have more of a tangible, positive impact on peoples’ lives. For instance, in dentistry, GANs have been used to make personalized crowns for individual patients. Insilico is using GANs to design medicinal drugs.The Batch: How much do you worry about bias in GAN output? The ability to produce realistic human faces makes it a pressing issue.Goodfellow: GANs can be used to counteract biases in training data by generating training data for other machine learning algorithms. If there’s a language where you don’t have as much representation in your data, you can oversample that. At Apple, we were able to generate data for a gestural text-entry feature called QuickPath. A startup called Vue.ai uses GANs to generate images of what clothing would look like on different models. Traditionally, there may not have been much diversity in who was hired to be a model to try on this clothing. Now you can get a model who looks like you, wearing a specific item of clothing you’re interested in. These are baby steps, but I hope there are other ways GANs can be used to address issues of underrepresentation in datasets.", "image_caption": "Ian Goodfellow", "metadata": {"article_id": "issue_59", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-2920at201.03.5020PM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-59/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_59.html"}}
{"id": 97650525003, "type": "news_chunk", "title": "GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes", "subtitle": "A Good Look for AI", "content": "Trying on new makeup is a hassle — apply, inspect, wash off, repeat. Not to mention the tribulation of visiting brick-and-mortar stores during the pandemic. Augmented reality is helping people try on all the makeup they want without leaving home.What’s new: Modiface, a subsidiary of beauty giant L’Oréal, uses GANs to let customers see how different colors of lipstick, eye shadow, and hair will look on them.How it works: Modiface’s approach is a hybrid of CycleGAN, StarGAN and StyleGAN, company operations chief Jeff Houghton told The Batch. It uses a CNN to track landmarks on a user’s hair and face. At L’Oreal’s website, users can select different lipsticks, eyeliners, blushes and hair dyes and fine-tune their shade, texture, and gloss. Then they can virtually apply those products to an uploaded selfie or a real-time video of their own faces. To train the algorithm, Modiface collected and annotated thousands of images of people wearing and not wearing makeup and hair coloring under various lighting and background conditions.A CycleGAN takes as input an image of a user’s face without makeup. When the user selects a specific makeup product, the system generates an image that shows what the selected product looks like when applied.Adding new makeup types to the model is as simple as adding a color swatch to the database. Modiface must retrain the model to include new products with properties, such as a metallic sheen, that aren’t represented in the training data.Collaborations with Amazon and Facebook enable users to access Modiface directly from those platforms. Behind the news: Augmented reality applications are reshaping the beauty industry. In 2018, the same year L’Oréal purchased Modiface, American chain Ulta acquired Glamst, a startup specializing in augmented reality. Meitu, a multi-billion dollar Chinese company, uses AI-driven face manipulation to make its users appear more attractive in social media posts, job applications, and other digital venues.Why it matters: E-commerce is increasingly important to the beauty industry’s bottom line, and the pandemic is driving even more business to the web. Tools like this make it easier for customers to try out products, which may boost sales.We’re thinking: It seems like we’re spending half our lives in video conferences. Do we still need to apply makeup at all, or can we let a GAN do it instead? Learn how to build a CycleGAN and control image features in Courses 1 and 2 of the GANs Specialization from DeepLearning.AI, available now on Coursera. Build the powerful StyleGAN in Course 3: Apply GANs, coming soon!", "image_caption": "Makeup applied on female faces with the help of augmented reality", "metadata": {"article_id": "issue_59", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-59/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_59.html"}}
{"id": 97650525004, "type": "news_chunk", "title": "GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes", "subtitle": "Making GANs More Inclusive", "content": "A typical GAN’s output doesn’t necessarily reflect the data distribution of its training set. Instead, GANs are prone to modeling the majority of the training distribution, sometimes ignoring rare attributes — say, faces that represent minority populations. A twist on the GAN architecture forces its output to better reflect the diversity of the training data.What’s new: IMLE-GAN learns to generate all the attributes of its training dataset, including rare ones. Ning Yu spearheaded the research with colleagues at University of Maryland, Max Planck Institute, University of California Berkeley, CISPA Helmholtz Center for Information Security, Princeton’s Institute for Advanced Study, and Google.Key insight: A GAN’s discriminator distinguishes whether or not the generator’s output is generated, while the generator learns to produce output that fools the discriminator. Ideally, a generator’s output would mirror the training data distribution, but in practice — since its only aim is to fool the discriminator, and the discriminator typically evaluates only one image at a time — it can learn to favor common types of examples. The authors had their model compare several generated works with examples from the training set, as well as interpolations between generated works, to encourage greater diversity in the output.How it works: IMLE-GAN enhances a GAN with Implicit Maximum Likelihood Estimation (IMLE). Instead of naively adding the IMLE loss to the usual adversarial loss, the authors modified the default IMLE loss and added a novel interpolation loss to compensate for fundamental incompatibilities between the adversarial and IMLE losses. IMLE generates a set of images and penalizes the network based on how different those images are from real images by making nearest-neighbor comparisons. Instead of comparing pixels, like in standard IMLE, the authors compare the images over the feature space. The switch from pixels to features makes the adversarial and IMLE losses more comparable.To compute the interpolation loss, the authors create an additional image that is interpolated between two generated images. Then, they compare the interpolated image’s features to those of the two non-generated images that were matched to the generated images during IMLE.To increase inclusion of underrepresented attributes, the algorithm samples data from a set of minority examples for the IMLE and interpolation losses, but from all examples for the adversarial loss. Results: The authors evaluated IMLE-GAN against StyleGAN and a handful of other models using Stacked MNIST, a variation of the MNIST dataset that includes handwritten digits in 1,000 distinct styles. IMLE-GAN reproduced 997 of the styles, while StyleGAN reproduced 940. Trained on CelebA, a large-scale dataset of celebrity faces, IMLE-GAN generated attributes present in less than 6 percent of training examples with increased precision compared to StyleGAN. For instance, it generated wearers of eyeglasses with 0.904 precision, compared to StyleGAN’s meager 0.719.Why it matters: Much of the time, we want our models to learn the data distribution present in the training set. But when fairness or broad representation are at stake, we may need to put a finger on the scale. This work offers an approach to making GANs more useful in situations where diversity or fairness is critical.We’re thinking: This work helps counter model and dataset bias. But it’s up to us to make sure that training datasets are fair and representative. Learn about what to consider when evaluating a GAN for your application and the potential impact of biases in Course 2: Build Better GANs, available now on Coursera.", "image_caption": "Data and examples related to IMLE-GAN", "metadata": {"article_id": "issue_59", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2021.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-59/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_59.html"}}
{"id": 97650525005, "type": "news_chunk", "title": "GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes", "subtitle": "More Data for Medical AI", "content": "Convolutional neural networks are good at recognizing disease symptoms in medical scans of patients who were injected with iodine-based dye, known as radiocontrast, that makes their organs more visible. But some patients can’t take the dye. Now synthetic scans from a GAN are helping CNNs learn to analyze undyed images.What’s new: Researchers from the U.S. National Institutes of Health and University of Wisconsin developed a GAN that generates labeled, undyed computerized tomography (CT) images of lesions on kidneys, spleens, and livers. They added these images to real-world training data to improve performance of a segmentation model that marks lesions in diagnostic scans.How it works: The work is based on CycleGAN and the DeepLesion dataset of CTs. CycleGAN has been used to turn pictures of horses into pictures of zebras without needing to match particular zebra and horse pics. This work takes advantage of that capability to map between dyed and undyed CTs. The authors used a CNN to sort DeepLesion into images of dyed and undyed patients. They trained the GAN on a portion of the dataset, including both dyed and undyed CTs, and generated fake undyed images.Using a mix of CycleGAN output and natural images, they trained a U-Net segmentation model to isolate lesions, organs, and other areas of interest.To compare their approach with alternatives, they trained separate U-Nets on variations of DeepLesion: dyed images in which the dye had been artificially lightened, images that had been augmented via techniques like rotation and cropping, and the dataset without alterations. Results: Tested on undyed, real-world CT scans, the U-Net trained on the combination of CycleGAN output and natural images outperformed the others. It was best at identifying lesions on kidneys, achieving a 57 percent improvement over the next-best model. With lesions on spleens, the spread was 4 percent; on livers, 3 percent. In estimating lesion volume, it achieved an average error of 0.178, compared to the next-highest score of 0.254. Tested on the remainder of the dyed DeepLesion images, all four U-Nets isolated lesions roughly equally well.Behind the news: The researchers behind this model have used it to improve screening for dangerous levels of liver fat and to identify patients with high risk of metabolic syndrome, a precursor to heart disease, diabetes, and stroke.Why it matters: Medical data can be hard to come by and labeled medical data even more so. GANs are making it easier and less expensive to create large, annotated datasets for training AI diagnostic tools. We’re thinking: Medical AI is just beginning to be recognized by key healthcare players in the U.S. Clever uses of CycleGAN and other architectures could accelerate the process. To learn more about using GANs to augment training datasets, including the pros and cons, stay tuned for GANs Specialization Course 3: Apply GANs, coming soon to Coursera.", "image_caption": "Examples of CT scans with different contrasts", "metadata": {"article_id": "issue_59", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2023.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-59/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_59.html"}}
{"id": 97650525006, "type": "news_chunk", "title": "GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes", "subtitle": "The Telltale Artifact", "content": "Deepfakes have gone mainstream, allowing celebrities to star in commercials without setting foot in a film studio. A new method helps determine whether such endorsements — and other images produced by generative adversarial networks — are authentic.What’s new: Lucy Chai led MIT CSAIL researchers in an analysis of where image generators fool and where they fail. They developed a technique to detect portions of an image that betray fakery.Key insight: Large-scale features of generated images are highly varied, but generated textures contain consistent artifacts. Convolutional neural networks (CNNs) are especially sensitive to textures, which makes them well suited to recognizing such artifact-laden areas. A CNN tailored for analyzing small pieces of images can learn to recognize parts dominated by generated textures.How it works: The authors built classifiers that survey images one patch at a time. They ran the classifiers on output from StyleGAN, Glow, and a generator model based on Gaussian mixture models (GMMs). They averaged the patchwise classifications to analyze each GAN’s vulnerability to detection. The authors created a dataset of images generated by a Progressive GAN trained on the CelebA-HQ dataset of celebrity portraits.They modified Resnet and Xception architectures to classify patches of user-determined size and trained them on the generated images. They removed the deeper layers, which analyze larger image areas, to concentrate the models on fine details.They used the classifications to produce heatmaps of image areas recognized as generated (blue) or not (red). Predominantly blue images were deemed to have been generated.By averaging the heatmaps over many images produced by the same GAN, the authors were able to identify the areas where that model is especially prone to leaving artifacts. For instance, StyleGAN and Glow generated high concentrations of artifacts in facial details, while GMM tended to generate them in backgrounds. Results: The authors’ best classifier achieved 100 percent average precision on StyleGAN output and 91.38 percent on GMM. These scores outperformed non-truncated MesoInception4, Resnet-18, Xception, and CNN models, which achieved average precision between 99.75 and 73.33 percent. On Glow, the authors’ best classifier achieved 95 percent average precision, whereas the best full model scored 97.32 percent.Why it matters: The better GANs become, the more useful they can be for both good and ill. In shedding light on areas where particular GANs produce more artifacts, this work illuminates pathways for researchers to improve them. But it also provides a map for malefactors to make their activities harder to detect. In fact, when the researchers trained a GAN to fool their classifiers, accuracy fell to less than 65 percent.We’re thinking: Building a discriminator that recognizes a particular generator’s output is easier than building a good generator. In fact, GAN researchers routinely degrade discriminators to give the generator a fighting chance to fool it. But social media platforms, among others, would like to catch all generated images, regardless of the generator that produced them. Looking for common artifacts offers a promising approach — until a variety of generators learn how to avoid producing them. Learn how image translation is used to create deepfakes in the upcoming Course 3: Apply GANs, available soon on Coursera.", "image_caption": "Data and examples related to a new technique to detect portions of an image", "metadata": {"article_id": "issue_59", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2022.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-59/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_59.html"}}
{"id": 97650525007, "type": "news_chunk", "title": "GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes", "subtitle": "Style and Substance", "content": "GANs are adept at mapping the artistic style of one picture onto the subject of another, known as style transfer. However, applied to the fanciful illustrations in children’s books, some GANs prove better at preserving style, others better at preserving subject matter. A new model is designed to excel at both.What’s new: Developed by researchers at Hacettepe University and Middle East Technical University, both in Turkey, Ganilla aims to wed photographic content and artistic style for illustrations in children’s books. It converts photos into virtual artwork in the styles of 10 published children’s book illustrators, including favorites like Patricia Polacco and Kevin Henkes, while staying true to scenes in photos. How it works: Ganilla is almost identical to CycleGAN except for a specially crafted generator. The researchers divided their generator into a downsampling stage and an upsampling stage.The downsampling stage is a modified Resnet-18 with additional skip connections to pass low-level features, such as textures and edges, from one layer to the next.The upsampling stage consists of layers of transposed convolutions that increase the size of the feature map and skip connections from the downsampling stage. The skip connections in this stage help preserve subject matter without overwriting style information.The authors trained the model on unpaired images from two datasets. The first contained nearly 5,500 images of landscape scenery, the second hundreds of works by each of 10 illustrators. Results: There’s no way to measure objectively how well a model generates landscapes in specific artistic styles, so the authors used quantitative and qualitative approaches to compare Ganilla’s output with that of a CycleGAN, DualGAN, and CartoonGAN trained on the same data. They trained a pair of CNNs to assess the GANs’ proficiency at transferring style (trained on small portions of images from each artist) and content (trained on full-size photos). The style classifier scored CycleGAN highest, while the content classifier gave DualGAN the edge. Ganilla ranked highest when style and content scores were averaged.The researchers asked 48 people to (a) rate whether each GAN-made illustration looked like the illustrator’s work, (b) describe what they thought the picture showed, and (c) rank generated images in terms of overall appeal. They scored Ganilla’s output highest for mimicking the human illustrators and depicting the source content. However, they rated DualGAN’s output slightly more appealing. Yes, but: Based on examples in the paper, the training illustrations tended to be heavy on stylized human and animal characters, while the photos contain very few characters. We’re curious to see what Ganilla would do with more photos of people and animals. Why it matters: GANs are powerful creative tools, and — like printmaking and photography before them — they’re spawning their own adversarial dynamic in the arts. Artists working in traditional media have raised concerns about GANs being trained to make derivatives of their work. Now, digital artists are accusing traditional artists of creative theft for making paint-on-canvas reproductions of their AI-abetted digital compositions.We’re thinking: When it comes to art, we favor GANs as a creative partner. Learn about human and algorithmic approaches to evaluating generative adversarial networks in GAN Specialization Course 2: Build Better GANs on Coursera. To build your own CycleGAN for style transfer, stay tuned for Course 3: Apply GANS, coming soon to Coursera!", "image_caption": "Examples of Generative Adversarial Networks used for image to illustration translation", "metadata": {"article_id": "issue_59", "chunk_index": 7, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2027.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-59/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_59.html"}}
{"id": 31736305001, "type": "news_chunk", "title": "Global Surveillance Survey, AI’s Crisis of Reproducibility...", "subtitle": "Ready or Not", "content": "Dear friends, I read an interesting paper comparing the results of traditional passive learning (sitting in a lecture) versus active methods like the flipped classroom, where students watch videos at home and work on exercises in class. The paper is nicely summarized by this figure: The leftmost pair of bars shows that students learn more from active learning. Ironically, they feel they are learning more from passive methods, shown by the remaining bars. I’ve been using a flipped classroom for much of my teaching, with great results. Students watch lectures on Coursera, then come to the classroom to ask questions and work in small groups. This paper explains why many instructors are reluctant to switch to active learning, even though it’s more effective. The world needs much better education everywhere. I hope more educators who teach in person will embrace active learning methods. Keep learning! Independent research lab OpenAI designed virtual agents to play hide-and-seek. They evolved increasingly clever strategies, eventually hacking the game world’s physics to gain advantage. What happened: The researchers trained the agents to navigate and manipulate their environment and juiced them with reinforcement learning. Then they divided their creations into teams of hiders and seekers and set them loose in a virtual world that included movable blocks, walls, and ramps. How it works: Seekers scored points if they caught sight of a hider. Hiders scored if they finished a game without being seen. An agent could move or lock objects in place; but only the agent that locked a given object could unlock it again. The agents figured out the basics over the first several million rounds. Around game 22 million, hiders — which were given a grace period at the start of each round to scramble for cover — began building shelters out of the movable objects.Roughly 100 million rounds in, seekers learned to infiltrate these hideaways using ramps. A few million later, the hiders stymied this strategy by locking the ramps.The researchers say they didn’t expect the agents to learn much more. But around game 450 million, seekers discovered they could push blocks around even if they were standing on top. This allowed them to surf to hiders’ walls and walk right into their hideaways (as seen in the animation above).Hiders eventually discovered the final, unbeatable strategy: Lock up every moveable object they wouldn’t be using as a barricade, then lock themselves inside a shelter of movable walls. Why it matters: Hide-and-seek strategies could map to many real-world applications. For instance, rescue robots could be programmed as seekers — with rules restricting which types of objects are okay to pick up or move — to sift through rubble for survivors after a disaster.We’re thinking: Reinforcement learning continues to find clever solutions. But the need to play 480 million rounds limits such techniques to simulated environments. We look forward to breakthroughs in small-data RL that make it possible to apply such techniques to physical robots that can play, say, thousands of games before they wear out. Meta learning, which organizations including OpenAI have worked on, could be an important step in this direction.", "image_caption": "Simulated hide-and-seek environment", "metadata": {"article_id": "issue_6", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize201201.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-6/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_6.html"}}
{"id": 31736305002, "type": "news_chunk", "title": "Global Surveillance Survey, AI’s Crisis of Reproducibility...", "subtitle": "AI Knows Who Labeled the Data", "content": "The latest language models are great at answering questions about a given text passage. However, these models are also powerful enough to recognize an individual writer’s style, which can clue them in to the right answers. New research measures such annotator bias in several data sets.What’s new: Researchers from Tel Aviv and Bar-Ilan Universities uncovered annotator bias in several crowdsourced data sets.Key insight: Only a few dozen people may generate the lion’s share of examples in a crowdsourced natural-language data set (see graph above). Having an overly small team of annotators introduces bias that can influence a model’s behavior.How it works: Mor Geva, Yoav Goldberg, and Jonathan Berant studied three data sets: MNLI, OpenBookQA, and CommonsenseQA. They fine-tuned the BERT architecture for each of three experiments: The authors measured the change in BERT’s performance after giving input sentences an annotator label. This experiment probed the degree to which the annotator’s identity encoded the correct answer.Then they used BERT to predict the annotator of individual text samples. This tested whether the annotator’s style encoded the person’s identity.Finally, they observed the difference in performance when the test and training sets had no annotators in common versus when the training set included samples from test-set annotators. An increase in performance further confirmed the presence of annotator bias. Results: Performance improved an average of 4 percent across the three data sets when input text included an annotator label. The model inferred annotators most accurately in data sets created by fewer contributors. In two of three data sets, mixing in samples from test-set annotators during training improved test accuracy, implying that the model doesn’t generalize to novel annotators.Why it matters: Annotator bias is pernicious and difficult to detect. This work raises a red flag around the number of contributors to data sets used in natural-language research.We’re thinking: Benchmark data sets are used to identify the best-performing models, which drives further research. If the data is biased, it may lead that research astray. Here’s hoping this work inspires further enquiry into sources of bias and ways to assess and mitigate it.", "image_caption": "Proportion of examples covered by number of annotators (sorted by number of annotations)", "metadata": {"article_id": "issue_6", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ANNOTATOR.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-6/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_6.html"}}
{"id": 31736305003, "type": "news_chunk", "title": "Global Surveillance Survey, AI’s Crisis of Reproducibility...", "subtitle": "Robots Put Down Stakes", "content": "Construction projects require teams of surveyors who continually map blueprints to precise, real-world locations. Drones might do it faster, saving time and money.What’s new: Civdrone, a startup with offices in New York and Tel Aviv, is developing a platform that uses drones to place surveying stakes around construction sites.How it works: The company uses off-the-shelf drones, each piloted by a human operator and equipped with a quiver of stakes.Where a survey marker is needed, a drone flies to the location, lands, and stabs a stake into the ground using a small pile driver.Each stake is topped with a QR code, which the drone encodes with the location’s GPS coordinates and elevation. The QR code can also contain information such as the presence of a gas pipe buried below.Construction workers can use a phone or dedicated QR-code reader to read the information. Behind the news: Construction is a hot area for drones, where mostly they provide a bird’s-eye view of job sites to help builders plan, track progress, and spot hazards. One maker of software for commercial and industrial drones says the construction industry is its fastest-growing customer. Why it matters: Surveying ensures that buildings stay true to their designs and plumb even as the ground shifts from day to day. Highly trained surveyors can insert around a hundred markers per day. Civdrone says it can do the work four times faster.We’re thinking: Construction companies live and die by their ability to stay on schedule and budget. Eliminating even the smallest delays — such as workers waiting for surveyors to finish their work — can keep projects on track and maintain wiggle room for when bigger snafus inevitably occur.", "image_caption": "Drone used for surveying on construction site", "metadata": {"article_id": "issue_6", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize201.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-6/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_6.html"}}
{"id": 31736305004, "type": "news_chunk", "title": "Global Surveillance Survey, AI’s Crisis of Reproducibility...", "subtitle": "The Long and Short of It", "content": "Not long ago, text-to-speech systems could read only a sentence at a time, and they were ranked according to their ability to accomplish that limited task. Now that they can orate entire books, we need new benchmarks.What’s new: A Google research team discovered that the usual measure of text-to-speech quality — having human judges rate single-sentence examples for human-like realism — varies widely depending on how samples are presented. That makes the standard procedure insufficient to evaluate performance on longer texts.Key insight: Rob Clark and colleagues tested samples of various lengths and formats to see how they affected quality ratings.How it works: Judges rated human and synthesized voices reading identical news articles and conversational transcripts. The judges evaluated samples in three forms: paragraphs, isolated sentences making up those paragraphs, and sentences preceded by the prior sentence or two (which were not rated).For sentences accompanied by preceding material, the preceding material was presented in human, synthesized, or text versions. Results: Samples that included prior sentences earned higher scores than sentences in isolation, regardless of whether they were spoken by humans or machines. That is, the additional context made the synthesized voices seem more realistic. Moreover, readings of paragraphs scored higher than readings of their component sentences, showing that isolated sentences aren’t a good gauge of long-form text-to-speech.Why it matters: Metrics that reflect AI’s performance relative to human capabilities are essential to progress. The authors show that the usual measure of text-to-speech performance doesn’t reflect performance with respect to longer texts. They conclude that several measures are necessary.We’re thinking: As natural language processing evolves to encompass longer forms, researchers are setting their sights on problems that are meaningful in that context. This work demonstrates that they also need to reconsider the metrics they use to evaluate success.", "image_caption": "Google text-to-speech logo", "metadata": {"article_id": "issue_6", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-2420at2012.32.1720PM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-6/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_6.html"}}
{"id": 31736305005, "type": "news_chunk", "title": "Global Surveillance Survey, AI’s Crisis of Reproducibility...", "subtitle": "Are Those Results Reproducible?", "content": "As deep learning becomes more resource-intensive, labs with better funding tend to achieve better results. One consequence is that less wealthy organizations often can’t replicate state-of-the-art successes. Some observers are calling it a crisis.What’s new: Members of the deep learning community are asking researchers to be more forthright about the hardware, software, and computing power they used to achieve their results, according to Wired. That could help other researchers economize in seeking to replicate them.How it works: NeurIPS asks that authors submitting papers to its December conference include a reproducibility checklist. Submissions must provide clearly written descriptions of algorithms, mathematical underpinnings, and models. Also, how much memory they needed, how much data they crunched, and — crucially — the number of times they ran the model. A link to the source code, too.Theoretical claims must include a list of assumptions, and the authors must publish the complete proof.Authors must back up figures and tables with either an open data set or a simulation. Behind the news: In 2005, Stanford professor John Ioannidis published the landmark paper, “Why Most Published Research Findings Are False.” The work pointed out that science in many disciplines — particularly social psychology and medicine — relies on foundational research that hasn’t been replicated. Many observers fear that AI could fall into the same trap.Why it matters: Science rests on hypotheses confirmed by experiments that yield consistent results every time they’re performed. AI is making rapid progress, but building on results that haven’t been verified puts that momentum at risk.We’re thinking: In the natural sciences, unverified results fuel skeptics of anthropogenic climate change, who appeal to uncertainty to avoid decisive action on the greatest challenge of our time. Maintaining the highest scientific standards in AI is the best protection against critics who might take advantage of this issue to impede progress in the field.", "image_caption": "Arxiv logo", "metadata": {"article_id": "issue_6", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-2420at2012.35.5620PM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-6/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_6.html"}}
{"id": 31736305006, "type": "news_chunk", "title": "Global Surveillance Survey, AI’s Crisis of Reproducibility...", "subtitle": "Watching the Watchers", "content": "A growing number of nations use AI to track their citizens. A new report sheds light on who’s watching and how. What’s new: Published by Carnegie Endowment for International Peace, “The Global Expansion of AI Surveillance” details which countries are buying surveillance gear, which companies are supplying it, and what technologies are most in-demand.What the report says: Of 176 countries surveyed, at least 75 use some combination of face recognition, location tracking, and predictive policing. This list of users spans advanced democracies, including the U.S., Germany, and the UK, to absolute dictatorships. Countries with the largest defense budgets are most likely to invest in AI-driven surveillance.The U.S. and China are the top producers of equipment. Huawei is the most prolific, selling to governments in 50 countries. IBM is the biggest U.S. dealer, providing surveillance systems to 11 countries.The report simply lists nations, suppliers, and applications. It doesn’t evaluate whether particular users or uses violate international human-rights agreements. Methodology: The authors drew their information from news reports. They accepted information as reported by established sources like The New York Times and Economist. They gathered corroborating accounts before relying on less rigorous sources like blogs.Why it matters: Surveillance networks are deeply rooted even in bastions of liberal democracy like London. They can support public safety, as in New South Wales, Australia, where smartcams spot drivers using a phone behind the wheel. But they also promote social biases and erode trust in authority and, at their worst, they’re powerful tools for repression. Baltimore’s secret drone-policing fiasco shows how an all-seeing eye can lead well-intentioned authorities in the direction of invasive dystopia.We’re thinking: Tracking which governments use which technology is important because it empowers citizens to react. The AI community, in particular, should take a proactive stance in promoting wise use of these technologies.", "image_caption": "AI tracking on pedestrians", "metadata": {"article_id": "issue_6", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize204.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-6/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_6.html"}}
{"id": 8922122001, "type": "news_chunk", "title": "Deepfakes Against Oppression, Alexa Hears With Her Eyes...", "subtitle": "News", "content": "Dear friends, There’s a lot we don’t know about the future: When will a Covid-19 vaccine be available? Who will win the next election? Or in a business context, how many customers will we have next year? With so many changes going on in the world, many people are feeling stressed about the future. I have a practice that helps me regain a sense of control. Faced with uncertainty, I try to: Make a list of plausible scenarios, acknowledging that I don’t know which will come to pass.Create a plan of action for each scenario.Start executing actions that seem reasonable.Review scenarios and plans periodically as the future comes into focus. For example, back in March, I did this scenario planning exercise. I imagined quick (three months), medium (one year), and slow (two years) recoveries from Covid-19 and made plans for managing each case. These plans have helped me prioritize where I can. The same method can apply to personal life, too. If you’re not sure you’ll pass an exam, get a job offer, or be granted a visa — all of which can be stressful — you can write out what you’d do in each of the likely scenarios. Thinking through the possibilities and following through on plans can help you navigate the future effectively no matter what it brings. Bonus: With a training in AI and statistics, you can calculate a probability to each scenario. I’m a fan of the Superforecasting methodology, in which the judgements of many experts are synthesized into a probability estimate. I refer to this site as a source of probability estimates as well. There will always be uncertainty, but with a little discipline, imagination, and foresight, we can still move forward with confidence. Keep learning! Documentary filmmakers often shield the identities of people who might be harmed for speaking out. But typical tactics like blurring faces and distorting voices can make it hard for audiences to connect emotionally. A new documentary uses deepfakes to protect the privacy of at-risk subjects.What’s new: The makers of the HBO documentary “Welcome to Chechnya” deepfaked faces of gay men and women fleeing the Russian republic of Chechnya, where LGBTQ people are being persecuted, the New York Times reported. How it works: Visual effects supervisor Ryan Laney developed the process, which he calls Censor Veil, to paint a realistic decoy face over each of the film’s 23 subjects. The process combines an autoencoder with conventional visual effects, Laney told The Batch.U.S. LGBTQ activists volunteered to have their faces stand in for those of interviewees. Their images were captured using an array of nine cameras.The filmmakers blurred the faces deliberately to signal to the audience that identities were hidden. What they’re saying: “This technology allowed us to just stretch the faces . . . over the images that I shot in the film. The face moves exactly the same way. It smiles, it cries in exactly the same way, but it is somebody else’s face.” — David France, director of “Welcome to Chechnya,” in Variety. Behind the news: An estimated 40,000 gay men and women live in Chechnya. They are at risk of arrest, torture, and detention in secret camps. Many have been killed.Why it matters: This technique provides a new way for journalists to preserve the impact of credible witnesses while protecting their privacy.We’re thinking: Deepfakes are infamous for their potential to propagate mistaken identities. This work (and similar initiatives like the BLM Privacy Bot) demonstrates that swapping one person’s face for another’s can have a socially beneficial use.", "image_caption": "Excerpts of HBO documentary \"Welcome to Chechnya\"", "metadata": {"article_id": "issue_60", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2026.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-60/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_60.html"}}
{"id": 41214192001, "type": "news_chunk", "title": "Mapping Wildfires, Compressing Video, Humanizing Benchmarks", "subtitle": "News", "content": "Dear friends, My father recently celebrated a milestone: He has completed 146 online courses since 2012. His studies have spanned topics from creative writing to complexity theory. Ronald Ng is a great example of lifelong learning. For him, learning is not a task or a responsibility. It’s a joy. “The joy of learning helps keep the mind sharp and allows us to appreciate the beauty of the subject matter,” he says. “We need to remain mentally young and have the same sense of wonderment” we had as children. And he’s not just taking online courses because he has nothing else to do. At age 74, he continues to work as a hematologist and serves as a court-appointed mediator in his spare time. You never know when learning will show its true value. As a doctor, my father had a patient who suspected he had been poisoned by mercury. The patient’s blood work didn’t show any evidence of this. But my father recalled a course in forensic medicine from Nanyang Technological University, where he had learned that mercury accumulates in hair. He took a hair sample from the patient and found the toxic metal in it. Then he was able to treat the patient appropriately. Growing up, I enjoyed having a father who played violin in the Hong Kong Philharmonic and followed the stars through a telescope on the roof of our apartment building. He taught me a lesson he learned as a volunteer in the army, where he discovered a truth that transcends the knowledge he gained studying subjects like military medicine and leadership: “We need very little in life to make us happy, provided we have the frame of mind to enjoy whatever we have.” You can read an interview with him along with a list of courses he has taken here. I hope his story inspires you to keep learning until you are 74, and well past that, too. Keep learning! An AI-powered eye in the sky is helping firefighters control woodland blazes.What’s new: California used maps drawn by neural networks to fight fires that threatened Yosemite National Park earlier this year, according to Wired. CalFire, the state’s firefighting agency, hopes the technology will help it better track wildfires, which can move quickly and erratically in windswept, mountainous terrain.How it works: U.S. military drones provide California with aerial imagery that human analysts use to map fire perimeters. But that process can take hours. The Pentagon’s Joint AI Center hired San Francisco startup CrowdAI to build a model that converts flyover videos into wildfire maps in less than 30 minutes. CalFire plans to make the maps available to firefighters through a mobile app. The system trained on infrared videos from MQ-9 Reaper drones. Human annotators had labeled and geotagged fires in their frames.CrowdAI used a proprietary image segmentation model to outline a fire’s extent, the company’s chief executive Devaki Raj told The Batch.Human analysts check the model’s output before passing it along to firefighters. Behind the news: A number of teams are working on AI systems designed to mitigate the impact of natural disasters. AI for Digital Response analyzes text and photos in Twitter to identify damaged infrastructure, calls for aid, and other relief-related topics. The platform has been used to evaluate damage of earthquakes and hurricanes, but it has yet to be used to respond to a crisis in real time.Disaster modeling startup One Concern, which uses AI to predict earthquake damage, works with several local U.S. governments and international financial institutions. However, critics have raised concerns about the system’s accuracy in predicting earthquake damage.NeurIPS 2020 will host a December workshop to bring together machine learning engineers and first responders. Why it matters: Wildfires move fast, and maps that are even a few hours out of date can put people and property at risk. As climate change makes wildfires more frequent and more destructive, firefighters need tools that will help them combat blazes quickly and efficiently.We’re thinking: DeepLearning.AI’s team in California has been experiencing the fallout from forest fires firsthand. We’re eager to see AI play a bigger role in disaster relief.", "image_caption": "Series of images related to a technology used to draw maps during a fight fire emergency", "metadata": {"article_id": "issue_61", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Fire2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-61/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_61.html"}}
{"id": 7978355001, "type": "news_chunk", "title": "AI Researchers Under Fire, RL Agents in Danger, Bias in Synthetic", "subtitle": "News", "content": "Dear friends, Today Landing AI, where I am CEO, launched LandingLens, an AI-powered platform that helps manufacturers develop computer vision solutions that can identify defective products. For AI to benefit a wide range of industries, we need platforms that enable experts in a variety of fields to build and deploy models. LandingLens is a step in this direction, and it’s available to manufacturers immediately. A major challenge to taking advantage of AI throughout the economy is the sheer amount of customization needed. To use computer vision to inspect manufactured goods, we need to train a different model for each product we want to inspect: each smartphone model, each semiconductor chip, each home appliance, and so on. How can Landing AI build models for thousands of products without hiring thousands of machine learning engineers? It’s much better to empower the manufacturers to build and deploy these models themselves. LandingLens enables experts in manufacturing — rather than experts in machine learning — to collect data, train models, deploy them, and carry out continuous learning. It helps them make sure their models work and scale up deployments. If the test data distribution drifts and the algorithm’s performance suddenly degrades, they’re empowered to collect new data and retrain the model without being beholden to an outside team. Here are a few unique features of LandingLens: Rather than holding the training set fixed and trying to improve the model, we hold the model fixed and help manufacturers improve the training set. We’ve found that this approach leads to faster progress in production settings.Rather than focusing on building models that recognize defects better than humans can, our tools aim to improve human-level performance. The better humans can recognize defects, the more consistently they’ll label those defects in training data, and the better the trained models will be. This is a very different philosophy from usual in AI research, where the goal often is to beat human-level performance. Having led AI teams at large consumer internet companies, I believe it’s time to take AI beyond the technology industry, to all industries. We’ve been building this platform for over a year, and I’m excited to be able to talk about it publicly. I hope that LandingLens — and other verticalized AI development platforms to come — will lower the bar for industrial deep learning and spread the benefits of AI throughout the economy. Keep learning! Controversy erupted over the need for transparency in research into AI for medicine.What’s new: Google Health introduced a system that purportedly identified breast cancer more accurately than human radiologists. But the search giant’s healthcare division failed to disclose details that would have enabled others to reproduce its results, dozens of critics wrote in a letter to Nature (also published on Arxiv).The critique: Researchers at Harvard, University of Toronto, Vector Institute, and elsewhere argue that AI systems used to diagnose life-threatening conditions should meet high standards of transparency. The Google research fell short on several counts: The authors didn’t release the trained model for others to verify their results.Although they mentioned the framework and libraries used, they omitted training details such as learning rate, type of optimizer, number of training epochs, and data augmentation techniques. That’s like listing the ingredients in a cake recipe without disclosing the amounts, Benjamin Haibe-Kains of the University of Toronto, who co-authored the critique, told The Batch.One dataset used in the study, Optimam, is readily available. However, the authors also used patient data that remains private. In lieu of that dataset, the critics argue, the authors should have disclosed labels and model predictions that would allow for independent statistical analysis.Other details were also missing, leading to questions such as whether the model trained on a given patient’s data multiple times in a single training epoch. The response: In a rebuttal published in Nature, the Google researchers said that keeping the model under wraps was part of “a sustainable venture to promote a vibrant ecosystem that supports future innovation.” The training details omitted are “of scant scientific value and limited utility to researchers outside our organization,” they added. They held back the proprietary dataset to protect patient privacy.Behind the news: AI researchers are struggling to balance trade secrets, open science, and privacy. The U.S. Food and Drug Administration hosted a workshop earlier this year aimed at developing best practices for validating AI systems that interpret medical images.Why it matters: Transparency makes it possible for scientists to verify and build on their colleagues’ findings, find flaws they may have missed, and ultimately build trust in the systems they deploy. Without sufficient information, the community can’t make rapid, reliable progress.We’re thinking: There are valid reasons to withhold some details. For instance, some datasets come with limitations on distribution to protect privacy. However, outside of circumstances like that, our view is that researchers owe it to each other to make research findings as reproducible as possible.", "image_caption": "Data related to a system that purportedly identified breast cancer", "metadata": {"article_id": "issue_62", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2023.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-62/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_62.html"}}
{"id": 52686398001, "type": "news_chunk", "title": "Halloween Special! Skeletons in the AI Closet including Bias", "subtitle": "Trick or Treat!", "content": "Dear friends, Welcome to this special Halloween issue of The Batch! In AI, we use many challenging technical terms. To help you keep things straight, I would like to offer some definitions that I definitely would not use. I hope you’ll find this alternative AI glossary a breath of fresh scare: Activation function: An incantation used to raise the deadDropout: A portal to another dimension that suddenly appears underfootEarly stopping: When you’re tired of collecting candy and you go home to bedFeature extraction: Getting a vampire’s fangs out of your neckGreedy policy: Self-explanatory when trick-or-treating Hinge loss: When the squeaky door falls off of a haunted houseLearning rate: How quickly werewolves realize they can’t break down your door but can climb through your windowMini-batch: The amount of candy you have after early stoppingOverfit: When you’ve eaten so much Halloween candy you can’t button your clothesRandom forest: Where random witches live Happy Halloween to all who celebrate it. Now let’s get this party started! Keep learning, As the days grow short, we peer into the gathering night to glimpse dark shapes amid the shadows. Last year at this season, we trembled before rogue AGI, ubiquitous surveillance, and the chill winds of AI winter. Those goblins still dance just beyond the jack o’lantern’s candle — yet other shades now join them: algorithms that exploit our basest instincts, models that consume every watt we can generate, tribal drumbeats that divide our community. But we need not cower. Build the bonfire high! Face the dire omens! Let our very fears spur us to extinguish these demons forevermore!", "image_caption": "Bonfire", "metadata": {"article_id": "issue_63", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Bonfire120copy.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-63/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_63.html"}}
{"id": 73801394001, "type": "news_chunk", "title": "Turning Tables on Face Recognition, Testing GPT-3, Recognizing", "subtitle": "News", "content": "Dear friends,As I write this letter, the vote count is underway in yesterday’s U.S. presidential election. The race has turned out to be tight. In their final forecast last night, the political analysts at fivethirtyeight.com suggested an 89 percent chance that Joe Biden would win. What did that mean? In repeated trials, such as dice rolls or cohorts of patients with potentially fatal illness, it’s easy to define the probability of a given event. We have a set of possible universes, and the probability is the fraction of those universes in which the event occurs. We can also ask if a set of probabilistic predictions is calibrated. If so, then out of all the events predicted to occur with an 89 percent chance, around 89 percent of them — neither many more nor many fewer — actually occur. We want our learning algorithms’ probabilistic outputs to be calibrated, and there is a body of literature on this topic. But an election is a one-time event. What does a probability mean in this case? When fivethirtyeight.com says that Biden has an 89 percent chance of winning, I mentally append the phrase “under a certain set of modeling assumptions made by the fivethirtyeight team.” The analysts made a set of assumptions under which they built a number of different universes — some that went for Biden, some Trump — and found that Biden won in 89 percent of them. It’s important to remember that these universes are artificial constructs built on the assumptions that Nate Silver and his team chose. I find that organizations such as fivethirtyeight.com generally make reasonable assumptions. For example, one assumption might be that a state’s vote tally for a given candidate follows a Gaussian distribution, with mean and variance estimated from the polling data. Yet every model has flaws and fails to capture some effects. A model might assume that each state’s outcome is independent of other states — but what if there are pervasive problems with the postal service delivery of mail-in ballots, or systematic biases in polling that result in undercounting some demographics? That’s why, while I consider election polls to be useful, I don’t take their predictions at face value. Even though every model is flawed, good ones allow us to understand the world better. No one knows with certainty if it will rain tomorrow, but my decision to carry an umbrella will differ depending on the probability. That’s why I use probabilities to quantify uncertainties when I make decisions. I find that if you think in probabilities consistently, you’ll start to develop an intuitive feeling for what the numbers mean. When someone tells me something has an 89 percent chance of happening, I’ve heard similar statements enough times in enough different contexts to have an intuition for what might happen next. Like many others, I stayed up late watching the election results trickle in, worried about the future of the U.S. and the potential global impact of this momentous election. Whatever the outcome, let us commit to keep on fighting for fairness, justice, and human decency, and to do our utmost to bring the greatest possible good to the greatest number of people. Keep learning! Private citizens are using AI-driven surveillance to turn the tables on law enforcement.What’s new: Activists are using face recognition to identify abusive cops, according to The New York Times.How it works: Many jurisdictions allow police to wear face masks or conceal their name tags, a practice that critics say protects officers who use excessive force against citizens. Activists around the world are using off-the-shelf software and crowdsourced datasets to develop systems that identify cops in photos and videos. In Portland, Oregon, self-taught coder Christopher Howell built a face recognition system that he used to identify at least one local officer. He does not plan to make it available to the public. Trained on images gathered from news, social media, and a public database called Cops.Photos, the model recognizes about 20 percent of the city’s police, he said. Portland law enforcement has been accused of improperly using pepper spray, and smoke grenades, and assaulting journalists.Belarusian AI researcher Andrew Maximov built a system that identifies masked officers by matching visible features to photos on social media. Police in Belarus have violently suppressed crowds in recent weeks.Last year, Hong Kong protester Colin Cheung posted a video that demonstrates a tool he built to identify officers who operated without badges. Behind the news: Police use of face recognition, such as the previously undisclosed DC-area system reported this week by the The Washington Post, has come under intense scrutiny. Public outcry has led to restrictions in some countries. Why it matters: Like many powerful technologies, face recognition is a double-edged sword. In the hands of private citizens, it could help increase police accountability and stem abuses. But it could also lead to harassment and worse against cops and others who have done nothing wrong.We’re thinking: It seems inevitable that ordinary citizens would harness face recognition to fight back against cops who allegedly have abused human or civil rights. Democratization of technology is a wonderful thing, but it comes with important responsibilities. Individuals — as well as governments and businesses — need to take care to use face recognition ethically.", "image_caption": "Face recognition system identifying cops", "metadata": {"article_id": "issue_64", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Facrec1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-64/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_64.html"}}
{"id": 7225203001, "type": "news_chunk", "title": "AI Predicts the Vote, Face Recognition Looks for Criminals", "subtitle": "News", "content": "Dear friends, Beating human-level performance (HLP) has been a goal of academic research in machine learning from speech recognition to X-ray diagnosis. When your model outperforms humans, you can argue that you’ve reached a significant milestone and publish a paper! But when building production systems, I’ve found that the goal of exceeding HLP isn’t always as useful. I believe the time has come to rethink it. Landing AI, where I’m CEO, has been automating visual inspection for manufacturers. We’ve built computer vision systems that can look at photos of products on an assembly line and classify defects such as scratches and dents. But we’ve run into an interesting challenge: Human experts don’t always agree on the appropriate label to describe the damage. “Is this really a scratch?” If even human experts disagree on a label, what is an AI system to do? In the past, when I built speech recognition systems, I encountered a similar problem. In some audio clips, the person speaking mumbles, or noise in the background overwhelms their words. Despite several listens, no human can transcribe them with confidence. Even when the words spoken are clear, transcriptions can be inconsistent. Is the correct transcription, “Um, today’s weather,” or “Erm . . . today’s weather”? If humans transcribe the same speech in different ways, how is a speech recognition system supposed to choose among the options? In academic research, we often test AI using a benchmark dataset with (noisy) labels. If a human achieves 90 percent accuracy measured against those labels and our model achieves 91 percent, we can celebrate beating HLP! But when building commercial systems, I’ve found this concept to be only occasionally useful. For example, if an X-ray diagnosis system outperforms human radiologists, does that prove — via incontrovertible logic — that hospital administrators should use it? Hardly. In practice, hospital administrators care about more than beating HLP on test-set accuracy. They also care about safety, bias, performance on rare classes, and other factors on which beating HLP isn’t feasible. So even if you beat HLP on test-set accuracy, your system isn’t necessarily superior to what humans do in the real world. I’ve found that there are better ways to use the concept of HLP. Briefly, our goal as machine learning engineers should be to raise, rather than beat, HLP. I’ll expand on that thought in a future letter. Working on visual inspection, my team has developed a lot of insights into applications of AI in this domain. I’ll keep sharing insights that are generally useful for machine learning practitioners here and in DeepLearning.AI’s courses. But I would like to share manufacturing-specific insights with people who are involved in that field. If you work in ML or IT in manufacturing, please drop me a note at [email protected]. I’d like to find a way to share insights and perhaps organize a discussion group. Keep learning! Major polling organizations took a drubbing in the press after they failed to predict the outcome in last week’s U.S. elections. At least one AI-powered model fared much better.What’s new: Several companies that offer analytics services used machine learning to predict the next U.S. president. Their results ranged from dead-on to way-off, as reported by VentureBeat.How they work: The companies analyzed social media posts to determine how large groups of people feel about a particular candidate. Expert.AI came closest. It analyzed 500,000 posts and found that challenger Joe Biden was more closely associated with words like “hope” and “success,” while incumbent Donald Trump was often mentioned alongside words like “fear” and “hatred.” Ranking these words according to their emotional intensity and frequency, the system predicted that Biden would win the popular vote by 2.9 percentage points. As of November 11, Biden’s actual margin was 3.4 percent according to The New York Times.KCore Analytics drew on a pool of 1 billion Twitter posts by influential users and those containing influential hashtags. It used the popularity of a given user or hashtag as a proxy for a subset of the voting population and scored positive or negative sentiment using an LSTM-based model to predict each candidate’s chance of victory. In July, it predicted Biden would win the popular vote by 8 to 9 percent — nearly triple the actual measure as of November 11 — and wrongly predicted the outcome in several swing states.Advanced Symbolics parsed public data from Facebook and Twitter to create a list of 288,659 users it considered a representative sample of U.S. voters. Its method relied on linking the way people talked about certain issues, like crime or Covid-19, to a certain candidate. The company predicted that Biden would sweep the electoral college with 372 electoral votes. The democratic nominee has gained 279 electoral votes as of November 11. Behind the news: AI systems have made more accurate political predictions in the past. In 2017, Unanimous.AI correctly forecasted that Trump’s public approval rating would be 42 percent on his 100th day in office. KCore last year successfully predicted election results in Argentina, while Advance Symbolics claims to have accurately predicted 20 previous elections. Why it matters: Human pollsters arguably performed poorly this year. But their jobs aren’t threatened by AI — yet. We’re thinking: There’s plenty of room for improvement in predictive modeling of elections. But, as we said in last week’s letter, probabilistic predictions — whether they’re calculated by a human or a machine — are intended to convey uncertainty. The better people understand probabilities and how they’re modeled, the more comfortable they’ll be when events don’t match the most likely outcome according to public polls.", "image_caption": "Person choosing answers from a poll", "metadata": {"article_id": "issue_65", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2036.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-65/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_65.html"}}
{"id": 28395875001, "type": "news_chunk", "title": "Bias In Surprising Places, Retail Models Adjust to Covid...", "subtitle": "News", "content": "Dear friends, Last week, I wrote about the limitation of using human-level performance (HLP) as a metric to beat in machine learning applications for manufacturing and other fields. In this letter, I would like to show why beating HLP isn’t always the best way to improve performance. In many machine learning problems, labels are determined by a person who evaluates the same sort of input as a learning algorithm would. For instance, a human labeler may look at a picture of a phone to determine if it’s scratched, and an algorithm would examine a similar picture to learn to detect scratches. (Note that this is not always the case. A human labeling a cancer diagnosis on an X-ray image may also rely on a tissue biopsy from the patient, while an algorithm would use the resulting dataset to learn to diagnose cancer based on images alone.) In cases where labels were determined by a human by looking at the same input that an algorithm would, what are we to make of situations in which HLP is well below 100 percent? This just means that different people labeled the data differently. For example, the ground-truth labeler who created a test set may have labeled a particular phone as scratched, while a different labeler thought the same phone was not scratched, and thus made a mistake in marking this example. If the second labeler disagreed with the ground-truth labeler on 1 out of 10 examples, then HLP in this task would be 90 percent. In this situation, rather than trying to build a learning algorithm that achieves 91 percent accuracy, it would be better to look into how the two labelers formed their judgements and try to help them make their labels more consistent. For example, all labelers may agree that scratches smaller than 1 mm are not significant (y=0), and scratches greater than 3 mm are significant (y=1), but they label scratches between 1 mm and 3 mm inconsistently. If we can spot this problem and get the labelers to agree on a consistent standard — say, that 1.5 mm is the point at which the labels should switch from y=0 to y=1 — then we’ll end up with less noisy labels. Setting standards that make labels more consistent will actually raise HLP, because humans now agree with one another more frequently. At the same time, having more consistently labeled data will result in better machine learning performance. This improvement is more important in many practical applications than the academic question of whether an algorithm beat HLP. HLP does have a role to play in establishing baseline performance for estimating irreducible, or Bayes, error, which in turn helps with error analysis. You can learn more about this in Deep Learning Specialization Course 3 and Machine Learning Yearning. But the message I hope you’ll take away from this letter is that, when a human labeler has created the class labels that constitute ground truth and HLP is significantly less than 100 percent, we shouldn’t just set out to beat HLP. We should take the deficit in human performance as a sign that we should explore how to redefine the labels to reduce variability. Keep learning! Social biases are well documented in decisions made by supervised models trained on ImageNet’s labels. But they also crept into the output of unsupervised models pretrained on the same dataset.What’s new: Two image classification models learned social biases from ImageNet photos, according to a study by researchers Carnegie Mellon and George Washington University.How it works: The authors measured the extent to which Google’s SimCLRv2 and OpenAI’s iGPT associated types of people with certain attributes. Using images from CIFAR-100 and Google Images, they assigned each picture either a category (such as man, woman, white, black, or gay) or an attribute (such as pleasant, unpleasant, career, or family).Then they fed the images to the model to generate features.They compared the features generated in response to different types of people (say, men or women) with features of opposing pairs of attributes (say, pleasant and unpleasant). In this way, they could determine the degree to which the model associated men versus women with those attributes. Results: Features generated by both models showed social biases such as associating white people with tools and black people weapons. While SimCLRv2 tended to associate stereotyped attributes with certain categories more strongly, iGPT showed such biases toward a broader range of categories. For instance, features generated by iGPT associated thin people with pleasantness and overweight people with unpleasantness, and also associated men with science and women with liberal arts. Behind the news: ImageNet 2012 contains 14 million images annotated by human workers, who passed along their prejudices to the dataset. ImageNet creator Fei-Fei Li is spearheading an effort to purge the dataset of labels that associated genders, races, or other identities with stereotypes and slurs.Why it matters: When unsupervised models pick up on biases in a dataset, the issue runs deeper than problematic labels. The authors believe that their models learned social stereotypes because ImageNet predominantly includes images of people in stereotypical roles: men in offices, women in kitchens, and non-white people in general excluded from images showing situations that have positive associations such as weddings. Machine learning engineers need to be aware that a dataset’s curation alone can encode common social prejudices.We’re thinking: Datasets are built by humans, so it may be impossible to eliminate social biases from them completely. But minimizing them will pay dividends in applications that don’t discriminate unfairly against certain social groups.", "image_caption": "Collage of self portraits", "metadata": {"article_id": "issue_66", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2033.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-66/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_66.html"}}
{"id": 60603048001, "type": "news_chunk", "title": "Government AI Falls Short, Face Recognition for Bears, Research", "subtitle": "News", "content": "Dear friends, Over the last two weeks, I described the importance of clean, consistent labels and how to use human-level performance (HLP) to trigger a review of whether labeling instructions need to be reviewed. When training examples are labeled inconsistently, an AI that beats HLP on the test set might not actually perform better than humans in practice. Take speech recognition. If humans transcribing an audio clip were to label the same speech disfluency “um” (a U.S. version) 70 percent of the time and “erm” (a UK variation) 30 percent of the time, then HLP would be low. Two randomly chosen labelers would agree only 58 percent of the time (0.72 + 0.33). An AI model could gain a statistical advantage by picking “um” all of the time, which would be consistent with 70 percent of the time with the human-supplied label. Thus, the AI would beat HLP without being more accurate in a way that matters. Labeling training data consistently is particularly important for small data problems. Innovations like data synthesis using generative adversarial networks, data augmentation, transfer learning, and self-supervision expand the possibilities for small data. But when I’m trying to train a neural network on 1,000 examples, the first thing I do is make sure they’re labeled consistently. Let’s continue with last week’s example of determining if a scratch is significant based on its length. If the labels are noisy — say, different labelers used different thresholds for labeling a scratch as significant (the left-hand graph in the image above) ̧— an algorithm will need a large number of examples to determine the optimal threshold. But if the data were clean — if all the labelers agree on the length that causes the label to switch from 0 to 1 (the right-hand graph) — the optimal threshold is clear. Learning theory affirms that the number of examples needed is significantly lower when the data is consistently labeled. In the simple example above, the error decreases on the order of {1 / √ m} in the case on the left, and {1/m} in the case on the right, where m is the training set size. Thus, error decreases much faster when the labels are consistent, and the algorithm needs many fewer examples to do well. Clean labels are generally helpful. You might be better able to get away with noisy labels when you have 1 million examples, since the algorithm can average over them. And it’s certainly much harder to revise 1 million labels than 1,000. But clean labels are worthwhile for all machine learning problems and particularly important if you’re working with small data. Keep learning! The U.S. government’s effort to take advantage of AI have not lived up to its promise, according to a new report.What’s new: Implementations of machine learning systems by federal agencies are “uneven at best, and problematic and perhaps dangerous at worst,” said authors of a survey by the Administrative Conference of the United States, Stanford Law School, and New York University School of Law.What they found: Less than half of civilian federal agencies surveyed used some form of AI, and about 7 percent of them accounted for the lion’s share of AI implementations evaluated. The most common implementations were in law enforcement, health care, and financial regulations. Examples include the Border Patrol’s use of face recognition for its Biometric Entry/Exit program and the Securities and Exchange Commission’s Corporate Issuer Risk Assessment, which helps regulators detect faults in companies’ financial reports. Only 12 percent of implementations used deep learning. The rest used approaches such as logistic regression with structured data (which the authors deemed lower sophistication) or random forests with hyperparameter tuning (which they judged medium sophistication).Government agencies are legally required to explain their decisions, such as why a person was denied benefits. But algorithms often reach conclusions for reasons that are not explainable, making it difficult to appeal.Around half of the systems evaluated were developed by outside contractors. The authors recommend greater investment in in-house talent because it’s more likely to tailor systems appropriately to government uses. Yes, but: The authors relied primarily on publicly available information, which may not contain sufficient technical perspective for such analysis. In addition, the survey period ended in August 2019, so the report excludes systems deployed since then.Why it matters: AI could help government agencies operate more effectively and efficiently, but this report shows that they have a long way to go to fulfill that vision.We’re thinking: Governments have an obligation to audit AI systems for performance, fairness, and compliance before rolling them out. Yet most agencies (and, for that matter, most corporations) don’t have the capability to assess these factors. We need tools that that enable a variety of stakeholders to define clear standards and assess whether they’ve been met, so we can spot problems, mitigate risks, and build trust in automated systems. We hope that companies such as Credo AI (which is backed by Andrew Ng’s AI Fund) can help.", "image_caption": "Graphs with data related to AI use cases", "metadata": {"article_id": "issue_67", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2039.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-67/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_67.html"}}
{"id": 1781682001, "type": "news_chunk", "title": "Phantom Menace, GAN for Pajama Zooming, When AI Goes Wrong", "subtitle": "News", "content": "Dear friends, The rise of AI creates opportunities for new startups that can move humanity forward. In the 1990s, the internet was embraced successfully by incumbent companies including Apple and Microsoft, but it also inspired hugely impactful startups like Amazon, Facebook, and Google. Similarly, AI now is empowering forward-looking incumbent companies — many of them former internet startups — and creating massive opportunities for new startups as well. I’ve been thinking about what I can do to help members of the DeepLearning.AI community who wish to create a company. At AI Fund (where I am managing general partner), I speak with many entrepreneurs who have either started or are thinking of starting a new company. I’ve noticed a few factors that increase the odds of success: Domain knowledge coupled with identification of a problem: Do you deeply understand an industry and a specific pain point? Have you experienced and struggled with solving the problem yourself?Initial hypothesis of a solution: Do you have a sense that AI-based automation can lead to a solution? Is it technically feasible and likely to solve the problem in a responsible and value-creating way? Large market opportunity: Is there a large number of potential customers who have a similar problem?Drive and grit: Startups move forward only because the people involved make it happen. Are you ready to struggle through the hard work, pain, and uncertainty that comes with starting a company? Many startup founders quietly obsess about startup ideas for years, since it can take a lot of thought and investigation to work out the nuances. (Before I cofounded Coursera, I had spent about five years obsessing over how to deliver effective online education. You can read more about my early experiences in “Origins of the Modern MOOC.”) Identifying a problem is one of the hardest steps. I didn’t understand this until I saw a lot of examples. So many things compete for attention in today’s world (in both business-to-business and business-to-consumer settings) that unless your offering creates compelling value, it’s hard to get people to pay attention. One test of a problem you’ve identified is: Have a number of people told you they would go to the trouble of exploring possible solutions? I’d love to hear from those of you who are, or aspire to become, entrepreneurs. My teams at DeepLearning.AI and AI Fund plan to hold a series of entrepreneur-oriented events next year. If the success factors I listed above describe you, and especially if you’re still in the early stages (say, from having identified a problem but not yet decided to start a company to having built a product and being ready to raise capital), please take this short survey and let us know how we can help you in your startup journey. Keep learning! A fighter pilot battled a true-to-life virtual enemy in midair.What’s new: In the skies over southern California, an airman pitted his dogfighting skills against an AI-controlled opponent that was projected onto his augmented-reality visor. How it works: The trial aimed to test the integration of an autonomous fighter agent developed by EpiSci with high-brightness, low-latency, augmented-reality technology from Red Six Aerospace. Red Six CEO Dan Robinson, an alumnus of the UK’s Royal Air Force, piloted a plane of his own design. EpiSci controlled a simulated Chinese J-20 stealth fighter using a combination of deep learning, reinforcement learning, and rules-based modeling.EpiSci’s agent previously ran on ground-based hardware in a simulation. The trial confirmed that it ran well on the resources available in the Red Six craft and responded to real-world input from GPS and inertial sensors, Chris Gentile, EpiSci’s VP of tactical autonomous systems, told The Batch.The event also confirmed that EpiSci could limit its agent to behaviors useful for training beginners — “It wasn’t kill-at-any-cost,” Gentile said — without compromising its ability to react to its human opponent’s tactics and errors. The U.S. Air Force plans to begin testing the system for pilot training next year. Behind the news: EpiSci honed its agent technology in the U.S. Defense Advanced Research Projects Agency (Darpa) Alpha Dogfight program, in which a pilot on the ground helmed a flight simulator to fight AI-controlled foes. (See our report on the program, “AI Versus Ace.”) Darpa recently awarded the company a grant to develop AI systems for air combat.Why it matters: Flight simulators don’t replicate all the challenges pilots face in the air — for instance, G-forces — and pitting human pilots against one another in the air is dangerous and expensive. Battling AI-controlled agents in augmented reality could make combat training more effective, safer, and cheaper.We’re thinking: The ethical boundaries of military AI demand careful navigation. Using machine learning to make training pilots safer may be a reasonable application. Building aircraft that can fight on their own, however, is a different matter. The AI community needs to draw bright red lines to ensure that AI is beneficial and human. To that end, we support the United Nations proposed ban on autonomous weapons.", "image_caption": "Fighter pilot in action", "metadata": {"article_id": "issue_68", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/DOGFIGHT.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-68/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_68.html"}}
{"id": 97877612001, "type": "news_chunk", "title": "Autonomous Helium Balloons, Seeing Eye AI, Muppet Models Estimate", "subtitle": "News", "content": "Dear friends, Like many people in the AI community, I am saddened by the sudden departure from Google of ethical AI researcher Timnit Gebru. Timnit is a tireless champion of diversity and fairness in AI. Her work, for example highlighting bias in face recognition systems, has been a productive influence on many researchers and companies. At the same time, my friend Jeff Dean built Google AI into a world-class engineering organization. I’ve seen him speak up for diversity when no one else in the room was doing so. Having not yet spoken to either of them, I hesitate to offer my opinion on the matter at this time. But the situation highlights a larger problem in the AI community: lack of a shared set of values (such as fairness, diversity, and transparency) and norms (such as what to do when there’s a problem). In academia, all scholars place high value on the pursuit and dissemination of knowledge. In medicine, all doctors recognize that the wellbeing of patients is their primary duty. We need that kind universal commitment in AI. We’re building technology that affects billions of people without a coherent set of guiding principles. Many companies and think tanks have published their own codes of ethics, and these statements are important — but they are far from sufficient. We need a set of values and norms that are shared across our entire community and transcend any one company. That way, we can collectively hold individuals, companies, and perhaps even governments accountable to them and operate for the common good even when we disagree. How can we bring the AI community together around shared values and norms? I encourage you to spend time with your teams, collaborators, and peers to discuss this difficult question. It’s past time to lay the foundation for a set of values and norms that all AI practitioners will proudly stand up for. Keep learning! Helium balloons that beam internet service to hard-to-serve areas are using AI to navigate amid high-altitude winds. What’s new: Loon, the Alphabet division that provides wireless internet via polyethylene blimps, used reinforcement learning to develop an autonomous control system that keeps the vehicles closer to their targets while consuming less energy than its hand-coded predecessor. The new algorithm controls Loon’s fleet over Kenya, where the company launched its first commercial service in July. How it works: Balloons navigate by ascending or descending to catch winds that push them in the direction desired. Loon used QR-DQN, a distributional reinforcement learning algorithm, to train a feed-forward network to determine when the balloon should ascend, descend, or stay put. Working with Google AI’s Montreal team, Loon researchers modified a weather dataset from the European Center for Medium-Range Weather Forecasts to generate a large number of wind scenarios. They modeled the physics of balloon flight within these synthesized wind fields to build simulations used to train and evaluate the model.In training, the model received the maximum reward when the balloon was within 50 kilometers of its base station, the range at which it reliably sends and receives signals. The reward halved with every 100 kilometers the balloon strayed.In use, instruments on board feed the model wind readings from the balloon’s current location and wake. It estimates wind conditions at nearby locations using a Gaussian process that analyzes weather readings from nearby balloons and forecasts from the European Center for Medium-Range Weather Forecasts. A pump inflates or deflates the balloon accordingly.In real world tests against the earlier flight control system, the new algorithm stayed on target 7 percent more often while cutting energy consumption by 4 watts day. Behind the news: Loon began within Alphabet’s experimental X division in the early 2010s and became a for-profit subsidiary in 2018. The company provided emergency internet access to Puerto Rico after hurricane Maria in 2017, and to Peru following a massive earthquake in 2019. A single balloon can serve several thousand individuals spread over 80 square kilometers. Why it matters: Billions of people, including two-thirds of all school-age children, don’t have access to the internet. In the Covid era, with students and workers alike staying home, the digital divide is more acute than ever. Cutting the cost of service to remote areas could bring many of those people into the information economy.We’re thinking: In Kenya, where Loon’s first balloons are flying, better connections could boost the growing community of AI engineers. To learn more about Kenya’s AI scene, check out our Working AI profile of data scientist and DeepLearning.AI ambassador Kennedy Kamande Wangari.", "image_caption": "AI-driven balloon reaching high altitude", "metadata": {"article_id": "issue_69", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/LOON.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-69/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_69.html"}}
{"id": 16482627001, "type": "news_chunk", "title": "Google Achieves Quantum Supremacy, Amazon Aims To Sway Lawmakers", "subtitle": "Quantum Leap", "content": "Dear friends, Thinking about the future of machine learning programming frameworks, I recently reread computer scientist Fred Brooks’ classic essay, “No Silver Bullet: Essence and Accidents of Software Engineering.” Three decades after its initial publication, it still holds important lessons for software engineers building ML tools. Despite progress from typewriters to text editors, why is writing still hard to do? Because text editors don’t address the most difficult part: thinking through what you want to say. Programming tools have the same limitation. I’m glad to be coding in Python rather than Fortran. But as Brooks points out, most advances in programming tools have not reduced the essential complexity of software engineering. This complexity lies in designing a program and specifying how it should solve a given problem, rather than in expressing that design in a programming language. Deep learning is revolutionary because it reduces the essential complexity of building, say, a computer vision system. Instead of writing esoteric, multi-step software pipelines comprising feature extractors, geometric transformations, and so on, we get data and train a neural network. Deep learning hasn’t just made it easier to express a given design; it has completely changed what we design. As we work on ML programming frameworks, we should think about how to further reduce the essential complexity of building ML systems. This involves not just specifying an NN architecture (which is indeed waaay easier to do in TensorFlow or PyTorch than C++), but also deciding what is the problem to be solved and designing all the steps from data acquisition to model training to deployment. I don’t know what will be the key ideas for reducing this essential complexity, but I suspect they will include software reuse, ML model reuse (such as libraries of pretrained models) and tools not just for code versioning and reuse (like github) but also for data versioning and reuse. Breakthroughs in unsupervised and other forms of learning could also play a huge role. Even as I occasionally struggle to get an ML system to work (it’s not easy for me either), I am excited to see how our community is pioneering this discipline. Keep learning! P.S. My best learning creation so far, seven month-old Nova, just said her first words! 🙂 A leaked paper from Google’s quantum computing lab claims “supremacy” over conventional computers.What’s new: The U.S. space agency NASA, whose scientists are collaborating with Google on a quantum computer, accidentally published a paper describing the breakthrough. The Financial Times snagged a copy before it was taken down, naming machine learning, chemistry, and materials science as likely uses for the technology. Google declined to comment pending the paper’s official release.How it works: Google designed the special-purpose system, called Sycamore, to determine whether sets of randomly generated numbers were truly random. Researchers estimate that it would have taken the world’s fastest conventional supercomputer, IBM’s Summit, 10,000 years to solve the problem. Sycamore solved it in 3 minutes and 20 seconds, an early demonstration of the capability known as quantum supremacy. Instead of bits, quantum computers process information using qubits that can hold the values 1 and 0 simultaneously.Qubits can be entangled with one another to represent the totality of all the states of a system’s qubits.For example, two qubits can represent 11, 10, 01, and 00 at once. Three qubits can represent 111, 110, 100, 000, 001, 011, 101 simultaneously, and so on. Sycamore has 53 qubits.A major challenge is keeping quantum processors cold enough to prevent ambient heat from disturbing the fragile qubits. Behind the news: Physicist Paul Benioff wrote a paper in 1980 describing how quantum-mechanical phenomena like superposition and entanglement could be applied to computing. Google, IBM, Intel, and Microsoft lately have made substantial progress in implementing those ideas.Why it matters: Quantum computing’s promise of exponentially faster processing in particular applications has many in the AI community excited to apply it to tasks like search and pattern matching. There’s no telling when quantum AI will emerge, but when it does, it probably will require new types of models tailored to the peculiar nature of qubits.We’re thinking: The problem Sycamore solved doesn’t have much practical value, as computer scientist Scott Aaronson points out in his excellent quantum-supremacy FAQ. It’s more “like the Wright Brothers’ flight” circa 1903, he says: The technology works, but it will be a while before actual users can climb aboard.", "image_caption": "A quantum processor and one qubit’s energy-relaxation time “T1” plotted as a function of it’s operating frequency and time", "metadata": {"article_id": "issue_7", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-10-0120at2010.10.2120AM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-7/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_7.html"}}
{"id": 16482627002, "type": "news_chunk", "title": "Google Achieves Quantum Supremacy, Amazon Aims To Sway Lawmakers", "subtitle": "Anonymous Faces", "content": "A number of countries restrict commercial use of personal data without consent unless they’re fully anonymized. A new paper proposes a way to anonymize images of faces, purportedly without degrading their usefulness in applications that rely on face recognition.What’s new: Researchers from the Norwegian University of Science and Technology introduced DeepPrivacy, a system that anonymizes images of people by synthesizing replacement faces. They also offer the Flickr Diverse Faces dataset, 1.47 million images of faces with supplemental metadata, which they used to train DeepPrivacy.Key insight: The original images are never exposed to the face generator. Authors Håkon Hukkelås, Rudolf Mester, and Frank Lindseth argue that this strategy preserves privacy more effectively than traditional anonymization techniques like pixelizing and blurring.How it works: DeepPrivacy is a conditional generative adversarial network that synthesizes novel images similar to previously observed ones. A discriminator classifies images as real or generated, while a generator based on the U-Net architecture is optimized to create images that fool the generator. Single Shot Scale Invariant Face Detector detects faces in images.For each face, Mask R-CNN locates keypoints for eyes, nose, ears, and shoulders.Then the faces are replaced with random values.The generator architecture receives keypoints, which define the deleted face’s orientation, and the corresponding faceless images. From these inputs, it learns to create replacement faces that the discriminator can’t distinguish from real-world images in the training data. Results: The researchers processed the WIDER-Face dataset (roughly 32,000 images containing around 394,000 faces) using DeepPrivacy as well as traditional anonymization methods. Subjected to traditional techniques, Dual Shot Face Detector retained 96.7 percent of its usual performance. With DeepPrivacy, it retained 99.3 percent. The researchers don’t provide metrics to evaluate the relative degree of anonymity imparted by the various methods.Why it matters: Laws like the European Union’s General Data Protection Regulation set a high bar for data-driven applications by placing tight limits on how personal data can be used. DeepPrivacy transforms photos of people into a less identifiable format that still contains faces recognizable to neural networks.Yes, but: DeepPrivacy addresses the privacy implications of faces only. An image purged of faces but still containing, say, clothing with identifiable markings, such as an athlete’s number, would allow a sophisticated model to infer the wearer’s identity.We’re thinking: Machine learning’s reliance on data is both a gift and a curse. Aggregation of data has allowed for great progress in the field. Yet privacy advocates are inclined to keep personal data under wraps. DeepPrivacy is an intriguing step toward a compromise that could satisfy both AI engineers and users alike.", "image_caption": "DeepPrivacy results on a diverse set of images", "metadata": {"article_id": "issue_7", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-2720at2011.52.2720AM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-7/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_7.html"}}
{"id": 16482627003, "type": "news_chunk", "title": "Google Achieves Quantum Supremacy, Amazon Aims To Sway Lawmakers", "subtitle": "Nothing but (Neural) Net", "content": "Basketball coaches often sketch plays on a whiteboard to help players get the ball through the net. A new AI model predicts how opponents would respond to these tactics.What’s new: A team of researchers in Taiwan trained a conditional generative adversarial network on data from National Basketball Association games. They trained their network to show how players on the opposing team likely would move in response to human-drafted plays.How it works: The researchers built a two-dimensional simulation of a half court complete with a three-point line and a net. A coach can draw motion paths for five players represented by dots, as well as ball movements including passes and shots. No dunking, however. Once a coach has drawn a play, a generator determines how the five defensive players would react.A discriminator evaluates these movements to make sure they match realistic gameplay.The model then displays the coach’s play and the defensive maneuvers. Results: A cohort of NBA pros, basketball fans, and basketball non-fans evaluated the generated defenses for realism. While the non-pro fans and non-fans had a hard time spotting the computer’s defensive plays, the NBA pros could tell they were not designed by a human coach.Behind the news: Stat SportVU has collected real time player motion data for the NBA since 2011. The system uses six cameras to collect each player’s position and track who has possession of the ball, 25 times per second. It uses machine learning to identify events like dribbles and passes, and play types like drives, isolations, and screens.Why it matters: Pro sports is a high-stakes industry that has embraced technology to optimize performance. It’s conceivable that a neural network someday might generate AlphaGo-like winning tactics that no human had envisioned.We’re thinking: This model isn’t a slam-dunk, given that the pros weren’t fooled. However, it appears to be sophisticated enough to help teach beginners how to think strategically off the court.", "image_caption": "Animation showing how players on the opposing team likely would move in response to human-drafted plays", "metadata": {"article_id": "issue_7", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/basketball2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-7/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_7.html"}}
{"id": 16482627004, "type": "news_chunk", "title": "Google Achieves Quantum Supremacy, Amazon Aims To Sway Lawmakers", "subtitle": "Amazon Prepares for a Crackdown", "content": "Amazon is writing what it hopes will become U.S. law governing use of face recognition technology. What happened: At a press event announcing new features for Amazon’s Alexa smart-home service, Jeff Bezos told a reporter that his company’s lawyers are drafting a statutory framework to guide what he views as an inevitable federal crackdown on face recognition. Amazon sells the cloud-based face recognition service Rekognition, whose use by law enforcement agencies has raised alarm among civil liberties advocates.What it says: The company has released no details about the model legislation in progress. However, in February, Amazon VP of Global Public Policy Michael Punke published a blog that could provide clues to the company’s aims. Face recognition should be used in accordance with existing laws, Punke writes in the post proposing ethical guidelines for the technology. He points out that the U.S. Constitution’s Fourth Amendment and Civil Rights Act of 1964 explicitly outline an individual’s right to privacy and freedom from discrimination.Law enforcement groups, government agencies, and businesses using face recognition should be held to high standards of transparency, the post says.Law enforcement should be allowed to use the technology only to narrow down groups of suspects, and only when a model is at least 99 percent confident in its prediction. Models should never be used as the final arbiter of a person’s guilt or innocence. Behind the news: Face recognition’s rapid proliferation has spawned a widespread backlash in the U.S. cities. San Francisco and Oakland, California, and Somerville, Massachusetts, have banned the technology. California’s legislature is considering a statewide ban. Several bills restricting its use are wending their way through Congress, and two representatives have vowed to propose further legislation.We’re thinking: Punke’s guidelines are sound, and Amazon is well situated to understand how the technology could be abused. When industries propose their own regulations, though, legislators need to take special care to make sure any resulting laws benefit society as a whole.", "image_caption": "Amazon Rekognition screen capture", "metadata": {"article_id": "issue_7", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-3020at202.08.4320PM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-7/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_7.html"}}
{"id": 16482627005, "type": "news_chunk", "title": "Google Achieves Quantum Supremacy, Amazon Aims To Sway Lawmakers", "subtitle": "Putting Text Generators on a Leash", "content": "Despite dramatic recent progress, natural language generation remains an iffy proposition. Even users of the muscular GPT-2 text generator have to press the button a number of times to get sensible output. But researchers are figuring out how to exert greater control over generated text.What’s new: Pre-trained text generators generally require fine-tuning for a specific sort of output. A team at Salesforce developed a model aptly named CTRL that lets users determine the output genre, from news story to horror script, without further training.Key insight: The model is guided by control codes, human-written text tags that describe a desired output genre — including, yes, jokes. The model learns relationships between a given code and the intended style and content.How it works: CTRL, like the state-of-the-art language model BERT, is a modified transformer network trained in an unsupervised fashion on large-scale text corpora. Its training data include Wikipedia, Reddit, and Project Gutenberg’s library of digitized books. CTRL predicts the next word in a sequence based on learned relationships among words.During training, each input sequence comes with a control code. For example, material drawn from a contract would be coded Legal.During generation, one of these codes directs the model to produce text similar to the associated subset of the training data. Results: The researchers provide qualitative results demonstrating that control codes generate different styles of text in response to the same prompt. For example, given the prompt “the knife,” the Reviews code produces “a knife is a tool and this one does the job well,” while the Horror code yields “a knife handle pulled through the open hole in the front.” The paper offers no quantitative evaluation.Why it matters: The ideal text generator would produce diverse, relevant passages appropriate to a wide variety of uses. CTRL suggests that a single model with unsupervised training could meet this requirement.We’re thinking: Many people including GPT-2’s creators worry that more-capable text generators invite misuse. Could a CTRL-style approach reduce abuse by suppressing certain genres (say, blatant political disinformation) as well as supporting more effective text generation?", "image_caption": "GPT-2 text generator", "metadata": {"article_id": "issue_7", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CTRL2.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-7/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_7.html"}}
{"id": 16482627006, "type": "news_chunk", "title": "Google Achieves Quantum Supremacy, Amazon Aims To Sway Lawmakers", "subtitle": "Prelude to a Quake?", "content": "Geologists call them slow slips: deep, low-frequency earthquakes that can last a month but have little effect on the surface. A model trained to predict such events could help with forecasting potentially catastrophic quakes.What’s new: French and American seismologists trained a model to recognize acoustic patterns associated with slow slips where one tectonic plate slides beneath another. Some seismologists believe that slow slips shift stress from deep in a geological fault up to the Earth’s brittle crust, presaging potentially catastrophic quakes.How it works: The authors began by simulating slow slips in the lab using two sheets of synthetic material, like acrylic plastic, separated by a thin layer of a granular, sandy medium. The video above is a microscopic view of the sheets in action. The researchers recorded the acoustic signals emitted by the sheets and granular layer as they compressed. Then they divided the recording into short segments and fed them into a random forest model.The model found that the signal’s gradual variance from mean — rather than big, sudden jumps just before a slip — was the best predictor that the sheets were about to experience a laboratory version of a slow slip.Having ingested seismic data from the tectonic plate that runs from Canada to California between 2007 and 2013, the model predicted four of the five slow slips that occurred between 2013 and 2018. We’re thinking: Seismologists already provide short-term risk assessments for a given location and time span. This research could lead to long-term forecasts, months or years out, allowing planners to expedite earthquake safety upgrades that otherwise may be delayed due to their cost.", "image_caption": "Map of the area analyzed in Cascadia and sketch of the subduction zone", "metadata": {"article_id": "issue_7", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-10-0120at205.28.1420PM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-7/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_7.html"}}
{"id": 5464624001, "type": "news_chunk", "title": "New Coronavirus Treatments, Reimagining Robotaxis, Opening", "subtitle": "News", "content": "Dear friends, When a researcher works for a company, what rights should they have to publish their work, and what rights should the company that sponsored the work have? This issue has come up many times in the AI community across many companies, most recently around Timnit Gebru’s very public departure from Google, which involved a disagreement over research she was preparing to publish. Researchers and companies often share a desire to contribute ideas that move AI forward. At the same time, they can also have completely legitimate interests that may differ. Researchers may want to make their work available to the community, while the organizations that fund that work may want to keep certain inventions secret or patent them. Researchers and companies may be willing or unwilling, to varying degrees, to point out inconvenient truths that need to be addressed. It’s not always obvious how to balance these interests. For example: Should researchers be allowed to release any technology they wish, as long as they don’t publish confidential information?Alternatively, should companies (and universities) have the final say, including the right to stop publication of papers when it’s in their interest to do so? (This is the de facto policy in many companies today.)Should a company be responsible for ensuring the quality of research published under its name, or should this be left only to peer review? Conversely, If a researcher publishes a scientifically flawed paper, does the fault lie with the researcher, or with both the researcher and the company?What would be a reasonable prepublication review process within companies, and how can we ensure that it is applied fairly and consistently?What rights and responsibilities do researchers and companies have with respect to patent filings of inventions in which they both played a part? I’ve submitted publications for review, and I’ve set policies that govern how others’ work should be reviewed. As a co-author, I’ve also pulled publications when I felt they were not up to standard. These experiences have shown me that the answers to these questions may differ, depending on the parties involved. What is clear, though, is that researchers and companies need to set clear expectations ahead of time, and then abide by them consistently. Both parties have an interest in avoiding situations where a researcher spends substantial time and energy working on ideas with the intent to publish them, only to be surprised that they’re unable to do so. I would like to see the AI community get together and establish a fair set of rules that balance everyone’s interests. Every researcher, company, and university is different, and possibly no one-size-fits-all answer will work for everyone. But if we set expectations collectively, we might be able to nudge companies toward a balanced set of policies around publications. What rules do you think would be fair? Let me know via social media or by sharing your ideas here. Keep learning! Covid Moonshot, an open source project to vet potential medicines using machine learning, is closing in on compounds that might help curb Covid-19.What’s new: Four new antiviral drugs identified by the project are ready to advance to animal trials, according to IEEE Spectrum. Unlike vaccines, which prevent infection, antivirals treat people who are already infected.How it works: Last spring, PostEra, a UK chemistry company, invited scientists to submit designs for molecules with potential to thwart the virus. It used a semisupervised deep learning platform to analyze more than 14,000 submissions. You can read our earlier report on the project here. More than 30 teams from industry, academia, and independent labs synthesized 1,000 of the most promising compounds.Of those, the project’s organizers determined that four related compounds had the most potential.Volunteers iteratively adjusted the molecules and re-analyzed them to improve their potency.In lab tests, at least one candidate killed the virus without damaging human cells. Behind the news: Covid Moonshot does not seek to profit from its effort. If any of its compounds successfully complete animal trials, which could happen by mid-2021, they will enter human clinical trials. If they pass that test, they will be made available to drug makers at no cost to manufacture and distribute.Why it matters: Antivirals typically are far less expensive to produce and easier to distribute than vaccines. These drugs could help keep the pandemic in check while inoculations make their way through the global population.We’re thinking: Although vaccines are beginning to roll out, now is no time to relax. Keep social distancing and hand washing until public-health experts say otherwise.", "image_caption": "Covid Moonshot animation", "metadata": {"article_id": "issue_70", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/MOONSHOT.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-70/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_70.html"}}
{"id": 5287648001, "type": "news_chunk", "title": "Biggest AI Stories of 2020: Covid Triage, Fun With GANs, GPT", "subtitle": "Coping With Covid", "content": "AI accelerated the search for a coronavirus vaccine, detected Covid-19 cases, and otherwise softened the pandemic’s blow.What happened: Machine learning researchers worldwide scrambled to harness the technology against the coronavirus. Among many misfires, they racked up important successes in detection, inoculation, other areas.Driving the story: The pandemic began with high hopes for AI-driven solutions among researchers and officials. But an April metastudy sounded a cautious note, finding that 145 models surveyed were poorly documented, yielded overly optimistic results, and were likely to be biased. Researchers persisted, ultimately delivering vaccines in record time. Outside the lab, deep learning teams tried to keep people safer and more connected. BlueDot, which analyzes news reports for significant events, detected the nascent pandemic several days ahead of the global health monitors and sent an early warning to its customers.The cities of Paris and Cannes evaluated compliance with masking regulations using computer vision in transit stations, buses, and markets. The government of Togo trained a model to identify regions of extreme poverty in satellite imagery. It used the output to guide distribution of relief funds to those most in need.Chatbots provided the locked-down and lonely with synthetic friends to chat and flirt with. For people working from home, videoconferencing companies trained models to filter background noises and virtually transform pajamas into business attire.A collaboration among many institutions in China developed a model that detects Covid-19 in CT scans with better than 90 percent accuracy. The model has been deployed in seven countries and the code has been downloaded 3 million times so far.Moderna, a U.S. biotech company whose vaccine was approved by the U.S. Food and Drug Administration in December, used machine learning to optimize mRNA sequences for conversion into molecules that could be tested. Behind the news: AI may yet play an important role in treating Covid-19. The nonprofit Covid Moonshot project used a semisupervised deep learning platform to filter 14,000 candidate antiviral drugs. The system validated four compounds that are expected to advance to animal trials.Where things stand: AI is no silver bullet, but the advent of this new, virulent, highly infectious strain of coronavirus has been a bracing test run of its capabilities to fight infectious diseases — and helped us live with them, too. Learn more: The Batch featured regular AI-Against-Covid news reports starting in April.", "image_caption": "Two reindeers with masks on a snowy night", "metadata": {"article_id": "issue_71", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_ReindeerMedicine_576x324.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-71/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_71.html"}}
{"id": 5287648002, "type": "news_chunk", "title": "Biggest AI Stories of 2020: Covid Triage, Fun With GANs, GPT", "subtitle": "This Snowman Does Not Exist", "content": "While generative adversarial networks were infiltrating cultural, social, and scientific spheres, they quietly transformed the web into a bottomless well of synthetic images of . . . well, you name it.What happened: Deepfakes showed up in mainstream entertainment, commercials, political campaigns, and even a documentary film in which they were used to protect onscreen witnesses. Amid the hoopla, a groundswell of online front-ends to image generators went largely unremarked.Driving the story: Inspired by 2019’s This Person Does Not Exist, a web app that produces realistic-looking personal portraits, engineers with a sense of humor implemented generative adversarial networks (GANs) that mimic real-world minutiae. Some of our favorites: Trained on images from Google Earth, This City Does Not Exist produces birds-eye-views of settlements large and small.Even non-equestrian types can appreciate This Horse Does Not Exist’s ability to produce a wide variety of poses, breeds, and situations. Sure, it occasionally spits out a horrific jumble of limbs, but that’s half the fun.Like many GANs, This Pizza Does Not Exist tends to average out distinctive features. Hence, its cheeses lack a gooey sheen, its sauce is rarely vibrant, and its crusts look underbaked. But, as the adage goes, even bad pizza is still pizza.The authors didn’t release a web version of This Chinese Landscape Painting Does Not Exist, but in tests, its output fooled human art aficionados around half of the time. Where things stand: Some observers worry that AI-generated fakes could undermine trust in public institutions by sowing confusion over what is and isn’t real. (Which is not to say GANs are required for that.) But the technology turns out to have a critically important use that outweighs any negative social consequences: Balancing pictures of cats on the internet with pictures of dogs. Learn more: The Batch’s GAN special issue features stories about detecting deepfakes, making GANs more inclusive, and an interview with GAN inventor Ian Goodfellow. To learn how to build GANs yourself, check out the Generative Adversarial Networks Specialization on Coursera.", "image_caption": "Dozens of snowmen with different characteristics", "metadata": {"article_id": "issue_71", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_GANS-Field-of-Snowmen6.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-71/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_71.html"}}
{"id": 5287648003, "type": "news_chunk", "title": "Biggest AI Stories of 2020: Covid Triage, Fun With GANs, GPT", "subtitle": "Representing the Underrepresented", "content": "Some of deep learning’s bedrock datasets came under scrutiny as researchers combed them for built-in biases.What happened: Researchers found that popular datasets impart biases against socially marginalized groups to trained models due to the ways the datasets were compiled, labeled, and used. Their observations prompted reforms as well as deeper awareness of social bias in every facet of AI.Driving the story: Image collections were in the spotlight — including ImageNet, the foundational computer-vision dataset. ImageNet creator Fei-Fei Li and colleagues combed the venerable dataset to remove racist, sexist, and otherwise demeaning labels that were inherited from WordNet, a lexical database dating back to the 1980s.A study found that even models trained on unlabeled ImageNet data can learn biases that arise from the dataset’s limited human diversity.MIT Computer Science & Artificial Intelligence Laboratory withdrew the Tiny Images dataset after outside researchers found that it was rife with disparaging labels.FlickrFaces-HQ (FFHQ), the dataset used to train StyleGAN, apparently also lacks sufficient diversity. This problem emerged when PULSE, a model based on StyleGAN that boosts the resolution of low-res photos, up-rezzed a pixelated image of President Barack Obama, the first Black U.S. president, into a portrait of a White man. Behind the news: In the wake of the PULSE fiasco, Facebook’s chief AI scientist Yann LeCun and Timnit Gebru, then head of Google’s ethical AI efforts, argued publicly over whether social biases in machine learning originate primarily in faulty datasets or systemic biases within the AI community. LeCun took the position that models aren’t biased until they learn from biased data, and that biased datasets can be fixed. Gebru pointed out — and we agree, as we said in a weekly letter — that such bias arises within a context of social disparities, and that eliminating bias from AI systems requires addressing those disparities throughout the field. Gebru and Google subsequently parted amid further disagreements around bias.Where things stand: The important work of ensuring that biases in datasets are documented and removed for particular tasks such as generating training data, has only just begun. Learn more: The Batch in the past year reported on bias mitigation techniques including Double-Hard Debias and Deep Representation Learning on Long-Tailed Data.", "image_caption": "Tree farm dataset", "metadata": {"article_id": "issue_71", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_TreeFarmDataSet_Fenced.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-71/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_71.html"}}
{"id": 5287648004, "type": "news_chunk", "title": "Biggest AI Stories of 2020: Covid Triage, Fun With GANs, GPT", "subtitle": "Algorithms Against Disinformation", "content": "The worldwide pandemic and a contentious U.S. election whipped up a storm of automated disinformation, and some big AI companies reaped the whirlwind. What happened: Facing rising public pressure to block inflammatory falsehoods, Facebook, Google’s YouTube division, and Twitter scrambled to update their recommendation engines. Members of the U.S. Congress grilled the companies, a popular Netflix documentary excoriated them, and public opinion polls showed that they had lost the trust of most Americans.Driving the story: The companies addressed the issue through various algorithmic and policy fixes — though they apparently stopped short of making changes that might seriously threaten the bottom line. After discovering hundreds of fake user profiles that included head shots generated by AI, Facebook cracked down on manipulated media it deemed misleading and banned deepfake videos outright. The company continues to develop deep learning tools to detect hate speech, memes that promote bigotry, and misinformation about Covid-19.YouTube developed a classifier to identify so-called borderline content: videos that comply with its rules against hate speech but promote conspiracy theories, medical misinformation, and other fringe ideas.Facebook and Twitter shut down accounts they considered fronts for state-backed propaganda operations.All three companies added disclaimers to content deemed to contain misleading information about the U.S. election. Twitter took its policy furthest, flagging falsehoods from President Donald Trump. Yes, but: The reforms may not stick. The companies have diluted some, and others have already backfired. In June, the Wall Street Journal reported that some Facebook executives had squelched tools for policing extreme content. The company later reversed algorithmic changes made during the election that boosted reputable news sources. Perceptions that Facebook’s effort was halfhearted prompted some employees to resign.YouTube’s algorithmic tweaks targeting disinformation has succeeded in cutting traffic to content creators who promote falsehoods. But they also boosted traffic to larger entities, like Fox News, that often spread the same dubious information. Where things stand: There’s no clear way to win the online cat-and-mouse game against fakers, cranks, and propagandists. But the big cats must stay ahead or lose public trust — and regulators’ forbearance. Learn more: For more details on using AI to stem the tide of disinformation and hate speech online, see our earlier stories on Facebook’s efforts here and here, and on YouTube’s here and here.", "image_caption": "Group of people having a snowball fight and covering with a giant Facebook like button", "metadata": {"article_id": "issue_71", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_SnowballFight_576x324px-2.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-71/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_71.html"}}
{"id": 5287648005, "type": "news_chunk", "title": "Biggest AI Stories of 2020: Covid Triage, Fun With GANs, GPT", "subtitle": "The Model Will See You Now", "content": "Institutional hurdles to AI for medicine began to fall, setting the stage for widespread clinical use of deep learning in medical devices and treatments.What happened: DeepMind’s AlphaFold model determined the three-dimensional shape of a protein in just hours, stealing the spotlight with promises of new blockbuster drugs and biological insights. Behind the scenes, the medical establishment took important steps to bring such technologies into mainstream medical practice.Driving the story: Institutional shifts boosted medical AI’s profile and heralded its growing acceptance. The largest medical insurers in the U.S., Medicaid and Medicare, agreed to reimburse doctors who use certain devices that incorporate machine learning. VizLVO from Viz.ai alerts doctors when a patient may have suffered a stroke. IDx-DR from Digital Diagnostics recognizes signs of a diabetes-related complication that can cause blindness.The U.S. Food and Drug Administration cleared several new AI-based treatments and devices, such as a system that conducts cardiac ultrasounds.An international, interdisciplinary group of medical experts introduced two protocols, Spirit and Consort, designed to ensure that AI-based clinical trials follow best practices and are reported in ways that enable external reviewers to verify the results. Where things stand: Many applications of AI in medicine require doctors and hospitals to reorganize their workflow, which has slowed adoption to some extent. Once they’ve cleared the FDA and Medicare, clinicians have a much greater incentive to make the changes needed to take full advantage of them. Learn more: Our AI For Medicine special issue features stories about deep learning in diagnosis, prognosis, and treatment, along with an exclusive interview with medical-AI godfather Eric Topol. Learn how to build your own medical models in the AI For Medicine Specialization on Coursera.", "image_caption": "Doctor examining a snowman holding a broom", "metadata": {"article_id": "issue_71", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_MedicalSnowman_576x324px.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-71/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_71.html"}}
{"id": 5287648006, "type": "news_chunk", "title": "Biggest AI Stories of 2020: Covid Triage, Fun With GANs, GPT", "subtitle": "Writer’s Unblock", "content": "Neural networks for natural language processing got bigger, more prolific, and more fun to play with.What happened: Language models, which already had grown to gargantuan size, continued to swell, yielding chatbots that mimic AI luminaries and have very strange ideas about horses.Driving the story: OpenAI’s colossal 175 billion-parameter text generator GPT-3 showcased ongoing progress in natural language processing. It also exemplified widespread trends in machine learning: exponential rise in parameter counts, growing prevalence of unsupervised learning, and increasing generalization. GPT-3 writes more coherent text than its predecessor, GPT-2 — so much so that tricksters used it to produce blog articles and Reddit comments that fooled human audiences. Other users showed off the technology’s inventiveness in unique ways, such as drafting philosophical essays and inventing conversations with historical figures.Language modeling boosted tools for businesses, for instance by helping Apple’s autocorrect differentiate among languages, enabling Amazon’s Alexa to follow shifts in conversation, and updating the DoNotPay robot lawyer to file lawsuits against telemarketers who unlawfully call U.S. citizens.Meanwhile, OpenAI trained GPT-2 on pixel data to produce iGPT, which is capable of filling in partially obscured pictures to generate images of uncanny weirdness. Where things stand: In language models, bigger clearly is better — but it doesn’t stop there. iGPT portends models trained on both images and words. Such models, which are in the works at OpenAI, at least, may be smarter, and weirder, than the giant language models of 2020. Learn more: Our NLP special issue includes stories about counteracting bias in word embeddings, making conversation, and choosing the right words, plus an exclusive interview with NLP pioneer Noam Shazeer. Learn how to build your own NLP models in the Natural Language Processing Specialization on Coursera.", "image_caption": "Bookstack and wrapping paper", "metadata": {"article_id": "issue_71", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_Holiday-Bookstack_576x324px.jpg", "source_url": "https://www.deeplearning.ai/the-batch/issue-71/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_71.html"}}
{"id": 89515041001, "type": "news_chunk", "title": "Clues to Mental Illness, Enterprise AI, Bias in Compressed Models", "subtitle": "Online Clues to Mental Illness", "content": "Dear friends, In my letter last week, I alluded to the way AI tends to concentrate power and wealth. This tendency worries me, and I believe it deserves more attention. The U.S. government has been looking into these winner-take-most dynamics at a few leading technology companies from an antitrust perspective. But the issue is much bigger than that. AI will concentrate power in many industries, including ones that haven’t traditionally relied on high tech, in the hands of a few winners. For instance, Amazon has come to dominate retailing at the expense of innumerable chains and mom-and-pop stores. Uber, Lyft, and Didi are concentrating power over the taxi industry, which used to support hundreds of thriving local companies. Retailing and taxi service are not traditionally viewed as tech industries. Driven by digitization and AI, this pattern will play out in many more industries in this decade. Covid-19 has added further fuel to these dynamics. Some retailers managed the shift to e-commerce. They are collecting data and implementing AI to optimize sales, and they’re becoming more powerful. But others were nearly destroyed as the pandemic choked off foot traffic in brick-and-mortar stores. They don’t have spare dollars to invest in AI, and they’re falling farther and farther behind. Even as AI creates tremendous wealth, I worry about the growing concentrations of power and wealth, and those who will be left behind. Government will have to step up to address this situation, but significant responsibility also lies with the all of us who conceive, build, and manage this technology. I ask each of you to use your knowledge wisely, in ways that benefit society at large rather than a select few — even if that “select few” is yourself. Keep learning! Can social media posts reveal early signs of mental illness? A new machine learning model shows promising results.What’s new: Researchers led by Michael Birnbaum at the Feinstein Institute for Medical Research and Raquel Norel at the IBM Watson Research Center developed a model that analyzes messages and images posted by Facebook users for indicators of psychological problems. Unlike earlier efforts to classify mental illness based on social media posts, which relied on subjects to report their condition, this one used actual diagnoses.How it works: The authors collected millions of messages and images posted over 18 months by 223 volunteers. Some posters had been hospitalized with schizophrenia-spectrum disorders, some had been diagnosed with mood disorders like depression, and some had no mental health issues. For text input, the authors labeled training examples using LIWC, which represents emotional tone, confidence, and authenticity. For images, they annotated measurements of hue, saturation, pixel density, and other factors.They trained a random forest to classify messages from each group. Results: The model identified people diagnosed with schizophrenia and mood disorders at a rate comparable to that of a standard 10-point questionnaire, according to Wired. The researchers found that individuals diagnosed as schizophrenic used “see,” “hear,” and other words related to perception more often than the others. Those with mood disorders tended to post more blue-tinted pictures. Both groups also used more swear words and posted smaller photos.Behind the news: Social media posts are a popular hunting ground for researchers aiming to gauge users’ mental states. Recent studies suggest that Reddit comments can indicate conditions like ADHD, anxiety, and bipolar disorder, and that Twitter users often telegraph their depression, postpartum mood disorder, suicidal ideation, and more.Why it matters: This tool could help doctors catch mental illness early — especially in young adults, who tend to be both prolific users of social media and at higher risk of developing mental illness — and could provide valuable context for treatment.We’re thinking: Useful though it might be in some cases, scanning social media posts for clues to a user’s mental state holds worrisome implications. Yet another reason social media companies must adopt stricter standards to protect privacy.", "image_caption": "Animation alternating sad and happy emojis", "metadata": {"article_id": "issue_73", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2070.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-73/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_73.html"}}
{"id": 89515041002, "type": "news_chunk", "title": "Clues to Mental Illness, Enterprise AI, Bias in Compressed Models", "subtitle": "Smaller Models, Bigger Biases", "content": "Compression methods like parameter pruning and quantization can shrink neural networks for use in devices like smartphones with little impact on accuracy — but they also exacerbate a network’s bias. Do compressed models perform less well for underrepresented groups of people? Yes, according to new research.What’s new: A Google team led by Sara Hooker and Nyalleng Moorosi explored the impact of compression on image recognition models’ ability to perform accurately across various human groups. The authors also proposed a way to rank individual examples by how difficult they are to classify.Key insight: In earlier work, members of the team showed that compressed image recognition models, although they maintained their accuracy overall, had trouble identifying classes that were rare in their training data. To learn whether that shortcoming translates into bias against underrepresented human types, the researchers trained models to recognize a particular class (people with blond hair), compressed them, and measured the differences in their accuracy across different types of people. This enabled them to evaluate the difference in performance between compressed and uncompressed models with respect to underrepresented groups.How it works: The authors trained a set of ResNet-18s on CelebA, a dataset of celebrity faces, to classify photos of people with blond hair. (CelebA is notorious for producing biased models.) Then they compressed the models using various combinations of pruning and quantization. Using both compressed and uncompressed models, they predicted blonde/not-blonde labels for the CelebA test set. They compared the performance of uncompressed and compressed models in classifying pictures of young people, old people, men, women, young men, old men, young women, and old women. This gave them a measure of how compression affected model bias against these groups.To rank examples for how difficult they were to classify, the authors found the difference between the number of “blond” predictions by uncompressed and compressed models for a given example, and added that to the difference between the number of “not blond” predictions by the same models. The sum yielded a score of how consistently the models labeled a given example.To make it easier to study various combinations of image and model, the researchers used a variable threshold to identify the least consistently labeled examples by percentage (designated “CIE” in the gallery above.) Results: Pruning 95 percent of model parameters boosted the false-positive “blond” rate for women (who made up 14 percent of the dataset) by an average 6.32 percent, but it increased that rate for men (less than 1 percent of the dataset) by 49.54 percent. (The authors didn’t report corresponding results for models compressed by quantization.) Furthermore, the ranking method succeeded in identifying the examples that were most difficult to classify. A 95-percent pruned model was 93.39 percent accurate over the entire dataset, but 43.43 percent accurate on the 1 percent least consistently labeled examples. An unpruned model had much the same trouble. It was 94.76 percent accurate over the entire dataset, but 55.35 percent accurate on the 1 percent of least consistently labeled examples.Why it matters: Model compression is an important part of practical deployments: Shipping a 10MB neural network for a mobile device is much more acceptable than shipping a 100MB model. But if compression exacerbates biases, we must systematically audit and address those issues. We’re thinking: This work is a reminder that it’s not enough to optimize overall classification accuracy. We need to make sure our models also perform well on various slices of the data.", "image_caption": "Image recognition examples", "metadata": {"article_id": "issue_73", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/COMPRESSED.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-73/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_73.html"}}
{"id": 89515041003, "type": "news_chunk", "title": "Clues to Mental Illness, Enterprise AI, Bias in Compressed Models", "subtitle": "U.S. New Year’s Resolutions for AI", "content": "U.S. lawmakers authorized a slew of national programs that promote artificial intelligence research, development, and deployment, and support efforts to make sure the results are ethical and trustworthy.What’s new: The 4,500 pages of the National Defense Authorization Act (NDAA), which primarily serves to authorize programs for the U.S. military, includes provisions that promote AI in both civilian and military agencies, and academic institutions, too. What it says: The NDAA only authorizes these programs; funding will come with further legislation. Among its provisions: The National AI Initiative will coordinate research and development across civilian, intelligence, and defense agencies.The National Science Foundation will begin planning a National Research Cloud, an aggregation of processing and data resources to be made available to academic and nonprofit researchers. (Fei-Fei Li described the National Research Cloud in our special New Year issue of The Batch.) The NSF will also build AI research institutes focused on health care, manufacturing, and other sectors; study the impact of AI on the nation’s workforce; and sponsor competitions that promote innovation.The National Institute of Standards and Technology will create a framework to grade AI systems on trustworthiness and define related terms like explainability, privacy, and transparency. The agency also will formulate privacy and security standards for training datasets, data management, and AI hardware. The Defense Department must ensure that any AI it acquires was developed “ethically and responsibly.”The Joint AI Center, a military organization launched in 2018, will report directly to the Deputy Secretary of Defense, giving the Pentagon leadership more direct control over its research and development priorities. The center’s biannual report must describe its work developing AI standards and its collaborations with other agencies. Yes, but: Some of these programs, such as the NSF’s AI research institutes, will cost money that Congress has yet to appropriate. Russell Wald, director of policy at Stanford’s Institute for Human-Centered Artificial Intelligence, told The Batch he’s optimistic that funding will be allocated where it’s needed. Behind the news: President Donald Trump vetoed the NDAA in December, saying he wanted it to include a repeal of Section 230 of the Communications Decency act, which protects internet companies from legal liability for user-generated content on their sites. Congress overrode the veto and passed the bill into law on New Year’s Day.Why it matters: The U.S. is a global leader in AI innovation and home to more big AI companies than anywhere else, but its government has lagged in issuing a comprehensive AI strategy. Directives like the National Research Cloud would give a healthy boost to AI researchers in many areas, and the impact likely would ripple across the world.We’re thinking: This bill is a major step forward in U.S. support for AI. We’ll keep our fingers crossed that the necessary funding comes through.", "image_caption": "United States Capitol", "metadata": {"article_id": "issue_73", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/NDAA20SIZED.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-73/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_73.html"}}
{"id": 89515041004, "type": "news_chunk", "title": "Clues to Mental Illness, Enterprise AI, Bias in Compressed Models", "subtitle": "Algorithms For Elephants", "content": "An AI-powered collar may help protect wild elephants from poachers, hunters, and other hostile humans.What’s new: Ten ElephantEdge wireless tracking collars will be fitted onto African elephants next year, TechCrunch reported. The product of an open source collaboration between hardware and software engineers, the collar serves as a platform for machine learning models designed to interpret elephant behavior and alert sympathetic humans when the animals are in trouble. How it works: The models included are winners of a competition organized by Hackster.io, a hardware engineering community, and Smart Parks, a Dutch conservation group. They were built using development tools from Edge Impulse and work with hardware from organizations including Institute Irnas and Avnet. Elephant AI contributed a model that recognizes human sounds picked up by the collar’s microphone and cross-references them with GPS coordinates to detect possible poachers. A different one uses data from the collar’s accelerometer to determine when elephants are eating, sleeping, or running.The Gajraj AI project built models to limit harm when elephants seek food from farms. For instance, one analyzes motions and vibrations of an elephant’s trunk for signs of distress from human interaction and alerts people nearby.Elephant Guardian provided models that interpret elephant activity, as well as one that alerts rangers to sounds of weapons commonly used by poachers, such as AK-47s. Behind the news: Defenders of wildlife are increasingly using AI to extend their reach and effectiveness. A machine learning model called PAWS suggests optimal patrol routes to help park rangers in Cambodia intercept poachers.Image recognition models associated with camera traps in the wild help conservationists keep track of numbers and movements of endangered species. For instance, a model from Google Earth recognizes 614 species and classifies 3.6 million images an hour. Why it matters: Africa’s elephant population has plummeted in recent years. Only about 350,000 wild elephants remain on the continent, and poachers illegally kill upward of 15,000 a year. These animals need all the help they can get. We’re thinking: This work addresses the elephant in the room.", "image_caption": "Series of images describing how an AI-powered collar for elephants operates", "metadata": {"article_id": "issue_73", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2064.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-73/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_73.html"}}
{"id": 89515041005, "type": "news_chunk", "title": "Clues to Mental Illness, Enterprise AI, Bias in Compressed Models", "subtitle": "Enterprise AI on the Rise", "content": "A survey of AI in large companies sees boom times ahead — if AI teams can get past issues that surround implementation.What’s new: Businesses of all sizes are using more machine learning, spending more on it, and hiring more engineers to wrangle it, according to a survey of 750 business leaders by Algorithmia, which provides tools that automate model deployment and management. Nonetheless, struggles with deployment, scaling, and other issues continue to hinder adoption.What they found: The survey questioned executives in a variety of sectors including finance, healthcare, education, and information technology. More than two-thirds of those who responded said their AI budgets are growing, while only 2 percent are cutting back. 40 percent of companies surveyed employed more than 10 data scientists, double the rate in 2018, when Algorithmia conducted its previous study. 3 percent employed more than 1,000 data scientists.Many respondents said they’re in the early stages, such as evaluating use cases and developing models.Many struggle with deployment. Half of those surveyed took between 8 days and three months to deploy a model. 5 percent took a year or more. Generally, larger companies took longer to deploy models, but the authors suggest that more mature machine learning teams were able to move faster.Scaling models is the biggest impediment, cited by 43 percent of respondents. In larger organizations, this may reflect siloing of machine learning teams in various departments. The authors believe that the solution is to centralize AI efforts in an innovation hub like those launched by Ericsson, IBM, and Pfizer. Behind the news: Several other recent surveys shed light on AI’s evolving role in the business world. For instance, MIT Technology Review looked at AI’s growth in different global regions, and McKinsey examined how different market sectors, like manufacturing, marketing, and supply chain management, are finding profitable uses for the technology. Why it matters: AI is new enough, and evolving fast enough, that every company’s experience is different. Spotting areas where industries where machine learning is having an impact, as well as trouble spots in deployment, can help guide crucial decisions.We’re thinking: In 2019, many companies experimented with AI. In 2020, a growing number started talking about how to productionize models. In the coming year, we hope for rapid progress in MLOps processes and tools to make building and productionizing machine learning systems repeatable and systematic. AI Fund (where Andrew is managing general partner) has seen a lot of startups jump into this space, which bodes well for the future.", "image_caption": "Results of Algorithmia's survey of 750 business leaders", "metadata": {"article_id": "issue_73", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ALGORITHMIA.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-73/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_73.html"}}
{"id": 60192496001, "type": "news_chunk", "title": "Propagandists Lie About AI, Language Models Grok Images...", "subtitle": "AI Truths, AI Falsehoods", "content": "Dear friends,Last Wednesday, the U.S. Capitol building was overrun by insurrectionists at the moment when members of Congress were certifying the results of a national election. Reading accounts of how close the mob came to where those representatives had sheltered, I believe the legislative branch came closer to falling than many people realize. This event was unprecedented, and its consequences will be playing out for a long time.U.S. democracy has taken a lot of damage in recent years. Citizens have become polarized. Some politicians have become brazen in their disregard for facts. Voters have been suppressed. The press has been vilified and attacked. Similar things have happened in other countries, and formerly healthy democracies have fallen into populism, authoritarianism, or totalitarianism. I hope this latest challenge will inspire a renewal of democracy. Organizations that are tested — and that survive the test — end up stronger. Democracy stands on several pillars, among them: Citizens who are informed by truthful perspectives supported by a free press and scientific enquiryInstitutions that create and enforce laws to make sure that society operates according to rulesFree and fair elections in which each individual has a vote that counts The AI community can help strengthen all three. As ambiguous information surfaces and is tossed into the grinder of social media, recommendation engines can drive polarization. How can we build recommenders that bring people together rather than driving them apart?Decisions to ban polarizing entities — including President Trump — from tech platforms have appeared to be made ad hoc. Instead, they need to be based on rules that are fair and consistently applied. If companies and regulators can develop such rules — which will not be easy — AI can play a significant role in implementing them at scale.Digital tools have been used to selectively discourage voting and to gerrymander. On the positive side, they’ve also been used to inform voters and drive turnout. We need to develop new categories of tools and muster the political will to use them o empower all voters. January 6, 2021, was a nadir for the U.S., and the path ahead will be long and hard. But I believe the country has reached a turning point. I hope the dire events of the past week will renew our appreciation of just how precious sound government is. Keep learning! Face recognition is being used to identify people involved in last week’s assault on the U.S. Capitol. It’s also being misused to support their cause.What’s new: Law enforcement agencies and online sleuths are using deep learning to put names to faces in images shot while supporters of U.S. President Trump overran the building in Washington, D.C. to stop certification of his defeat in the recent national election, leaving several people dead and many injured. At the same time, pro-Trump propagandists are making false claims that the technology shows left-wing infiltrators led the attack.What happened: Police arrested few of the perpetrators. In the aftermath, the abundant images have fed AI-powered sleuthing to find those who were allowed to leave the scene. University of Toronto researcher John Scott-Railton used face identification and image enhancement to help identify a man who was photographed inside the Senate chamber wearing body armor and carrying zip-tie handcuffs as retired Air Force Colonel Larry Rendall Brock, Jr. Subsequently Brock was arrested.Clearview AI, a face recognition company used by thousands of U.S. law enforcement agencies, saw a 26 percent jump in search requests following the attack. At least two police agencies have acknowledged using the service to identify perpetrators.Even as face recognition determined that some of the most visible leaders of the assault were Trump supporters, the right-leaning Washington Times erroneously reported that face recognition vendor XRVision had identified individuals leading the assault as left-wing Antifa activists. XRVision called the story “outright false, misleading, and defamatory.” Deepfakes, too: Falsehoods also circulated regarding deepfake technology. Users of 4chan and social media site Parler wrongly asserted that President Trump’s post-insurrection speech, in which he called the participants “criminals” and “unpatriotic,” was faked by AI. The White House debunked this claim.Why it matters: The Capitol assault, apart from its aim to disrupt the democratic process (and apparently to assassinate officials), highlights that face recognition and deepfakes are two sides of the machine learning coin: One is a powerful tool for uncovering facts, the other a powerful tool for inventing them. While the police are relying on the former capability, propagandists are exploiting both by spreading believable but false claims.We’re thinking: Paranoia about artificial intelligence once centered on fear that a malicious superintelligence would wreak havoc. It turns out that humans using AI — and lies about AI — to spread disinformation pose a more immediate threat.", "image_caption": "Face detection being used on a person during assault on the U.S. Capitol", "metadata": {"article_id": "issue_74", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/PARANOIA.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-74/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_74.html"}}
{"id": 60192496002, "type": "news_chunk", "title": "Propagandists Lie About AI, Language Models Grok Images...", "subtitle": "Tell Me a Picture", "content": "Two new models show a surprisingly sharp sense of the relationship between words and images.What’s new: OpenAI, the for-profit research lab, announced a pair of models that have produced impressive results in multimodal learning: DALL·E, which generates images in response to written prompts, and Contrastive Language-Image Pretraining (CLIP), a zero-shot image classifier. The company published a paper that describes CLIP in detail; a similar Dall-E paper is forthcoming.How they work: Both models were trained on text-image pairs. DALL·E (whose name honors both Salvador Dalí and Pixar’s WALL·E) is a decoder-only transformer model. OpenAI trained it on images with text captions taken from the internet. Given a sequence of tokens that represent a text and/or image, it predicts the next token. Then it predicts the next token given its previous prediction and all previous tokens.This allows DALL·E to generate images from a wide range of text prompts and to generate fanciful images that aren’t represented in its training data, such as “an armchair in the shape of an avocado.”CLIP uses a text encoder (a modified transformer) and an image encoder (a vision transformer) trained on 400 million image-text pairs drawn from the internet. Using contrastive loss function adopted from ConVIRT, it learned to predict which of nearly 33,000 text snippets would match an image.Since CLIP can predict which text best matches an image among any number of texts, it can perform zero-shot classification in any image classification task. At inference, CLIP is given a list of all potential classes in the form of “a photo of a {object}.” Then, fed an image, it returns the most likely class from the list. Yes, but: Neither model is immune to goofs. Asked to produce a pentagonal clock, for instance, DALL·E rendered some timepieces with six or seven sides. CLIP, meanwhile, has trouble counting objects in an image and differentiating subclasses like car brands or flower species.Behind the news: The new models build on earlier research at the intersection of words and images. A seminal 2016 paper from the University of Michigan and Max Planck Institute for Informatics showed that GANs could generate images from text embeddings. Other work has resulted in models that render images from text, among them Generative Engine and Text to Image. Judging by the examples OpenAI has published so far, however, DALL·E seems to produce more accurate depictions and to navigate a startling variety of prompts with flair.Why it matters: As OpenAI chief scientist (and former post-doc in Andrew’s lab) Ilya Sutskever recently wrote in The Batch, humans understand concepts not only through words but through visual images. Plus, combining language and vision techniques could overcome computer vision’s need for large, well labeled datasets.We’re thinking: If we ever build a neural network that exhibits a sense of wonder, we’ll call it GOLL·E.", "image_caption": "AI-generated images with the model DALL-E", "metadata": {"article_id": "issue_74", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/AVOCADO.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-74/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_74.html"}}
{"id": 60192496003, "type": "news_chunk", "title": "Propagandists Lie About AI, Language Models Grok Images...", "subtitle": "Striding Toward the Minimum", "content": "When you’re training a deep learning model, it can take days for an optimization algorithm to minimize the loss function. A new approach could save time.What’s new: Juntang Zhuang and colleagues at Yale, University of Illinois at Urbana-Champaign, and University of Central Florida proposed AdaBelief, a more efficient variation on the popular Adam optimizer.Key insight: The popular optimization methods of stochastic gradient descent (SGD) and Adam sometimes take small steps, requiring more time to reach their destination, when they could take larger ones. Given a small learning rate and a point in a large, steep area of a loss function’s landscape, SGD takes small steps until the slope becomes steeper, while Adam’s steps become smaller as it progresses. In both scenarios, an ideal optimizer would predict that the slope is long and take larger steps.How it works: AdaBelief adjusts its step size depending on the difference between the current gradient and the average of previous gradients. Like Adam, AdaBelief moves along a function step by step and calculates an exponential moving average of the gradient, assigning exponentially smaller weights to previous gradients. Also like Adam, at each step, a steeper average gradient generally calls for a larger step size.Unlike Adam, AdaBelief treats the weighted average as a prediction of the gradient at the next step. If the difference between the prediction and the actual gradient is small, the function’s steepness probably isn’t changing much, and AdaBelief takes a relatively larger step. Conversely, if the difference is large, the landscape is changing, and AdaBelief decreases the step size. Results: The authors provide videos showing that, in experiments on functions with known minimums, AdaBelief was faster than both Adam and SGD with momentum (as shown above). To demonstrate their method’s accuracy, they compared AdaBelief to SGD, Adam, and other adaptive optimizers on tasks including image classification, image generation, and language modeling. AdaBelief basically matched SGD’s accuracy and exceeded that of all other adaptive optimizers. For instance, on ImageNet, AdaBelief increased a ResNet18’s highest top-1 accuracy, or accuracy of its best prediction, to 70.08 percent, on par with SGD’s 70.23 percent and 2 percent better than the best adaptive optimizers.Why it matters: Faster optimization means faster training, and that means more time to experiment with different models.We’re thinking: The authors’ video demonstrations suggest that AdaBelief could be a valuable alternative to Adam. However, they don’t supply any numbers that would make for a precise speed comparison. We look forward to the authors of the Deep Learning Optimizer Benchmark Suite, who have evaluated over a dozen optimizers in various tasks, running AdaBelief through its paces.", "image_caption": "Graphs comparing SGD + Momentum, Adam and AdaBelief", "metadata": {"article_id": "issue_74", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2061.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-74/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_74.html"}}
{"id": 60192496004, "type": "news_chunk", "title": "Propagandists Lie About AI, Language Models Grok Images...", "subtitle": "The Fax About Tracking Covid", "content": "A pair of neural networks is helping to prioritize Covid-19 cases for contact tracing.What’s new: The public health department of California’s Contra Costa County is using deep learning to sort Covid-19 cases reported via the pre-internet technology known as fax.How it works: Hospitals and medical labs document cases of coronavirus infection using hand-written forms. Many transmit the documents to public health officials over telephone landlines. Stanford University researchers developed Covid Fast Fax to evaluate them so that public health workers, who still manually review each case, can spot the most critical ones. The system comprises two convolutional neural networks. One model culls Covid-19 reports from other incoming faxes. The researchers trained it using 25,000 copies of the five forms used most frequently by area hospitals. They augmented the dataset by adding blurs, streaks, and other distortions commonly seen in fax transmissions.The second model determines which reports are most urgent. It ranks the severity of each case by reading checkboxes that indicate a patient’s symptoms, gender, isolation status, and other details. To train it, the researchers wrote 130 fake reports, transmitted them by fax, and augmented them by flipping, blurring, and adding noise.The researchers evaluated their system on 1,224 faxes received over a two week period. The system was able to read 88 percent of the documents. Of these, it detected Covid-19 reports with 91 percent recall, a measurement for accuracy that docks the model for mislabeling high-priority cases. Behind the news: The use of fax in health care persists despite billions of dollars to promote digital health records. Digital systems face roadblocks, as many professionals find them difficult to use, and for-profit hospitals aren’t always eager to make it easy for patients to share their information with competitors. Why it matters: According to a 2019 survey, 89 percent of U.S. health organizations still rely on fax to transmit medical information. Anything that accelerates the processing of that information is a plus — especially during a pandemic.We’re thinking: It’s 2021, and hospitals are still relying on fax to make critical decisions? AI can help hospitals cope with outmoded communications technology, but it’s no substitute for updating U.S. health care infrastructure.", "image_caption": "Covid Fast Fax operating", "metadata": {"article_id": "issue_74", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/FAX.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-74/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_74.html"}}
{"id": 60192496005, "type": "news_chunk", "title": "Propagandists Lie About AI, Language Models Grok Images...", "subtitle": "It’s a Small World Model After All", "content": "World models, which learn a compressed representation of a dynamic environment like, say, a video game, have delivered top results in reinforcement learning. A new method makes them much smaller.What’s new: Jan Robine and colleagues at Heinrich Heine University Düsseldorf present Discrete Latent Space World Models. Their approach matches the performance of the state of the art in six Atari games, SimPLe, with far fewer parameters.Key insight: Researchers have devoted significant effort to making reinforcement learning algorithms efficient, but they’ve given less attention to making models themselves efficient. Using high-performance architectures for the various components of a world model ought to improve the entire system — in this case, by reducing its size.How it works: Following the typical world models approach, the authors trained separate neural networks to generate a representation of the environment (the representation model), predict how actions would affect the environment (the dynamics model), and choose the action that will bring the greatest reward (the policy model). For the representation model, the authors used a vector quantized variational autoencoder (VQ-VAE) that’s smaller than the autoencoder in SimPLe. The VQ-VAE takes as input the pixels of a game’s most recent four frames. Its encoder generates a 6×6 matrix of indices, each pointing to a vector in an embedding that represents the environment. (After training, the decoder is no longer needed.)For the dynamics model, they used a convolutional LSTM that takes as input the encoder’s output. They trained it to predict the reward and features of the next four frames. Errors backpropagate through to the embedding, so eventually it encodes information about predicted rewards and states. (After training, the dynamics model is no longer needed.)For the policy model, they used a small convolutional neural network that also receives the encoder’s output. They trained it to choose an action using proximal policy optimization.To train the system, the authors used the same iterative procedure as SimPLe. They let the system interact with the environment, trained the representation and dynamics models, and then trained the policy network; then they repeated the cycle. Results: The authors compared their method to SimPLe in six Atari games. SimPLe uses 74 million parameters, while their method uses 12 million during training and 3 million during inference. Nonetheless, their method’s mean scores over five training runs beat SimPLe in five out of six games when given 100,000 observations.Yes, but: Although the authors’ method beat SimPLe on average, SimPLe racked up higher scores in four out of six games.Why it matters: Smaller models consume less energy, require less memory, and execute faster than larger ones, enabling machine learning engineers to perform more experiments in less time.We’re thinking: World models are young enough that something as simple as changing the components used can make a big difference. This suggests that plenty of opportunity remains to improve existing models.", "image_caption": "Graphs related to world models", "metadata": {"article_id": "issue_74", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2069.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-74/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_74.html"}}
{"id": 15272133001, "type": "news_chunk", "title": "Detecting Guns, Fighting Lead Poisoning, Adversarial Training", "subtitle": "Working AI: Stoking GPU Clusters", "content": "Dear friends, Experience gained in building a model to solve one problem doesn’t always transfer to building models for other problems. How can you tell whether or not intuitions honed in one project are likely to generalize to another? I’ve found that two factors can make the difference: the size of the training set and whether the data is unstructured or structured.For instance, I’ve heard blanket statements like, “you should always have at least 1,000 examples before tackling a problem.” This is good advice if you’re working on a pedestrian detector, where data is readily available and prior art shows that large datasets are important. But it’s bad advice if you’re building a model to diagnose rare medical conditions, where waiting for 1,000 examples might mean you’ll never get started. Unstructured data includes text, images, and audio clips, which lend themselves to interpretation by humans. Structured data, on the other hand, includes things like transaction records or clickstream logs, which humans don’t process easily. This difference leads to very different strategies for training and deploying models: Unstructured data: Because the examples are easy for humans to understand, you can recruit people to label them and benchmark trained models against human-level performance (HLP). If you need more examples, you might be able to collect them by capturing more text/images/audio or by using data augmentation to distort existing examples. Error analysis can take advantage of human intuition.Structured data: This class of data is harder for humans to interpret, and thus harder for humans to label. Algorithms that learn from structured data often surpass HLP, making that measure a poor benchmark. It can also be hard to find additional examples. For instance, if the training dataset comprises records of your customers’ purchases, it’s hard to get data from additional customers beyond your current user base. Dataset size has implications as well: Small dataset: If the dataset includes <1,000 examples, you can examine every example manually, check if the labels are correct, and even add labels yourself. You’re likely to have only a handful of labelers, so it’s easy to hash out any disagreements together on a call. Every single example is a significant fraction of the dataset, so it’s worthwhile to fix every incorrect label.Large dataset: If the dataset is >100,000 examples, it’s impractical for a single engineer to examine every one manually. The number of labelers involved is likely to be large, so it’s critical to define standards clearly, and it may be worthwhile to automate labeling. If a significant number of examples are mislabeled, it may be hard to fix them, and you may have to feed the noisy data to your algorithm and hope it can learn a robust model despite the noise. If you find yourself in need of advice while working on, say, a manufacturing visual inspection problem with 100 examples, the best person to ask would be someone who has worked on a manufacturing visual inspection problem with 100 examples. But if you can’t find such a person, consider looking for someone with expertise in the same dataset size/type quadrant as the problem you’re working on. As you develop your career, you might also consider whether you want to stay in one quadrant and develop deep expertise there, or move across quadrants and develop more general skills. Keep learning! As a senior deep learning engineer at Nvidia, Swetha Mandava helps make models run more efficiently on large-scale hardware. Learn about her onramp to AI and how she stays on track. Read more", "image_caption": "SWETHA1 576x324", "metadata": {"article_id": "issue_75", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/SWETHA120576x324.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-75/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_75.html"}}
{"id": 15272133002, "type": "news_chunk", "title": "Detecting Guns, Fighting Lead Poisoning, Adversarial Training", "subtitle": "AI Versus Lead Poisoning", "content": "An algorithm is helping cities locate pipes that could release highly toxic lead into drinking water.What’s new: BlueConduit, a startup that focuses on water safety, is working with dozens of North American municipal governments to locate lead water lines so they can be replaced, Wired reported.How it works: For each city, the company develops a custom model that ranks the likelihood that any given property has lead pipes. The company starts by collecting comprehensive data on building locations, ages, market values, occupants, and other variables, BlueConduit executives told The Batch. It works with the local government to gather details from a representative set of properties, including known pipe materials and results of water tests if they’re available.It trains a gradient boosted tree on the data, tuning the model to account for uncertainties in the dataset.The model’s output is used to produce maps that officials can use to prioritize removal of potentially hazardous pipes and residents can use to request removal. Behind the news: Founded by faculty at Georgia Tech and University of Michigan, BlueConduit developed its technology to help manage a wave of lead poisoning in Flint, Michigan, between 2014 and 2019. There it achieved 70 percent accuracy in classifying properties with lead pipes. Contaminated water in Flint exposed thousands of people to dangerously high levels of lead.Why it matters: Lead exposure can impair development in children, and it’s linked to heart, kidney, and fertility problems in adults. Yet digging up older water lines that may use lead pipes can cost thousands of dollars. Cities can save millions if they can focus on the most dangerous locations and avoid replacing pipes in houses that are already safe.We’re thinking: Flint stopped using BlueConduit’s system in 2018 partly because some residents complained they were being passed over by the AI-driven replacement strategy — a sign of how little they trusted their local government and the unfamiliar technology. The city reinstated the system the following year under pressure from the state and legal actions, but the lesson remains: When you’re deploying a major AI system, establishing trust is as important as tuning parameters.", "image_caption": "Series of images related to water lines replacement and poisoned water", "metadata": {"article_id": "issue_75", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/LEAD20576x324.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-75/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_75.html"}}
{"id": 15272133003, "type": "news_chunk", "title": "Detecting Guns, Fighting Lead Poisoning, Adversarial Training", "subtitle": "Adversarial Helper", "content": "Models that learn relationships between images and words are gaining a higher profile. New research shows that adversarial learning, usually a way to make models robust to deliberately misleading inputs, can boost vision-and-language performance.What’s new: Vision-and-language models based on transformer networks have shown strong performance on tasks such as answering questions about images. Zhe Gan of Microsoft and colleagues at Microsoft and the University of Maryland improved such models via Vision-and-Language Large-scale Adversarial (VILLA) training.Key insight: Vision-and-language models often are pretrained, for instance, to fill in blanks in image captions, and then fine-tuned for a specific task, such as answering questions about images. Previous work with language models showed that adversarial fine-tuning — that is, giving the model input that’s designed to fool it and training it not to be fooled — can increase accuracy. The team extended this idea to vision-and-language models in both pretraining and fine-tuning.How it works: The authors worked with UNITER, which has achieved state-of-the-art performance on several vision-and-language tasks. UNITER embeds images and text separately. Then it feeds the embeddings into a BERT-like model to create a multimodal embedding. The authors used a variation on FreeLB, an adversarial training technique. FreeLB perturbs embeddings by learning a small vector that, when added to embeddings, is likely to fool the network, and then training the model to answer correctly regardless.The authors perturbed both image and text embeddings, but not at the same time. The model’s objective was threefold: predict the correct answer using unperturbed embeddings, predict the correct answer using perturbed embeddings, and to keep those predictions and confidence in them close to one another.They pretrained UNITER to perform masked language modeling (guessing which words are missing from a text passage, usually based on surrounding words, but in this case based on an accompanying image) and image-text matching (guessing whether a text and image are paired). Pretraining involved four large image-and-caption datasets.They fine-tuned and tested on several vision-and-language tasks. For instance, visual question answering required answering questions about images like, “what color are her eyes?” Visual commonsense reasoning required answering multiple-choice questions such as, “why is [person4] pointing at [person1]?” followed by “I think so because...” Results: UNITER trained with VILLA outperformed a standard UNITER in six vision-and-language tasks. In visual question answering, UNITER with VILLA answered 73.67 percent correctly, while the plain model answered 72.91 percent correctly. In the two-stage visual commonsense reasoning task of answering a question and justifying the answer, UNITER with VILLA scored 59.75 percent, while its standard counterpart succeeded 57.76 percent of the time.Why it matters: We understand the world through several modalities, and that makes us smarter. For instance, to describe a tree, neither an image nor a biological description is sufficient, but together they have a revealing synergy. Current models still struggle to grasp the meaning of images and language individually, but they will always be missing something until they can draw connections between them.We’re thinking: Vision: check. Language: check. Now sound, aroma, touch . . .", "image_caption": "Data related to adversarial learning", "metadata": {"article_id": "issue_75", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/VILLA20576x324.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-75/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_75.html"}}
{"id": 15272133004, "type": "news_chunk", "title": "Detecting Guns, Fighting Lead Poisoning, Adversarial Training", "subtitle": "Draw a Gun, Trigger an Algorithm", "content": "Computer vision is alerting authorities the moment someone draws a gun.What’s new: Several companies offer deep learning systems that enable surveillance cameras to spot firearms and quickly notify security guards or police, according to Vice.No people were harmed in the training of this model: Some developers of gun detection models have gone to great lengths to produce training data. Virginia-based Omnilert trained its Gun Detect system using simulations from video game software, scenes from action movies, and thousands of hours of video depicting employees holding toy or real guns.Alabama-headquartered Arcarithm, which makes systems for gun detection, produced training data by photographing guns in front of a green screen and compositing them into scenes such as offices. The company created 30,000 to 50,000 images of each of America’s 10 most popular rifles and handguns to train its Exigent-GR software.Other companies including Actuate, Defendry, Scylla, and ZeroEyes offer similar systems. Behind the news: The use of computer vision in such offerings updates earlier systems based on sounds. For instance, ShotSpotter is used by over 100 police departments in the U.S. The system picks up gunshot sounds from acoustic sensors placed around a community and uses machine learning to compare them with an audio database. When it recognizes a gunshot, it triangulates the location and alerts police.Why it matters: Gun violence is endemic across the U.S, including hundreds of mass shootings. By warning police or security guards before a shooter opens up, AI-powered gun detection could save lives.We’re thinking: Like any machine learning system applied to the real world, gun detection algorithms aren’t perfect. One such system used in New York state schools was found to mistake broom handles for guns. Such mistakes could be dangerous if they prompt police to enter possible crime scenes with their own weapons drawn and pulses pounding.", "image_caption": "Gun detecting system working and alerting the police", "metadata": {"article_id": "issue_75", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/GUN.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-75/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_75.html"}}
{"id": 15272133005, "type": "news_chunk", "title": "Detecting Guns, Fighting Lead Poisoning, Adversarial Training", "subtitle": "Annual Report, Robot Edition", "content": "Corporations are tailoring their financial reports to be read by machines.What’s new: Automated systems download far more company financial reports than humans, according to a study by the U.S. nonprofit National Bureau of Economic Research. Consequently, companies are filling those reports with data that looks good to computers.What they did: The study analyzed 50 years of quarterly and annual financial reports submitted by public companies to the U.S. Securities and Exchange Commission. Drawing on SEC download logs, the authors examined the IP address associated with each download to determine whether a person or a machine initiated it. They found that automated downloads grew from 360,862, or 39 percent of the total, in 2003 to around 165 million, or 78 percent, in 2016.Companies that served large numbers of machines-initiated downloads were more likely to make their reports machine-readable by, say, adhering to ASCII standards, separating tables from text, and ensuring that documents contained all the information required to interpret them.Moreover, these companies use language more likely to produce positive scores from sentiment-analysis models. For instance, they tend to avoid words associated with negative emotions, lawsuits, or uncertainty. Behind the news: Computer systems increasingly drive the stock market. Last year, Deutsche Bank estimated that automated systems made buying and selling decisions for 80 percent of equity trading and 90 percent of equity futures trading. Corporate financials are following suit.Why it matters: The study found that the more easily a computer can digest a company’s financial reports, the faster its stock is traded after a report has been published. This suggests that the market’s pace, already lightning-fast, is bound to accelerate.We’re thinking: Companies have every incentive to tweak their reports to impress their audience, whether readers consist of wetware or software. But there’s a slippery slope between painting a rosy picture and exaggerating in ways that border on fraud. Regulators, analysts, and AI practitioners alike have a responsibility to guard against market manipulation.", "image_caption": "Some findings from a study by the U.S. nonprofit National Bureau of Economic Research.", "metadata": {"article_id": "issue_75", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/CORPORATE.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-75/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_75.html"}}
{"id": 37601953001, "type": "news_chunk", "title": "Reading Viruses, Liberating Drones, Detecting Earthquakes...", "subtitle": "The Language of Viruses", "content": "Dear friends, Last week, I talked about how best practices for machine learning projects are not one-size-fits-all, and how they vary depending on whether a project uses structured or unstructured data, and whether the dataset is small or big. Another dimension that affects best practices is which phase of development a project is in: proof of concept or production. During the proof of concept (POC) phase, the primary goal is to determine if a system is worth building and deploying. During this phase, you might ask: For a visual inspection system, can we build a model that matches the performance of human inspectors?For face detection, can we build an edge (on-device) implementation that’s nearly as accurate as the cloud version while avoiding an unacceptable level of bias?For a sales-lead scoring application, how much will estimated revenue increase by using machine learning to prioritize leads? When building a POC, my goal is to move fast. We’ve all been told we should build replicable, robust, and scalable systems — but when I haven’t even determined if a project is technically feasible, I often trade replicability for speed. I hope I don’t get too much hate mail for this, but if it buys you speed, it is okay to hard-code parameters, compute key variables in a Jupyter notebook, use local copies of data, and operate with lightweight code review or versioning processes. If you already have a platform for experimentation, you may be able to build POCs in a systematic and robust way without sacrificing speed. But if you don’t, avoid over-investing in infrastructure at this stage. Instead, focus on getting the key information you need: whether this project is worth taking to production. (Those of you who are familiar with the lean startup philosophy will see the parallel to building a minimum viable product, which is often a clunky piece of software that helps validate or falsify a hypothesis.) In contrast, during the production phase, the goal is to build and deploy a system that generates practical value. I might go back to the messy POC and make sure that every step is replicable and documented. I put a lot of thought into scalable data pipelines, monitoring systems, and reliability. For example, if a researcher wrote preprocessing routines (say, a sequence of scripts and regexps to remove data associated with spam accounts), these now need to be documented, tested, and incorporated into the system. You’ll likely want to document everything to make sure models can be replicated and maintained: hyperparameters, model choices, data provenance (where the data came from), data lineage (how it was processed). During this phase, tools like TensorFlow Transform and Apache Beam can be lifesavers. If you’re building a project, don’t confuse the POC and production phases! Both are important, but the best practices depend on whether you’re deciding as quickly as possible if a project is worth putting into production or building a system that delivers real results to real users. Keep learning! A neural network learned to read the genes of viruses as though they were text. That could enable researchers to page ahead for potentially dangerous mutations.What’s new: Researchers at MIT trained a language model to predict mutations that would enable infectious viruses — including the SARS-CoV-2 virus that causes Covid-19 — to become even more virulent.Key insight: The authors suggest that the immune system’s response to viruses is similar to the way people understand natural language. A virus that causes infection has a “grammar” that’s biologically correct, and it also has a semantic “meaning” to which the immune system does or doesn’t respond. Mutations can enhance these worrisome qualities. How it works: The authors trained a bidirectional LSTM on the genetic equivalent of making a language model guess a missing word in a sentence. The training set included gene sequences from a variety of infectious bugs: 45,000 variants of influenza, 60,000 of HIV, and 4,000 of SARS-CoV-2. The researchers trained the biLSTM to fill in a missing amino acid in a sequence. Along the way, the model generated embeddings that represent relationships among sequences.Then they generated mutated sequences by changing one amino acid at a time.To rank a given mutation, they took a weighted sum of the likelihood that the mutated virus retained an infectious grammar and the degree of semantic difference between the original and mutated sequence’s embeddings. Results: The researchers compared their model’s highest-ranked mutations to those of actual viruses according to the area under curve (AUC), where 0.5 is random and 1.0 is perfect. The model achieved 0.85 AUC in predicting SARS-CoV-2 variants that were highly infectious and capable of evading antibodies. It achieved 0.69 AUC for HIV, and 0.77 AUC and 0.83 AUC respectively for two strains of influenza.Behind the news: Other researchers have also explored similarities between language and gene sequences. For example, Salesforce researchers trained a language model to treat amino acids like words and build grammatically correct “sentences” of functional proteins that could be used in medicine.Why it matters: Discovering dangerous viral mutations typically takes weeks, as scientists must analyze DNA taken from patients. The ability to predict harmful mutations could help them find dangerous variants sooner, helping epidemiologists update their models and giving researchers a head start on vaccines and therapies.We’re thinking: The Batch is grammatically correct but not infectious. Though we wouldn’t mind if it went viral!", "image_caption": "Data related to a language model that predicts mutations that would enable infectious viruses", "metadata": {"article_id": "issue_76", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/MUTATIONS.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-76/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_76.html"}}
{"id": 37601953002, "type": "news_chunk", "title": "Reading Viruses, Liberating Drones, Detecting Earthquakes...", "subtitle": "Quake Watch", "content": "Detecting earthquakes is an important step toward warning surrounding communities that damaging seismic waves may be headed their way. A new model detects tremors and provides clues to their epicenter.What’s new: S. Mostafa Mousavi and colleagues at Stanford and Georgia Institute of Technology built EQTransformer to both spot quakes and measure characteristics that help seismologists determine where they originated.Key insight: Language models based on transformer networks use self-attention to track the most important associations among tokens, such as words, in a sentence. The authors applied self-attention to seismic waves globally to track the most important associations among their features. Since clues to a quake’s epicenter appear in portions of the waveform, they also used self-attention locally to find patterns over shorter periods of time.How it works: The authors passed seismic waves through an encoder that fed three decoders designed to detect earthquakes and spot two types of location signal. The authors trained and tested the system using the Stanford Earthquake Dataset (STEAD), which contains over one million earthquake and non-earthquake seismographs. They augmented the data by adding noise, adding earthquake signals to non-quake waves, and shifting quake start times. Self-attention requires a great deal more computation as the input’s size grows, so the encoder, which comprised convolutional and LSTM layers, compressed the input into a high-level representation. A pair of transformer layers were included to focus on earthquake signals.In the detection decoder, convolutional layers determined whether an earthquake was occurring.The other two decoders tracked the arrival of p-waves (primary waves that push and pull the ground) and s-waves (secondary waves that move the ground up and down or side to side). The difference in these arrival times indicates distance from a quake’s epicenter. These decoders used LSTM and local self-attention layers to examine small windows of time, which fed convolutional layers that detected the signals. Results: EQTransformer outperformed state-of-the-art models in both detecting earthquakes and tracking p- and s-waves. In detection, EQTransformer achieved an F1 score of 1.0, a 2 percent improvement over the previous state of the art. In tracking p-waves, it improved mean absolute error over the earlier state of the art in that task from 0.07 to 0.01. With s-waves, it improved mean absolute error from .09 to .01. The training dataset didn’t include seismographs from Japan, so the authors tested their model’s ability to generalize on aftershocks from a Japanese quake that occurred in 2000. In this test, EQTransformer’s ability to spot the arrival of p-waves varied from human performance by an average .06 seconds, while its ability to spot the arrival of s-waves varied from human performance by an average .05 seconds.Why it matters: Applied at both global and local scales, self-attention could be useful in tasks as diverse as forecasting weather, product demand, and power consumption.We’re thinking: We applaud this earth shattering research!", "image_caption": "Data and graphs related to a new model capable of detecting tremors", "metadata": {"article_id": "issue_76", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2075.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-76/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_76.html"}}
{"id": 37601953003, "type": "news_chunk", "title": "Reading Viruses, Liberating Drones, Detecting Earthquakes...", "subtitle": "Every Picture Tells a Story", "content": "Facebook expanded a system of vision, language, and speech models designed to open the social network to users who are visually impaired.What’s new: A Facebook service that describes photos in a synthesized voice now recognizes 1,200 visual concepts — 10 times more than the previous version. Known as automatic alternative text, the system can recognize and explain what’s happening in a picture, including the relative size and position of people and objects, in any of 45 languages.How it works: Launched in 2016, the system initially learned from hand-labeled data to recognize 100 common concepts, like tree and mountain. Facebook added face recognition the following year, allowing users to opt into a more personalized experience. The new upgrade extends automatic alternative text in several ways: Facebook engineers used a weakly supervised approach to train ResNeXt image recognition models on 3.5 billion Instagram images and 17,000 hashtags that users put with them. Using a similar architecture, they applied transfer learning to train linear classification heads to recognize concepts including selfies, national monuments, and foods like rice and French fries.They used an existing object detection library to build a Fast R-CNN that recognizes the number, size, and position of various items in an image and determines its primary subject.The system starts each description with the humble phrase, “May be...,” and it doesn’t describe concepts that it can’t identify reliably. Users can request extra details, and the model will display a page that itemizes a picture’s elements by their position (top, middle, left, or bottom), relative size (primary, secondary, or minor), and category (people, activities, animals, and so on). Behind the news: Facebook, along with other popular websites, has struggled with how to serve visually impaired users. Some have complained that the site doesn’t work well with common accessibility equipment like screen readers that speak text aloud. For instance, earlier versions of automated alternative text didn’t inform users when the images it described were advertisements. However, some users have applauded Facebook’s use of face recognition with automatic alternative text, which can tell them when a photo depicts a friend or loved one.Why it matters: Around 285 million people worldwide are visually impaired and 39 million are blind, the World Health Organization estimates. People who don’t see well are as reliant on information as anyone — and they represent a sizable market.We’re thinking: Disabled web users in the U.S. file hundreds of lawsuits annually against Internet companies that don’t make their services accessible. Increasingly, online accessibility is recognized as a right, not a privilege.", "image_caption": "Facebook service describing a photo on Instagram", "metadata": {"article_id": "issue_76", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/BLIND.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-76/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_76.html"}}
{"id": 37601953004, "type": "news_chunk", "title": "Reading Viruses, Liberating Drones, Detecting Earthquakes...", "subtitle": "Drones Unleashed", "content": "U.S. regulators for the first time allowed commercial operators of autonomous aerial vehicles to fly out of operators’ sight.What’s new: The U.S. Federal Aviation Administration generally requires people on the ground to keep an eye on drones, but it authorized drone maker American Robotics to fly without requirement.How it works: The company’s 20-pound quadcopters travel predetermined paths and automatically avoid collisions with birds, aircraft, and other obstacles. When they’re not in the air, the drones charge their battery in a weatherproof launch pad, which also houses computing horsepower for navigation.An acoustic sensing system recognizes the presence and direction of airborne objects. It commands the robot to descend if it detects an object flying within a two-mile perimeter.A human technician must run through a safety checklist and inspect drones before takeoff, but these functions can be performed remotely. Flights are limited to daylight hours, altitudes under 400 feet, and limited areas in Kansas, Massachusetts, and Nevada, according to The Verge. Behind the news: Companies can apply to the FAA for a waiver of the line-of-sight rule. American Robotics became the first company to receive one after four years of testing. The agency recently issued rules governing flights in populated areas and at night — a step toward a full regulatory framework for drone delivery services.Last August, the agency granted Amazon and Wing limited permission to deliver packages via drones.The U.S. approach to drone regulations is relatively permissive. Most countries restrict flights to an operator’s line of sight. Why it matters: The ability to operate without a human in visual contact is a critical step to making drone flights easier to manage and more economical to operate.We’re thinking: Andrew used to work with Pieter Abbeel, Adam Coates, and others on reinforcement learning to get autonomous helicopters to fly stunts. He crashed quite a few copters in the process! (Safely, of course, in empty fields.) With drones now flying out of an operator’s line of sight, it’s more important than ever to subject their hardware and software to robust safety testing and verification.", "image_caption": "Drone flying over a massive field", "metadata": {"article_id": "issue_76", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/DRONES.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-76/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_76.html"}}
{"id": 37601953005, "type": "news_chunk", "title": "Reading Viruses, Liberating Drones, Detecting Earthquakes...", "subtitle": "Images From Noise", "content": "Generative adversarial networks and souped-up language models aren’t the only image generators around. Researchers recently upgraded an alternative known as score-based generative models. What’s new: Yang Song and Stefano Ermon at Stanford derived a procedure for selecting hyperparameter values for their earlier score-based generator, which produces images from noise. Finding good hyperparameters enabled the authors to generate better images at higher resolution. Key insight: Score-based image generation uses a model that learns how to change images corrupted by additive noise to reproduce the original pictures, and an algorithm that executes the changes to produce fresh images. The earlier work relied on manual tuning to find good values for hyperparameters such as how much noise to add to training images. Real-world data distributions are hard to analyze mathematically, so, in the new work, the authors approximated them with simplified distributions. Given the simpler scenario, they could analyze how each hyperparameter would affect both training and inference, enabling them to derive methods to compute hyperparameter values. About score-based generation: The process starts with producing many versions of each training dataset by adding various magnitudes of noise. A modified RefineNet is trained to minimize the difference between its prediction of the way, and the actual way, to change a noisy example into a clean one. RefineNet learns a vector field: Given a point in space that corresponds to an image, it returns a vector that represents the direction toward a more realistic image. Then an algorithm based on Langevin dynamics (a set of equations developed to model the way molecules interact in a physical system) moves the point in that direction. The process of finding a vector and moving in that direction repeats for a finite number of steps. How it works: The authors followed their score-based procedure but used new methods to compute hyperparameters that governed the noise added to the training dataset and the size and number of steps computed by Langevin dynamics. We’ll focus on the noise hyperparameters in this summary. Very noisy datasets are needed to train the network to produce an image from noise. However, too many highly noisy training examples make it hard for the network to learn. So it’s necessary to balance noisy and less-noisy examples carefully.What’s the greatest amount of noise to add? To train the network to generate images that reflect the entire training data distribution, Langevin dynamics must be able to transition between any two training examples. So the greatest noise, as measured by the Euclidean distance between noisy and noise-free examples, should be equal to the maximum distance between any pair of noise-free training examples.What should be the difference between the greatest and next-greatest amounts of noise added, and how many increments should there be? The authors examined a scenario with only one training example. For RefineNet to supply good directions, it must learn to chart a path from any point in the vector field to that example. To do that, the added noise must leave no areas where, randomly, noisy data doesn’t occur. Based on that principle, they derived an equation to determine how many noisy datasets to produce and what increments of noise to apply. Results: The authors evaluated their new model’s output using Frechet Inception Distance (FID), a measure of how well a generated data distribution resembles the original distribution, where lower is better. Trained on 32×32 images in CIFAR-10, the model achieved 10.87 FID, a significant improvement over the earlier model’s 25.32 FID. It also beat SNGAN, which achieved 21.7 FID. The paper doesn’t compare competing FID scores at resolutions above 32×32 and omits FID scores altogether at resolutions higher than 64×64. It presents uncurated samples up to 256×256.Why it matters: GANs often don’t learn to produce good images because the objectives of their generator and discriminator are at odds. Score-based generative models optimize for only one objective, which eliminates this risk. That said, they may fail to converge for other reasons.We’re thinking: We love the idea of using mathematical reasoning to derive optimal hyperparameter values: More time to develop good models!", "image_caption": "Examples of images being produced from noise", "metadata": {"article_id": "issue_76", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2074.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-76/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_76.html"}}
{"id": 84751803001, "type": "news_chunk", "title": "Drivers Under Surveillance, What Is Fairness?, Cancer Treatment", "subtitle": "Eyes On Drivers", "content": "Dear friends, Over the last several decades, driven by a multitude of benchmarks, supervised learning algorithms have become really good at achieving high accuracy on test datasets. As valuable as this is, unfortunately maximizing average test set accuracy isn’t always enough. I’ve heard too many conversations like this:Machine learning engineer: It did well on the test set!Product manager: But it doesn’t work for my application.Machine learning engineer: But . . . It did well on the test set! What else is there?Robustness and generalization: In a production deployment, performance can degrade due to concept drift (where the function mapping from x->y changes; say, the model predicts housing prices y and inflation causes prices to rise) and data drift (where the input distribution changes). One important subset of data drift relates to performance on classes that are rare in or absent from the training set. For example, a speech recognition system may achieve high average accuracy despite poor performance on speakers with a British accent, because the training and test sets included few examples of British speakers. If the product takes off in the U.K. and a lot more British speakers jump in, its accuracy will plummet. A more robust system would fare better. Performance on relatively important examples: Some examples are more important than others, and even if average test set accuracy is high, a system that performs poorly on important examples may be unacceptable. For example, users might forgive a search engine that doesn’t always return the best results to informational and transactional queries like “apple pie recipe” or “wireless data plan.” But when they enter a navigational query such as “stanford,” “youtube,” or “reddit,” they have a specific website in mind, and the search engine had better return the right URL or risk losing the user’s trust. In theory, weighting test examples according to their importance can address this issue, but it doesn’t always work in practice. Performance on key slices of data: Say a machine learning system predicts whether a prospective borrower will repay a loan, so as to decide whether to approve applications. Even if average accuracy is high, if the system is disproportionately inaccurate on applications by a specific minority group, we would be foolhardy to blindly deploy it. While the need to avoid bias toward particular groups of people is widely discussed, this issue applies in contexts beyond fairness to individuals. For example, if an ecommerce site recommends products, we wouldn’t want it to recommend products from large sellers exclusively and never products from small sellers. In this example, poor performance on important slices of the data — such as one ethnicity or one class of seller — can make a system unacceptable despite high average accuracy. My advice: If a product manager tells us that our AI system doesn’t work in their application, let’s recognize that our job isn’t only to achieve high average test accuracy — our job is to solve the problem at hand. To achieve this, we may need visualizations, larger datasets, more robust algorithms, performance audits, deployment processes like human-in-the-loop, and other tools. Keep learning! Amazon is monitoring its delivery drivers with in-vehicle cameras that alert supervisors to dangerous behavior.What’s new: The online retail giant rolled out a ceiling-mounted surveillance system that flags drivers who, say, read texts, fail to use seatbelts, exceed the speed limit, or ignore a stop sign, CNBC reported.How it works: The system, Netradyne Driveri, uses street-facing and in-cab cameras along with an accelerometer and gyroscope to spot 16 unsafe behaviors. When it detects an offending behavior, the system warns the driver and automatically uploads a video to Amazon.Drivers can upload videos manually to document potentially problematic events such as a person approaching the vehicle or an inaccessible delivery location.Netradyne said its system reduces collisions by two thirds, according to The Verge. Yes, but: Some Amazon drivers said that the system violates their privacy and exacerbates pressure to meet the company’s aggressive delivery schedules.Behind the news: Amazon has expanded its force of local delivery drivers to more than 400,000 as of November. It has used a similar computer vision system from SmartDrive to monitor its long-haul truckers for sleepiness and distraction. Delivery competitor United Parcel Service also has tested a system, Lytx DriveCam, that monitors drivers of its delivery vans.Why it matters: Investigations by BuzzFeed and the The New York Times charge that Amazon pressures drivers to make deliveries at a dangerously fast clip, resulting in numerous accidents and several deaths. While in-car surveillance is intrusive, proponents point out that it might help reduce human errors that can occur when people are under stress.We’re thinking: There are many ways that AI can enhance productivity and safety. Let’s make sure to do it in a way that’s empowering rather than dehumanizing.", "image_caption": "Netradyne Driveri system used to monitore Amazon's delivery drivers working", "metadata": {"article_id": "issue_78", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/CARCAM.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-78/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_78.html"}}
{"id": 84751803002, "type": "news_chunk", "title": "Drivers Under Surveillance, What Is Fairness?, Cancer Treatment", "subtitle": "Better Language Through Vision", "content": "For children, associating a word with a picture that illustrates it helps them learn the word’s meaning. New research aims to do something similar for machine learning models.What’s new: Hao Tan and Mohit Bansal at University of North Carolina Chapel Hill improved a BERT model’s performance on some language tasks by training it on a large dataset of image-word pairs, which they call visualized tokens, or vokens.Key insight: Images can illuminate word meanings, but current datasets that associate images with words have a small vocabulary relative to the corpuses typically used to train language models. However, these smaller datasets can be used to train a model to find correspondences between words and images. Then that model can find such pairings in separate, much larger datasets of images and words. The resulting pairings can help an established language model understand words better.How it works: The authors trained a system called the vokenizer to pair BERT-style tokens — generally individual words or characters — with related images. They used the resulting visualized tokens to train BERT to predict such pairings and fine-tuned it on various language tasks. The vokenizer comprised a pretrained ResNeXt-101 vision model and a pretrained BERT, each followed by a two-layer neural network that generated representations separately for input images and tokens. To train it, the authors split COCO, which depicts roughly dozens of object types with captions, into token-image pairs, associating an image with every token in a given caption. They trained the vokenizer to predict pairings by encouraging it to make the distance between pairs of images and tokens larger than the distance between unpaired images and tokens.To create a large number of token-image pairs, the vokenizer paired images in the Visual Genome, which depicts millions of objects, with words from English Wikipedia. First it generated a representation for each image. Then, for each token, it used a nearest neighbor search to find the image whose representation was closest.Using a separate BERT with an extra fully-connected layer, the authors removed some tokens from Wikipedia sentences at random. They pretrained the model to predict both the missing tokens and the image paired with each token. Then they fine-tuned the model on GLUE (which includes several language understanding tasks), SQuAD (question answering), and SWAG (language reasoning). Results: BERT pretrained with the token-image pairs outperformed the same architecture trained in the same way but without the pairs on tasks in GLUE, SQuAD, and SWAG. For instance, it achieved 92.2 percent accuracy on SST2, predicting the sentiment of movie reviews, compared to 89.3 percent for BERT without visual training. Similarly, on SQuAD v1.1, it achieved an F1 score of .867 on SQuAD compared to .853 for BERT without visual training.Why it matters: This work suggests the potential of visual learning to improve even best language models.We’re thinking: If associating words with images helps a model learn word meaning, why not sounds? Sonic tokens — sokens! — would pair, say, “horn” with the tone of a trumpet and “cat” with the sound of a meow.", "image_caption": "Graphs and data related to visualized tokens (or vokens)", "metadata": {"article_id": "issue_78", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/VOKEN-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-78/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_78.html"}}
{"id": 84751803003, "type": "news_chunk", "title": "Drivers Under Surveillance, What Is Fairness?, Cancer Treatment", "subtitle": "Fairness East and West", "content": "Western governments and institutions struggling to formulate principles of algorithmic fairness tend to focus on issues like race and gender. A new study of AI in India found a different set of key issues.What’s new: Researchers at Google interviewed dozens of activists, academic experts, and legal authorities about the ways AI is deployed in India, especially with respect to marginalized groups. In part, their goal was to demonstrate how Western notions of bias and power don’t always apply directly to other cultures.What they found: The report highlights three ways in which issues surrounding AI in India differ from Western countries and may call for different approaches to achieve fairness. Dataset bias: Half the Indian population lacks access to the internet — especially women and rural residents — so datasets compiled from online sources often exclude large swathes of society. Fixing the problem goes beyond data engineering. It requires a comprehensive approach that includes bringing marginalized communities into the digital realm.Civil rights: Many Indian citizens are subjected to intrusive AI, unwillingly or unwittingly. For example, some cities use AI to track the operational efficiency of sanitation workers, many of whom come from lower-caste groups. To address perceived abuses, Westerners typically appeal to courts, journalists, or activists. Many Indians, though, perceive such avenues to be largely unavailable.Technocracy: India is eager to modernize, which motivates politicians and journalists to embrace AI initiatives uncritically. Compared with Western countries, fewer people in positions of power are qualified to assess such initiatives — a prerequisite to making their fairness a priority. Behind the news: Other groups have sought to highlight the outsized influence that Western notions of ethics have on AI worldwide. The IEEE Standards Association recently investigated how applying Buddhist, Ubuntu, and Shinto-inspired ethics could improve responsible AI.A 2019 study looked at how responsible AI guidelines should accommodate the massive influx of people who are newly online, many of whom live in countries like Brazil, India, and Nigeria.A report published last year examined the social implications of AI in China and Korea. Why it matters: Most research into AI fairness comes from a U.S.-centric perspective rooted in laws such as the Civil Rights Act of 1964, which outlaws discrimination based on race, sex, and religion. Guidelines based on a single country’s experience are bound to fall short elsewhere and may even be harmful.We’re thinking: Many former colonies struggle with legal and educational systems imposed by Western powers. It’s important to avoid repeating similar patterns with AI systems.", "image_caption": "Scale of justice symbol over a map of India", "metadata": {"article_id": "issue_78", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker-97.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-78/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_78.html"}}
{"id": 84751803004, "type": "news_chunk", "title": "Drivers Under Surveillance, What Is Fairness?, Cancer Treatment", "subtitle": "Shortcut to Cancer Treatment", "content": "Doctors who treat breast cancer typically use a quick, inexpensive tumor-tissue stain test to diagnose the illness and a slower, more costly one to determine treatment. A new neural network could help doctors to go straight from diagnosis to treatment.What’s new: The stain in the test for treatment highlights a key visual clue to the choice of therapy that’s otherwise invisible to human pathologists. Nikhil Naik at Salesforce and colleagues at University of Southern California built ReceptorNet to detect that clue in the diagnostic test.Key insight: The presence of estrogen receptor proteins is a sign that hormone therapy may work. In the diagnostic test, known as hematoxylin and eosin (H&E), these proteins are invisible to the human eye. An attention mechanism, which identifies the most important parts of an input (in this case, a portion of an H&E slide) in determining the output (a label that the proteins are present), can aggregate image areas where they occur so a vision network can classify the slide.How it works: ReceptorNet comprises a ResNet-50 pretrained on ImageNet followed by an attention layer and a fully connected layer. The researchers trained and tested ReceptorNet on images of H&E slides, and augmentations of them, in the Australian Breast Cancer Tissue Bank and The Cancer Genome Atlas datasets. The authors isolated the images’ foreground using Otsu’s method, which distinguishes foreground from background based on variance in each pixel’s grayscale value, to remove background regions. They magnified the foregrounds 20 times and divided them into tiles of 256×256 pixels.During training, they fed ReceptorNet 50 randomly sampled tiles per slide. The ResNet extracted representations of each tile and passed them en masse to the attention layer, which weighted their importance. The fully connected layer used the weighted representations to classify slides according to whether they contain estrogen receptors.To combat overfitting, the authors used mean pixel regularization, randomly replacing 75 percent of tiles with a single-color image of the dataset’s mean pixel value. Results: ReceptorNet achieved an area under the curve of 0.92 AUC, a measure of true versus false positives where 1 is a perfect score. The authors experimented with alternatives to the attention layer that didn’t perform as well, which suggests that attention was key. Yes, but: The authors had access only to H&E images, so they couldn’t compare ReceptorNet’s performance against the test that’s typically used to guide treatment.Why it matters: ReceptorNet had a label for each tissue slide but not for each tile derived from it. The success of attention in aggregating and evaluating the representations extracted from each tile bodes well for this approach in reading medical images.We’re thinking: Where else could computer vision augment or replace slow, expensive medical tests?", "image_caption": "Series of images and graphs related to cancer detection", "metadata": {"article_id": "issue_78", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/RECEPTORNET.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-78/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_78.html"}}
{"id": 84751803005, "type": "news_chunk", "title": "Drivers Under Surveillance, What Is Fairness?, Cancer Treatment", "subtitle": "How Much For That Vintage Gucci?", "content": "Computer vision is helping people resell their used designer handbags.What’s new: Rebag, a resale company for luxury handbags, watches, and jewelry, launched Clair AI, an app that automatically appraises second-hand bags from brands like Gucci, Hermes, and Prada, Vogue reported.How it works: Users take a close-up picture of a handbag against a neutral background. The app finds between one and five potential matches for designer, model, and style. Users choose the potential match that comes closest and adds details about the used bag’s condition and color. The system then calculates dollar figures for retail price and trade-in value.Users can also submit photos of bags in magazines, videos, or other fashionista’s hands to find out what other people are carrying.Rebag’s CEO said in a promotional video that the app achieved 90 percent accuracy rate and took six years and millions of data points to develop. Behind the news: Rebag’s revenue grew by 50 percent in 2020, riding a surge in demand for second-hand luxury goods. The market for used high-end items like watches, jewelry, fine art, and yachts grew in 2019 by 12 percent to $26.5 billion.Why it matters: Consumers are mindful of the resale value of high-ticket goods. An app that makes it easier to tap into that market could drive sales of both new and used items — and make it easy to unload the hideous thing that somehow looked fetching last summer.We’re thinking: We tested this system on the bags in our closet, but it wasn’t impressed by our prized NeurIPS tote.", "image_caption": "Rebag app working on a cellphone", "metadata": {"article_id": "issue_78", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker-100.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-78/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_78.html"}}
{"id": 16808143001, "type": "news_chunk", "title": "Untested Medical AI?, Art Appreciation For Robots, Why Models", "subtitle": "Medical AI’s Hidden Data", "content": "Dear friends, When a lot of data is available, machine learning is great at automating decisions. But when data is scarce, consider using the data to augment human insight, so people can make better decisions. Let me illustrate this point with A/B testing. The common understanding of the process is: Build two versions of your product. For example, on the DeepLearning.AI website, one version might say, “Build your career with DeepLearning.AI,” and another, “Grow your skills with DeepLearning.AI.”Show both versions to groups of users chosen at random and collect data on their behavior.Launch the version that results in better engagement (or another relevant metric). But this is not how I typically use A/B testing. Often I run such tests to gain insight, not to choose which product to launch. Here‘s how it works: Build two versions of your product.Have the product team make predictions about which version will perform better.Test both versions and collect data on user behavior.Show the results to the team, and let them influence their beliefs about users and their reactions. If someone says, “Oh, that’s weird. I didn’t realize our users wanted that!” then we’ve learned something valuable.Based on the team’s revised intuitions, have them decide what to launch. It could be version A, version B, or something else.Repeat until you reach diminishing returns in terms of learning. On major websites, where the developers may run thousands of automated experiments a day — for example, trying out different ad placements to see who clicks on what — it’s not possible for people to look at every experimental result to hone their intuition. In this case, fully or mostly automated decision making works well. An algorithm can try multiple versions and pick the one that achieves the best metrics (or use the data to learn what to show a given user). But when the number of daily experiments is small, using such experiments to hone your intuition allows you to combine limited trials with human insight to arrive at a better decision.Beyond A/B testing, the same concept applies to building machine learning systems. If your dataset size is modest, combining data-derived insights with human insights is critical. For example, you might do careful error analysis to derive insights and then design a system architecture that captures how you would carry out the task. If you have a massive amount of data, more automation — perhaps a large end-to-end learning algorithm — can work. But even then, error analysis and human insight still play important roles. Keep learning! U.S. government approval of medical AI products is on the upswing — but information about how such systems were built is largely unavailable.What’s new: The U.S. Food and Drug Administration (FDA) has approved a plethora of AI-driven medical systems. But, unlike drugs, there’s a dearth of publicly available information about how well they work, according to an investigation by the health-news website Stat News.What they found: The FDA doesn’t require makers of AI systems to provide systematic documentation of their development and validation processes, such as the composition of training and test datasets and the populations involved. The data actually provided by manufacturers varies widely. Stat News compiled a list of 161 products that were approved between 2012 and 2020. Most are imaging systems trained to recognize signs of stroke, cancer, or other conditions. Others monitor heartbeats, predict fertility status, or analyze blood loss.The makers of only 73 of those products disclosed the number of patients in the test dataset. In those cases, the number of patients ranged from less than 100 to more than 15,000.The manufacturers of fewer than 40 products revealed whether the data they used for training and testing had come from more than one facility — an important factor in proving the product’s general utility. Makers of 13 products broke down their study population by gender. Seven did so by race.A few companies said they had tested and validated their product on a large, diverse population, but that information was not publicly available. Behind the news: The rate at which the FDA approves medical AI products is rising and could reach 600 products annually by 2025, according to Stat News. Most such products currently are approved under a standard that requires demonstrating “substantial equivalence” in safety and efficacy to similar, already-approved systems. This standard, known as 510(k), was established in 1976 without medical AI in mind.A recent FDA action plan for regulating AI aims to compel manufacturers to evaluate their products more rigorously. Why it matters: Without consistent requirements for testing and reporting, the FDA can’t ensure that AI systems will render accurate diagnoses, recommend appropriate treatments, or treat minority populations fairly. This leaves health care providers to figure out for themselves whether a product works as advertised with their particular equipment and patients.We’re thinking: If you don’t know how an AI system was trained and tested, you can’t evaluate the risk of concept or data drift as real-world conditions and data distributions change. This is a problem even in drug testing: A vaccine validated against the dominant Covid-19 variant may become less effective as the virus mutates. Researchers are developing tools to combat such drifts in AI systems. Let’s make sure they’re deployed in medical AI.", "image_caption": "Series of images showing a variety of medical AI products", "metadata": {"article_id": "issue_79", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/FDA.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-79/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_79.html"}}
{"id": 16808143002, "type": "news_chunk", "title": "Untested Medical AI?, Art Appreciation For Robots, Why Models", "subtitle": "Facing Failure to Generalize", "content": "The same models trained on the same data may show the same performance in the lab, and yet respond very differently to data they haven’t seen before. New work finds this inconsistency to be pervasive.What’s new: Researchers explored this largely unexamined phenomenon, which they call underspecification. The team, led by Alexander D’Amour, Katherine Heller, and Dan Moldovan, spanned Google, MIT, Stanford, University of California San Diego, U.S. Department of Veterans Affairs, Aravind Eye Hospital, and Shri Bhagwan Mahavir Vitreo-Retinal Services.Key insight: A well specified model pipeline — a model architecture, hyperparameters, training and test sets, and training procedure — should produce models that behave consistently. In practice, though, the same pipeline can produce many distinct models that achieve near-optimal performance, only some of which generalize to real-world conditions. Building a plethora of models and testing each one is the only way to know which is which.How it works: The authors built many models per pipeline across a range of machine learning applications. Then they compared their performance on an appropriate test set and alternative data. The tests fell into three categories: The authors probed whether models produced using the same pipeline performed equally well on particular subsets of a test set. For example, with vision models that were trained to recognize an eye disease, they compared performance on images taken by different cameras.They compared performance on an established test set and a similar one with a different distribution. For instance, they compared the performance of ImageNet-trained models on both ImageNet and ObjectNet, which depicts some ImageNet classes from different angles and against different backgrounds.They also compared performance on examples that were modified. For instance, using a model that was trained to evaluate similarity between two sentences, they switched genders, comparing the similarity of “a man is walking” and “a doctor is walking” versus “a woman is walking” and “a doctor is walking.” Results: The authors found highly variable performance in models produced by identical model pipelines for several practical tasks in language, vision, and healthcare. For instance, they trained 50 ResNet-50 models on ImageNet using the same pipeline except for differing random seeds. On ImageNet’s test set, the standard deviation from top-1 accuracy was 0.001. On ImageNet-C, which comprises corrupted ImageNet examples that are still recognizable to humans, the standard deviation was 0.024. A given model’s performance on one dataset didn’t correlate with its performance on the other.Why it matters: If our models are to be useful and trustworthy, they must deliver consistent results. Underspecification is a significant barrier to that goal.We’re thinking: This work offers a helpful framework to evaluate the model performance on similar-but-different data. But how can we specify model pipelines to produce consistent models? We eagerly await further studies in this area.", "image_caption": "Different data related to the phenomenon called underspecification", "metadata": {"article_id": "issue_79", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/UNDERSPEC-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-79/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_79.html"}}
{"id": 16808143003, "type": "news_chunk", "title": "Untested Medical AI?, Art Appreciation For Robots, Why Models", "subtitle": "Computation as a National Resource", "content": "How much processing power do various nations have on hand to drive their AI strategy? An international trade group aims to find out.What’s new: The Organisation for Economic Co-operation and Development (OECD) is launching an effort to measure the computing capacity available in countries around the world. The organization, which serves 37 member nations, wants to help leaders invest wisely in AI by giving them a sense of their computational assets and how they compare to their peers.How it works: A task force led by Nvidia vice president Keith Strier will include around 30 policy makers, researchers, hardware experts, and data center operators. The task force will develop a framework for benchmarking national and regional processing resources.Once the framework is in place, they’ll survey each country, focusing on government agencies and national AI clouds. They will omit military capabilities, commercial services, and edge devices.It’s unclear whether the survey will include public-private partnerships, such as Google’s collaboration with Aramco, Saudi Arabia’s state-run oil company, to provide cloud infrastructure for large companies. Behind the news: The project is part of OECD’s One AI initiative, which also classifies AI systems, develops trustworthy AI, and crafts guidance on AI policies. The organization developed a set of AI principles that 40 nations had signed as of June.Why it matters: The OECD has cataloged over 300 AI policy initiatives across 60 countries, but the computing power available to each is hugely unequal. A tool that helps policymakers see where investment is most needed could help them set sensible targets.We’re thinking: Governments have an incentive to improve their standard metrics, such as gross domestic product. Companies that sell processing power have an incentive to encourage governments to boost those metrics by investing in computing infrastructure. If this dynamic provides more resources to AI researchers, we’re all for it.", "image_caption": "Data showing information related to AI strategy status in OECD countries", "metadata": {"article_id": "issue_79", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker-2021-02-09T103736.475.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-79/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_79.html"}}
{"id": 16808143004, "type": "news_chunk", "title": "Untested Medical AI?, Art Appreciation For Robots, Why Models", "subtitle": "How Art Makes AI Feel", "content": "An automated art critic spells out the emotional impact of images.What’s new: Led by Panos Achlioptas, researchers at Ecole Polytechnique, King Abdullah University, and Stanford University trained a deep learning system to generate subjective interpretations of art.How it works: The robot reviewer is a showcase for the authors’ dataset ArtEmis, which combines images with subjective commentary. ArtEmis comprises around 81,500 paintings, photos, and other images from the online encyclopedia WikiArt along with crowdsourced labels that describe the emotional character of each work (“amusement,” “awe,” “sadness,” and so on) and brief explanations of how the work inspired those emotions (to explain amusement, for instance, “His mustache looks like a bird soaring through the clouds.”) The researchers trained a model based on Show-Attend-Tell, which combines a convolutional neural network and an LSTM outfitted with attention, to replicate the annotations.As a baseline, they used a ResNet-32 pretrained on ImageNet. Given a test image, they used a nearest neighbor search to find the most similar image in the ArtEmis training set. Then they chose one of that image’s captions at random. Results: Volunteers guessed whether a given caption came from Show-Attend-Tell or a human, and roughly half the model’s captions passed as human-written. Nonetheless, the authors found the model’s output on average less accurate, imaginative, and diverse than the human annotations. The team also compared generated and baseline captions using a number of natural language metrics. Show-Attend-Tell achieved a ROUGE-L score of 0.295 versus the baseline 0.208 (a perfect score being 1.0). It achieved a METEOR score of 0.139 versus the baseline 0.1 (out of a perfect 1.0).Behind the news: Other AI systems have probed the elusive relationship between images and emotions, especially images of human faces. For instance, a GAN has been built to generate synthetic faces that express one of eight emotions, and some software vendors dubiously claimed to evaluate job candidates based on facial expressions.Why it matters: When humans look at an image, they perceive meanings beyond the subject matter displayed in the frame. Systems that help people make sense of the world could benefit from the ability to make such subjective judgments, whether they’re evaluating artworks, product recommendations, medical images, or flaws in manufactured goods.We’re thinking: Show-Attend-Tell’s soft deterministic attention mechanism makes us feel like we’re looking at a dream.", "image_caption": "Art pieces with subjective commentary regarding their emotional impact", "metadata": {"article_id": "issue_79", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker-98.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-79/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_79.html"}}
{"id": 16808143005, "type": "news_chunk", "title": "Untested Medical AI?, Art Appreciation For Robots, Why Models", "subtitle": "Autonomous Weapons Gain Support", "content": "A panel of AI experts appointed by the U.S. government came out against a ban on autonomous weapons.What’s new: A draft report from the National Security Commission on Artificial Intelligence, led by former Google CEO Eric Schmidt, recommends against a proposed international global prohibition of AI-enabled autonomous weapon systems, an idea that many other countries have agreed to. The report encourages the U.S. to expand its efforts in military AI, which include the weapons-ready Northrop Grumman X-47B drone pictured above.What they said: The commission acknowledges the risks of autonomous weaponry but concludes that an international ban would be both difficult to enforce and antithetical to America’s interest. Commission member and former deputy defense secretary Robert Work told Reuters that autonomous weapons would make fewer mistakes than humans, resulting in fewer battlefield casualties. The members believe that the U.S.’ geopolitical rivals will use AI for intelligence, propaganda, and espionage. Rogue states, terrorists, and criminals will, too. The U.S. must scale up its own AI programs to defend against such threats.They recommend that defense agencies build a shared infrastructure for developing AI systems, with access for trusted contractors. Each agency should invest in more tech training for its personnel, too. To accomplish these and other goals, the U.S. government should allocate $32 billion annually by 2026.The final version of the report is scheduled for publication in March. Behind the news: Nongovernmental organizations have been campaigning to ban autonomous weapons for nearly a decade. The United Nations has held meetings on the subject since 2014, and at least 30 countries are in favor.Why it matters: As the world’s preeminent power in both military force and AI, the U.S.’ decisions will be a major influence on those of other countries.We’re thinking: National defense policy is full of complicated issues, and simplistic slogans won’t lead to the best decisions. On balance, though, we continue to support a global ban on autonomous weapons.", "image_caption": "Autonomous aircraft taking off and landing", "metadata": {"article_id": "issue_79", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/MORAL-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-79/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_79.html"}}
{"id": 31315514001, "type": "news_chunk", "title": "Tesla Acquires DeepScale, France Backs Face Recognition, Robots", "subtitle": "Tesla Bets on Slim Neural Nets", "content": "Dear friends, Last week, I saw a lot of social media discussion about a paper using deep learning to generate artificial comments on news articles. I’m not sure why anyone thinks this is a good idea. At best, it adds noise to the media environment. At worst, it’s a tool for con artists and propagandists. A few years ago, an acquaintance pulled me aside at a conference to tell me he was building a similar fake comment generator. His project worried me, and I privately discussed it with a few AI colleagues, but none of us knew what to do about it. It was only this year, with the staged release of OpenAI’s GPT-2 language model, that the question went mainstream. Do we avoid publicizing AI threats to try to slow their spread, as I did after hearing about my acquaintance’s project? Keeping secret the details of biological and nuclear weapon designs has been a major force slowing their proliferation. Alternatively, should we publicize them to encourage defenses, as I’m doing in this letter? Efforts like the OECD’s Principles on AI, which state that “AI should benefit people and the planet,” give useful high-level guidance. But we need to develop guidelines to ethical behavior in practical situations, along with concrete mechanisms to encourage and empower such behavior. We should look to other disciplines for inspiration, though these ideas will have to be adapted to AI. For example, in computer security, researchers are expected to report vulnerabilities to software vendors confidentially and give them time to issue a patch. But AI actors are global, so it’s less clear how to report specific AI threats. Or consider healthcare. Doctors have a duty to care for their patients, and also enjoy legal protections so long as they are working to discharge this duty. In AI, what is the duty of an engineer, and how can we make sure engineers are empowered to act in society’s best interest? To this day, I don’t know if I did the right thing years ago, when I did not publicize the threat of AI fake commentary. If ethical use of AI is important to you, I hope you will discuss worrisome uses of AI with trusted colleagues so we can help each other find the best path forward. Together, we can think through concrete mechanisms to increase the odds that this powerful technology will reach its highest potential. Keep learning! Elon Musk has promised a fleet of autonomous Tesla taxis by 2020. The company reportedly purchased a computer vision startup to help meet that goal.What’s new: Tesla acquired DeepScale, a Silicon Valley startup that processes computer vision on low-power electronics, according to CNBC. The price was not reported. DeepScale, founded in 2015 by two UC Berkeley computer scientists, had raised nearly $19 million prior to Tesla’s purchase.The company’s platform, called Carver21, uses a high-efficiency neural network architecture known as SqueezeNet.The systems uses three parallel networks to perform object detection, lane identification, and drivable area identification.Carver21 imposes a computational budget of 0.6 trillion operations per second. That’s a relatively small demand on Tesla’s custom chipset, which is capable of 36 trillion operations per second. Behind the news: Tesla’s stock is down 25 percent this year due to manufacturing problems and a drop in demand for electric vehicles. In July, the company lost around 10 percent of its self-driving dev team after Musk expressed displeasure at their inability to adapt its highway-specific autopilot software to urban driving, according to a report in The Information. The recent debut of Tesla’s Smart Summon feature, which enables cars to drive themselves from a parking space to their waiting owner, was marred by reports of accidents. Why it matters: Cars operate within tight constraints on electrical power, and self-driving cars consume lots of power-hungry processing. Tesla is betting that leaner processing will help it reach full autonomy within the power budget of an electric vehicle. Fleets of self-driving taxis would certainly bolster the company’s bottom line.We’re thinking: Low-power processing is just one of many things that will make fully self-driving systems practical. There’s widespread skepticism about Tesla’s ability to deliver on its promises on time, but every piece will help.", "image_caption": "DeepScale's automated vehicle technology", "metadata": {"article_id": "issue_8", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Deep20Scale20Resized.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-8/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_8.html"}}
{"id": 31315514002, "type": "news_chunk", "title": "Tesla Acquires DeepScale, France Backs Face Recognition, Robots", "subtitle": "Hidden Findings Revealed", "content": "Drugs undergo rigorous experimentation and clinical trials to gain regulatory approval, while dietary supplements get less scrutiny. Even when a drug study reveals an interaction with supplements, the discovery tends to receive little attention. Consequently, information about interactions between drugs and supplements — and between various supplements — is relatively obscure. A new model brings it to light.What’s new: Lucy Lu Wang and collaborators at the Allen Institute created supp.ai, a website that scans medical research for information about such interplay. Users can enter a supplement name to find documented interactions.Key insight: Language describing drug interactions is similar to that describing interactions involving supplements, so an approach that spots drug interactions should work for supplements.How it works: The researchers modified an earlier model that finds drug-to-drug interactions in medical literature to support supplements. The authors compiled a list of supplements and drugs in the TRC Natural Medicines database of 1,400 supplements and the Unified Medical Language System’s database of 2 million medical terms and their relationships.They used a sentence extraction tool to search abstracts of publications indexed by the Medline database for sentences containing references to multiple supplements.They fine-tuned a BERT language model on the Merged-PDDI archive of documents describing drug-to-drug interactions.Based on patterns in that archive, the model predicted whether a sentence describes drug-to-drug, supplement-to-drug, or supplement-to-supplement interactions. Results: Among 22 million abstracts, the system classified 1.1 million sentences describing interactions. To assess accuracy, the authors hand-labeled 400 sentences that contained references to supplements. On this subset, the system was 87 percent accurate in identifying supplement interactions, compared with 92 percent for drug interactions, the state of the art in that task.Why it matters: Most U.S. adults use a dietary supplement, yet their interactions with drugs or one another are virtually unknown. Supp.ai makes it easy for anyone with a web browser to look them up.We’re thinking: The researchers took advantage of the similarity between text discussing drug and supplement interactions to adapt a drug-oriented model for an entirely different, less-scrutinized class of remedies — a clever approach to a difficult problem.", "image_caption": "Pipeline for identifying sentences containing evidence of SDIs and SSIs", "metadata": {"article_id": "issue_8", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Supplements20Resized.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-8/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_8.html"}}
{"id": 31315514003, "type": "news_chunk", "title": "Tesla Acquires DeepScale, France Backs Face Recognition, Robots", "subtitle": "Want Your Pension? Send a Selfie", "content": "The French government plans to roll out a national identification service based on face recognition. Critics warn that the new system violates citizens’ privacy.What’s new: Beginning in November, President Emmanual Macron’s administration plans to implement a digital ID program based on an Android app. While French citizens aren’t required to enroll, the app will be the only digital portal to many government services.How it works: Called Alicem, the app is designed to authenticate the user’s identity for interactions such as filing taxes, applying for pension benefits, and paying utility bills. Alicem starts by capturing video of the user’s face from various angles.Then it compares the video to the user’s passport photo to determine whether they depict the same individual.The app will delete the video selfie once it has completed enrollment, according to France’s Ministry of Interior. Behind the news: France isn’t the first government to use face recognition in this way. Singapore also offers access to government services via face print.Yes, but: Emilie Seruga-Cau, who heads France’s privacy regulator, says Alicem’s authentication scheme violates the European Union’s General Data Protection Regulation by failing to offer an alternative to face recognition. A privacy group called La Quadrature du Net has sued the government over the issue. France’s Interior Ministry has shown no sign of bowing to such concerns.We’re thinking: Any democratic government aiming to use face recognition for identification must protect its citizens on two fronts. Laws must restrict use of the data to its intended purpose, and due care must be taken to secure the data against hacks.", "image_caption": "Alicem website home", "metadata": {"article_id": "issue_8", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Alicem.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-8/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_8.html"}}
{"id": 31315514004, "type": "news_chunk", "title": "Tesla Acquires DeepScale, France Backs Face Recognition, Robots", "subtitle": "A Robot in Every Kitchen", "content": "Every home is different. That makes it difficult for domestic robots to translate skills learned in one household — say, fetching a soda from the fridge — into another. Training in virtual reality, where the robot has access to rich information about three-dimensional objects and spaces, can make it easier for robots to generalize skills to the real world.What’s new: Toyota Research Institute built a household robot that users can train using a virtual reality interface. The robot learns a new behavior based on a single instance of VR guidance. Then it responds to voice commands to carry out the behavior in a variety of real-world environments.How it works: Toyota’s robot is pieced together from off-the-shelf parts, including two cameras provide stereoscopic vision. Classical robotics software controls the machine, while convolutional neural networks learn unique embeddings. To teach the robot new tasks, a user wears a VR headset to see through its eyes and drive it via handheld paddles.During training, the system maps each pixel to a wealth of information including object class, a vector pointing to the object’s center, and other features invariant to view and lighting.When the robot carries out a learned action in the real world, it establishes a pixel correspondence between its training and the present scene, and adjusts its behavior accordingly. Results: The Toyota researchers trained the bot in the virtual environment on three tasks: retrieving a bottle from a refrigerator, removing a cup from a dishwasher, and moving multiple objects to different locations. Then they had the robot perform each task 10 times in two physical homes. They ran the experiments with slight alterations, for instance asking the robot to retrieve a bottle from a higher shelf than the virtual one it was trained on, or doing so with the lights turned off. The robot achieved an 85 percent success rate — though it took an average 20 times longer than a human would.Why it matters: Researchers have given a lot of attention lately to the use of reinforcement learning on robots that are both trained and tested in a simulated environment. Getting such systems to generalize from a simulation to the real world is an important step toward making them useful.We’re thinking: Birth rates have been slowing for decades in Japan, China, the U.S., and much of Europe. The World Health Organization estimates that 22 percent of the world’s population will be over 60 years old by 2050. Who will care for the elderly? Robots may be part of the answer.", "image_caption": "Robot being trained using a virtual reality interface", "metadata": {"article_id": "issue_8", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Toyota20Robot20Resized.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-8/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_8.html"}}
{"id": 31315514005, "type": "news_chunk", "title": "Tesla Acquires DeepScale, France Backs Face Recognition, Robots", "subtitle": "Deep Learning Tackles Skin Ailments", "content": "Skin conditions are the fourth-largest cause of nonfatal disease worldwide, but access to dermatological care is sparse. A new study shows that a neural network can do front-line diagnostic work.What’s new: Researchers at Google Health, UCSF, MIT, and the Medical University of Graz trained a model to examine patient records and predict the likelihood of 26 common skin diseases. The researchers believe that their system could improve the diagnostic performance of primary-care centers for skin disease.Key insight: The system is designed to mimic the typical diagnostic process in a teledermatology setting. It accepts a patient’s medical history and up to six images, and returns a differential diagnosis, or a ranked list of likely diagnoses.How it works: Yuan Liu and her colleagues collected anonymized patient histories and images from a dermatology service serving 17 sites across two U.S. states. They trained on data collected a few years ago, and they tested on data generated more recently to approximate real-world conditions. The system includes: A separate Inception-v4 convolutional neural network for each patient image, all of which used the same weights.A module that converts patient metadata into a consistent format via predefined rules. A fully connected layer combines images and metadata to predict the probability of a particular disease or an “other” class. Results: The model classified diseases more accurately than primary care physicians and nurse practitioners. Allowed three guesses, it was more accurate than dermatologists by 10 percent. The system proved robust to skin color and type and its performance remained consistent across variations.Why it matters: Most previous models consider only a single image and classify a single disease. Inspired by current medical practices, this model uses a variable number of input images, makes use of non-visual patient information as well, and classifies a variety of conditions. The research also shows how to establish model robustness by comparing performance across characteristics like skin color, age, and sex.Yes, but: This study drew data from a limited geographic area. It remains to be seen whether the results generalize to other regions or whether such systems need to be trained or fine-tuned to account for specific geographic areas.We’re thinking: Computer vision has been making great progress in dermatology. Still, there are many difficult steps between encouraging results and deployment in clinical settings.", "image_caption": "Overview of the development and validation of our deep learning system (DLS)", "metadata": {"article_id": "issue_8", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Skin20Resized.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-8/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_8.html"}}
{"id": 31315514006, "type": "news_chunk", "title": "Tesla Acquires DeepScale, France Backs Face Recognition, Robots", "subtitle": "AI Startups in Demand", "content": "AI startups are being scooped up at an accelerating pace, many by companies outside the tech sphere.What’s new: A report by CB Insights shows that, as of August, 2019 was on track to surpass last year’s record number of AI startup acquisitions. The annual tally has grown an average of 38 percent every year since 2010.Who’s buying: While tech giants buy more startups on average, non-tech companies account for the overwhelming majority of purchases. Apple has the biggest portfolio, having acquired 20 AI startups since 2010, including the companies behind popular features like Siri and FaceID.Amazon, Facebook, Google, Microsoft, and Intel are the other notable customers, each having acquired at least seven companies working on computer vision, natural language processing, speech recognition, and the like.Most acquisitions by far have been one-off purchases by incumbents outside tech. For instance, John Deere, McDonalds, and Nike snatched up companies that help do things like harvest crops, develop customer relationships, and manage inventory. What they’re paying: Seven AI acquisitions topped a billion dollars. The most recent happened in April, when pharma giant Roche Holdings closed its $1.9 billion purchase of cancer analytics provider Flatiron Health. The report doesn’t provide annual spending totals.Why it matters: The report makes a strong case that AI’s strategic value is rising steadily throughout the economy. AI is still a tech-giant specialty, but it’s becoming essential in industries well beyond the internet and software.We’re thinking: Exciting startups attract talent, and their work leads to acquisitions that supercharge innovation with bigger budgets and wider reach, drawing still more people into the field. The latest numbers show that this virtuous cycle has staying power — enough, perhaps, to overcome the ongoing shortage of machine learning engineers.", "image_caption": "Chart with number of AI startup acquisitions from 2010 to 2019", "metadata": {"article_id": "issue_8", "chunk_index": 6, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/CB20Insights20Graph20Resized.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-8/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_8.html"}}
{"id": 82079209001, "type": "news_chunk", "title": "Face Datasets Under Fire, Baking With AI, Human Disabilities", "subtitle": "Cutting Corners to Recognize Faces", "content": "Dear friends, AI-enabled automation is often portrayed as a binary on-or-off: A process is either automated or not. But in practice, automation is a spectrum, and AI teams have to choose where on this spectrum to operate. It’s important to weigh the social impact of our work, and we must ameliorate automation’s impact on jobs. In addition to this important consideration, the best choice often depends on the application and what AI can and cannot do.Take the problem of diagnosing medical patients from X-rays. The deployment options include: Human only: No AI involved.Shadow mode: A human doctor reads an X-ray and decides on a diagnosis, but an AI system shadows the doctor with its own attempt. The system’s output doesn’t create value for doctors or patients directly, but it is saved for analysis to help a machine learning team evaluate the AI’s performance before dialing it up to the next level of automation.AI assistance: A human doctor is responsible for the diagnosis, but the AI system may supply suggestions. For example, it can highlight areas of an X-ray for the doctor to focus on.Partial automation: An AI system looks at an X-ray image and, if it has high confidence in its decision, renders a diagnosis. In cases where it’s not confident, it asks a human to make the decision.Full automation: AI makes the diagnosis. These options can apply to medical diagnosis, visual inspection, autonomous navigation, media content moderation, and many other tasks. In many cases, I’ve found that picking the right one is critical for a successful deployment, and that using either too much or too little automation can have a significant negative impact.When you’re choosing a point along the automation spectrum, it’s worth considering what degree of automation is possible given the AI system’s accuracy, availability of humans to assist with the task, and desired rate of decision making (for example, human-in-the-loop options won’t work if you need to select an ad to place on a webpage within 100 milliseconds). Today’s algorithms are good enough only for certain points on the spectrum in a given application. As an AI team gains experience and collects data, it might gradually move to higher levels of automation within ethical and legal boundaries.Some people say that we should focus on IA (intelligence augmentation) rather than AI — that AI should be used to help humans perform tasks rather than automate those tasks. I believe we should try to create value for society overall. Automation can transform and create jobs (as when taxi cabs created new opportunities for cab drivers) as well as destroy them. Even as we pick a point on this spectrum, let’s take others’ livelihoods into account and create value that is widely and fairly shared. Keep learning! Datasets for training face recognition models have ballooned in size — while slipping in quality and respect for privacy. What’s new: In a survey of 130 datasets compiled over the last four decades, Mozilla fellow Inioluwa Deborah Raji and AI consultant Genevieve Fried traced how the need for increasing quantities of data led researchers to relax their standards. The result: datasets riddled with blurred photos, biased labels, and images of minors, collected and used without permission, the authors told MIT Technology Review. What they found: The study divides the history of face datasets into four periods. Starting in 1964, face images were captured in photo shoots using paid models and controlled lighting. Gathering these datasets was expensive and time-consuming; the biggest comprised 7,900 images.The U.S. Department of Defense kicked off the second period in 1996 by spending $6.5 million to develop FERET, which contained 14,126 images of 1,200 individuals. Like most other datasets of this era, it was compiled from photo shoots with consenting subjects. Models trained on these datasets faltered in the real world partly due to their relatively homogenous lighting and poses.Released in 2007, Labeled Faces in the Wild was the first face dataset scraped from the web. LFW’s 13,000 images included varied lighting conditions, poses, and facial expressions. Other large datasets were gathered from Google, Flickr, and Yahoo as well as mugshots and surveillance footage.In 2014, Facebook introduced DeepFace, the first face recognition model that used deep learning, which identified people with unprecedented accuracy. Researchers collected tens of millions of images to take advantage of this data-intensive approach. Obtaining consent for every example became impossible, as did ensuring that each one’s label was accurate and unbiased. Why it matters: People deserve to be treated fairly and respectfully by algorithms as well as other people. Moreover, datasets assembled without due attention to permission and data quality erode the public’s trust in machine learning. Companies like Clearview.ai and FindFace stand accused of harvesting online images without consent and using them in ways that violate individuals’ privacy, while shaky algorithms have contributed to biased policing. In the U.K., Canada, and certain U.S. jurisdictions, lawmakers and lawsuits are calling for restrictions on the use of face images without consent. We’re thinking: Andrew and his teams have worked on many face recognition systems over the years. Our practices have evolved — and continue to do so — as both society and AI practitioners have come to recognize the importance of privacy. As we gather data, we must also work toward fairer and more respectful standards governing its collection, documentation, and use. Fun fact: Andrew’s face appears (with permission!) in a Carnegie Mellon University face dataset collected by Tom Mitchell in 1996. Here’s what Andrew looked like in those days.", "image_caption": "Dozens of different faces shown in a series of images", "metadata": {"article_id": "issue_80", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-80/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_80.html"}}
{"id": 82079209002, "type": "news_chunk", "title": "Face Datasets Under Fire, Baking With AI, Human Disabilities", "subtitle": "Bigger, Faster Transformers", "content": "Performance in language tasks rises with the size of the model — yet, as a model’s parameter count rises, so does the time it takes to render output. New work pumps up the number of parameters without slowing down the network.What’s new: William Fedus, Barret Zoph, and Noam Shazeer at Google Brain developed the Switch Transformer, a large-scale architecture (the authors built a version comprising 1.6 trillion parameters) that’s nearly as fast as a much smaller model.Key insight: The approach known as mixture-of-experts uses only a subset of a model’s parameters per input example. Like mixture-of-experts, Switch Transformer chooses which of many layers would best process a given input.How it works: The authors trained Switch Transformer to predict words that had been removed at random from a large text dataset scraped from the web. The dataset was preprocessed to remove offensive language, placeholder text, and other issues. A typical transformer extracts a representation from each input token, such as a word, and then uses self-attention to compare the representations before passing them to a fully connected layer. Switch Transformer replaces the fully connected layer with one of a number (determined by a hyperparameter) of fully connected layers.A softmax layer calculates the probability that any particular fully connected layer is best for processing a given token. Then it uses the chosen layer in the usual manner.The fully connected layers process tokens in parallel. The authors added a loss to encourage them to be equally active. On a hardware chip, a separate processor core handles each layer, so this loss encourages equal distribution of the load on each core. Results: The authors compared Switch Transformer (7.4 billion parameters) to T5 (223 million parameters), a variant similar to the original transformer that was trained on the same dataset, using negative log perplexity, a measure of the model’s uncertainty (higher is better). The new model achieved -1.561 negative log perplexity compared to T5’s -1.731. Switch Transformer ran at two-thirds the speed of T5 — it executed 1,000 predictions per second compared to T5’s 1,600 — with 33 times the number of parameters. It beat a mixture-of-experts transformer, presumably of roughly the same size, on both counts.Why it matters: In deep learning, bigger is better — but so is a manageable computation budget.We’re thinking: Transformers come in an increasing variety of flavors. We hope this summary helps you remember which is switch.", "image_caption": "Different graphs showing switch transformer data", "metadata": {"article_id": "issue_80", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-1-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-80/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_80.html"}}
{"id": 82079209003, "type": "news_chunk", "title": "Face Datasets Under Fire, Baking With AI, Human Disabilities", "subtitle": "Human Disabilities Baffle Algorithms", "content": "Facebook’s content moderation algorithms block many advertisements aimed at disabled people. What’s new: The social media platform’s automated systems regularly reject ads for clothing designed for people with physical disabilities. The algorithms have misread such messages as pornography or sales pitches for medical devices, The New York Times reported. How it works: Automated systems at Facebook and Instagram examines the images and words in ads that users try to place on the sites. They turn down ads they deem to violate their terms of service. The system tells would-be ad buyers when it rejects their messages, but not why, making it difficult for advertisers to bring rejected materials into compliance. Companies can appeal rejections, but appeals often are reviewed by another AI system, creating a frustrating loop. Facebook disallowed an ad for a sweatshirt from Mighty Well bearing the words “I am immunocompromised — please give me space.” The social network’s algorithm had flagged it as a medical product. Mighty Well successfully appealed the decision.Facebook and Instagram rejected ads from Slick Chicks, which makes underwear that clasps on the side as a convenience for wheelchair users, saying the ads contained “adult content.” Slick Chicks’ founder appealed the decision in dozens of emails and launched an online petition before Facebook lifted the ban.The social-networking giant routinely rejects ads from Yarrow, which sells pants specially fitted for people in wheelchairs. Facebook doesn’t allow ads for medical equipment, and apparently the algorithm concluded that the ads were for wheelchairs. Yarrow has successfully appealed the rejections, which takes an average of 10 days each.Patty + Ricky, a marketplace that sells clothing for people with disabilities, has appealed Facebook’s rejection of 200 adaptive fashion products. Behind the news: Other social media platforms have been tripped up by well-intentioned efforts to control harmful speech. YouTube blocked a popular chess channel for promoting harmful and dangerous content. Apparently, its algorithm objected to words like “black,” “white,” “attack,” and “threat” in descriptions of chess matches.In 2019, TikTok admitted to suppressing videos made by users who were disabled, queer, or overweight, purportedly an effort to discourage bullying. Why it matters: Millions of small businesses advertise on Facebook and Instagram, many of which serve niche communities. For such companies, being barred from promoting their wares on these platforms is a major blow. We’re thinking: Moderating content on platforms as big as Facebook would be impossible without AI. But these cases illustrate how far automated systems are from being able to handle the job by themselves. Humans in the loop are still required to mediate between online platforms and their users.", "image_caption": "Person in wheelchair, person in side profile, person wearing a hoodie", "metadata": {"article_id": "issue_80", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-80/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_80.html"}}
{"id": 82079209004, "type": "news_chunk", "title": "Face Datasets Under Fire, Baking With AI, Human Disabilities", "subtitle": "Cake + Cookie = Cakie", "content": "AI may help revolutionize the human diet – or dessert, at least. What’s new: Google applied AI engineer Dale Markowitz and developer advocate Sara Robinson trained a model to predict whether a recipe is a bread, cake, or cookie. They brightened the recent holiday season by using it to develop novel hybrid recipes. How it works: The engineers conceived their project to demonstrate Google’s AutoML, a software suite for easy-bake machine learning. They compiled and labeled roughly 600 recipes for breads, cakes, or cookies and limited ingredient lists to 16 essentials, like eggs, flour, and milk.They trained a model to classify recipes as bread, cake, or cookies with high accuracy.For each recipe, AutoML’s explainability feature assigned the ingredients a percentage that described their importance to the classification. Butter, sugar, and yeast were most important overall.The authors adjusted ingredient ratios until they developed a recipe that the model classified as equal parts cookie and cake, and another that was equal parts bread and cookie. They baked and tasted these creations: a cakie (“nasty”) and a breakie (“pretty good”). Behind the news: Machine learning’s culinary education is coming along, though some of its creations are tastier than others. Sony AI’s Flagship Gastronomy Project recently launched an app to help professional chefs develop new recipes, food pairings, and menus.In 2019, Facebook trained a vision model to recognize different types of food and generate recipes to produce them.In 2016, IBM debuted Chef Watson, an app trained partly on Bon Appétit’s recipe archive, which generates recipes based on ingredients specified by users.Blogger Janelle Shane prompted GPT-2 to generate recipes for dish names that were themselves generated by AI, producing gustatory horrors like Chocolate Chicken Chicken Cake. Why it matters: Experimenting with new recipes isn’t just fun for home cooks. Commercial test kitchens are on the lookout for novel flavors, textures, and dishes. AI could help chefs invent smorgasbords of culinary delights. We’re thinking: These AI-powered recipes may seem half-baked, but suddenly we have a craving for Chocolate Chicken Chicken Cake.", "image_caption": "Model predicting ingredients in a recipe and woman cooking", "metadata": {"article_id": "issue_80", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-3.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-80/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_80.html"}}
{"id": 82079209005, "type": "news_chunk", "title": "Face Datasets Under Fire, Baking With AI, Human Disabilities", "subtitle": "Sharper Eyes For Vision+Language", "content": "Models that interpret the interplay of words and images tend to be trained on richer bodies of text than images. Recent research worked toward giving such models a more balanced knowledge of the two domains.What’s new: Pengchuan Zhang and Xiujun Li led a team at Microsoft and University of Washington raised the bar in several vision-and-language tasks. They call their system Oscar+, building on earlier work that used class names of objects in an image to improve matching of image and text representations.Key insight: Recent progress in vision-and-language models has come mostly by combining learned image and text representations more effectively rather than improving the representations themselves, the authors observed. Honing these representations through additional pretraining ought to boost their performance.How it works: The authors started with pretrained representations for images and text generated by separate models for vision (ResNeXt-152 C4 pretrained on ImageNet-5k) and language (pretrained BERT). They honed the image representations by further pretraining the vision model on new data. Then they generated image-and-text representations as they pretrained Oscar+ as a whole. Finally, they fine-tuned the system on specific vision-and-language tasks. In the additional pretraining step, the authors pretrained the ResNeXt-152 C4 to detect 1,848 objects or attributes (such as labels describing colors or textures) in 2.49 million images in four object detection datasets.A transformer fused image and text representations as the authors pretrained Oscar+ on 8.85 million examples from four image caption datasets with generated image tags, image tag datasets with generated captions, and visual question-and-answer datasets. At this stage, the system optimized two loss terms. One term encouraged the system to predict randomly hidden words in a caption or an image’s tags. The other term encouraged the system to match an image and its tags, or an answer with its question and its image.They fine-tuned the system to perform seven specific tasks. Results: Oscar+ achieved state-of-the-art results in all seven tasks, from matching images with captions (and vice-versa) to determining the truth of a statement about two images. The system boosted NoCaps accuracy (captioning images that contain objects not seen in training) to 92.5 percent from 86.6 percent — its biggest gain. To show that performance was substantially improved by separately pretraining the object detector on additional data, the authors compared performance with and without that step. That step boosted visual question-answering accuracy, for instance, to 74.90 percent from 71.34 percent.Why it matters: Performance in multimodal tasks can improve with additional learning in just one of the modes involved.We’re thinking: If Oscar is a grouch, is Oscar+ nicer — or even more grumpy?", "image_caption": "System Oscar+ working", "metadata": {"article_id": "issue_80", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-4.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-80/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_80.html"}}
{"id": 41720944001, "type": "news_chunk", "title": "Google Overhauls Ethics Team, Covid-19 Triage, Facebook Glasses", "subtitle": "Google Overhauls Ethical AI Team", "content": "Dear friends, One of the most important skills of an AI architect is the ability to identify ideas that are worth working on. Over the years, I’ve had fun applying machine learning to manufacturing, healthcare, climate change, agriculture, ecommerce, advertising, and other industries. How can someone who’s not an expert in all these sectors find meaningful projects within them? Here are five steps to help you scope projects effectively.Step 1: Identify a business problem (not an AI problem). I like to find a domain expert and ask, “What are the top three things that you wish worked better? Why aren’t they working yet?” For example, if you want to apply AI to climate change, you might discover that power-grid operators can’t accurately predict how much power intermittent sources like wind and solar might generate in the future.Step 2: Brainstorm AI solutions. When I was younger, I used to execute on the first idea I was excited about. Sometimes this worked out okay, but sometimes I ended up missing an even better idea that might not have taken any more effort to build. Once you understand a problem, you can brainstorm potential solutions more efficiently. For instance, to predict power generation from intermittent sources, we might consider using satellite imagery to map the locations of wind turbines more accurately, using satellite imagery to estimate the height and generation capacity of wind turbines, or using weather data to betterpredict cloud cover and thus solar irradiance. Sometimes there isn’t a good AI solution, and that’s okay too.Step 3: Assess the feasibility and value of potential solutions. You can determine whether an approach is technically feasible by looking at published work, what competitors have done, or perhaps building a quick proof of concept implementation. You can determine its value by consulting with domain experts (say, power-grid operators, who can advise on the utility of the potential solutions mentioned above). Step 4: Determine milestones. Once you’ve deemed a project sufficiently valuable, the next step is to determine the metrics to aim for. This includes both machine learning metrics such as accuracy and business metrics such as revenue. Machine learning teams are often most comfortable with metrics that a learning algorithm can optimize. But we may need to to stretch outside our comfort zone to come up with business metrics such as those related to user engagement, revenue, and so on. Unfortunately, not every business problem can be reduced to a matter of optimizing test set accuracy! If you aren’t able to determine reasonable milestones, it may be a sign that you need to learn more about the problem. A quick proof of concept can help supply the missing perspective. Step 5: Budget for resources. Think through everything you’ll need to get the project done including data, personnel, time, and any integrations or support you may need from other teams. For example, if you need funds to purchase satellite imagery, make sure that’s in the budget.This is an iterative process. If, at any step, you find that the current direction is infeasible, return to an earlier step and proceed with your new understanding. Is there a domain that excites you where AI might make a difference? I hope these steps will guide you in exploring it — even if you don’t yet have deep expertise in that field. AI won’t solve every problem, but as a community, let’s look for ways to make a positive impact wherever we can. Keep learning! Having dismissed two key researchers, Google restructured its efforts in AI ethics. What’s new: Marian Croak, an accomplished software engineer and vice president of engineering at Google, will lead a new center of expertise in responsible AI, the company announced. The move came amid uproar over the exits of her predecessors Timnit Gebru and Margaret Mitchell. What happened: Google’s Ethical AI group has been in flux since last December when Gebru, the group’s technical co-lead with Mitchell, left the company. Gebru says she was fired, while Google’s latest communiqué refers to Gebru’s “exit.” In December, members of Ethical AI demanded that Google CEO Sundar Pichai reinstate Gebru and make other changes. More than 2,600 Google employees signed a letter expressing solidarity with the ethics researcher.Google put Croak in charge of the newly established Responsible AI Research and Engineering Center of Expertise, which will oversee Ethical AI and coordinate research into fairness and bias among 10 Google teams.One day after announcing the new organization, Google dismissed Margaret Mitchell. She had been under investigation internally for allegedly copying documents related to Gebru’s departure to a personal computer. Mitchell’s termination triggered another wave of employee outrage. Behind the news: In December, Gebru was on the verge of publishing a paper that criticized large language models including Google’s own BERT. Executives asked her to either retract the paper or remove the names of all Google co-authors. Gebru responded with a request that Google take certain actions as a condition of her employment. Her managers interpreted this as an ultimatum and told her they had accepted her resignation. Gebru has said that she did not offer to resign.On Friday, Dean apologized in an email to the staff for Google’s handling of Gebru’s exit and announced new policies prompted by an internal review. Why it matters: Google is a leader in AI and one of the most powerful companies in the world. Its approach to ethical challenges — and its treatment of employees — are highly influential throughout the tech industry. We’re thinking: Under Gebru and Mitchell, Google’s Ethical AI team developed tools to improve model transparency, examined how social constructs of race manifest in AI, and released a framework for identifying risks posed by models in development. We hope the people who carry on this work will pursue similarly ambitious projects.", "image_caption": "Margaret Mitchell, Marian Croak and Timnit Gebru pictured", "metadata": {"article_id": "issue_81", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/TIMNIT.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-81/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_81.html"}}
{"id": 41720944002, "type": "news_chunk", "title": "Google Overhauls Ethics Team, Covid-19 Triage, Facebook Glasses", "subtitle": "Covid-19 Triage", "content": "The pandemic has pushed hospitals to their limits. A new machine learning system could help doctors make sure the most severe cases get timely, appropriate care.What’s new: Anuroop Sriram, Matthew Muckley, and colleagues at Facebook, NYU School of Medicine, and NYU Abu Dhabi developed a system that examines X-ray images to predict which Covid-19 patients are at greatest risk of decline.Key insight:Previous methods assess Covid risk based on a single chest X-ray. But when making assessments, clinicians often look for relative changes between successive X-rays to determine whether a patient’s condition is improving or deteriorating. The researchers used consecutive X-rays to improve risk assessment.How it works: The authors trained their system to predict the probability that a patient would die, require intubation, need intensive care, or need more oxygen over the next 24, 48, 72, or 96 hours. The authors augmented two datasets that comprise chest X-rays of Covid patients via cropping, flipping, or random noise. Using the augmented data, they pretrained two DenseNet-121 encoders using a contrastive loss function. The contrastive loss encouraged the models to produce similar representations if the two images had the same parent X-ray and dissimilar representations otherwise.They fine-tuned the system on NYU Covid, a dataset that contains sequences of chest X-rays labelled with the patients’ outcomes.After pretraining, the first encoder generated a representation of each X-ray in a sequence. A transformer processed these representations. The researchers summed its output into a single vector.A linear classifier used this vector to make the final prediction. Results: The system achieved a mean AUC (area under the curve, a measure of true versus false positives where 1 is a perfect score) of 0.785, 0.801, 0.790, and 0.790 when predicting adverse outcomes at 24, 48, 72, and 96 hours into the future, respectively. Those scores were comparable to those of two clinicians who achieved an average AUC of 0.784, 0.787, 0.761 and 0.754.Why it matters: Pretraining followed by fine-tuning opens up important applications where data is too scarce for simpler learning approaches.We’re thinking: The pandemic has been an early test of AI’s utility in medicine. The record so far has been mixed, but we’re glad to see research that shows promising results for both fighting Covid and improving healthcare in general.", "image_caption": "Graph showing system that examines X-ray images to predict which Covid-19 patients are at greatest risk of decline", "metadata": {"article_id": "issue_81", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/COVID.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-81/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_81.html"}}
{"id": 41720944003, "type": "news_chunk", "title": "Google Overhauls Ethics Team, Covid-19 Triage, Facebook Glasses", "subtitle": "ID By Eyeglasses?", "content": "Smart glasses in the works at Facebook may be equipped with face recognition. What’s new: The social media colossus plans to market augmented-reality headgear, and it’s considering a feature that would overlay a person’s name on their face, according to Buzzfeed. What’s happening: Announced by Mark Zuckerberg in 2017 (shown in the video clip above), the glasses are set to drop this year. Some capabilities — including face recognition — may be added later, Facebook vice president Andrew Bosworth told Bloomberg News. Bosworth said the technology could remind users of the names of people whom they had met previously. Similarly, it could help people with face blindness, a neurological condition that makes it hard to recognize familiar faces.Facebook is assessing the legal ramifications, since this capability may not be lawful everywhere. For instance, an Illinois law against collecting biometric data might bar the product in that state.Where local laws don’t pose a barrier, the company may formulate its own rules factoring in the potential for harm, said Facebook diversity officer Maxine Williams. Behind the news: Facebook’s smart glasses, which will be manufactured by Ray-Ban, will compete against Snapchat Spectacles and Google Glass (lately refocused from consumer to enterprise applications).Why it matters: Wearable hardware that recognizes faces raises serious questions about privacy. Facebook has an incentive to tread carefully: It was the least trusted of nine major social media platforms in a recent survey.We’re thinking: A Facebook foray into mass-market face recognition could force U.S. lawmakers finally to issue rules on how the technology can and can’t be used.", "image_caption": "Mark Zuckerberg talking about Facebook's smart glasses", "metadata": {"article_id": "issue_81", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/ezgif.com-optimize-10.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-81/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_81.html"}}
{"id": 41720944004, "type": "news_chunk", "title": "Google Overhauls Ethics Team, Covid-19 Triage, Facebook Glasses", "subtitle": "Transformer Variants Head to Head", "content": "The transformer architecture has inspired a plethora of variations. Yet researchers have used a patchwork of metrics to evaluate their performance, making them hard to compare. New work aims to level the playing field. What’s new: Yi Tay and colleagues at Google developed Long-Range Arena, a suite of benchmarks designed to standardize comparisons between transformers. The term long-range refers to transformers’ ability to capture dependencies between tokens in an input sequence that are far apart. Key insight: The power of the original transformer lies in its ability to track relationships between tokens anywhere in an input sequence. But that power comes at a cost: The model slows down and its memory requirement rises dramatically as the length of its input increases. Many variants were created to alleviate this issue. These models cry out for tests that challenge their ability over long sequences. How it works: The suite comprises six tests designed to probe different aspects of transformer behavior. Long ListOps requires a model to calculate the numeric output from a long list of ordered operations; for instance, to determine that [MAX 4 3 [MIN 2 3 ] 1 0 [MEDIAN 1 5 8 9, 2]] equals 5. It investigates how well a model can parse long sequences.Character-Level Text Classification is a binary sentiment classification task based on movie reviews in the IMDb dataset. Its objective is to test a model’s accuracy when processing documents up to 4,000 characters long.Character-Level Document Retrieval evaluates similarity between two documents using the ACL Anthology Network dataset, which identifies when one paper cites another. It assesses a model’s ability to compress text inputs for comparison.Image Classification on Sequences of Pixels classifies objects in CIFAR-10 images that have been flattened into a one-dimensional sequence. This tests how well a model learns spatial relationships between pixels.Pathfinder and Pathfinder-X require a model to decide whether two circles are connected by a path that consists of dashes in a generated image. Pathfinder-X increases the difficulty by making the image area 16 times larger. Both tasks test how well a model learns long-range spatial relationships. Pathfinder-X also gauges how the results change if the sequence length is much longer. Results: The authors tested 10 transformers. While some shined in one task or another, none was the clear front runner. Big Bird achieved the highest average accuracy – 55.01 percent across five tasks — but it didn’t achieve the top score in any single task. Performer, dominated character-level text classification, performing 5.7 times faster than a vanilla transformer. Linformer used the least memory, 9.58 times less than the vanilla transformer. All models failed Pathfinder-X: Their ability to classify the image was no better than random chance, showing that longer input sequences inhibited performance. Why it matters: Standardized comparisons not only help application developers choose the right model for their needs, they also provide a deeper understanding of a model’s performance characteristics and spur researchers to improve the state of the art. We’re thinking: You can get involved, too. The team open sourced its work and encourages others to contribute to the Long-Range Arena leaderboard.", "image_caption": "Graph showing information about different transformer models", "metadata": {"article_id": "issue_81", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/BENCHMARK.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-81/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_81.html"}}
{"id": 41720944005, "type": "news_chunk", "title": "Google Overhauls Ethics Team, Covid-19 Triage, Facebook Glasses", "subtitle": "Data Science Is Full of Newbies", "content": "Machine learning is spreading from big corporations to smaller companies, and many of its practitioners are relatively new to the technology. What’s new: Almost one in five data scientists active on Kaggle, which hosts machine learning competitions, have been in the field less than one year, according to the company’s latest State of Data Science and Machine Learning survey. The report covers demographics and employment as well as popular platforms, frameworks, applications, and techniques. The data includes answers from 200,000 people. What they found: The report tallies responses by 2,675 Kaggle users who identified themselves as employed data scientists. A majority of the respondents had less than three years of machine learning experience, and 18 percent had been in the field less than one year.37 percent worked at businesses with fewer than 50 employees, a 7 percent rise over last year’s survey. 51 percent worked on teams of fewer than five people.81.9 percent of respondents identifed as male.Most respondents were in India, making up 21.8 percent of the total. 14.5 percent were in the U.S., and 4.6 percent in Brazil.The U.S. is by far the most lucrative place to be a data scientist, as 73 percent of U.S. respondents said they made over $100,000. In India, the median salary range was between $7,500 and $10,000. Behind the news: Users of the employment site Glassdoor consistently rank data scientist as one of America’s best jobs, citing good pay and working conditions. But just because workers are happy doesn’t mean they’re sitting still. About a third of the engineers who responded to Anaconda’s 2020 State of Data Science survey said they plan to look for a new job in the coming year. The expected turnover is highest in IT, where 44 percent of data scientists either are actively looking for new employment or plan to do so soon.Why it matters: This survey underlines how data science is diffusing, not only among businesses but among nations. It highlights trends that hiring managers, among others, should bear in mind, including the field’s ongoing gender imbalance.We’re thinking: All those newcomers to data science represent a huge pool of fresh ideas and new talent coming in to the field.", "image_caption": "Different graphs showing information about data scientists", "metadata": {"article_id": "issue_81", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/03/ezgif.com-optimize-6.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-81/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_81.html"}}
{"id": 25366456001, "type": "news_chunk", "title": "Facebook's Algorithms Under Fire, Voice Clones Invade...", "subtitle": "Social Engagement vs. Social Good", "content": "Dear friends, Over the past weekend, I happened to walk by a homeless encampment and went over to speak with some of the individuals there. I spoke with a homeless man who seemed to be partially speaking with me, and partially speaking with other people that I could not see. I also spoke with a woman who said she fled her abusive home at the age of 21, and wished that she had a tent — like some of the others — so she could sleep with something over her head rather than be exposed to the elements at night. I feel grateful and privileged every day to have enough food, to have a place to live, and to even have a modern computer with internet access. I’m going to come out and say this (knowing some people will disagree): Every one of us has an obligation to serve others. While we can try to help a handful of people at a time with a meal or a donation — and this is to be celebrated — I don’t know how to systematically help the large and growing number of homeless. But I will keep thinking on this, and am determined to find a way. Even as we build amazing products and technologies, let’s keep thinking about how we can scalably serve the many wonderful, resilient individuals like the ones I met last weekend. Keep learning! Facebook’s management obstructed the architect of its recommendation algorithms from mitigating their negative social impact, MIT Technology Review reported.What’s new: The social network focused on reining in algorithmic bias against particular groups of users at the expense of efforts to reduce disinformation and hate speech, according to an in-depth profile of Joaquin Quiñonero Candela, who designed Facebook’s early recommenders and now leads its Responsible AI team. The story: The article traces Quiñonero’s effort to balance the team’s mission to build trustworthy technology with management’s overriding priorities: boosting user engagement and avoiding accusations that it favored one political faction over another. Quiñonero joined Facebook in 2012 to lead an effort to build models that matched advertisements with receptive users. That effort successfully boosted user engagement with ads, so he designed similar systems to fill news feeds with highly engaging posts, comments, and groups.His team went on to build a machine learning development platform, FBLearner Flow, that was instrumental to helping Facebook scale up its AI efforts. It enabled the company to build, deploy, and monitor over a million models that optimize engagement through tasks like image recognition and content moderation, inadvertently amplifying disinformation and hate speech.In 2018, Quiñonero took charge of Responsible AI to investigate and resolve such issues. The team developed models that attenuated the flow of disinformation and hate speech, but they diminished engagement, and management redirected and disincentivized that work.Facebook’s leadership, under pressure from critics who charged that the network favored left-wing over right-wing political views, directed the team to focus on mitigating bias. The new direction diverted attention away from staunching extremist content and toward tools like Fairness Flow, which measures models’ relative accuracy when analyzing data from different user demographics. The response: Facebook denied that it interfered with moves to reduce disinformation and hate speech. It also denied that politics motivated its focus on mitigating bias. Facebook head of AI research Yann LeCun said the article mischaracterized how Facebook ranks content, Quiñonero’s role, and how his group operates.The article made little mention of the company’s efforts to reduce the spread of divisive content, detect hateful memes, and remove hate speech. AI flagged around 95 percent of the hate speech removed from the network between last July and September, according to the company.Facebook publicly supports regulations that would govern social media including rules that would limit the spread of disinformation. Why it matters: Facebook, like many AI companies, is struggling to balance business priorities with its social impact. Teams like Responsible AI are crucial to achieving that balance, and business leaders need to give them authority to set technical priorities and limits.We’re thinking: The powers of AI can put machine learning engineers in the difficult position of mediating between business priorities and ethical imperatives. We urge business leaders to empower employees who try to do the responsible thing rather than throttling their work, even if it negatively impacts the bottom line.", "image_caption": "Facebook like and dislike buttons", "metadata": {"article_id": "issue_83", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/Social-Engagement-vs-Social-Good-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-83/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_83.html"}}
{"id": 25366456002, "type": "news_chunk", "title": "Facebook's Algorithms Under Fire, Voice Clones Invade...", "subtitle": "Old Drugs for New Ailments", "content": "Many medical drugs work by modulating the body’s production of specific proteins. Recent research aimed to predict this activity, enabling researchers to identify drugs that might counteract the effects of Covid-19.What’s new: Thai-Hoang Pham and colleagues at The Ohio State University and The City University of New York developed DeepCE, a system designed to predict how particular drugs will influence the amounts of RNA, and therefore the amounts of various proteins, produced by a cell.Key insight: In machine learning, attention layers learn to represent how the various parts of two input sequences interact with one another. In biology, genes mediate the production of RNA, while drugs can affect the action of genes. Given separate embeddings that represent genes and chemical structures of drugs, attention can capture how a drug affects RNA production. How it works: Given a drug, a dose, and a line of cells cloned from a particular patient, DeepCE predicts the amount of RNA produced by each of roughly 1,000 genes. (Collectively, this information constitutes a gene expression profile). The training and test data included more than 600 drugs for a total of over 4,000 gene expression profiles from seven human cell lines in the L1000 database. The authors used the node2vec method to generate embeddings of proteins in a database of relationships among genes and proteins. From these embeddings, they extracted representation of the genes in L1000.A chemical can be represented as a graph in which each node stands for an element in the periodic table. The authors used a convolutional graph neural network to generate embeddings of drugs in L1000. The network represented each node of a given compound based on its surrounding nodes.Given the gene and drug embeddings, a multi-headed attention network generated a matrix that represented gene-drug and gene-gene interactions. Given information about drug doses and cell lines in L1000, separate feed-forward networks generated embeddings of these factors.A fully connected network accepted all of these representations and learned how to predict RNA production. Results: The authors compared DeepCE’s predictions with those of several baseline methods using the Pearson correlation coefficient, a measure of the correlation between predictions and ground truth. DeepCE outperformed all of them with a score of 0.4907. The next-best method, a two-layer feed-forward network, scored 0.4270. They also used DeepCE to look for existing drugs that might treat Covid-19. They compared the predictions for more than 11,000 drugs with corresponding profiles of Covid-19 patients, looking for the greatest negative correlations — an indicator that the drug would fight the illness. Of 25 drugs surfaced by DeepCE, at least five already had shown potential as Covid-19 treatments; others had been used for different viruses with similar symptoms. Why it matters: Complex datasets may have features that aren’t processed easily by a single network. By using a different network for each type of input and combining their outputs, machine learning engineers can extract useful information that otherwise might be inaccessible. We’re thinking: The next blockbuster antiviral (or antidepressant, anti-inflammatory, or heart medicine) may already be on pharmacy shelves. Wouldn’t it be wonderful if deep learning found it?", "image_caption": "Data related to DeepCE, a system designed to predict how particular drugs will influence the amounts of RNA", "metadata": {"article_id": "issue_83", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/Old-Drugs-for-New-Ailments-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-83/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_83.html"}}
{"id": 25366456003, "type": "news_chunk", "title": "Facebook's Algorithms Under Fire, Voice Clones Invade...", "subtitle": "Who Audits the Auditors?", "content": "Auditing is a critical technique in the effort to build fair and equitable AI systems. But current auditing methods may not be up to the task.What’s new: There’s no consensus on how AI should be audited, whether audits should be mandatory, and what to do with their results, according to The Markup, a nonprofit investigative reporting outfit. What’s Happening: Auditing firms are doing brisk business analyzing AI systems to determine whether they’re effective and fair. But such audits are often limited in scope, and they may lend legitimacy to models that haven’t been thoroughly vetted. HireVue, a vendor of human resources software, used an independent company to audit one of its hiring tools by interviewing stakeholders about possible problems. But the audit stopped short of evaluating the system’s technical design.An audit of hiring software made by Pymetrics, which evaluates candidates through simple interactive games (illustrated above), did examine its code and found it largely free of social biases. But the audit didn’t address whether or not the software highlighted the best applicants for a given job.AI vendors are under no obligation to have their systems audited or make changes if auditors find problems. Behind the news: In the U.S., members of Congress and the New York City Council have proposed bills that would require companies to audit AI systems. The laws have yet to be passed.Why it matters: AI systems increasingly affect the lives of ordinary people, influencing whether they land a job, get a loan, or go to prison. These systems must be trustworthy — which means the audits that assess them must be trustworthy, too.We’re thinking: Makers of drugs and medical devices must prove their products are effective and safe. Why not makers of AI, when its output can dramatically impact people’s lives? The industry should agree on standards and consider making audits mandatory for systems that affect criminal justice, allocating health care resources, and offering loans.", "image_caption": "Operation of a hiring software which evaluates candidates through simple interactive games", "metadata": {"article_id": "issue_83", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/Who-Audits-the-Auditors-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-83/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_83.html"}}
{"id": 25366456004, "type": "news_chunk", "title": "Facebook's Algorithms Under Fire, Voice Clones Invade...", "subtitle": "Your Words, Their Voices", "content": "Voice clones — the audio counterpart to deepfaked images — are poised to invade popular media and entertainment.What’s new: Professionals and amateurs alike are using AI to emulate the voices of human actors, Wired reported.Cloned like a pro: Game developers and marketers are cloning voices to save money and make their products more immersive. Sonantic, a UK-based startup, claims it can reproduce an actor’s voice from less than 20 minutes of training data. Its technology enables media creators to impart a variety of emotional inflections — such as angry, happy, or fearful — at varying levels of intensity. Sonantic shares revenue generated by voice cloning with the human originals.U.S.-based Replica Studios trains its system by having actors read 20 short sentences that cover the gamut of English phonetics. The company’s modification of the game Cyberpunk 2077 enables non-player characters to address the player by name. Like Sonantic, Replica shares voice-cloning revenue with human speakers.MSCHF, a marketing firm, synthesized the voice of rapper Gucci Mane and put his doppelgänger to work narrating Pride and Prejudice, Don Quixote, and other literary classics. Remixers join in: Much of the entertainment industry is sorting out who owns which rights to an actor’s voice, but some amateur content creators have embraced the technology with abandon. Tim McSmythers, a researcher who goes by the handle Speaking of AI on social media, trained models to mimic the voices of celebrities like Adam Driver, Ellen Degeneres, and Jerry Seinfeld and composite their likenesses into famous movie and TV scenes. Our favorite: Homer Simpson telling Anakin Skywalker the legend of Darth Plagueis the Wise in a clip from Star Wars: The Phantom Menace.15.ai, previously profiled in The Batch, allows users to generate custom dialogue using character voices from My Little Pony, Rick and Morty, and other games and TV shows. (The site is currently on hiatus.) Why it matters: Voice cloning opens new avenues of creativity and productivity. For instance, generated voices can help developers road-test dialogue before bringing in the human talent and expand the conversational role of background characters. Yet the technology also holds potential for abuse, and guarding against them will require new kinds of vigilance.We’re thinking: Have you ever been yelled at? We would love to build a system to transcribe the yeller’s words, and then re-synthesize their voice in a more polite tone.", "image_caption": "Homer Simpson talking to Anakin Skywalker in a clip from Star Wars: The Phantom Menace.", "metadata": {"article_id": "issue_83", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/Your-Words--Their-Voices-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-83/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_83.html"}}
{"id": 25366456005, "type": "news_chunk", "title": "Facebook's Algorithms Under Fire, Voice Clones Invade...", "subtitle": "Good Labels for Cropped Images", "content": "In training an image recognition model, it’s not uncommon to augment the data by cropping original images randomly. But if an image contains several objects, a cropped version may no longer match its label. Researchers developed a way to make sure random crops are labeled properly. What’s new: Led by Sangdoo Yun, a team at Naver AI Lab developed ReLabel, a technique that labels any random crop of any image. They showcased their method on ImageNetKey insight: Earlier work used knowledge distillation: Given a randomly cropped image, a so-called student model learned from labels predicted by a teacher model. That approach requires that the teacher predict a label for each of many cropped versions of a given example. In this work, an image was divided into a grid, and the teacher predicted a label for each grid square, creating a map of regions and their labels that was used to determine a label for any given portion of the image. This way, the teacher could examine each example only once, making the process much more efficient. How it works: The teacher was an EfficientNet-L2 that had been pretrained on Google’s JFT-300M dataset of 300 million images. The student was a ResNet-50. The authors removed the teacher’s final pooling layer, so the network would predict a label for each region in a 15×15 grid instead of one label for the whole image. They used the teacher to predict such a “label map” for every image in ImageNet.The researchers trained the student using random crops of images in ImageNet and their corresponding label maps. Given a cropped image, they used RoIAlign to find the regions within the label map that aligned with the crop and pooled the corresponding regions into a vector. Then they used softmax to turn the vector into the probability distribution that is the label. Results: The researchers compared a ResNet-50 trained on ImageNet using their labels to one trained using the standard labels. The new labels improved test classification accuracy from 77.5 percent to 78.9 percent.Why it matters: Images on social and photo-sharing sites tend to be labeled with tags, but a tag that reads, say, “ox” indicates only that an ox appears somewhere in the image. This approach could enable vision models to take better advantage of data sources like this.We’re thinking: A bounding box around every object of interest would ameliorate the cropping problem — but such labels aren’t always easy to get.", "image_caption": "Graphs and data related to ReLabel, a technique that labels any random crop of any image.", "metadata": {"article_id": "issue_83", "chunk_index": 5, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/Good-Labels-for-Cropped-Images-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-83/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_83.html"}}
{"id": 21690475001, "type": "news_chunk", "title": "Networking License Plate Readers, Calling Out Unreproducible", "subtitle": "Partners in Surveillance", "content": "Dear friends,Earlier today, I spoke at a DeepLearning.AI event about MLOps, a field that aims to make building and deploying machine learning models more systematic. AI system development will move faster if we can shift from being model-centric to being data-centric. You can watch a video of the event here. Unlike traditional software, which is powered by code, AI systems are built using both code (including models and algorithms) and data:AI systems = Code + DataWhen a system isn’t performing well, many teams instinctually try to improve the Code. But for many practical applications, it’s more effective instead to focus on improving the Data.Progress in machine learning has been driven for decades by efforts to improve performance on benchmark datasets, in which researchers hold the Data fixed while improving the Code. But for many applications — especially ones where the dataset size is modest (<10,000 examples) — teams will make faster progress by focusing instead on making sure the dataset is good: Is the definition of y given x clear and unambiguous? For example, do different data labelers draw bounding boxes consistently? Do speech transcriptionists label ambiguous audio consistently, for instance, writing “um, yes please” rather than “um ... yes please”?Does the input distribution x sufficiently cover the important cases?Does the data incorporate timely feedback from the production system, so we can track concept and data drift? It’s a common joke that 80 percent of machine learning is actually data cleaning, as though that were a lesser task. My view is that if 80 percent of our work is data preparation, then ensuring data quality is the important work of a machine learning team.Rather than counting on engineers to chance upon the best way to improve a dataset, I hope we can develop MLOps tools that help make building AI systems, including building high-quality datasets, more repeatable and systematic. MLOps is a nascent field, and different people define it differently. But I think the most important organizing principle of MLOps teams and tools should be to ensure the consistent and high-quality flow of data throughout all stages of a project. This will help many projects go more smoothly. I have much more to say on this topic, so check out my talk here. Thanks to my team at Landing AI for helping to crystalize these thoughts.Keep learning!Andrew Police are increasingly able to track motor vehicles throughout the U.S. using a network of AI-powered cameras — many owned by civilians. What’s new: Flock, which sells automatic license plate readers to homeowners associations, businesses, and law enforcement agencies, is encouraging enforcers to use its network to monitor cars and trucks outside their jurisdiction, according to an investigation by Vice. How it works: Flock owners can opt to share data with police. In turn, police can share data with Flock’s Total Analytics Law Officers Network, or Talon. Talon collects as many as 500 million vehicle scans each month. The network’s cameras store video and send alerts when they spot vehicles flagged on watch lists. In addition to license plate numbers, users can search by model, color, and features like spoilers or roof racks.Talon data can also be used in conjunction with the National Crime Information Center, an FBI database that contains records on fugitives, missing persons, and stolen vehicles.Over 500 U.S. police departments have access to Talon. Flock claims that it helps solve between four and five cases an hour. The system stores data for only 30 days, but police can download information for use as evidence in a case.Roving scanners are mounted on tow trucks and garbage trucks, The Wall Street Journal reported. License plate data played a role in arrests of suspects in the riot at the U.S. Capitol on January 6. Behind the news: AI-powered cameras are increasingly popular with law enforcement, but their use is fueling concerns about overreach. Police used data from Ring, a division of Amazon that sells AI-enhanced surveillance cameras to residences and businesses (but which lack license plate reader technology), to target Black Lives Matter protesters in Los Angeles last summer.License plate readers by Vigilant have contributed to arrests for driving vehicles incorrectly identified as stolen.In South Africa, critics say that Vumacam’s camera systems, which recognize objects, behaviors, and license plate numbers, reinforce law enforcement biases against Blacks. Why it matters: Commercial surveillance networks have been deployed without much oversight or consent, and police are rarely accountable for how they use such systems. Permissive policies around these devices amount to warrantless monitoring of millions of innocent people by police as well as fellow citizens. We’re thinking: While AI can help police catch criminals, we do not condone a silent erosion of civil liberties and privacy. We support clear, consistent guidelines on appropriate uses of face recognition, license plate readers, and other tracking technologies.", "image_caption": "Neighborhood being monitored by AI-powered cameras", "metadata": {"article_id": "issue_84", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/partners-in-surveillance-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-84/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_84.html"}}
{"id": 21690475002, "type": "news_chunk", "title": "Networking License Plate Readers, Calling Out Unreproducible", "subtitle": "Deciphering The Brain’s Visual Signals", "content": "What’s creepier than images from the sci-fi TV series Doctor Who? Images generated by a network designed to visualize what goes on in peoples’ brains while they watch Doctor Who.What’s new: Lynn Le, Luca Ambrogioni, and colleagues at Radboud University and Max Planck Institute for Human and Brain Cognitive Sciences developed Brain2Pix, a system that reconstructs what people saw from scans of their brain activity.Key insight: The brain uses neurons nearby one another to represent visual features nearby one another. Convolutional neural networks excel at finding and using spatial patterns to perform tasks such as image generation. Thus, a convolutional neural network can use the spatial relationships between active neurons in a brain scan to reconstruct the corresponding visual image. How it works: The authors used a picture-to-picture generative adversarial network (GAN) to try to produce an image of what a person was looking at based on functional magnetic resonance imaging (fMRI): 3D scans that depict blood oxygenation in the brain, which indicates neuron activity. They trained the GAN on Doctor Who fMRI, a collection of video frames from 30 episodes of Doctor Who and corresponding fMRIs captured as an individual watched the show. The authors converted each 3D scan into 2D images, each of which represented distinct sections of the brain, using a neuroscientific device known as a receptive field estimator .They trained the GAN’s discriminator to classify whether an image came from Doctor Who or the GAN’s generator. They trained the generator with a loss function that encouraged it to translate the 2D images of neuron activity into an image that would fool the discriminator.The generator used two additional loss terms. The first term aimed to minimize the difference between the pixel values of a video frame and its generated counterpart. The second term aimed to minimize the difference between representations, extracted by a pretrained VGG-16, of a video frame and its generated counterpart.The generator used a convolutional architecture inspired by U-Net in which residual connections passed the first layer’s output to the last layer, second layer’s output to the penultimate layer, and so on. This arrangement helped later layers in the network to preserve spatial patterns in the brain scans. Results: The researchers used an AlexNet to extract representations of Brain2Pix images and Doctor Who frames and compared the distance between them. Brain2Pix achieved an average distance of 4.6252, an improvement over the previous state-of-the-art method’s average of 5.3511.Why it matters: The previous state-of-the-art used 3D convolutions directly on the raw fMRIs, yet the new approach fared better. For some problems, engineering features — in this case, converting fMRIs into 2D — may be the best way to improve performance. We’re thinking: We wouldn’t mind sitting in an fMRI machine for hours on end if we were binge-watching Doctor Who.", "image_caption": "Images generated by a network designed to visualize what goes on in peoples’ brains while they watch Doctor Who", "metadata": {"article_id": "issue_84", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/What-the-brain-sees-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-84/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_84.html"}}
{"id": 21690475003, "type": "news_chunk", "title": "Networking License Plate Readers, Calling Out Unreproducible", "subtitle": "Spotlight on Unreproducible Results", "content": "A new website calls out AI research that may not lend itself to being reproduced. What’s new:Papers Without Code maintains a directory of AI systems that researchers tried but failed to reproduce. The site (the name of which is a play on the indispensable Papers With Code), aims to save researchers time wasted trying to replicate results published with insufficient technical detail.How it works: Users can submit a link to a paper, a link to their attempt to reproduce it, and an explanation of why their effort failed. After reviewing the links, the site’s administrators contact the original authors and request data, code, and pointers necessary to reproduce their work. If the authors don’t reply or provide insufficient information, the administrators add the paper to a public list.To date, the website has received more than 10 submissions, six of which have been posted. Two authors have uploaded their code. Once a paper passes muster, its author is encouraged to post it to Papers With Code, which documents 40,000 replicated computer science studies.The researcher behind Papers Without Code, who goes by the user name ContributionSecure14 on Reddit, started the website after wasting a week trying to replicate a machine learning study.They advise authors who can’t release their code, data, or infrastructure for proprietary reasons to work directly with others trying to replicate their efforts. “There’s no point in publishing the paper in the public domain if others cannot build off it,” they told TechTalks. Behind the news: Google engineer Pete Warden proclaimed a “machine learning reproducibility crisis” in 2018. Since then the issue has emerged as a widespread concern. Last year, 31 researchers criticized the lack of technical detail in a Google paper that describes a cancer system that purportedly outperformed human doctors.One of that paper’s coauthors, Joelle Pineau of McGill University and Facebook, worked with NeurIPS to ensure that papers submitted to the conference come with working code and data. She also published a Machine Learning Reproducibility Checklist.Rescience C is a peer-reviewed journal that publishes replication efforts of computer science papers. Why it matters: Reproducibility is an essential part of science, and AI is one of many fields facing a so-called replication crisis brought on by growing numbers of papers that report unreliable results.We’re thinking: While we applaud the spirit of this effort, without a transparent review process and a public list of reviewers, it could be used to demean researchers unfairly. We urge other research venues and institutions to take up the cause.", "image_caption": "Square brackets with lines disappearing inside", "metadata": {"article_id": "issue_84", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/Spotlight-on-Unreproducible-Results-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-84/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_84.html"}}
{"id": 21690475004, "type": "news_chunk", "title": "Networking License Plate Readers, Calling Out Unreproducible", "subtitle": "Chatbots Against Depression", "content": "A language model is helping crisis-intervention volunteers practice their suicide-prevention skills. What’s new: The Trevor Project, a nonprofit organization that operates a 24-hour hotline for LGBTQ youth, uses a “crisis contact simulator” to train its staff in how to talk with troubled teenagers, MIT Technology Review reported.How it works: The chatbot plays the part of a distraught teenager while a counselor-in-training tries to determine the root of their trouble. In-house engineers developed the system with help from Google. The team tested several models before settling on GPT-2.The model was pretrained on 45 million web pages and fine-tuned on transcripts of role-playing between trainees and The Trevor Project staffers.A different model helps triage incoming calls, also developed in collaboration with Google. When people log in to the chat system, a prompt asks them to describe their feelings. An ALBERT implementation analyzes their response for indicators of self-harm, flags those at highest risk, and prioritizes them to converse with a counselor. Behind the news: AI is being used in a growing number of mental health settings. ReachVet, a program of the U.S. Department of Veterans Affairs, scans military records and generates a monthly list of former military members at high risk of suicide.Chatbots like Flow, Lyssn, and Woebot (where Andrew Ng is chairman) aim to alleviate mood disorders like anxiety and depression in lieu of a human therapist. Why it matters: Suicide rates among LGBTQ teens are two to seven times higher than among their straight peers, and they’re twice as likely to think about taking their own lives, according to the U.S. government. The Trevor Project fields over 100,000 crisis calls, chats, and texts annually. Speeding up the training pipeline could save lives.We’re thinking: As a grad student at MIT, Andrew tried to volunteer for a crisis call line, but his application was rejected. Maybe training from this system would have helped!", "image_caption": "Commercial about The Trevor Lifeline", "metadata": {"article_id": "issue_84", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/Chatbots-Against-Depression-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-84/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_84.html"}}
{"id": 21690475005, "type": "news_chunk", "title": "Networking License Plate Readers, Calling Out Unreproducible", "subtitle": "Pictures From Words and Gestures", "content": "A new system combines verbal descriptions and crude lines to visualize complex scenes. What’s new: Google researchers led by Jing Yu Koh proposed Tag-Retrieve-Compose-Synthesize (TReCS), a system that generates photorealistic images by describing what they want to see while mousing around on a blank screen.Key insight: Earlier work proposed a similar system to showcase a dataset, Localized Narratives, that comprises synchronized descriptions and mouse traces captured as people described an image while moving a cursor over it. That method occasionally produced blank spots. The authors addressed that shortcoming by translating descriptive words into object labels (rather than simply matching words with labels) and distinguishing foreground from background.How it works: The Local Narratives dataset provides an inherent correspondence between every word in a description and a mouse trace over an image. TReCS uses this correspondence to translate words into labels of objects that populate a scene. The authors trained the system on the portion of Localized Narratives that used images in COCO and tested it on the portion that used Open Images. Given a description, a BERT model assigned an object label to each word in the description. The authors obtained ground-truth labels by matching the mouse traces for each word to object segmentation masks (silhouettes) for the images described. Then they fine-tuned the pretrained BERT to, say, attach the label “snow” to each of the words in “skiing on the snow.”For each label assigned by BERT, the system chose a mask from a similar image (say, a photo taken in a snowy setting). The authors trained a cross-modal dual encoder to maximize the similarity between a description and the associated image, and to minimize the similarity between that description and other images. On inference, given a description, the authors used the resulting vectors to select the five most similar training images.The system used these five images differently for foreground and background classes (an attribute noted in the mask dataset). For foreground classes such as “person,” it retrieved the masks with the same label and chose the one whose shape best matched the label’s corresponding mouse trace. For background classes such as “snow,” it chose all of the masks from the image whose masks best matched the labels and combined shape of the corresponding mouse traces.The authors arranged the masks on a blank canvas in the locations indicated by the mouse traces. They positioned first background and then foreground masks, reversing the order in which they were described. This put the first-mentioned object in front.A generative adversarial network learned to generate realistic images from the assembled masks. Results: Five judges compared TReCS’ output with that of AttnGAN, a state-of-the-art, text-to-image generator that did not have access to mouse traces. The judges preferred TReCS’ image quality 77.2 percent to 22.8 percent. They also preferred the alignment of TReCS output with the description, 45.8 percent to 40.5 percent. They rated both images well aligned 8.9 percent of the time and neither image 4.8 percent of the time.Why it matters: The authors took advantage of familiar techniques and datasets to extract high-level visual concepts and fill in the details in a convincing way. Their method uncannily synthesized complex scenes from verbal descriptions (though the featured example, a skier standing on a snowfield with trees in the background, lacks the railing and mountain mentioned in the description). We’re thinking: Stock photo companies may want to invest in systems like this. Customers could compose photos via self-service rather than having to choose from limited options. To provide the best service, they would still need to hire photographers to produce raw material.", "image_caption": "Tag-Retrieve-Compose-Synthesize (TReCS)", "metadata": {"article_id": "issue_84", "chunk_index": 5, "image_url": "https://dl-staging-website.ghost.io/content/images/2022/10/pictures-from-words-and-gestures-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-84/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_84.html"}}
{"id": 33845180001, "type": "news_chunk", "title": "Tesla Under Investigation, Star Trek: The Chatbot, Attention", "subtitle": "Tesla Safety Under Investigation", "content": "Dear friends, I have a two-year-old daughter, and am expecting my son to be born later this week. When I think about what we can do to build a brighter future for our children, the most important thing is to create a foundation for education. Because education is knowledge, and knowledge is human progress.Today Coursera, which I co-founded almost nine years ago to transform lives through learning, became a publicly listed company. I remember building the machine learning course that wound up being the first course on Coursera. There were many Friday nights when I met friends for dinner and then headed back to the office to record videos until 3 a.m. I felt privileged and humbled sitting in a room by myself speaking to a webcam, knowing I was playing a small role in helping thousands of learners.Of course, Coursera quickly became much bigger than a professor and a webcam. I’m grateful to my cofounder Daphne Koller, my early team members, our university partners, instructors, investors, advisors, executives, board members, and 1,000-plus employees over the years. Special shout-out to the company’s CEO Jeff Maggioncalda, who treasures the education mission as much as I do. Most of all, I want to thank all the learners. Let's face it — learning is fun, but it can also be hard work. I remember once reading an article about the percentage of programmers who were self-taught. I couldn’t understand anything less than 100 percent, because I think all learners are self taught. Teachers can play a role, but ultimately it's up to learners to learn. So thank you for watching the online videos, doing homework, and spending your spare time to master these materials.Coursera was launched on April 18, 2012 (the company and I share a birthday!). I hope we’ll continue to reach more learners, because everyone should be a lifelong learner, and everyone should have the opportunity to transform their life through learning.The education mission is bigger than any person or single institution. If we can unlock the full potential in every person, we will move humanity forward. (This letter is excerpted from a speech I made at Coursera’s IPO event earlier today.) Keep learning! U.S. authorities are investigating Tesla’s self-driving technology. What’s new: Federal regulators launched a probe of nearly two dozen accidents, some of them fatal, that involved Tesla vehicles equipped for self-driving, Reuters reported. The inquiry: The National Highway Traffic Safety Administration is looking into 23 crashes of Tesla vehicles that occurred when the cars’ autonomous driving systems may have been engaged. The agency previously completed four investigations into Tesla crashes, most famously one from 2016 in which a Florida driver was killed when his car plowed into a big rig. Tesla’s technology was found to be partly to blame for that incident but not the other three.In separate investigations of the Florida incident and one in California two years later, the National Transportation Safety Board (a different federal oversight group) found Tesla’s system at fault.Tesla insisted its vehicles are safe. Data it collects from its fleet shows that cars under autonomous control experience fewer accidents per mile than those driven by humans, the company said. The company has not revealed whether Autopilot was engaged during the accidents under investigation. Behind the news: Tesla has two self-driving modes. Autopilot, which comes standard on all new vehicles, controls the steering wheel, brakes, and accelerator. It’s meant to be used on highways with a center divider.Drivers can upgrade to what Tesla calls the Full Self-Driving option for $10,000. Despite the option’s name, last November, a Tesla lawyer disclosed to California regulators that the system should not be considered fully autonomous.Tesla advises drivers using either mode to keep their hands near the steering wheel and eyes on the road. However, the systems remain engaged even if drivers don’t follow these instructions, and videos on social media show drivers using Autopilot on roads that are not divided highways. Why it matters: The new investigations are aimed at finding facts and will not directly result in new rules for Tesla or the self-driving industry at large. Still, the company’s reputation could take a battering, and hype about self-driving technology makes it harder for the AI community as a whole to gain trust and make progress. We’re thinking: While it may be true that Tesla’s self-driving technology is safer on average than human drivers, it doesn’t fit the description “full self-driving.” While Tesla’s work to promote clean energy has had widespread positive impact, it’s time for the company to drop that branding and for car makers to provide clear, consistent information about their autonomous capabilities.", "image_caption": "Person driving a Tesla car", "metadata": {"article_id": "issue_85", "chunk_index": 1, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/TESLA%20TG.gif?upscale=true&name=TESLA%20TG.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-85/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_85.html"}}
{"id": 33845180002, "type": "news_chunk", "title": "Tesla Under Investigation, Star Trek: The Chatbot, Attention", "subtitle": "Vision Models Get Some Attention", "content": "Self-attention is a key element in state-of-the-art language models, but it struggles to process images because its memory requirement rises rapidly with the size of the input. New research addresses the issue with a simple twist on a convolutional neural network. What’s new: Aravind Srinivas and colleagues at UC Berkeley and Google introduced BoTNet, a convolutional architecture that uses self-attention to improve average precision in object detection and segmentation.Key insight: Self-attention and convolution have complementary strengths. Self-attention layers enable a model to find relationships between different areas of an image, while convolutional layers help the model to capture details. Self-attention layers work best when inputs are small, while convolutional layers can shrink input size. Combining the two offers the best of both worlds.How it works: BoTNet-50 is a modified ResNet-50. The authors trained it for COCO’s object detection and segmentation tasks — that is, to draw bounding boxes around objects and determine what object each pixel belongs to — via Mask R-CNN, a method that details how to train and set up the network architecture for these tasks. Some ResNets use bottleneck blocks, which perform three layers of convolutions. The first layer reduces the input size, the second extracts representations, and the third converts its input back to the original size.BoTNeT adopts this structure, but in the last three blocks of the network, the authors replaced the second convolutional layer with a self-attention layer. Results: BoTNet-50 beat a traditional ResNet-50 in both object detection and segmentation. Averaged over all objects in COCO, more than half of pixels that BoTNet associated with a given object matched the ground-truth labels 62.5 percent of the time, while the ResNet-50 achieved 59.6 percent. For a given object, more than half of BoTNet’s predicted bounding box overlapped with the ground-truth bounding box 65.3 percent of the time, compared to 62.5 percent for the ResNet-50.Why it matters: Good ideas in language processing can benefit computer vision and vice versa. We’re thinking: Convolution is almost all you need.", "image_caption": "Sequence related to image processing", "metadata": {"article_id": "issue_85", "chunk_index": 2, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-22T175640.664-1.gif?upscale=true&name=ezgif.com-gif-maker%20-%202021-03-22T175640.664-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-85/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_85.html"}}
{"id": 33845180003, "type": "news_chunk", "title": "Tesla Under Investigation, Star Trek: The Chatbot, Attention", "subtitle": "Star Trek: The Videobot Generation", "content": "A digital doppelgänger of Star Trek’s original star will let fans chat with him — possibly well beyond his lifetime. What’s new: AI startup StoryFile built a lifelike videobot of actor William Shatner, best known for playing Captain James T. Kirk of the Starship Enterprise in the 1960’s-vintage Star Trek television series. The Shatbot is scheduled to go online in May. How it works: The company honed its approach by building avatars of Holocaust survivors, a socially distanced interactive Santa Claus, and a platform that lets people talk with scientists about climate change. StoryFile recorded hours of Shatner, who recently turned 90, answering questions while volumetric cameras captured his image in three dimensions.The volumetric picture was shot in front of a green screen, enabling the team to isolate Shatner’s image and display it against a living-room setting.The team trained a proprietary language model called Conversa to associate questions and answers. When a user asks a question, the model will find a closely related answer and serves it up. Behind the news: Shatner imagines that the system might enable his descendents to interact with him after his death. Other companies are also using chatbots to help people feel connected to departed loved ones. Last December, Microsoft was awarded a patent for a bot that re-creates a specific person by processing their text messages, audio, videos, and other digital remains.The creator of Replika, a chatbot for people experiencing loneliness, trained the original system using old texts from a friend who had died in a car accident. Why it matters: Technological replicas of human beings are a long-standing science fiction trope, and few stories have shaped our vision of the future as profoundly as Star Trek. A lifelike avatar of William Shatner is a fitting — and fun — way to celebrate that legacy. We’re thinking: We support free Enterprise.", "image_caption": "Star Trek actor William Shatner", "metadata": {"article_id": "issue_85", "chunk_index": 3, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/SHATNER%20TG.gif?upscale=true&name=SHATNER%20TG.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-85/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_85.html"}}
{"id": 33845180004, "type": "news_chunk", "title": "Tesla Under Investigation, Star Trek: The Chatbot, Attention", "subtitle": "Same Patient, Different Views", "content": "When you lack labeled training data, pretraining a model on unlabeled data can compensate. New research pretrained a model three times to boost performance on a medical imaging task.What’s new: Shekoofeh Azizi and colleagues at Google developed Multiple-Instance Contrastive Learning (MICLe), a training step that uses different perspectives of the same patient to enhance unsupervised pretraining.Key insight: Presented with similar images, a model trained via contrastive learning produces representations that are nearby in vector space. Training via contrastive learning on images of the same patient taken from various angles can produce similar representations of an illness regardless of the camera’s viewpoint.How it works: The authors started with a ResNet-50 (4x) pretrained on ImageNet. They added contrastive pretraining steps and fine-tuning to diagnose 26 skin conditions from acne to melanoma. The training data was a private set of 454,295 images that included multiple shots of the same patients. To refine the general representations learned from ImageNet for medical images, the authors pretrained the model according to SimCLR, an earlier contrastive learning technique. The model regarded augmented versions of the same parent image as similar and augmented versions of different images as dissimilar.To sharpen the representations for changes in viewpoint, lighting, and other variables, they further pretrained the model on multiple shots of 12,306 patients. In this step — called MICLe — the model regarded randomly cropped images of the same patient as similar and randomly cropped images of different patients as dissimilar.To focus the representations for classifying skin conditions, they fine-tuned the model on the images used in the previous step. Results: The authors compared the performance of identical ResNet-50s pretrained and fine-tuned with and without MICLe. The authors’ method boosted the model’s accuracy by 1.18 percent to 68.81 percent, versus 67.63 percent without it. Why it matters: A model intended to diagnose skin conditions no matter where they appear on the body may not have enough data to gain that skill through typical supervised learning methods. This work shows that the same learning can be accomplished using relatively little data through judicious unsupervised pretraining and contrastive losses.We’re thinking: The combination of SimCLR and MICLe is a study in contrasts.", "image_caption": "Sequence showing a training step that uses different perspectives of the same patient to enhance unsupervised pretraining", "metadata": {"article_id": "issue_85", "chunk_index": 4, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-08T122032.068-1.gif?upscale=true&name=ezgif.com-gif-maker%20-%202021-03-08T122032.068-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-85/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_85.html"}}
{"id": 33845180005, "type": "news_chunk", "title": "Tesla Under Investigation, Star Trek: The Chatbot, Attention", "subtitle": "On Her Majesty’s Secret Algorithm", "content": "The UK’s electronic surveillance agency published its plan to use AI. What’s new: Government Communications Headquarters (GCHQ) outlined its intention to use machine learning to combat security threats, human trafficking, and disinformation — and to do so ethically — in a new report. What it says: GCHQ said its AI will augment, rather than supplant, human analysts. Moreover, the agency will strive to use AI with privacy, fairness, transparency, and accountability by emphasizing ethics training and thoroughly reviewing all systems. Such systems will: Analyze data on large computer networks to prevent cyber attacks, identify malicious software, and trace them back to their origins.Intercept sexually explicit imagery featuring minors and messages from sexual predators.Combat drug smuggling and human trafficking by analyzing financial transactions and mapping connections between the individuals behind them.Counter misinformation using models that detect deepfakes, assist with checking facts, and track both content farms that pump out fake news and botnets that spread it. Behind the news: While intelligence agencies rarely detail their AI efforts, several examples have come to light. German law enforcement agencies use AI-generated images of minors to trap online predators.The U.S. National Reconnaissance Office is developing a system to guide surveillance satellites.The U.S. National Security Agency is training models to audit regulatory compliance by other intelligence agencies as they search for international criminals and look for warning of emerging crises. Why it matters: The GCHQ plan emphasizes the utility of AI systems in securing nations and fighting crime — and highlights the need to ensure that sound ethical principles are built into their design and use.We’re thinking: GPT-007 prefers its data shaken, not perturbed.", "image_caption": "Graph showing key AI characteristics", "metadata": {"article_id": "issue_85", "chunk_index": 5, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/UKSPY-1.gif?upscale=true&name=UKSPY-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-85/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_85.html"}}
{"id": 33845180006, "type": "news_chunk", "title": "Tesla Under Investigation, Star Trek: The Chatbot, Attention", "subtitle": "A MESSAGE FROM FOURTHBRAIN", "content": "Become a machine learning engineer in only 16 weeks with FourthBrain! Meet our grads at a free info session on April 8, 2021. Ask them questions and learn more about the program. Register now", "image_caption": "The Batch - March 29th", "metadata": {"article_id": "issue_85", "chunk_index": 6, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20-%20March%2029th.png?upscale=true&name=The%20Batch%20-%20March%2029th.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-85/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_85.html"}}
{"id": 45554589001, "type": "news_chunk", "title": "Doctors Distrust AI, New Life For Old Songs, ImageNet...", "subtitle": "De-Facing ImageNet", "content": "Dear friends, Each year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latest Edelman Trust Barometer contains a worrisome finding: While technology was ranked the most trusted industry in the U.S. last year, this year we plunged to ninth place. Trust in the tech industry fell to new lows in the majority of 27 countries surveyed.Tech can be a huge force for moving the world forward, but many well meaning efforts will run into headwinds if we aren’t able to gain others’ trust. It’s more urgent than ever that we collectively act in a way that is genuinely deserving of the rest of society’s trust.Trust is much harder to build than to destroy. One company that hypes AI can do more damage than 10 others that speak about it responsibly. One company that makes misleading statements can do more damage than 10 that speak honestly. How can we regain trust? Several steps are needed, but to my mind, chief among them are: Straight talk. I think we’re all tired of hearing tech companies say they’re fighting for small businesses when they’re just fighting for their own bottom line. I realize that no company can address every issue under the sun, but when we speak about something, we owe it to the public to tell it like it is.Take responsibility. Tech’s influence on what people see and hear has a huge impact on their perception of reality. Our collective influence on automation has a huge impact on jobs. I hope that each organization will acknowledge the power it has and use it to benefit society.Engage and empathize. When someone who is honest and well meaning has a problem with what we do, our first step should be to try to understand their point of view, not to dismiss their concerns. Society has reasonable worries about tech’s concentration of power, fairness, and impact on jobs. Whether we agree or disagree in a certain instance, let's acknowledge the concern and see if we can address it honestly. Trying to fool the public and government officials doesn’t work. We often read in the news about politicians who know little about tech, and say things that reflect their lack of understanding. But let me tell you this: Every large government has at least a handful of people who are tech-savvy enough to see through the spin to the heart of an issue. Companies shouldn’t try to fool people and instead do the harder — but more effective — work of solving problems thoughtfully. On the plus side, 62 percent of respondents to Edelman’s survey agreed that employees have the power to force corporations to change. CEO aren’t the only people responsible for what companies do. All employees have a responsibility to help build trustworthy businesses. Wherever you work, I hope you’ll support straight talk, taking responsibility, and engaging and empathizing. Keep learning! ImageNet now comes with privacy protection. What’s new: The team that manages the machine learning community’s go-to image dataset blurred all the human faces pictured in it and tested how models trained on the modified images on a variety of image recognition tasks. The faces originally were included without consent. How it worked: The team used Amazon’s Rekognition platform to find faces in ImageNet’s nearly 1.5 million examples. Rekognition drew a bounding box around each of over 500,000 faces. (Some images contained more than one face.) Crowdsourced workers checked the model’s work and corrected errors where necessary. Then the team applied Gaussian blur to the area within bounding boxes.The authors trained 24 image recognition architectures on the original ImageNet and copies of the same architectures on the blurred version, and compared their performance. The models trained on the blurred images were, on average, less accurate by under 1 percent. However, the decline was severe with respect to objects typically found close to a face, such as masks (-8.71 percent) and harmonicas (-8.93 percent).They tested the blurred data’s effect on transfer learning by pretraining models using the unmodified and modified ImageNet and fine-tuning them for object recognition, scene recognition, object detection, and facial attribute classification (whether a person is smiling, wearing glasses, and the like). The models trained on blurred images performed roughly as well as those trained on unmodified ImageNet.The face-blurred ImageNet will become the new official version, according to VentureBeat. Behind the news: This work is part of a wider movement toward protecting privacy in machine learning data. For instance, papers submitted to CVPR in recent years proposed models to automatically blur faces and license plates in Google Street View as well as data for training autonomous vehicles, and action recognition models. Why it matters: Machine learning datasets need not violate privacy. We can develop datasets that both protect privacy and train good models. We’re thinking: Any loss of accuracy is painful, but a small loss is worthwhile to protect privacy. There’s more to life than optimizing test-set accuracy! We expect that most ImageNet-trained applications won’t suffer from the change, as they don’t involve objects that typically appear near to faces. Fine-tuning on a dataset obtained with permission might help for the rest.", "image_caption": "Blurred human faces in different pictures", "metadata": {"article_id": "issue_86", "chunk_index": 1, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/BLUR%20TG.gif?upscale=true&name=BLUR%20TG.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-86/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_86.html"}}
{"id": 45554589002, "type": "news_chunk", "title": "Doctors Distrust AI, New Life For Old Songs, ImageNet...", "subtitle": "Pretraining on Uncurated Data", "content": "It’s well established that pretraining a model on a large dataset improves performance on fine-tuned tasks. In sufficient quantity and paired with a big model, even data scraped from the internet at random can contribute to the performance boost.What’s new: Facebook researchers led by Priya Goyal built SElf-supERvised (SEER), an image classifier pretrained on a huge number of uncurated, unlabeled images. It achieved better fine-tuned ImageNet accuracy than models pretrained on large datasets that were curated to represent particular labels.Key insight: Large language models pretrained on billions of uncurated documents found in the wild, such as GPT-3, have achieved state-of-the-art performance after fine-tuning on a smaller dataset. Computer vision should benefit likewise from such scale.How it works: The authors used a 1.3-billion parameter RegNet, a convolutional neural network architecture similar to ResNet, pretrained on 1 billion images randomly scraped from Instagram. The pretraining procedure followed SwAV, which was devised by several of the same researchers. SwAV receives representations — in this case, from the RegNet — and learns to group related images into a number of clusters by emphasizing similarities among them (similar to contrastive learning).The authors fine-tuned the model on over 1 million images from ImageNet. Results: SEER achieved 84.2 percent top-1 accuracy on the ImageNet test set, 1.1 percent better than the best previous self-supervised, pretrained model (a ResNet of 795 million parameters pretrained on ImageNet using SimCLRv2). It was 4.3 percentage points better than a 91-million parameter ViT pretrained on JFT-300M, a curated dataset of 300 million images from Google Search. SEER also excelled at few-shot learning: Fine-tuned on 10 percent of ImageNet, it achieved 77.9 percent accuracy, 2.2 percentage points lower than a SimCLRv2 model pretrained on 100 percent of ImageNet and fine-tuned on 10 percent of ImageNet.Why it matters: Scraping the internet could be as productive in computer vision as it has been in language processing. Just keep in mind that training models on such data risks violating privacy and consent as well as absorbing the various biases — including objectionable social biases — inherent on the internet.We’re thinking: This paper suggests a tradeoff between the costs of building a curated dataset and training on a larger corpus plucked from the wild. If the cost of curation is high for your application, maybe you can cut it and spend more on training.", "image_caption": "Data related to SElf-supERvised (SEER), an image classifier pretrained on uncurated, unlabeled images", "metadata": {"article_id": "issue_86", "chunk_index": 2, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/SEER%20(1).gif?upscale=true&name=SEER%20(1).gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-86/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_86.html"}}
{"id": 45554589003, "type": "news_chunk", "title": "Doctors Distrust AI, New Life For Old Songs, ImageNet...", "subtitle": "Would Your Doctor Take AI’s Advice?", "content": "Some doctors don’t trust a second opinion when it comes from an AI system. What’s new: A team at MIT and Regensburg University investigated how physicians responded to diagnostic advice they received from a machine learning model versus a human expert. How it works: The authors recruited doctors to diagnose chest X-rays. The physicians fell into two groups: 138 radiologists highly experienced in reading X-rays and 127 internal or emergency medicine specialists with less experience in that task.For each case, the doctors were given either accurate or inaccurate advice and told that it was generated by either a model or human expert.The physicians rated the advice and offered their own diagnosis. Results: The radiologists generally rated as lower-quality advice they believed was generated by AI. The others rated AI and human advice to be roughly of equal quality. Both groups made more accurate diagnoses when given accurate advice, regardless of its source. However, 27 percent of radiologists and 41 percent of the less experienced offered an incorrect diagnosis when given inaccurate advice. Behind the news: AI-powered diagnostic tools are proliferating and becoming more widely accepted in the U.S. and elsewhere. These tools may work about as well as traditional methods at predicting clinical outcomes. Those that work well may only do so on certain populations due to biased training data. Why it matters: It’s not enough to develop AI systems in isolation. It’s important also to understand how humans use them. The best diagnostic algorithm in the world won’t help if people don’t heed its recommendations. We’re thinking: While some doctors are skeptical of AI, others may trust it too much, which also can lead to errors. Practitioners in a wide variety of fields will need to cultivate a balance between skepticism and trust in machine learning systems. We welcome help from the computer-human interface community in wrestling with these challenges.", "image_caption": "Data related to a diagnostic advice received from a machine learning model vs a human expert", "metadata": {"article_id": "issue_86", "chunk_index": 3, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/INFLUENCE%20TG.gif?upscale=true&name=INFLUENCE%20TG.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-86/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_86.html"}}
{"id": 45554589004, "type": "news_chunk", "title": "Doctors Distrust AI, New Life For Old Songs, ImageNet...", "subtitle": "New Life for Old Songs", "content": "Neural networks can tease apart the different sounds in musical recordings. What’s new: Companies and hobbyists are using deep learning to separate voices and instruments in commercial recordings, Wired reported. The process can improve the sound of old recordings and opens new possibilities for sampling, mash-ups, and other fresh uses. How it works: Finished recordings often combine voices, instruments, and other sounds recorded in a multitrack format into a smaller number of audio channels; say, one for mono or two for stereo. The mingling of signals limits how much the sonic balance can be changed afterward, but neural networks have learned to disentangle individual sounds — including noise and distortion — so they can be rebalanced or removed without access to the multitrack recordings. Audio Research Group was founded by an audio technician at Abbey Road Studios, who developed a deep learning system to remix the Beatles’ 1964 hit, “She Loves You,” which was produced without multitracking.Audionamix separates mono recordings into tracks for vocals, drums, bass guitar, and other sounds. The service has been used to manipulate old recordings for use in commercials and to purge television and film soundtracks of music, inadvertently playing in the background, that would be expensive to license.French music streaming service Deezer offers Spleeter, an open-source system that unmixes recordings (pictured above). Users have scrubbed vocals to produce custom karaoke tracks, create oddball mashups, and cleanse their own recordings of unwanted noises. Why it matters: Many worthwhile recordings are distorted or obscured by noises like an audience’s cheers or analog tape hiss, making the quality of the musicianship difficult to hear. Others could simply use a bit of buffing after decades of improvement in playback equipment. AI-powered unmixing can upgrade such recordings as well as inspire new uses for old sounds. We’re thinking: Endless remixes of our favorite Taylor Swift tracks? We like the sound of that!", "image_caption": "Recording unmixed", "metadata": {"article_id": "issue_86", "chunk_index": 4, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/upmix.gif?upscale=true&name=upmix.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-86/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_86.html"}}
{"id": 45554589005, "type": "news_chunk", "title": "Doctors Distrust AI, New Life For Old Songs, ImageNet...", "subtitle": "Attention for Image Generation", "content": "Attention quantifies how each part of one input affects the various parts of another. Researchers added a step that reverses this comparison to produce more convincing images.What’s new: Drew A. Hudson at Stanford and C. Lawrence Zitnick at Facebook chalked up a new state of the art in generative modeling by integrating attention layers into a generative adversarial network (GAN). They call their system GANsformer.Key insight: Typically, a GAN learns through competition between a generator that aims to produce realistic images and a discriminator that judges whether images are generated or real. StyleGAN splits the generator into (a) a mapping network and (b) a synthesis network, and uses the output of the mapping network to control high-level properties (for example, pose and facial expression) of an image generated by the synthesis network. The output of the mapping layer can be viewed as a high-level representation of the scene, and the output of each layer of the synthesis network as a low-level representation. The authors devised a two-way version of attention, which they call duplex attention, to refine each representation based on the other.How it works: GANsformer is a modified StyleGAN. The authors trained it on four types of subject matter: faces in FFHQ; scenes composed of cubes, cylinders, and spheres in CLEVR; pictures of bedrooms in LSUN; and urban scenes in Cityscapes. Given a random vector, the mapping network produced an intermediate representation via a series of fully connected layers. Given a random vector, the synthesis network produced an image via alternating layers of convolution and duplex attention.The authors fed the mapping network's intermediate representation to the synthesis network’s first duplex attention layer.Duplex attention updated the synthesis network’s representation by calculating how each part of the image influenced the parts of the intermediate representation. Then it updated the intermediate representation by calculating how each of its parts influenced the parts of the image. In this way, the system refined the mapping network’s high-level view according to the synthesis network’s low-level details and vice versa.The discriminator used duplex attention to iteratively hone the image representation along with a learned vector representing general scene characteristics. Like the synthesis network, it comprised alternating layers of convolution and duplex attention. Results: GANsformer outperformed the previous state of the art on CLEVR, LSUN-Bedroom, and Cityscapes (comparing Fréchet Inception Distance based on representations produced by a pretrained Inception model). For example, on Cityscapes, GANsformer achieved 5.7589 FID compared to StyleGAN2’s 8.3500 FID. GANsformer also learned more efficiently than a vanilla GAN, StyleGAN, StyleGAN2, k-GAN, and SAGAN. It required a third as many training iterations to achieve equal performance.Why it matters: Duplex attention helps to generate scenes that make sense in terms of both the big picture and the details. Moreover, it uses memory and compute efficiently: Consumption grows linearly as input size increases. (In transformer-style self-attention, which evaluates the importance of each part of an input with respect to other parts of the same input, memory and compute cost grows quadratically with input size.)We’re thinking: Transformers, which alternate attention and fully connected layers, perform better than other architectures in language processing. This work, which alternates attention and convolutional layers, may bring similar improvements to image processing.", "image_caption": "Examples of image generators using GANsformer", "metadata": {"article_id": "issue_86", "chunk_index": 5, "image_url": "https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif?upscale=true&name=ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-86/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_86.html"}}
{"id": 15252301001, "type": "news_chunk", "title": "Robots Supervise Robots, Bad Labels Plague Datasets, Large", "subtitle": "Labeling Errors Everywhere", "content": "Dear friends, Machine learning development is highly iterative. Rather than designing a grand system, spending months to build it, and then launching it and hoping for the best, it’s usually better to build a quick-and-dirty system, get feedback, and use that feedback to improve the system.The iterative aspect of machine learning applies to many steps. For example:Data labeling: It’s hard to come up with fully fleshed-out labeling guidelines that result in clean and consistent labels on your first attempt. It might be better to use an initial set of guidelines to label some data, see what problems arise, and then improve the guidelines.Model training: Building an AI system requires deciding what data, hyperparameters, and model architecture to use. Rather than overthinking these choices, it’s often better to train an initial model, then use error analysis to drive improvements.Deployment and monitoring: When deploying a machine learning system, you might implement dashboards that track various metrics to try to spot concept drift or data drift. For example, if you’re building a product recommendation system, you might track both software metrics such as queries per second and statistical metrics such as how often the system recommends products of different categories. What metrics should we track? Rather than try to design the perfect set of dashboards before launch, I find it more fruitful to pick a very large set of metrics, evolve them, and prune the ones that prove less useful. Iteration is helpful in other phases of machine learning development as well. It make sense to take an empirical, experimental approach to decision making whenever: Multiple options are available and it's hard to know the best choice in advance.We can run experiments to get data quickly about the performance of different options. These two properties hold true for many steps in a typical ML project. One implication is that, if we can build tools and processes that enable high-throughput experimentation, we can make faster progress. For instance, if you have an MLOps platform that enables you to quickly train and evaluate new models, this will allow you to improve models more quickly. This principle applies to other aspects of ML development that are iterative. That’s why time spent optimizing your team's capacity to run many experiments can pay off well. Keep learning! Key machine learning datasets are riddled with mistakes. What’s new: Several benchmark datasets are shot through with incorrect labels. On average, 3.4 percent of examples in 10 commonly used datasets are mislabeled, according to a new study — and the detrimental impact of such errors rises with model size. The research: Curtis Northcutt and Anish Athalye at MIT and Jonas Mueller at Amazon trained a model to identify erroneous labels in popular datasets such as ImageNet, Amazon Reviews, and IMDB. Following confident learning, the authors considered an example mislabeled if it met two conditions: The model’s predicted classification didn't match the label, and the model’s confidence in its classification was greater than its average confidence in its predictions of the labeled class over all examples bearing that label.Human reviewers vetted the mislabeled examples. They found many obvious mistakes: an image of a frog labeled “cat,” an audio clip of a singer labeled “whistling,” and negative movie reviews misinterpreted as positive. QuickDraw had the highest rate of inaccurately labeled data, 10.1 percent. MNIST had the lowest, 0.15 percent.The authors fixed the bad labels and revised the test sets. Then they measured how well different models classified the corrected test sets. Smaller models like Resnet-18 or VGG-11 outperformed larger ones like NasNet or VGG-19. Why it matters: It’s well known that machine learning datasets contain a fair percentage of errors. Previous inquiries into the problem focused on training rather than test sets, and found that training on a small percentage of incorrect labels didn’t hurt deep learning performance. But accuracy on a test set that’s rife with errors is not a true measure of a model’s ability, and bad labels in the test set have a disproportionate impact on bigger models. We’re thinking: It’s time for our community to shift from model-centric to data-centric AI development. Many state-of-the-art models work well enough that tinkering with their architecture yields little gain in many problems, and the most direct path to improved performance is to systematically improve the data your algorithm learns from. You can check out Andrew’s recent talk on the subject here. #DataCentricAI", "image_caption": "Model identifying erroneous labels in popular datasets", "metadata": {"article_id": "issue_87", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-12.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-87/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_87.html"}}
{"id": 15252301002, "type": "news_chunk", "title": "Robots Supervise Robots, Bad Labels Plague Datasets, Large", "subtitle": "Image Generation Transformed", "content": "A recent generative adversarial network (GAN) produced more coherent images using modified transformers that replaced fully connected layers with convolutional layers. A new GAN achieved a similar end using transformers in their original form. What’s new: Yifan Jiang and collaborators at the University of Texas at Austin and the MIT-IBM Watson AI Lab unveiled TransGAN, a transformer-based GAN that doesn’t use any convolutions. Key insight: Traditionally, GANs rely on convolutional neural networks, which integrate information in pixels far away from one another only in the later layers. The upshot could be an image of a person with two different eye colors or mismatched earrings. A GAN based on transformers, which use self-attention to determine relationships among various parts of an input, would learn relationships between pixels across an entire image from the get-go. That should enable it to produce more realistic images. How it works: Like other GANs, TransGAN includes a generator (which, given a random input, generates a new image) and a discriminator (which, given an image, predicts whether or not it’s generated). Both components contain a sequence of transformer layers, each comprising a fully connected layer and a self-attention layer. The authors trained them simultaneously. Where a typical GAN’s generator uses convolutions to manipulate a two-dimensional representation, TransGAN uses transformers to manipulate a sequence and project it into a sequence of pixels. To cut the amount of computation required, the generator produces a small number of representations at the first layer and increases the number in subsequent layers.Convolutions typically focus on small, adjacent areas of an input to avoid unnaturally abrupt transitions. To encode similar smoothness without convolutions, TransGAN’s generator applied a mask during training that limited attention to neighboring parts of an image. The mask gradually enlarged until it covered the entire image.The discriminator receives an image divided into an 8x8 grid, which it converts into a sequence of 64 patches. The sequence passes through the transformer layers, ending with a linear layer that classifies the image. Results: TransGAN set a new state of the art on the STL-10 dataset, which includes relatively few labeled examples and many unlabeled examples in a similar distribution. It achieved a Fréchet Inception Distance — a measure of the difference in distribution between generated images and training data (lower is better) — of 25.32 FID, compared to the previous state of the art’s 26.98 FID. Yes, but: On the Celeb-A dataset of relatively high-res celebrity faces, TransGAN achieved a Fréchet Inception Distance of 12.23 FID versus HDCGAN, which is designed for higher-res output and scored 8.44 FID. Why it matters: The transformer takeover continues! Meanwhile, TransGAN’s expanding training mask gives its output the smooth look of convolutions with better coherence across generated images. Maybe such purposeful training schedules can stand in for certain architectural choices. We’re thinking: Transformers, with their roots in language processing, might answer the age-old question of how many words an image is worth.", "image_caption": "A generative adversarial network (GAN)", "metadata": {"article_id": "issue_87", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-13.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-87/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_87.html"}}
{"id": 15252301003, "type": "news_chunk", "title": "Robots Supervise Robots, Bad Labels Plague Datasets, Large", "subtitle": "Who Watches the Welders?", "content": "A robot inspector is looking over the shoulders of robot welders. What’s new: Farm equipment maker John Deere described a computer vision system that spots defective joints, helping to ensure that its heavy machinery leaves the production line ready to roll. How it works: Like other manufacturers, John Deere uses robotic welders to assemble metal parts on its machines for farming, forestry, and construction. But industrial-strength welding has a longstanding problem: Bubbles of gas can form inside a joint as it cools, weakening it. An action recognition model developed by Intel spots such defects in real time. The model was trained on videos of good and bad welds. The clips were lit only by welding sparks, so that lighting conditions wouldn’t affect the model’s performance.The model is deployed on a ruggedized camera perched on the welding gun 12 to 14 inches away from the molten metal.When it detects a bad weld, it stops the robot and alerts human workers. Behind the news: AI-powered quality assurance is gaining ground. Systems from Landing AI (a sister company to DeepLearning.AI) and others recognize defects in a growing number of manufacturing processes. Why it matters: Skilled human inspectors are in short supply, expensive to hire, and not always able to inspect every joint in a factory full of robotic welders, so defects may go unnoticed until after a subpar part has become part of a larger assembly. A single welded part can cost up to $10,000. By spotting errors as they occur, computer vision can save manufacturers time and money. We’re thinking: Good to see AI making sure the job is weld done", "image_caption": "Operators working with factory machinery", "metadata": {"article_id": "issue_87", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-14.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-87/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_87.html"}}
{"id": 15252301004, "type": "news_chunk", "title": "Robots Supervise Robots, Bad Labels Plague Datasets, Large", "subtitle": "Large Language Models for Chinese", "content": "Researchers unveiled competition for the reigning large language model GPT-3. What’s new: Four models collectively called Wu Dao were described by Beijing Academy of Artificial Intelligence, a research collective funded by the Chinese government, according to Synced Review. Power quartet: Wu Dao’s constituent models were developed by over 100 scientists at leading Chinese universities and tech companies. In January, researchers associated with the project told Wired that it could help citizens navigate China’s bureaucracy, including the Beijing Motor Vehicles Administration. Wen Yuan is a 2.6 billion parameter language model that matched or exceeded GPT-3’s performance in Chinese- and English-language tasks. The group plans to scale the model up to 100 billion parameters later this year, according to a report in AI Technology Review.Wen Lan associates images and video with text. The learning algorithm enabled the researchers to train the model on 50 million image/text pairs containing a high percentage of negative examples: data labeled according to what it is not. The model outperformed CLIP on a text-image retrieval task and beat the previous top scorer on AIC-ICC image captioning by 5 percentage points.Wen Hui, with 11.3 billion parameters, is a text-generation model pretrained for general language skills. The team has spun out applications that write poetry, generate videos, and generate images from text prompts (shown above).Wen Su, based on BERT, is trained to predict the shapes of biomolecules including proteins and DNA from human blood cells and drug-resistant bacteria.The project also includes FastMoE, a method for training models with more than 1 trillion parameters, along with a 2 terabyte Chinese-language database. Behind the news: The Beijing Academy of Artificial Intelligence was founded in 2018 to help the Chinese government achieve its goal of becoming the global center of AI. Its other projects include research into the cognitive roots of neural networks, a proposal for standardized AI notation, and a program to develop AI-specific computer chips. Why it matters: This effort reflects China’s growing confidence and capability in AI. Such ambitious projects could also curb China’s AI brain drain, as many of its most talented engineers wind up leaving for work overseas. We’re thinking: AI has long been dominated by organizations clustered in a few geographic hotspots. China’s effort to shift AI’s center of gravity away from the west could have far-reaching repercussions into the types of systems that get built and how they’re deployed.", "image_caption": "CogView home website", "metadata": {"article_id": "issue_87", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-16.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-87/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_87.html"}}
{"id": 15252301005, "type": "news_chunk", "title": "Robots Supervise Robots, Bad Labels Plague Datasets, Large", "subtitle": "Motion Mapper", "content": "In some animated games, different characters can perform the same actions — say, walking, jumping, or casting spells. A new system learned from unlabeled data to transfer such motions from one character to another. What’s new: Cinjon Resnick at New York University and colleagues at Nvidia, Technical University of Berlin, and Google developed a system designed to isolate changes in the pose of a two-dimensional figure, or sprite, and apply them to another sprite. While earlier approaches to solving this problem require labeled data, the new system is self-supervised. Key insight: A 2D animation consists of three elements: a sprite, the sprite’s motion and any special effects, and a background (which remains static in this work). Separate neural networks optimizing a variety of loss terms can learn to disentangle these elements, compute their changes from frame to frame, and recombine them to produce a novel frame. How it works: The system comprises four convolutional neural networks: two encoders, a transformation network, and a decoder. It generates a new frame given an image of a target sprite, a background, and two frames of animation showing a source sprite in motion — say, the initial frame and the one showing the pose, position, or other attributes to be mapped onto the target. During training, the images of the target sprite, background, and first frame of the animation were identical. The training and test sets consisted of several hundred animated video game characters performing various motions. One encoder generated a representation of the background based on the background reference image. The other generated separate representations of the target sprite and two animation frames.The transformation network used the representations of the two animation frames to generate a matrix describing how the sprite changed. The authors combined the various representations by multiplying the matrix by the target sprite’s representation and adding the background representation.The decoder used the result to produce an image of the target sprite, against the background, in the source sprite’s position in the second animation frame.The authors trained these components at once using a loss function consisting of three terms. The first term encouraged the background representation to remain constant from frame to frame. The second encouraged the transformed representation of the target sprite — that is, the transformation network’s matrix multiplied by the initial target sprite representation — to be similar to that of the source sprite in the second animation frame. The third minimized the pixel difference between the generated image and the second animation frame. Results: The authors compared their system with Visual Dynamics. It underperformed the competition, achieving a mean squared error of ~20 versus ~16 — but Visual Dynamics is a supervised system that requires labeled training data. Why it matters: A collection of networks that study different aspects of a dataset, and then compare and combine the representations they generate, can yield valuable information when labels aren’t available. We’re thinking: Possibly a useful tool for animators. Definitely a new toy for remix culture.", "image_caption": "System designed to isolate changes in the pose of a two-dimensional figure", "metadata": {"article_id": "issue_87", "chunk_index": 5, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-17.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-87/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_87.html"}}
{"id": 88404900001, "type": "news_chunk", "title": "Top AI Startups, Muting Griefers, Re-Creating Lost Masterpieces", "subtitle": "Haters Gonna [Mute]", "content": "Dear friends, Last Sunday was my birthday. That got me thinking about the days leading to this one and those that may lie ahead.As a reader of The Batch, you’re probably pretty good at math. But let me ask you a question, and please answer from your gut, without calculating.How many days is a typical human lifespan? 20,000 days100,000 days1 million days5 million days When I ask friends, many choose a number in the hundreds of thousands. (Many others can’t resist calculating the answer, to my annoyance!)When I was a grad student, I remember plugging my statistics into a mortality calculator to figure out my life expectancy. The calculator said I could expect to live a total of 27,649 days. It struck me how small this number is. I printed it in a large font and pasted it on my office wall as a daily reminder. That’s all the days we have to spend with loved ones, learn, build for the future, and help others. Whatever you’re doing today, is it worth 1/30,000 of your life?Let’s make every day count. Keep learning! P.S. Don’t worry about me. I’m healthy and plan to stick around for awhile. P.P.S. A huge thank-you to everyone who responded to my earlier online note about my birthday! ❤️ A new tool aims to let video gamers control how much vitriol they receive from fellow players. What’s new: Intel announced a voice recognition tool called Bleep that the company claims can moderate voice chat automatically, allowing users to silence offensive language. The system is in beta-test and scheduled for release later this year. How it works: Chip maker Intel worked with Spirit AI, which develops technology for content moderation, to let users of voice chat fine-tune how much of specific types of offensive language can reach their ears. Bleep combines speech detection technology with Spirit’s flagship product, which determines whether a phrase constitutes harassment in the context of surrounding chatter.The system classifies offensive speech in nine categories including misogyny, sexually explicit language, and anti-LGBTQ hate speech. Users can opt to filter out none, some, most, or all content in any category. For a tenth category called N-word, the system offers an on/off switch.It runs on Windows PCs and, since it interacts directly with Windows’ audio controls, it can work with a variety of voice-chat apps. Behind the news: ToxMod also aims to moderate video game voice chat and provides a dashboard for human moderators to track offensive speech across servers. Hive’s system is designed to moderate audio, video, text, and images. Its customers include Chatroulette, which uses Hive’s technology to help users avoid unwanted nudity. Two-Hat’s text-moderation system detects efforts to subvert moderation by, say, intentionally misspelling slurs and other potentially offensive language. Why it matters: There’s a clear need for tools that help people enjoy networked communications without being targeted by abuse. Twenty-two percent of U.S. online gamers stopped playing certain games after experiencing verbal harassment, according to a survey by the Anti-Defamation League. We’re thinking: For those whose first thought is, “Censorship!,” note that users will control this auto-moderation capability locally. At the same time, there’s a fine line between blocking harassment and shutting out perspectives we don't currently share. In an ideal world, players would take it upon themselves to keep their conversations civil. Until that day comes, AI will play a valid — if worrisome at times — role.", "image_caption": "Voice recognition tool \"Bleep\" working", "metadata": {"article_id": "issue_88", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-6.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-88/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_88.html"}}
{"id": 88404900002, "type": "news_chunk", "title": "Top AI Startups, Muting Griefers, Re-Creating Lost Masterpieces", "subtitle": "Cream of the Startup Crop", "content": "AI startups continue to roared ahead, global pandemic or no. What’s new: Tech industry analyst CB Insights published its fifth annual list of the 100 most promising private AI companies. What they found: The list of 100 was drawn from over 6,000 contenders based on measures including number and type of investors, R&D activity, news sentiment analysis, and competitive landscape. (Disclosure: Landing AI, where Andrew is CEO, is on the list.) Just over half of the companies selected provide services such as machine learning operations (MLOps) that feed tech’s appetite for AI. The rest cater to 18 other industries, mostly healthcare, transportation, retail services, and logistics.Collectively, they’ve raised $11.7 billion since 2010. The most richly funded entries include Chinese chipmaker Horizon Robotics ($1.6 billion), American autonomous driving company Aurora ($1.16 billion), and Chinese self-driving outfit Momenta ($783 million). More than a dozen are valued at more than $1 billion.Many of the companies are still in early stages. Over a third haven’t made it past Series A funding.Sixty-four companies on the list are based in the U.S. The UK has eight, and China and Israel have six each. Whatever happened to . . . : Twenty-one companies from last year’s list made it to this year’s. Three of last year’s cohort had successful IPOs, one went public outside regular investment channels, and two were acquired. All are still in business. Why it matters: In the midst of massive global economic turmoil, the AI industry continues to prosper. But, while AI’s impacts are global, U.S. companies continue to scoop up most of the rewards. We’re thinking: Building companies is hard. To quote Theodore Roosevelt, credit should be given to the person “who is actually in the arena, whose face is marred by dust and sweat and blood.” To everyone working on a startup, we wish you success!", "image_caption": "Annual list of the most promising private AI companies in different fields", "metadata": {"article_id": "issue_88", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-7.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-88/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_88.html"}}
{"id": 88404900003, "type": "news_chunk", "title": "Top AI Startups, Muting Griefers, Re-Creating Lost Masterpieces", "subtitle": "Crouching Beggar, Hidden Painting", "content": "Neural networks for image generation don’t just create new art — they can help recreate works that have been lost for ages. What’s new: Oxia Palus, a UK startup dedicated to resurrecting lost art through AI, combined deep learning and 3D printing to reproduce a painting that had been hidden beneath one of Pablo Picasso’s works. How it works: In 2018, researchers used an x-ray technique to reveal that Picasso had painted his “Crouching Beggar” on top of another artwork. Art experts believe the underlying composition, which depicts a park in Barcelona, was painted by Picasso’s contemporary Santiago Rusiñol. Anthony Bourached and George Cann, the company’s CEO and CTO respectively, manually derived a black-and-white outline of the hidden painting from the x-ray image.They divided the outline into built structures, greenery, and sky. For each component, they chose an existing Rusiñol painting and used a style transfer method to map its style to the relevant areas, producing a composite generated image.To capture the topography of the artist’s brushstrokes, they mapped the style of a Rusiñol work painted via the impasto technique, in which paint is applied thickly so that color correlates well with height, onto the generated image. They made a grayscale version of the output to produce a heightmap.Working with a specialty 3D printing company, they used the generated image and heightmap to produce a facsimile painting, taking extra care not to deform the canvas beneath layers of paint. The process embeds a unique code in each print as a copy-protection measure.Oxia Palas plans to sell 100 copies of the resulting print titled “Parc Del Laberint D’horta” ($11,111.11, NFT included). Behind the news: The team used an earlier version of its style-transfer method to recreate a portrait of a woman hidden beneath Picasso’s “The Old Guitarist.” More recently, it trained a conditional generative adversarial network on 225 paintings made by Leonardo da Vinci and his students to recreate painted-over portions of da Vinci’s “Virgin on the Rocks.” Why it matters: The false starts and abandoned ideas hidden under later works of art can offer valuable glimpses into a painter’s creative process. This combination of style transfer and 3D printing reveals what might have been. We’re thinking: It’s fitting that Picasso, who revolutionized art in the 20th century, is providing inspiration for a new, AI-powered avant garde.", "image_caption": "X-ray technique reproducing a painting that had been hidden beneath one of Pablo Picasso’s works", "metadata": {"article_id": "issue_88", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-8.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-88/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_88.html"}}
{"id": 88404900004, "type": "news_chunk", "title": "Top AI Startups, Muting Griefers, Re-Creating Lost Masterpieces", "subtitle": "Toward Better Video Search", "content": "Video search engines are often evaluated based on how they rank a single video when presented with a brief description that accompanies that video in the test set. But this criterion may not reflect a system's utility in the real world, where numerous videos may be highly relevant to the search terms. New work aims to solve this problem.What’s new: Researchers at the University of Bristol led by Michael Wray propose a new benchmark, Semantic Similarity Video Retrieval (SVR), that evaluates video retrieval systems by their ability to rank many similar videos. They also built a system that performed well on it.Key insight: To evaluate a video retrieval system based on how similar the top-ranked videos are to an input description, the evaluation process needs a ground-truth measure of similarity between descriptions and videos. There isn’t an automatic way to compare a description to a video, but there are several ways to compare a description to other descriptions. The authors assessed the similarity between existing descriptions to approximate ground-truth similarity between descriptions and videos. This enabled them to train their system to rank the similarity of input text to a variety of videos, and to evaluate the quality of its search results.How it works: The authors generated separate representations for captions and videos and honed the similarity of matching descriptions and videos. Given a description, the system learned to rank clips whose video representation best matched that of the input (and vice-versa). They trained and tested it on videos with descriptions from movies, news, how-tos, and other sources. The authors calculated similarity between each description and every other description using METEOR. If the similarity between two descriptions exceeded a threshold, they matched the description with the video bearing the other caption.They used these matches to train a system that included a GPT-based language model, which generated representations of descriptions, and a combination of convolutional neural networks, which generated representations of videos. A triplet loss encouraged the system to produce similar representations of matched descriptions and videos and dissimilar representations of unmatched ones.Given input text (for the purpose of evaluation, an existing description), they ranked the top-matching videos according to the cosine similarity between the representations of the text and the representation of the videos. Results: The authors measured how well their system ranked each video with respect to every description (and vice-versa) using nDCG. This method rewards high rankings of similar representations (as measured by METEOR) and penalizes high rankings of dissimilar representations. The authors’ system scored 0.840 out of a perfect 1.0. A baseline system that used two vanilla neural networks to create video and description embeddings scored .833.Why it matters: Rather than designing a system to ace a common test, the authors devised a new test that better reflects what users expect from such systems. That approach should lead to more useful systems all around.We’re thinking: The more machine learning improves, the more we need benchmarks that are capable of measuring the latest improvements.", "image_caption": "Semantic Similarity Video Retrieval (SVR) working", "metadata": {"article_id": "issue_88", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-9.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-88/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_88.html"}}
{"id": 43934345001, "type": "news_chunk", "title": "Europe's AI Backlash, Robot Debater, Car Wreck Recognition", "subtitle": "The Coming Crackdown", "content": "Dear friends, How much data do you need to collect for a new machine learning project? If you’re working in a domain you’re familiar with, you may have a sense based on experience or from the literature. But when you’re working on a novel application, it’s hard to tell. In this circumstance, I find it useful to ask not how much data to collect but how much time to spend collecting data.For instance, I’ve worked on automatic speech recognition, so I have a sense of how much data is needed to build this kind of system: 100 hours for a rudimentary one, 1,000 hours for a basic one, 10,000 hours for a very good one, and perhaps 100,000-plus hours for an absolutely cutting-edge system. But if you were to give me a new application to work on, I might find it difficult to guess whether we need 10 or 10,000 examples.When starting a project, it’s useful to flip the question around. Instead of asking, How many days do we need to collect m training examples?I ask,How many training examples can we collect in d days? Taking a data-centric approach to model development, let’s say it takes about two days to train a model and two days to perform error analysis and decide what additional data to collect (or how to tweak the model). How many days should you spend collecting data before training and error analysis? Allocating comparable amounts of time to each step seems reasonable, so I would advocate budgeting a couple of days — a week at most — for data collection. Then iterate through the loop. I’ve seen many teams spend far too much data collecting data before jumping into the model development loop. I’ve rarely seen a team spend too little time. If you don’t collect enough data the first time around, usually there’s time to collect more, and your efforts will be more focused because they’ll be guided by error analysis.When I tell a team, “Let’s spend two days collecting data,” the time limit often spurs creativity and invention of scrappy ways to acquire or synthesize data. This is much better than spending two months collecting data only to realize that we weren’t correcting the right data (say, the microphone we used was too noisy, leading to high Bayes/irreducible error).So, next time you face an unfamiliar machine learning problem, get into the model iteration loop as quickly as possible, and set a limited period of time for collecting data the first time around, at least. You’re likely to build a better model in less time.Keep learning! P.S. Once I created an unnecessarily scramble when asked a team to make sure that data collection took no longer than two days. Because of a bad Zoom connection, they thought I said “today.” Now I've learned to hold up two fingers whenever I say “two days” on a video call. The European Union proposed sweeping restrictions on AI technologies and applications. What’s new: The executive arm of the 27-nation EU published draft rules that aim to regulate, and in some cases ban, a range of AI systems. The proposal is the first to advance broad controls on the technology by a major international body. What it says: The 100-plus page document divides AI systems into three tiers based on their level of risk. The definition of AI includes machine learning approaches, logic-based approaches including expert systems, and statistical methods. The rules would forbid systems deemed to pose an “unacceptable” risk. These include real-time face recognition, algorithms that manipulate people via subliminal cues, and those that evaluate a person’s trustworthiness based on behavior or identity.The “high risk” category includes systems that identify people; control traffic, water supplies and other infrastructure; govern hiring, firing, or doling out essential services; and support law enforcement. Such systems would have to demonstrate proof of safety, be trained using high-quality data, and come with detailed documentation. Chatbots and other generative systems would have to let users know they were interacting with a machine.For lower-risk applications, the proposal calls for voluntary codes of conduct around issues like environmental sustainability, accessibility for the disabled, and diversity among technology developers.Companies that violate the rules could pay fines of up to 6 percent of their annual revenue. Yes, but: Some business-minded critics said these rules would hinder innovation. Meanwhile, human rights advocates said the draft leaves loopholes for applications that are nominally prohibited. For example, face recognition is prohibited only if it’s conducted in real time; it could still be used on video captured in the past. Behind the news: Governments worldwide are moving to regulate AI. The U.S. Federal Trade Commission last week signaled its intent to take legal action against companies that make biased systems. A number of other countries including Australia, China, Great Britain, and India have enacted laws aimed at reining in big tech companies. Why it matters: The EU’s AI proposal is the spiritual successor to its 2018 privacy law, the General Data Protection Regulation (GDPR). That law sparked a global trend as Brazil, China, India, and other countries proposed or enacted laws to protect user data. The new plan could have a similar impact. We’re thinking: Despite its flaws, the GDPR drew a line in the sand and advanced the conversation about uses of personal data. While this new set of rules is bound to provoke criticism —some it valid, no doubt — we welcome moves to promote regulation around AI and look forward to a spirited, global discussion.", "image_caption": "Forbidden sign over security cameras, handprint and face recognition system", "metadata": {"article_id": "issue_89", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-89/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_89.html"}}
{"id": 43934345002, "type": "news_chunk", "title": "Europe's AI Backlash, Robot Debater, Car Wreck Recognition", "subtitle": "Wreck Recognition", "content": "Automobile insurers are increasingly turning to machine learning models to calculate the cost of car repairs. What’s new: The pandemic has made it difficult for human assessors to visit vehicles damaged in crashes, so the insurance industry is embracing automation, Wired reported. How it works: When drivers get into an accident, insurance companies direct them to download an app that guides them through documenting the effects. These systems are particularly good at assessing damage from minor collisions and determining when a car has been totaled. Such apps classify damage using a model trained on crash photos of a variety of makes and models. The app determines whether the damaged part needs to be inspected by a human. If not, it analyzes what needs to be fixed and estimates a repair cost using data from local mechanics and parts suppliers. Then a human adjustor reviews the model’s work.Tractable, which makes such software, says its system correctly estimates 25 percent of cases without human intervention.CCC Information Services, which makes an app called Smart Estimate, claims that adjusters who use its system are 30 percent more productive.Such models are particularly good at assessing minor damage and determining when a car has been totaled. Yes, but: Several body shop owners said that automated estimates weren’t accurate and often failed to spot hard-to-see damage such as a misaligned frame. Bad estimates resulted in substandard repairs and delays as mechanics haggled with insurance companies for more money. Why it matters: Smart damage-assessment apps can inspect vehicles far more quickly than a human who examines the damage first-hand. Accurate output helps insurance companies save money and drivers settle claims more quickly. We’re thinking: Will self-driving cars that get into a fender bender use an app to assess the damage?", "image_caption": "Tractable app determining the cost of a car's damage", "metadata": {"article_id": "issue_89", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-1.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-89/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_89.html"}}
{"id": 43934345003, "type": "news_chunk", "title": "Europe's AI Backlash, Robot Debater, Car Wreck Recognition", "subtitle": "Up for Debate", "content": "IBM’s Watson question-answering system stunned the world in 2011 when it bested human champions of the TV trivia game show Jeopardy! Although the Watson brand has fallen on hard times, the company’s language-processing prowess continues to develop. What’s new: Noam Slonim led a team at IBM to develop Project Debater, which is designed to compete in formal debates. Key insight: A debate opens with four-minute opening statements by both sides followed by rounds of rebuttals and finally closing statements. To perform well, a debater must quickly prepare arguments supported by evidence, address competing arguments, and organize statements logically — a set of tasks too diverse for an end-to-end system. Instead, the team built a pipeline of independent components, each a complex system in its own right. How it works: Project Debater receives a motion to argue in favor of or against. Then it’s off to the races finding facts, arguments, and counterarguments and stitching them together into speeches. The argument mining component searches the 400 million articles in LexisNexis for relevant opinions and extracts evidence that backs or refutes them. A model based on a gated recurrent unit (a type of recurrent neural network) in conjunction with an SVM classifies whether an opinion supports or opposes the motion.The argument knowledge base is a compendium of arguments, quotes, and analogies grouped into thematic classes. The system classifies the theme of the motion it’s arguing to find relevant arguments, both supporting and opposing. Claims are linked to counterclaims, so the system can rebut common opposing arguments and avoid concurring accidentally.A rebuttal module turns an opponent’s speech into text using Watson Speech to Text. It compares the opponent’s arguments with those discovered by the earlier components using a combination of models including LSTMs, hand-written rules, and logistic regression. It uses the most relevant argument to form a rebuttal.The debate construction component clusters arguments based on their theme. A rule-based system filters out similar arguments, picks the best paragraphs, and organizes them into a speech. Finally, a text-to-speech service synthesizes audio output. Results: Project Debater is the first system of its kind, and no established benchmark exists to evaluate it. The researchers compared the quality (judged by humans on a scale of one to five) of the system’s opening statement with a speech on the same topic generated by a GPT-2 pretrained on a large text corpus and fine-tuned on speeches. Project Debater achieved an average score of 4.1, far outperforming the fine-tuned GPT-2’s score of 3.2. Yes, but: Project Debater lost a 2019 competition with debate champion Harish Natarajan — albeit narrowly. Why it matters: Building a system that can beat humans at competitive debate isn’t a multi-decade, multi-team project like winning at chess or Go, but it’s a substantial endeavor. So far, Project Debater has generated over 50 papers and spawned the subfields in claim detection and evidence detection. We’re thinking: The AI community is embroiled in its own debates, including an annual event in Montreal. Maybe this system can participate next time around?", "image_caption": "Diagram showing how Project Debater works", "metadata": {"article_id": "issue_89", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-3.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-89/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_89.html"}}
{"id": 43934345004, "type": "news_chunk", "title": "Europe's AI Backlash, Robot Debater, Car Wreck Recognition", "subtitle": "Boosting Biomedicine", "content": "The U.S. government aims to turbocharge biomedical AI research. What’s new: The National Institutes of Health, which invests $41.7 billion annually in medical research, announced a program called Bridge to Artificial Intelligence (Bridge2AI) to promote machine learning in human biology and medicine. Take it to the bridge: The program’s primary goal is to develop new datasets. It also aims to standardize data from different sources and develop automated tools to help create datasets and ensure that they adhere to FAIR principles, which aim to enable machines to use data with little human intervention. Bridge2AI will fund research into two areas: Creating datasets geared toward solving research “grand challenges” such as understanding how genes respond to disease, modeling physiological movement, and monitoring biological processes that lead from illness to health.Establishing an administration center for Bridge2AI projects. The center will focus on developing best practices to meet grand-challenge goals in areas like teamwork, ethics, standards, tool optimization, and workforce development.The NIH will begin accepting applications in June and will award funds the following spring. Behind the news: U.S. government agencies bringing AI into mainstream healthcare. A recent study estimated that U.S. regulators have approved 222 AI-powered medical devices.The Food and Drug Administration released a plan to update its medical-device regulations for machine learning.The Centers for Disease Control and Prevention uses machine learning to forecast annual flu outbreaks. Last year it deployed a chatbot to help screen people for Covid-19 infections. Why it matters: Bigger, better datasets specially designed for machine learning could help illuminate human biological processes and the diseases that disrupt them. We’re thinking: AI for medicine has tremendous potential. Datasets designed specifically to help us realize that potential may be just what the doctor ordered.", "image_caption": "Timeline for biomedical AI projects", "metadata": {"article_id": "issue_89", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/image-4.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-89/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_89.html"}}
{"id": 12470561001, "type": "news_chunk", "title": "TensorFlow Versus PyTorch, Autonomous Drone Races, State-of-art", "subtitle": "Autonomous Drones Ready to Race", "content": "Dear friends, I just replaced my two-year-old phone with a new one and figured out how to take long-exposure photos of Nova even while she’s asleep and the lights are very low. This piece of technology brought me a surprising amount of joy! I wrote about ethics last week, and the difficulty of distilling ethical AI engineering into a few actionable principles. Marie Kondo, the famous expert on de-cluttering homes, teaches that if an item doesn’t spark joy, then you should throw it out. When building AI systems, should we think about whether we’re bringing joy to others? This leaves plenty of room for interpretation. I find joy in hard work, helping others, increasing humanity’s efficiency, and learning. I don’t find joy in addictive digital products. I don’t expect everyone to have the same values, but perhaps you will find this a useful heuristic for navigating the complicated decision of what to work on: Is your ML project bringing others joy? This isn’t the whole answer, but I find it a useful initial filter. Keep learning! Pilots in drone races fly souped-up quadcopters around an obstacle course at 120 miles per hour. But soon they may be out of a job, as race organizers try to spice things up with drones controlled by AI.What’s new: The Drone Racing League, which stages contests to promote this so-called sport of the future, recently unveiled an autonomous flier called RacerAI. The new drone includes Nvidia’s Jetson AGX Xavier inference engine, four stereoscopic cameras, and propellers that deliver 20 pounds of thrust.What’s happening: RacerAI serves as the platform for AI models built by teams competing in AlphaPilot, a competition sponsored by the DRL and Lockheed Martin. 420 teams entered and tested their models on a simulated track.Virtual trials whittled the teams down to nine, which will compete in four races throughout fall 2019.Team USRG from Kaist University in South Korea won the first race on October 8. The second is scheduled for November 2 in Washington D.C.The series winner will take a $1 million prize. In early 2020, that model will face a top-rated human pilot for an additional $250,000 purse. Behind the news: Drone Racing League pilots use standardized drones built and maintained by the league, and train on the same simulator used to train RacerAI. Races are typically a mile long and take place in event spaces across the U.S. and Europe.Why it matters: Drone racing is fun and games, but the skills learned by autonomous racing models could be transferable to real-world applications like automated delivery.We’re thinking: A recent DRL video shows that current models have a way to go before they graduate from passing through rings to making high-speed maneuvers. Human pilots still have a significant edge — for now.", "image_caption": "Drone race", "metadata": {"article_id": "issue_9", "chunk_index": 1, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/drone20gif20sized.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-9/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_9.html"}}
{"id": 12470561002, "type": "news_chunk", "title": "TensorFlow Versus PyTorch, Autonomous Drone Races, State-of-art", "subtitle": "High Accuracy, Low Compute", "content": "As neural networks have become more accurate, they’ve also ballooned in size and computational cost. That makes many state-of-the-art models impractical to run on phones and potentially smaller, less powerful devices. A new technique makes convolutional neural networks much less computationally intensive without significantly degrading their performance. What’s new: Zhonghui You and colleagues at Peking University and Momenta, an autonomous-vehicle startup, propose a way to remove parameters that aren’t critical to a model’s performance: Gate Decorator.Key insight: The new technique removes functional groups of parameters (specifically convolutional filters), rather than individual parameters.How it works: Gate Decorator adds to the model a scaling factor that represents each filter’s importance to the model’s output. It ranks filters by their impact on the model’s loss function. Then it removes the least effective ones. The model processes a subset of training data to learn the scaling factor’s value for each filter. The original model’s parameters retain their existing values.The scaling factors are randomly initialized. For each filter, the model is encouraged to learn the smallest scaling factor that, multiplied by the filter’s output, takes the least toll on performance.A user-specified fraction of filters with the smallest scaling factor are deleted. The pruned network is fine-tuned on the entire training set.The process is repeated for a user-defined number of iterations. Results: The researchers compared the accuracy and computational cost of original and pruned networks. Gate Decorator cut the computational cost of an ImageNet-trained ResNet by 55 percent and a CIFAR-trained ResNet by 70 percent. Accuracy for these models decreased by 0.67 percent and increased by 0.03 percent, respectively. That’s state-of-the-art accuracy for such a reduction in computational cost.Why it matters: Unlike most weight-pruning techniques, Gate Decorator’s efficiency gains are straightforward to achieve in practice, not just in theory. A model shorn of filters can still run existing algorithms, while removing weights from a densely connected neural network ultimately requires specialized algorithms that we don’t yet have.We’re thinking: A pruning method like this might work with other parameter groupings to cut the computational demand of architectures beyond CNNs. The resulting models could be further compressed using other methods such as quantization.", "image_caption": "An illustration of filter pruning", "metadata": {"article_id": "issue_9", "chunk_index": 2, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/pruning2.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-9/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_9.html"}}
{"id": 12470561003, "type": "news_chunk", "title": "TensorFlow Versus PyTorch, Autonomous Drone Races, State-of-art", "subtitle": "Clash of the Frameworks", "content": "Most deep learning applications run on TensorFlow or PyTorch. A new analysis found that they have very different audiences.What’s new: A researcher at Cornell University compared references to TensorFlow and PyTorch in public sources over the past year. PyTorch is growing rapidly within the research community, while TensorFlow maintains an edge in industry, according to a report in The Gradient. (deeplearning.ai, which publishes The Batch, provides the TensorFlow Specialization course available on Coursera.)Findings: Horace He used proxy data to determine whether users were from the research or business community. To represent the research community, he surveyed abstracts submitted to five top AI conferences in 2018. He found an average increase of 275 percent in researchers using PyTorch, and an average decrease of roughly 0.5 percent for TensorFlow, over the year.To track business users, he analyzed 3,000 job listings. Businesses looking for experience in TensorFlow outnumbered those asking for experience in PyTorch. He also surveyed articles on LinkedIn and found a ratio of 3,230 to 1,200 in favor of TensorFlow.TensorFlow also outnumbered PyTorch in terms of GitHub stars used by coders to save repositories for later use. He considers this a key metric for tracking projects in production. Competitive strengths: TensorFlow has a large, well established user base, and industry is typically slower to pick up on new technologies.TensorFlow is much more efficient than PyTorch. Even modest savings in model run times can help a company’s bottom line.PyTorch integrates neatly with Python, making the code simple to use and easy to debug.According to He, many researchers prefer PyTorch’s API, which has remained consistent since the framework’s initial release in 2016. We’re thinking: If there is to be a reckoning between the two top frameworks, it could happen soon. The newly released TensorFlow 2.0 adds many of the benefits PyTorch users love, particularly Python integration and making Eager mode the default for execution. However, deep learning is driven largely by research, so today’s students may bring PyTorch with them as they trickle into the job market.", "image_caption": "Tensorflow and Pytorch logos", "metadata": {"article_id": "issue_9", "chunk_index": 3, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TF-PT2.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-9/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_9.html"}}
{"id": 12470561004, "type": "news_chunk", "title": "TensorFlow Versus PyTorch, Autonomous Drone Races, State-of-art", "subtitle": "Power of Babel", "content": "More than 900 indigenous languages are spoken across the Americas, nearly half of all tongues in use worldwide. A website tracks the growing number of resources available for natural language processing researchers interested in studying, learning from, and saving these fading languages.What’s happening: Naki collects NLP efforts involving indigenous American languages.What’s inside: Researchers at the National Autonomous University of Mexico noticed a distinct rise in NLP papers focused on Native American languages over the past five years. They organized all the papers and research tools they could find. The researchers found NLP tools for 35 languages.North American languages receive the most research attention, despite having fewer speakers on average than those in Mesoamerica and South America.Native American tongues offer a diversity of dialects within an individual language. That makes it difficult for NLP models to develop standardized dictionaries and syntaxes. Behind the news: Language families are linguistic groupings with similar origins and closely related syntax and definitions, such as Indo-European, or Sino-Tibetan. The Americas are home to more than 70 such groups, according to some researchers. Why it matters: The resources collected on Naki are part of a growing effort to apply NLP to less common languages. The effort poses fundamental research problems. Like other rare tongues, Native American languages suffer from small written datasets — while NLP is very data-hungry — as well as broad variation from speaker to speaker and high complexity. For example, like Mandarin, many languages from Central Mexico shift vocal pitch to give identical words different meanings. NLP could benefit immeasurably by solving these problems.We’re thinking: While it’s valuable to study rare languages for their own sake, there’s a huge opportunity in giving people who rely on them access to capabilities that much of the world takes for granted: voice recognition, speech to text, automatic translation, and the like. The usual techniques won’t get us there, but working with these languages could lead researchers to the necessary breakthroughs.", "image_caption": "Map of Northamerica showing different indigenous languages by location", "metadata": {"article_id": "issue_9", "chunk_index": 4, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/languages2.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-9/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_9.html"}}
{"id": 12470561005, "type": "news_chunk", "title": "TensorFlow Versus PyTorch, Autonomous Drone Races, State-of-art", "subtitle": "Two Steps to Better Summaries", "content": "Summarizing a document using original words is a longstanding problem for natural language processing. Researchers recently took a step toward human-level performance in this task, known as abstractive summarization, as opposed to extractive summarization consisting of sentences drawn from the input text. “We present a method to produce abstractive summaries of long documents,” their abstract reads — quoting words generated by the model they propose.What’s new: Rather than generating abstractive summaries directly, researchers from Element AI and Montreal Institute for Learning Algorithms started with an extractive summary that guides the generated language.Key insight: Providing an extractive summary along with source text can help a pre-trained language model generate a higher-quality abstractive summary.How it works: Summarization proceeds in two steps: extraction and abstraction. The researchers trained a neural network to identify the most important sentences in a document. In essence, they assign a real-valued score to each sentence based on relationships among all sentences (in terms of content and style, for example). The highest-scoring sentences form an extractive summary.A GPT-like architecture, trained on ground-truth abstractive summaries, generates an abstractive summary by predicting words in a sequence. The model receives the extractive summary after the source document, so the summary has greater influence over its output. Results: The authors tested four corpora, all of which include human-written summaries: arXiv (research papers), PubMed (medical research papers), bigPatent (patent documents) and Newsroom (news articles). The authors compared summarization quality using ROUGE scores, which capture the overlap between generated and ground-truth summaries. For three out of the four datasets, the proposed method achieved state-of-the-art summarization quality without copying entire sentences from the input. Extractive summarization models yielded the best ROUGE scores for the Newsroom corpus.Why it matters: The ability to generate high-quality abstractive summaries could boost worker productivity by replacing long texts with concise synopses.We’re thinking: Yikes! We hope this doesn’t put The Batch team out of a job.", "image_caption": "Proposed model for abstractive summarization of a scientific article", "metadata": {"article_id": "issue_9", "chunk_index": 5, "image_url": "https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-2720at2012.44.4820PM.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-9/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_9.html"}}
{"id": 43816555001, "type": "news_chunk", "title": "Rise of the Robocoders, Banks Embrace Face Recognition...", "subtitle": "Robocoders", "content": "Dear friends, It can take 6 to 24 months to bring a machine learning project from concept to deployment, but a specialized development platform can make things go much fasterMy team at Landing AI has been working on a platform called LandingLens for efficiently building computer vision models. In the process, I’ve learned important lessons about how such platforms can help accelerate the machine learning project lifecycle: Data collection: Ambiguity in labels (what is the “correct” value of y?) plagues many projects. If the labels are inconsistently defined, it’s impossible to achieve a high test-set accuracy. But it’s difficult to find these inconsistencies manually and to convince stakeholders (often subject-matter experts) to resolve them. An MLOps platform can identify problems and encourage consistency.Model training: The ability to write code to train a model in TensorFlow or PyTorch is a valuable skill. But even for skilled engineers, it’s faster to use a no-code platform that lets you do this via mouse clicks (to manage data augmentation, link the data and model, manage GPU training resources, keep track of data/model versions, and provide visualizations and metrics for error analysis).Production deployment: Many teams can execute a successful proof of concept and achieve high-test set accuracy. But to secure budgets and approval for deployment, a small demo can help others see a project’s value. A platform can make it easy to implement a demo that runs not just in a Jupyter notebook but in a lightweight deployment environment such as a mobile app or simple edge device. It used to take me months to deploy a model. With a no-code platform, I can train a RetinaNet demo, carry out error analysis, use a data-centric approach to clean up inconsistent data, retrain, and deploy to an edge device — all in 60 minutes. I get a thrill every time I go through the machine learning project lifecycle so quickly.Platforms like this can help a variety of AI projects across all industries. LandingLens works well for visual inspection in areas as diverse as automotive, semiconductor, and materials, I’m hoping to make it more widely available. Its sweet spot is computer vision problems (detection or segmentation) with 30 to 10,000 images. If you have a business problem in computer vision that falls in this sweet spot, I’d like to hear from you. Please get in touch by filling out this form. Keep learning! Language models are starting to take on programming work. What’s new: SourceAI uses GPT-3 to translate plain-English requests into computer code in 40 programming languages. The French startup is one of several companies that use AI to ease coding, according to Wired. How it works: Companies have trained language models to anticipate programmers’ needs. SourceAI, currently in beta test, enables users to describe the function they want, then select a programming language. Between 80 and 90 percent of code generated by the beta version works as intended, founder Furkan Bektes told The Batch. He plans to charge $0.04 to $0.10 per piece of code.GPT-3 also powers Debuild, which builds web applications like buttons and text input fields based on plain English descriptions.Belgian startup Tabnine has a GPT-2-powered tool that automatically suggests follow-on lines of code as programmers type. Behind the news: Other companies are also using machine learning to increase coders’ productivity and sniff out bugs. Facebook’s Aroma lets developers search code databases for snippets similar to whatever they’re working on.Intel’s Machine Inferred Code Similarity is a similar tool that compares pieces of code to determine their function.DeepMind published a model that rewrites human-generated code to make it run more efficiently. Why it matters: In the hands of a skilled programmer, such tools can save time, freeing up brainpower for more complex tasks. In the hands of the newbie, they make it possible to create applications with little experience and — with diligent attention — gain skills more quickly. We’re thinking: No AI system should replace a sacred rite of passage for neophyte coders: print (“Hello World!”).", "image_caption": "Animation of SourceAI working", "metadata": {"article_id": "issue_90", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/code_revised-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-90/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_90.html"}}
{"id": 43816555002, "type": "news_chunk", "title": "Rise of the Robocoders, Banks Embrace Face Recognition...", "subtitle": "Banking on Computer Vision", "content": "AI-powered surveillance is becoming a staple in U.S. banks. What’s new: Several banks are using cameras equipped with computer vision to bolster security and boost employee productivity, according to Reuters. What’s up: The companies have a variety of aims and approaches. JPMorgan Chase is testing systems from vendors including AnyVision and Vintra at several Chase branches in Ohio. The systems collect data on customer and employee behavior to improve staff scheduling and interior layouts, the company said.City National Bank of Florida plans to use face recognition at 31 branches to identify employees, customers, and eventually suspects on government watch lists.An unnamed bank in the southern U.S. uses such systems to alert employees to issues such as suspicious loiterers and open safes. Behind the news: The latest moves build on earlier attempts by financial institutions to take advantage of image recognition technology. Before settling on a private vendor, JPMorgan Chase put together its own system drawing on technology from Amazon Web Services, Google, and IBM.Bank of America bought AI-powered surveillance cameras in the early 2010s to catch people loitering in ATM kiosks.Wells Fargo in 2007 used CrimeDex, a crime-prevention network that reportedly offered “facial recognition technology and the ability to search videos such as ATM surveillance records” and listed “14,000 suspects,” to identify a thief who taken $400,000 from automated teller machines. Why it matters: If banks can get regulators and consumers to accept AI-assisted surveillance in branch offices, it will add momentum to wider adoption of the technology. We’re thinking: Many of these use cases seems more like surveillance than security. Without sufficient sensitivity to public concerns, such efforts is likely to inspire backlash. Organizations that aim to take advantage of this technology: Tread cautiously.", "image_caption": "Series of videos showing AI-powered surveillance inside a bank", "metadata": {"article_id": "issue_90", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/BANK576x324.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-90/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_90.html"}}
{"id": 43816555003, "type": "news_chunk", "title": "Rise of the Robocoders, Banks Embrace Face Recognition...", "subtitle": "Greener Machine Learning", "content": "A new study suggests tactics for machine learning engineers to cut their carbon emissions. What’s new: Led by David Patterson, researchers at Google and UC Berkeley found that AI developers can shrink a model’s carbon footprint a thousand-fold by streamlining architecture, upgrading hardware, and using efficient data centers. What they did: The authors examined the total energy used and carbon emitted by five NLP models: GPT-3, GShard, Meena, Switch Transformer, and T5. They reported separate figures for training and inference. Generally, they found that inference consumes more energy than training. The authors point to several model-design strategies that trim energy use. Transfer learning, for instance, eliminates the need to train new models from scratch. Shrinking networks through techniques such as pruning and distillation can increase energy efficiency by a factor of 3 to 7.Hardware makes a difference, too. Chips designed specifically for machine learning are both faster and more efficient than GPUs. For instance, a Google TPU v2 ran a transformer 4.3 times faster and used 1.3 times less energy than an Nvidia P100.Cloud computing centers with servers optimized for machine learning are twice as efficient as traditional enterprise data centers. Data centers using renewable energy sources are greener, and centers built near their energy source bring further savings, as transmitting energy over long distances is relatively expensive and inefficient. Behind the news: The authors joined the Allen Institute and others in calling for greener AI. To this end, MLCommons, the organization behind the MLPerf benchmark, recently introduced new tools to measure a model’s energy consumption alongside traditional performance metrics. Why it matters: Training and deploying a large model can emit five times more carbon dioxide than a single car over the course of its lifetime. As AI becomes more widespread, energy efficiency becomes ever more important. We’re thinking: There are bigger levers for reducing carbon emissions, such as transitioning the world away from coal power. Still, as a leading-edge industry, AI has an important role in building a the green future.", "image_caption": "Animation showing a methaphorical transition from AI to a green environment", "metadata": {"article_id": "issue_90", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/ezgif.com-gif-maker---2021-04-27T113917.099-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-90/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_90.html"}}
{"id": 43816555004, "type": "news_chunk", "title": "Rise of the Robocoders, Banks Embrace Face Recognition...", "subtitle": "3D Object Factory", "content": "In the open-ended video game Minecraft, players extract blocks of virtual materials from a 3D environment to assemble objects of their own design, from trees to cathedrals. Researchers trained neural networks to generate these structures. What’s new: Shyam Sudhakaran and researchers at University of Copenhagen, University of York, and Shanghai University used a neural cellular automaton algorithm to construct 3D objects. The work demonstrates the potential for such algorithms to generate structures in three dimensions, as typically they’re limited to two. Key insight: A cellular automaton generates complex patterns on a 2D grid by changing each cell’s state iteratively based on simple rules that depend on the states of its neighbors. A neural cellular automaton updates cells depending on the output of a neural network and the states of neighboring cells. Using 3D convolutions enables a neural cellular automaton to generate patterns in 3D. How it works: The authors trained several 3D convolutional neural networks to reproduce structures found on the community website Planet Minecraft. Each different structure required its own model. The structures comprised 50 block types mostly corresponding to materials (stone, glass, metals, and so on), including piston blocks that push or pull adjacent blocks to produce animated objects. The system spawned block types directly without needing to virtually mine them out of the virtual ground. The authors initialized a single block in a 3D grid.The network updated each cell in the grid depending on whether a neighboring cell was activated. The updates ran for a set number of steps, growing the structure at each step.The loss function encouraged the generated structure to match the original in block type and placement. Results: The authors reported few quantitative results. However, the trained models grew static structures like castles, temples, and apartments that appear to be accurate inside and out. One model learned to grow an animated caterpillar. Why it matters: Cellular automata may have certain benefits. For instance, if part of the resulting structure is destroyed, the automaton can use what’s left to regenerate the missing part. This approach can produce resilient digital 3D structures with no human intervention after the first step. We’re thinking: Machine learning engineers looking for an excuse to play Minecraft need look no further!", "image_caption": "Minecraft video capture", "metadata": {"article_id": "issue_90", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/05/ezgif.com-gif-maker---2021-04-20T095340.976-1.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-90/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_90.html"}}
{"id": 4139786001, "type": "news_chunk", "title": "Surgical Robots Go Autonomous, Virtual Reality On Speed, AI", "subtitle": "Medical AI Gets a Grip", "content": "Dear friends, I decided last weekend not to use a learning algorithm. Sometimes, a non-machine learning method works best.Now that my daughter is a little over two years old and highly mobile, I want to make sure the baby gate that keeps her away from the stairs is always shut. It’s easy to forget and leave it open when walking through. How do you do this?I started designing a system where I’d collect images of the gate both open and shut, and train a neural network to distinguish between the two. Then I would use TensorRT to deploy the model on a Raspberry Pi computer, which would beep if the gate were left open for more than 60 seconds. I got as far as wiring up the system. Then I found a refrigerator-door alert widget that does the same job by sensing when a magnet is separated from a detector. It goes to show that sometimes you don’t need a big neural network to do the job. (But when you do need one, it’s handy.) That’s why it’s nice to have a portfolio of techniques. Then we can better pick the right one for a given job. Perhaps one lesson here is to pick the right sensor: To do the job with a camera, I needed a computer vision algorithm. But with a magnetic sensor, making the decision to beep when the gate is left open becomes trivial. Keep learning! Surgical robots perform millions of delicate operations annually under human control. Now they’re getting ready to operate on their own. What’s new: Researchers at UC Berkeley, UC San Francisco, and SRI International trained a machine learning system to pilot a da Vinci two-armed surgical robot through a task that tested its dexterity, precision, and speed, The New York Times reported. How it works: The system learned via imitation learning to lift tiny plastic rings off a pegboard, pass them from one claw to the other, and slide them onto different pegs. The task is a exercise for surgeons learning to perform laparoscopic procedures, in which a camera and other specialized instruments are inserted into the patient’s body through a small incision. The authors trained an ensemble of four convolutional neural networks on 180 RGBD (red, green, blue, plus depth) video clips of human surgeons using the robot to demonstrate an error and how to correct it, as well as information about the robot’s joint positions. The system learned to perform the task, but its precision degraded over time as the cables that control the robot’s limbs stretched, causing the model to miss its targets.To compensate for the gradual loss of precision, the authors trained an LSTM on motion-capture data of the robot’s joint positions as the machine performed random motions autonomously.Together, the two models proved more agile, precise, and rapid on the ring-and-peg test than human surgeons. Behind the news: AI already assists physicians in a few small but important procedures. For instance, a robotic tool from the Dutch company Microsure, which helps suture tiny incisions on blood vessels, uses AI to stabilize shaking in the operator’s hands. Why it matters: This is a nice example of an algorithm that handles concept drift in robotic control. A lot of work in model-based reinforcement learning assumes a fixed model. But just as the dynamics of a human arm change as the arm tires — and a surgeon must adapt to control that tiring arm — we want learning algorithms to adapt to gradual changes in the robot’s dynamics. We’re thinking: We’re looking to AI systems that help optimize nutrition, exercise, and sleep to help steer us clear of AI systems that wield a scalpel!", "image_caption": "Alert door widget", "metadata": {"article_id": "issue_92", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-20-at-9.35.26-AM-copy--1-.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-92/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_92.html"}}
{"id": 33495889001, "type": "news_chunk", "title": "Face Recognition for the Masses, Labeling Libel, Document...", "subtitle": "Face Recognition for the Masses", "content": "Dear friends, Benchmarks have been a significant driver of research progress in machine learning. But they've driven progress in model architecture, not approaches to building datasets, which can have a large impact on performance in practical applications. Could a new type of benchmark spur progress in data-centric AI development?Remember: AI System = Code (model/algorithm) + DataMost benchmarks provide a fixed set of Data and invite researchers to iterate on the Code. This makes it possible to compare algorithms: By running many models on the same dataset, we can find the ones that perform best. To spur innovation on data-centric AI approaches, perhaps it’s time to hold the Code fixed and invite researchers to improve the Data.A huge amount of innovation — in algorithms, ideas, principles, and tools — is needed to make data-centric AI development efficient and effective. When AI was shifting toward deep learning over a decade ago, I didn’t foresee how many thousands of innovations and research papers would be needed to flesh out core tenets of the field. But now I think an equally large amount of work lies ahead to support a data-centric approach. For example, we need to develop good ways to: Surface and address inconsistencies in data labelsDetect and address data drift and concept driftHelp developers with error analysisSelect and apply the most effective data augmentation techniquesDecide what additional data to collect (rather than collecting more of everything)Merge inconsistent data sourcesTrack data provenance and lineage, so we can address problems in the data, such as bias, that may be discovered later Benchmarks and competitions in which teams are asked to improve the data rather than the code would better reflect the workloads of many practical applications. I hope that such benchmarks also will spur research and help engineers gain experience working on data. The Human Computer Interface (HCI) community also has a role in designing user interfaces that help developers and subject-matter experts work efficiently with data. I asked for feedback on the idea of a data-centric competition on social media (Twitter, LinkedIn, Facebook). I’ve read all the responses so far — thanks to all who replied. If you have thoughts on this, please join the discussion there. Keep learning! A secretive start-up matches faces online as a free service. What’s new: Face recognition tech tends to be marketed to government agencies, but PimEyes offers a web app that lets anyone scan the internet for photos of themself — or anyone they have a picture of. The company says it aims to help people control their online presence and fight identity theft, but privacy advocates are concerned that the tool could be used to monitor or harass people, The Washington Post reported. You can try it here. How it works: PimEyes has extracted geometric data from over 900 million faces it has found online. It claims not to crawl social media sites, but images from Instagram, Twitter, and YouTube have shown up in its results. The company compares the geometry of faces in pictures uploaded by users to those in its database and returns any matches.Anyone can search for free. Paying subscribers can see the web address of any images found and receive alerts when the system finds new matches. The company claims its accuracy is around 90 percent.The service doesn't verify user identities, leaving it ripe for abuse. Cyberstalkers on 4Chan have used it to stalk women photographed in public, and activists on Twitter have used it to try to identify people who stormed the U.S. Capitol on February 6.PimEyes, which is registered in the Seychelles, has declined interviews with several news outlets. It does not identify any of its personnel, and it answers questions via email through an anonymous spokesperson. Behind the news: Free online face matching is part of a broader mainstreaming of face recognition and tools to counter it. Google’s FaceNet, released in 2015, has become the basis of many face recognition tools.The Russian app FindFace, which is used by government officials to track political dissidents, earned notoriety in 2016 when people used it to identify women who had appeared anonymously in pornography.Exposing.AI uses face recognition to warn users when their Flickr images are used to train an AI model. Why it matters: The widespread ability to find matches for any face online erodes personal privacy. It also adds fuel to efforts to regulate face recognition, which could result in restrictions that block productive uses of the technology. We’re thinking: We’re all poorer when merely posting a photo on a social network puts privacy at risk. The fact that such a service is possible doesn’t make it a worthy use of an engineer’s time and expertise.", "image_caption": "Conventional and data-centric benchmarks", "metadata": {"article_id": "issue_93", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-05-26-at-9.46.41-AM-copy--1-.png", "source_url": "https://www.deeplearning.ai/the-batch/issue-93/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_93.html"}}
{"id": 82456618001, "type": "news_chunk", "title": "Autonomous Weapons Used in Combat, Tesla Doubles Down on Computer", "subtitle": "Deadly Drones Act Alone", "content": "Dear friends, In school, most questions have only one right answer. But elsewhere, decisions often come down to a difficult choice among imperfect options. I’d like to share with you some approaches that have helped me make such decisions. When I was deciding where to set up a satellite office outside the U.S., there were many options. My team and I started by listing important criteria such as supply of talent, availability of local partners, safety and rule of law, availability of visas, and cost. Then we evaluated different options against these criteria and built a matrix with cities along one axis and our criteria along the other. That clarified which country would make a great choice. When I feel stuck, I find it helpful to write out my thoughts: What options am I choosing among?What criteria are driving the choice?How does each option rate with respect to the criteria?if I need more information, how can I get it? Documenting decisions in this way also builds a foundation for further choices. For example, over the years, I’ve collected training data for many different kinds of problems. When I need to select among tactics for acquiring data, having been through the process many times, I know that some of the most important criteria are (i) the time needed, (ii) the number of examples, (iii) accuracy of the labels, (iv) how representative the input distribution is, and (v) cost. If I’m making a decision as part of a team, I check with teammates at each step to make sure we’re accurately capturing the top options, criteria, and so on. (The comments feature in Google Docs is a great way to facilitate open debate within a team.) This helps me avoid losing track of some criteria and acting based on an incomplete set; for example, picking the satellite office’s location based only on the availability of talent. It also helps align everyone on the final decision. As you may know, I wound up setting up a satellite office in Colombia because of the availability of talent and a supportive ecosystem of partners. The team there has become a key part of many projects. Lately I’ve worried about their wellbeing amid Covid-19 and widespread unrest. But in hindsight, setting up in Colombia was one of my best decisions, and I remain as committed as ever to supporting my friends there. Keep learning!Andrew Autonomous weapons are often viewed as an alarming potential consequence of advances in AI — but they may already have been used in combat. What’s new: Libyan forces unleashed armed drones capable of choosing their own targets against a breakaway rebel faction last year, said a recent United Nations (UN) report. The document, a letter from the organization’s Panel of Experts on Libya to the president of the Security Council, does not specify whether the drones targeted, attacked, or killed anyone. It was brought to light by New Scientist. Killer robots: In March of 2020, amid Libya’s ongoing civil war, the UN-supported Government of National Accord allegedly attacked retreating rebel forces using Kargu-2 quadcopters manufactured by Turkish company STM. The fliers are equipped with object-detection and face-recognition algorithms to find and strike targets without explicit human direction.Upon acquiring a target, the drone flies directly at it and detonates a small warhead just before impact.STM claims that its systems can distinguish soldiers from civilians.The Turkish military bought at least 500 such units for use in its border conflict with Syria. STM is negotiating sales to three other nations, according to Forbes. Behind the news: Many nations use machine learning in their armed forces, usually to bolster existing systems, typically with a human in the loop. In the most recent battle between Israel and Palestinians in Gaza, the Israeli Defense Force deployed machine learning systems that analyzed streams of incoming intelligence. The analysis helped its air force identify targets and warn ground troops about incoming attacks.The U.S. Army is testing a drone that uses computer vision to identify targets up to a kilometer away and determine whether they’re armed.The European Union has funded several AI-powered military projects including explosive device detection and small unmanned ground vehicles that follow foot soldiers through rough terrain. Why it matters: Observers have long warned that deploying lethal autonomous weapons on the battlefield could ignite an arms race of deadly machines that decide for themselves who to kill. Assuming the UN report is accurate, the skirmish in Libya appears to have set a precedent. We’re thinking: Considering the problems that have emerged in using today’s AI for critical processes like deploying police, sentencing convicts, and making loans, it’s clear that the technology simply should not be used to make life-and-death decisions. We urge all nations and the UN to develop rules to ensure that the world never sees a real AI war.", "image_caption": "A group of drones flying over a field", "metadata": {"article_id": "issue_94", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/06/Kargu-Redo-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-94/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_94.html"}}
{"id": 82456618002, "type": "news_chunk", "title": "Autonomous Weapons Used in Combat, Tesla Doubles Down on Computer", "subtitle": "One Model for Vision-Language", "content": "Researchers have proposed task-agnostic architectures for image classification tasks and language tasks. New work proposes a single architecture for vision-language tasks.What’s new: Led by Tanmay Gupta, researchers at the Allen Institute for AI and University of Illinois at Urbana-Champaign designed a general-purpose vision architecture and built a system, GPV-I, that can perform visual question answering, image captioning, object localization, and image classification.Key insight: Model architectures usually are designed for specific tasks, which implies certain types of output. To classify ImageNet, for instance, you need 1,000 outputs, one for each class. But text can describe both tasks and outputs. Take classification: the task “Describe this image” leads to the output, “this image is a dog.” By generating a representation of text that describes a task, a model can learn to perform a variety of tasks and output text that completes it without task-specific alterations in its architecture.How it works: Given a text description of a task — say, “describe the image” — and an image, GPV-I generates separate representations of the text and image, determines their relative importance to one another, and outputs a relevant text response and a copy of the image with bounding boxes. The authors trained it on COCO image captioning, VQA question answering, and RefCOCO+ object localization datasets. The system uses BERT to produce a representation of the task. It extracts an initial image representation using a ResNet-50 and passes it to a transformer borrowed from DETR. The transformer splits the representation into a grid, each cell of which contains a representation for the corresponding location in the image.A so-called cross-modal module accepts the representations of the image (one for each grid cell) and text (that is, the task) and produces new ones that reflect their relationship. It uses co-attention between transformer layers to compare image and text representations and a sigmoid layer to compute the relevance of the image representations to the task. Then it weights each image representation by its relevance.An image decoder uses the DETR representations to generate a bounding box for each object detected and the relevance scores to select which boxes to draw over the image. The text decoder (a transformer) uses the BERT representations and weighted representations to generate text output. Results: The researchers evaluated GPV-I on COCO classification, COCO captioning, and VQA question answering. They compared its performance with models trained for those tasks. On classification, GPV-I achieved accuracy of 83.6 percent, while a ResNet-50 achieved 83.3 percent. On captioning, GPV-I achieved 1.023 CIDEr-D — a measure of the similarity of generated and ground-truth captions, higher is better — compared to a VLP’s 0.961 CIDEr-D. On question answering, GPV-I achieved 62.5 percent accuracy compared to ViLBERT’s score of 60.1 percent, based on the output’s similarity to a human answer.Why it matters: A single architecture that can learn several tasks should be able to share concepts between tasks. For example, a model trained both to detect iguanas in images and to answer questions about other topics might be able to describe what these creatures look like even if they weren’t represented in the question-answering portion of the training data.We’re thinking: Visual classification, image captioning, and visual questioning answering are a start. We look forward to seeing how this approach performs on more varied tasks.", "image_caption": "Architecture of vision-language tasks", "metadata": {"article_id": "issue_94", "chunk_index": 2, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/06/ezgif.com-gif-maker---2021-05-04T162716.343-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-94/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_94.html"}}
{"id": 82456618003, "type": "news_chunk", "title": "Autonomous Weapons Used in Combat, Tesla Doubles Down on Computer", "subtitle": "Radiologists Eye AI", "content": "AI lately has achieved dazzling success interpreting X-rays and other medical imagery in the lab. Now it’s catching on in the clinic. What’s new: Roughly one-third of U.S. radiologists use AI in some form in their work, according to a survey by the American College of Radiology. One caveat: Many who responded positively may use older — and questionable — computer-aided detection, a technique for diagnosing breast cancer that dates to the 1980s, rather than newer methods. What they found: The organization queried its membership via email and received 1,861 responses. Of respondents who said they use AI, just over half use it to interpret images, and another 11 percent for image enhancement. The most common applications were breast (45.7 percent), thoracic (36.2 percent), and neurological (30.1 percent) imaging.12 percent of AI users said they use the technology to manage work lists, 11 percent to manage operations.Nearly 10 percent of AI users built their own algorithms rather than buying from outside vendors.94 percent of AI users said their systems perform inconsistently. Around 6 percent said they always work, and 2 percent said they never work.More than two thirds of respondents said they don’t use AI, and 80 percent of those said they see no benefit in it. Many believe that the technology is too expensive to implement, would hamper productivity, or wouldn’t be reimbursed. Behind the news: AI’s role in medical imaging is still taking shape, as detailed by Stanford radiology professor Curtis Langlotz in the journal Radiology: Artificial Intelligence. In 2016, a prominent oncologist wrote in the New England Journal of Medicine, “machine learning will displace much of the work of radiologists.” Two years later, Harvard Business Review published a doctor-penned essay headlined, “AI Will Change Radiology, but It Won’t Replace Radiologists.” Radiology Business recently asked, “Will AI replace radiologists?” and concluded, “Yes. No. Maybe. It depends.” Why it matters: AI’s recent progress in medical imaging is impressive. Although the reported 30 percent penetration rate probably includes approaches that have been uses for decades, radiologists are on their way to realizing the technology’s promise. We’re thinking: One-third down, two-thirds to go! Machine learning engineers can use such findings to understand what radiologists need and develop better systems for them.", "image_caption": "X-rays and charts about AI use in radiology", "metadata": {"article_id": "issue_94", "chunk_index": 3, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/06/radiology-redo-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-94/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_94.html"}}
{"id": 82456618004, "type": "news_chunk", "title": "Autonomous Weapons Used in Combat, Tesla Doubles Down on Computer", "subtitle": "Tesla All-In For Computer Vision", "content": "Tesla is abandoning radar in favor of a self-driving system that relies entirely on cameras. What’s new: The electric car maker announced it will no longer include radar sensors on Model 3 sedans and Model Y compact SUVs sold in North America. Tesla is the only major manufacturer of autonomous vehicles to bet solely on computer vision. Most others rely on a combination of lidar, radar, and cameras. How it works: Tesla has dropped radar only in the U.S. and only in its two most popular models. It aims to gather data and refine the technology before making the change in Model S, Model X, and vehicles sold outside the U.S. The eight-camera system called Tesla Vision will provide sensory input for Autopilot driver-assistance features such as lane controls as well as the Full Self-Driving upgrade, which automatically parks and summons vehicles, slows for stop signals, and automates highway driving. Such features will be “limited or inactive” during the transition.The move comes on the heels of earlier statements that touted cameras. “When radar and vision disagree, which one do you believe?” Musk said in a tweet on April 10. “Vision has much more precision, so better to double down on vision than do sensor fusion.”CEO Elon Musk predicted that Tesla Vision would help the company’s vehicles achieve full autonomy by the end of 2021. (Musk has a history of declaring ambitious goals his company has failed to meet.) Behind the news: Some people in the self-driving car industry favor using relatively expensive lidar and radar sensors in addition to low-cost cameras because they provide more information and thus greater safety. Camera-only advocates counter that humans can drive safely perceiving only images, so we should build AI that does the same. Most companies working on autonomous vehicles have chosen the more expensive route as the fastest way to reach full autonomy safely. Once they get there, the thinking goes, they can attend to bringing the cost down. Why it matters: If Tesla’s bet on cameras pays off, it could have an outsize influence on future self-driving technology. We’re thinking: While it’s great to see ambitious plans to commercialize computer vision, Tesla’s initiative will require tests on public streets. That means countless drivers will be the company’s unwitting test subjects — a situation that, as ever, demands strong oversight by road-safety authorities.", "image_caption": "Animation showing Tesla car's vision system", "metadata": {"article_id": "issue_94", "chunk_index": 4, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/06/tesla-redo_orange-background-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-94/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_94.html"}}
{"id": 82456618005, "type": "news_chunk", "title": "Autonomous Weapons Used in Combat, Tesla Doubles Down on Computer", "subtitle": "What AI Knows About Proteins", "content": "Transformer models trained on sequences of amino acids that form proteins have had success classifying and generating viable sequences. New research shows that they also capture information about protein structure. What’s new: Transformers can encode the grammar of amino acids in a sequence the same way they do the grammar of words in a language. Jesse Vig and colleagues at Salesforce Research and University of Illinois at Urbana-Champaign developed methods to interpret such models that reveal biologically relevant properties. Key insight: When amino acids bind to one another, the sequence folds into a shape that determines the resulting protein’s biological functions. In a transformer trained on such sequences, a high self-attention value between two amino acids can indicate that they play a significant role in the protein’s structure. For instance, the protein’s folds may bring them into contact. How it works: The authors studied a BERT pretrained on a database of amino acid sequences to predict masked amino acids based on others in the sequence. Given a sequence, they studied the self-attention values in each layer of the model. For each sequence in the dataset, the authors filtered out self-attention values below a threshold to find amino acid pairs with strong relationships. Consulting information in the database, they tallied the number of relationships associated with a given property of the protein’s shape (for example, pairs of amino acids in contact).Some properties depended on only one amino acid in a pair. For example, an amino acid may be part of the protein site that binds to molecules such as drugs. (The authors counted such relationships if the second amino acid had the property in question.) Results: The authors compared their model’s findings with those reported in other protein databases. The deeper layers of the model showed an increasing proportion of related pairs in which the amino acids actually were in contact, up to 44.7 percent, while the proportion of all amino acids in contact was 1.3 percent. The chance that the second amino acid in a related pair was part of a binding site didn’t rise steadily across layers, but it reached 48.2 percent, compared to a 4.8 percent chance that any amino acid was part of a binding site. Why it matters: A transformer model trained only to predict missing amino acids in a sequence learned important things about how amino acids form a larger structure. Interpreting self-attention values reveals not only how a model works but also how nature works. We’re thinking: Such tools might provide insight into the structure of viral proteins, helping biologists discover ways to fight viruses including SARS-CoV-2 more effectively.", "image_caption": "Protein structures", "metadata": {"article_id": "issue_94", "chunk_index": 5, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/06/PROTEIN-1-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-94/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_94.html"}}
{"id": 98808602001, "type": "news_chunk", "title": "Face Recognition at the Border, Robot Manicurists...", "subtitle": "3D Scene Synthesis for the Real World", "content": "Researchers have used neural networks to generate novel views of a 3D scene based on existing pictures plus the positions and angles of the cameras that took them. In practice, though, you may not know the precise camera positions and angles, since location sensors may be unavailable or miscalibrated. A new method synthesizes novel perspectives based on existing views alone. What’s new: Chen-Hsuan Lin led researchers at Carnegie Mellon University, Massachusetts Institute of Technology, and University of Adelaide in developing the archly named Bundle-Adjusting Neural Radiance Fields (BARF), a technique that generates new 3D views from images of a scene without requiring further information. Key insight: The earlier method called NeRF requires camera positions and angles to find values that feed a neural network. Those variables can be represented by a learnable vector, and backpropagation can update it as well as the network’s weights. How it works: Like NeRF, BARF generates views of a scene by sampling points along rays that extend from the camera through each pixel. It uses a vanilla neural network to compute the color and transparency of each point based on the point’s position and the ray’s direction. To determine the color of a given pixel, it combines the color and transparency of all points along the associated ray. Unlike NeRF, BARF’s loss function is designed to learn camera positions and angles, and it uses a training schedule to learn camera viewpoints before pixel colors. As input, BARF takes images plus their viewpoint vectors. Given a novel viewpoint, it learns to minimize the difference between the predicted and ground-truth color of each pixel.Points along separate rays that are close to one another have similar coordinates. The similarity makes it difficult to distinguish details and object boundaries in such areas. To work around this issue, BARF (like NeRF) represents points as fixed position vectors such that a small change in a point’s location causes a large change in its position vector.This positional encoding helps the system reproduce scene details, but it inhibits learning of viewpoint vectors, since a large shift in the representation of nearby points causes the learned camera viewpoint to swing wildly without converging. To solve this problem, BARF zeroes out most of each position vector at the start of training and fills it in progressively as training progresses. Consequently, the network learns the correct camera perspective earlier in training and how to paint details in the scene later. Results: The researchers compared BARF to NeRF, measuring their ability to generate a novel view based on several views of an everyday scene, where the viewpoints were unknown to BARF and known to NeRF. BARF achieved 21.96 competitive peak signal-to-noise ratio, a measure of the difference between the generated and actual images (higher is better). NeRF achieved 23.25 competitive peak signal-to-noise ratio. Why it matters: Data collected in the wild rarely are perfect, and bad sensors are one of many reasons why. BARF is part of a new generation of models that don’t assume accurate sensor input, spurring hopes of systems that generalize to real-world conditions. We’re thinking: In language processing, ELMo kicked off a fad for naming algorithms after Sesame Street characters. Here’s hoping this work doesn’t inspire its own run of names.", "image_caption": "Neural networks generating novel views of a 3D scene based on existing pictures", "metadata": {"article_id": "issue_95", "chunk_index": 1, "image_url": "https://info.deeplearning.ai/hs-fs/hubfs/BARF.gif?width=1200&upscale=true&name=BARF.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-95/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_95.html"}}
{"id": 98808602002, "type": "news_chunk", "title": "Face Recognition at the Border, Robot Manicurists...", "subtitle": "Every Problem Looks Like a Nail", "content": "Robots are brushing their way into the beauty market. What’s new: A trio of companies is developing automated nail-painting devices that integrate robotics and computer vision, The New York Times reported. How it works: Users select a color and place a hand or finger into a slot in a toaster-sized machine. The system scans the fingertips, and an automated paint dispenser — in some cases, a mechanical arm tipped by a brush — coats each nail. These machines update earlier nail-decorating gadgets that, say, applied decals without using AI. Clockwork aims to install its machines in offices and retail stores. The company recently opened a storefront in San Francisco.Nimble and Coral aim their devices at home users.All three companies are still tweaking their products ahead of official launches. Behind the news: The beauty industry has embraced a variety of AI techniques. Makeup wearers can upload a portrait to Estée Lauder and L’Oreal, which use face recognition to determine color combinations that match or highlight a person’s skin tone.Neutrogena’s Skin360 scans a user’s face to identify blemishes and provide targeted skin-care advice.Photo-filtering apps like Meitu automatically touch up users’ selfies. Why it matters: Americans spent $8.3 billion on nail care last year. Automated systems could appeal to people who are looking for a fast makeover as well as those who want to continue social distancing without foregoing manicures. But such systems also could also displace workers who already contend with low wages. We’re thinking: Paint your nails or don’t, but everyone who writes code should take good care of their hands.", "image_caption": "Automated nail-painting devices working", "metadata": {"article_id": "issue_95", "chunk_index": 2, "image_url": "https://info.deeplearning.ai/hs-fs/hubfs/MANICURE2.gif?width=1200&upscale=true&name=MANICURE2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-95/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_95.html"}}
{"id": 98808602003, "type": "news_chunk", "title": "Face Recognition at the Border, Robot Manicurists...", "subtitle": "Irresponsible AI", "content": "Few companies that use AI understand the ethical issues it raises. What’s new: While many companies are ramping up investments in AI, few look for and correct social biases in their models, according to a report by the credit-scoring company Fico. The report surveyed 100 C-level executives in data, analytics, and AI departments at companies that bring in revenue of $100 million or more annually. What they found: Nearly half of respondents said their company’s investment in AI had grown in the last 12 months. But there was no corresponding rise in efforts to make sure AI was ethical, responsible, and free of bias. Over 60 percent of respondents reported that their company’s executives had a poor or partial understanding of AI ethics. Even higher percentages found limited understanding among customers, board members, and shareholders.21 percent had prioritized AI ethics in the past year. Another 30 percent said they would do so this year. Still, 73 percent reported difficulty getting buy-in from colleagues on ethical AI goals.There is little consensus on corporate responsibility with regard to AI. Some respondents said they had no responsibility beyond legal and regulatory compliance, while others supported standards of fairness and transparency.Around half said they evaluated data and models for bias. 11 percent hired outside evaluators to test models for bias.51 percent of respondents did not monitor models after deployment. Behind the news: This is Fico’s second annual report, and it shows some improvement over the previous survey: Last year, 67 percent of respondents said they did not monitor systems after deployment. Why it matters: Never mind technical issues — taking the survey’s results at face value, a substantial percentage of large companies aren’t ready for AI transformation on an ethical level. Businesses that pursue AI without paying attention to ethical pitfalls run the risk of alienating customers and violating laws. We’re thinking: Companies that pay attention to ethics — in AI and elsewhere — will reap rewards in the form of better products, happier customers, and greater fairness and justice in the world.", "image_caption": "Charts and graphs showing relevant information regarding ethics in AI", "metadata": {"article_id": "issue_95", "chunk_index": 3, "image_url": "https://info.deeplearning.ai/hs-fs/hubfs/fico-redo-2.gif?width=1200&upscale=true&name=fico-redo-2.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-95/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_95.html"}}
{"id": 15343775001, "type": "news_chunk", "title": "Computers Spawn Computers, Self-Riding Bike, AI-Against-Covid", "subtitle": "AI Against Covid: Progress Report", "content": "A new report assessed how AI has helped address Covid-19 and where it has fallen short. What’s new: Machine learning systems haven’t lived up to their promise in some areas, but in others they’ve made a substantial impact, biomedical engineer Maxime Nauwynck wrote in The Gradient, an online journal of machine learning. Application areas: The author surveyed only systems specifically designed or adapted to fight Covid-19. Clinical Applications: In the pandemic’s early months, hundreds of research papers described systems allegedly capable of diagnosing the illness from lung scans. Few made it into clinical practice. Most were tripped up by poorly constructed public datasets, unexplainable output, or inadequate quality control.Epidemiology: Early AI models were hobbled by lack of data, but public health officials in the U.S. and UK ultimately developed ensemble systems to track the disease’s spread and anticipate its impacts.Treatments: The FDA granted emergency approval to treatments developed by biomedicine startups BenevolentAI and AbCellera. Both companies used AI to aid drug discovery. Moderna credits AI with helping it develop one of the first vaccines with extraordinary speed.Information: Chatbots helped overburdened health workers in China and the U.S. manage the deluge of patient questions, appointment scheduling, and other services.Public Safety: Computer vision systems are helping cities and businesses monitor social distancing. In France, systems detect whether individuals are wearing masks in public places. Behind the news: AI-powered health monitoring systems from BlueDot and Healthmap made headlines early last year when they reported a novel disease outbreak in the Wuhan area one week before the World Health Organization issued its first warnings. Why it matters: While AI is no panacea, this inventory makes clear that the technology has made significant contributions to the fight against Covid-19. We’re thinking: When new technology meets a previously unknown illness, there are bound to be hits and misses. The successes should help us prepare for — or, better yet, avoid — the next contagion.", "image_caption": "Covid-19 over a graph", "metadata": {"article_id": "issue_96", "chunk_index": 1, "image_url": "https://info.deeplearning.ai/hs-fs/hubfs/covid.gif?width=1200&upscale=true&name=covid.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-96/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_96.html"}}
{"id": 98142608001, "type": "news_chunk", "title": "Wildfire Alert Network, AI Invades Campuses, Synthetic Videos", "subtitle": "Machine Learning for Human Learners", "content": "AI is guiding admissions, grading homework, and even teaching classes on college campuses. What’s new: In a bid to cut costs, many schools are adopting chatbots, personality-assessment tools, and tutoring systems according to The Hechinger Report, an online publication that covers education. Critics worry that these systems may cause unseen harm. What they found: AI is used to help manage students at nearly every step in gaining higher education. Baylor University, Boston University, and others use personality-assessment software from Kira Talent to score applicants on traits such as openness, motivation, and “neuroticism.” Human administrators make the final call on who gets accepted.After accepting a new crop of candidates, Georgia State University uses a chatbot to send them encouraging messages. The system has increased the percentage who pay a deposit and enroll.Australia’s Deakin University developed Genie, a chatbot that monitors student behaviors and locations. If it determines that a would-be scholar is dawdling in the dining hall, for instance, it will send a message to get back on-task.Southern New Hampshire University is developing systems to grade homework and class participation. It monitors speech, body language, and how rapidly students respond to online lessons.ElevateU produces instructional programs called “AI textbooks” that tailor the learning experience based on student preferences, actions, and responses. Yes, but: Some observers say these systems may be giving inaccurate grades, contributing to bias in admissions, or causing other types of harm. An AI grading system tested by researchers at MIT gave high marks to gibberish essays studded with key phrases that contributed to a good score.University of Texas at Austin abandoned a system that evaluated graduate candidates after it was found to favor people whose applications resembled those of past students.Last year, the British government abandoned high-school rankings determined by an algorithm when the system gave 40 percent of students lower grades than their teachers would have assigned. Why it matters: The pandemic exacerbated an ongoing decline in U.S. university enrollment, which has left colleges scrambling. Automated systems that are carefully designed and sensibly deployed could help streamline processes, reduce costs, and increase access. We’re thinking: AI has its place on campus. For instance, chatbots can help students figure out where their classes meet. The technology doesn’t yet offer a substitute for good human judgement when it comes to sensitive tasks like assessing performance, but if it can show consistently fair and accurate judgement, it could help reduce the noise that currently afflicts human grading.", "image_caption": "Series of universities entrances", "metadata": {"article_id": "issue_97", "chunk_index": 1, "image_url": "https://info.deeplearning.ai/hs-fs/hubfs/highered.gif?width=1200&upscale=true&name=highered.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-97/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_97.html"}}
{"id": 98142608002, "type": "news_chunk", "title": "Wildfire Alert Network, AI Invades Campuses, Synthetic Videos", "subtitle": "Sorting Shattered Traditions", "content": "Computer vision is probing the history of ancient pottery. What’s new: Researchers at Northern Arizona University developed a machine learning model that identifies different styles of Native American painting on ceramic fragments and sorts the shards by historical period. How it works: The researchers started with an ensemble of VGG16 and ResNet50 convolutional neural networks pretrained on ImageNet. They fine-tuned the ensemble to predict pottery fragments’ historical period. The researchers collected 3064 photographs of pottery fragments from the southwestern U.S. Four experts labeled each photo as belonging to one of nine periods between 825 AD and 1300 AD. A majority of the experts had to agree on the type of pottery in each image for it to be included in the fine-tuning dataset, which contained 2,407 images.To make their training data more robust, the researchers randomly rotated, shrunk, or enlarged every photo prior to each training cycle.Heat maps generated using Grad-CAM highlighted the design features that were most influential in the model’s decisions. Results: In tests, the model classified tens of thousands of unlabeled fragments. It scored higher than two experts and roughly equal to two others. Behind the news: AI is helping archaeologists discover long-lost civilizations and make sense of clues they had already uncovered. Researchers found evidence of ancient settlements by training a model to interpret lidar readings taken during flights over Madagascar and the U.S.Using a similar method, archaeologists developed a network that identified underground tombs in aerial photography.A model that reads cuneiform is helping scholars translate ancient Persian tablets. Why it matters: For human archaeologists, learning to recognize the patterns on ancient pottery takes years of practice, and they often disagree on a given fragment’s provenance. Machine learning could sift through heaps of pottery shards far more quickly, allowing the humans to focus on interpreting the results. We’re thinking: Even when experts correctly identify a fragment, they can’t always explain what features led them to their conclusion. Heat maps from machine learning models could help teach the next generation of archaeologists how to read the past.", "image_caption": "Computer vision is probing the history of ancient pottery", "metadata": {"article_id": "issue_97", "chunk_index": 2, "image_url": "https://info.deeplearning.ai/hs-fs/hubfs/ezgif.com-gif-maker%20-%202021-05-25T145524.475.gif?width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-05-25T145524.475.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-97/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_97.html"}}
{"id": 77125490001, "type": "news_chunk", "title": "Amazon's Grab-And-Go Grocery, The Trouble With Ethical AI", "subtitle": "Pattern for Efficient Learning", "content": "Getting high accuracy out of a classifier trained on a small number of examples is tricky. You might train the model on several large-scale datasets prior to few-shot training, but what if the few-shot dataset includes novel classes? A new method performs well even in that case. What’s new: Eleni Triantafillou of Google and Vector Institute, along with colleagues at both organizations, designed Few-shot Learning with a Universal Template (FLUTE). Key insight: Training some layers on several tasks while training others on only one reduces the number of parameters that need to be trained for a new task. Since fewer parameters need training, the network can achieve better performance with fewer training examples. How it works: The authors trained a ResNet-18 to classify the eight sets in Meta-Dataset: ImageNet, Omniglot, Aircraft, Birds, Flowers, Quickdraw, Fungi, and Textures. Then they fine-tuned the model on 500 examples and tested it separately on Traffic Signs, MSCOCO , MNIST, CIFAR-10, and CIFAR-100. The authors trained the model’s convolutional layers on all training sets. Prior to training on each set, they swapped in new batch normalization layers. These were Feature-wise Linear Modulation (FiLM) layers, which scale and shift their output depending on the dataset the input belongs to. They also swapped in a fresh softmax layer.Prior to fine-tuning on each test set, the authors initialized the FiLM layers as follows: They trained a set encoder to find the training dataset most similar to the test set. A so-called blender network weighted the FiLM layer parameter values according to the set encoder’s output. Then it combined the weighted parameters in all first layers, all second layers, and so on.The authors fine-tuned the FiLM layers to minimize nearest-centroid classifier loss: Using up to 100 labeled examples in each class (capped at 500 total), the authors created a centroid for each class, an average of the network’s outputs for all examples in that class. Then, using individual examples, they trained the FiLM layers to minimize the distance between the output and the centroid for the example’s class.The model classified test examples by picking the class whose centroid was most similar to the example’s output. Results: Averaged across the five test sets, FLUTE’s 69.9 percent accuracy exceeded that of other few-shot methods trained on the same datasets. The closest competitor, SimpleCNAPs, achieved 66.8 percent accuracy. Why it matters: The combination of shared and swappable layers constitutes a template that can be used to build new classifiers when relatively few examples are available. We’re thinking: We will con-template the possibility of using this approach for tasks beyond image classification.", "image_caption": "Few-shot Learning with a Universal Template (FLUTE)", "metadata": {"article_id": "issue_98", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/06/FEWSHOT.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-98/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_98.html"}}
{"id": 77024492001, "type": "news_chunk", "title": "Zillow's New Neural Net, Optimizing Traffic City-Wide...", "subtitle": "News", "content": "Dear friends, In a recent letter, I noted that one difference between building traditional software and AI products is the problem of complex product specification. With traditional software, product managers can specify a product in ways that communicate clearly to engineers what to build — for example, by providing a wireframe drawing. But these methods don’t work for AI products. For an AI product, among the most important parts of the specification are: The space of acceptable operating conditions (also called the operational design domain)The level of performance required under various conditions, including machine learning metrics such as accuracy and software metrics such as latency and throughput Consider the problem of how to build a self-driving car. We might decide the acceptable road conditions for autonomous operation and the acceptable rate of collisions with particular objects at various speeds (for example, gently bumping a traffic cone at five miles per hour every 1 million miles may be okay, but hitting a pedestrian at 20 miles per hour every 1,000 miles is not). Or take reading electronic health records. What is an acceptable error rate when diagnosing a serious disease? How about the error rate when diagnosing a minor disease? What if human-level performance for a particular illness is low, so physicians tend to misdiagnose it, too? Specifying the metrics, and the dataset or data distribution on which the metrics are to be assessed, gives machine learning teams a target to aim for. In this process, we might decide how to define a serious versus a minor disease and whether these are even appropriate concepts to define a product around. Engineers find it convenient to optimize a single metric (such as average test-set accuracy), but it’s not unusual for a practical specification to require optimizing multiple metrics. Here are some ideas that I have found useful for specifying AI products. Clearly define slices (or subsets) of data that raise concerns about the system’s performance. One slice might be minor diseases and another major diseases. If the system is intended to make predictions tied to individuals, we might check for undesirable biases by specifying slices that correspond to users of different age groups, genders, ethnicities, and so on.For each slice, specify a level of performance that meets the user’s need, if it’s technically feasible. Also, examine performance across slices to ensure that the system meets reasonable standards of fairness.If the algorithm performs poorly on one slice, it may not be fruitful to tweak the code. Consider using a data-centric approach to improve the quality of data in that slice. Often this is the most efficient way to address the problem. I’ve found it very helpful to have sufficient data and a clear target specification for each slice. This isn’t always easy or even possible, but it helps the team advance toward a reasonable target.As a team performs experiments and develops a sense of what’s possible as well as where the system might falter, the appropriate slices can change. If you’re a machine learning engineer who is part-way through the project, and the product manager changes the product specification, don’t be frustrated! Ask them to buy you a coffee (or tea or other beverage of your choice) for your trouble, but recognize that this is part of developing a machine learning system. Hopefully such changes will happen less frequently as the team gains experience. Keep learning!Andrew Traffic signals controlled by AI are keeping vehicles rolling citywide. What’s new: Several U.S. cities are testing systems from Israel-based startup NoTraffic that promise to cut both commute times and carbon emissions, according to MotorTrend. The company plans to expand to 41 cities by the end of 2021. How it works: NoTraffic uses a combination of neural networks and other techniques to optimize intersections and coordinate traffic signals throughout a city. The system is outfitted to integrate with pavement sensors and connected-vehicle protocols. Cameras installed at intersections run models that detect and classify oncoming vehicles, bikes, and pedestrians, and calculate their speed and location.They stream anonymized data to control modules housed in traffic signals, which aggregate the sensor outputs and optimize signal operation. For instance, the system can turn a green light red if there are no cars coming, or change a red light to green as an emergency vehicle approaches.The data is streamed to the cloud for optimization over larger areas and transmitted back to control modules to account for broader traffic patterns. For example, the system can coordinate multiple lights to reroute traffic around a road that has been closed due to an accident.In a two month trial, Redlands, CA, found that installing the systems in 2 percent of intersections spared commuters a total of 900 hours of gridlock, which translated to an extra $331,380 in economic productivity.The Redlands trial also staved off 11 tons of greenhouse gas emissions. The company estimates that installing its technology in every traffic signal in the U.S. would forestall the equivalent of 20 million vehicles’ worth of exhaust annually. Behind the news: Machine learning is combating congestion outside the U.S. as well. Delhi deployed its own AI-powered traffic signal network at over 7,500 intersections.At least 23 cities in China and Malaysia use Alibaba’s CityBrain to control gridlock. Why it matters: Worldwide, congestion costs hundreds of billions of dollars in annual productivity, pollutes cities, and burdens the planet with greenhouse gases. AI-driven traffic control doesn’t eliminate those impacts, but it can take the edge off. We’re thinking: Many traffic lights already are geared to prioritize passage of emergency vehicles, for example by recognizing patterns of flashing lights — but networked sensors stand to improve traffic routing globally.", "image_caption": "Video showing traffic signals controlled by AI", "metadata": {"article_id": "issue_99", "chunk_index": 1, "image_url": "https://dl-staging-website.ghost.io/content/images/2021/07/NOTRAFFIC-REVISED.gif", "source_url": "https://www.deeplearning.ai/the-batch/issue-99/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/issue_99.html"}}
{"id": 52310939001, "type": "news_chunk", "title": "Johnson & Johnson Reveals its Revised AI Strategy", "subtitle": "Johnson & Johnson Reveals its Revised AI Strategy", "content": "The world’s biggest pharmaceutical company by revenue shed light on its AI strategy. What’s new: Johnson & Johnson, after experimenting broadly with generative AI, settled on a short list of projects that aid in sales, drug development, supply-chain management, and internal communications. A company executive described the process and results to the venture-capital firm Greylock and The Wall Street Journal. How it works: The 140-year-old medical company spent roughly a year experimenting with various AI applications throughout the company, according to Chief Information Officer Jim Swanson. A centralized governing board oversaw as many as 900 experiments. After finding that 10 percent to 15 percent of use cases drove about 80 percent of the value, the company shifted responsibility for AI projects to specific departments to focus on high-value applications. In the end, the criteria for choosing a project was threefold: (i) how readily it could be implemented, (ii) how useful it would be throughout the company, and (iii) how much it would benefit the business. A division that develops cancer treatments integrated a sales copilot into its customer relationship management system. The system supplies medically validated, legally reviewed information about products and information about particular customers. The application is being adapted for salespeople who sell hardware such as robotics and artificial hip joints.AI systems are accelerating drug development. One system helps design chemical processes, such as determining the optimal moment to add a compound that will turn a liquid into a solid. An image-analytics model helps identify compounds that are safe and effective.The company developed a system that monitors and predicts risks to supply chains, such as a fire that may affect supplier locations, materials, or products. The system provides early warnings that helps managers anticipate and mitigate disruptions.AI tools are helping to organize and execute clinical trials more efficiently. Models that identify patients who qualify for trials help ensure that trial populations are sufficiently diverse. A model that helps enroll patients in trials more than doubled enrollment in some cases.The Global Services department implemented a chatbot to answer employees’ questions about benefits, policies, and procedures and sends links to relevant documents.Separate organizations that oversee AI development and data management help keep projects moving forward, meet ethical standards, and scale appropriately. Meanwhile, employees undergo “digital boot camp” training (including a course in generative AI). Behind the news: Generative AI is expected to bring in up to $110 billion in annual revenue across the pharmaceutical industry, according to McKinsey. The consultancy breaks down this number into the following categories, in order of their contribution to the total: commercial (AI for sales and marketing), research (AI for designing, screening, and manufacturing molecules), clinical (AI to facilitate trials), enterprise, operations, and medical (processing medical literature). Why it matters: Johnson & Johnson’s experience offers a peek into AI development at a major legacy company in a key sector. The company has identified high-value opportunities in enterprise-wide operations, departmental priorities, and core products. It’s pursuing all three. We’re thinking: Notably, this medical stalwart is building AI applications for human resources, sales, and supply-chain management. Similar opportunities exist at companies old and new, big and small, far and wide.", "image_caption": "Gloved hand holds Johnson & Johnson vaccine vial with syringe, representing pharmaceutical and vaccination concepts.", "metadata": {"article_id": "johnson_johnson_reveals_its_revised_ai_strategy", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--86--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/johnson-johnson-reveals-its-revised-ai-strategy/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/johnson_johnson_reveals_its_revised_ai_strategy.html"}}
{"id": 77063364001, "type": "news_chunk", "title": "Learn the Language of Software", "subtitle": "Learn the Language of Software", "content": "Dear friends, Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful! In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment. As coding becomes easier, more people should code, not fewer! Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step. (By the way, to learn more about AI assisted coding, check out our video-only short course, “Build Apps with Windsurf’s AI Coding Agents.”) I wrote previously that I see tech-savvy people coordinating AI tools to move toward being 10x professionals — individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively. One question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that. When I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result. Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools continue to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do. Keep building!", "image_caption": "Illustration of a programmer at a computer displaying PyTorch code, while a smiling colleague gives a thumbs-up in approval.", "metadata": {"article_id": "learn_the_language_of_software", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F03%2Funnamed--54--2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/learn-the-language-of-software/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/learn_the_language_of_software.html"}}
{"id": 48083508001, "type": "news_chunk", "title": "Lessons From Our First AI Dev Conference", "subtitle": "Lessons From Our First AI Dev Conference", "content": "Dear friends, Last Friday on Pi Day, we held AI Dev 25, a new conference for AI Developers. Tickets had (unfortunately) sold out days after we announced their availability, but I came away energized by the day of coding and technical discussions with fellow AI Builders! Let me share here my observations from the event. I'd decided to start AI Dev because while there're great academic AI conferences that disseminate research work (such as NeurIPS, ICML and ICLR) and also great meetings held by individual companies, often focused on each company's product offerings, there were few vendor-neutral conferences for AI developers. With the wide range of AI tools now available, there is a rich set of opportunities for developers to build new things (and to share ideas on how to build things!), but also a need for a neutral forum that helps developers do so. Based on an informal poll, about half the attendees had traveled to San Francisco from outside the Bay Area for this meeting, including many who had come from overseas. I was thrilled by the enthusiasm to be part of this AI Builder community. To everyone who came, thank you! Other aspects of the event that struck me: First, agentic AI continues to be a strong theme. The topic attendees most wanted to hear about (based on free text responses to our in-person survey at the start of the event) was agents!Google's Paige Bailey talked about embedding AI in everything and using a wide range of models to do so. I also particularly enjoyed her demos of Astra and Deep Research agents.Meta's Amit Sangani talked compellingly as usual about open models. Specifically, he described developers fine-tuning smaller models on specific data, resulting in superior performance than with large general purpose models. While there're still many companies using fine-tuning that should really just be prompting, I'm also seeing continued growth of fine-tuning in applications that are reaching scale and that are becoming valuable.Many speakers also spoke about the importance of being pragmatic about what problems we are solving, as opposed to buying into the AGI hype. For example, Nebius' Roman Chernin put it simply: Focusing on solving real problems is important! Lastly, I was excited to hear continued enthusiasm for the Voice Stack. Justin Uberti gave a talk about OpenAI’s realtime audio API to a packed room, with many people pulling out laptops to try things out themselves in code! DeepLearning.AI has a strong “Learner First” mentality; our foremost goal is always to help learners. I was thrilled that a few attendees told me they enjoyed how technical the sessions were, and said they learned many things that they're sure they will use. (In fact, I, too, came away with a few ideas from the sessions!) I was also struck that, both during the talks and at the technical demo booths, the rooms were packed with attendees who were highly engaged throughout the whole day. I'm glad that we were able to have a meeting filled with technical and engineering discussions. I'm delighted that AI Dev 25 went off so well, and am grateful to all the attendees, volunteers, speakers, sponsors, partners, and team members that made the event possible. I regretted only that the physical size of the event space prevented us from admitting more attendees this time. There is something magical about bringing people together physically to share ideas, make friends, and to learn from and help each other. I hope we'll be able to bring even more people together in the future. Keep building! P.S. I'm thrilled to share our newest course series: the Data Analytics Professional Certificate! Data analytics remains one of the core skills of data science and AI, and this professional certificate takes you up to being job-ready for this. Led by Netflix data science leader Sean Barnes, this certificate gives you hands-on experience with essential tools like SQL, Tableau, and Python, while teaching you to use Generative AI effectively as a thought partner in your analyses. Labor economists project a 36% growth in data science jobs by 2033. I'm excited to see rising demand for data professionals, since working with data is such a powerful way to improve decision-making, whether in business, software development, or your private life. Data skills create opportunities at every level—I’m excited to see where they take you! Sign up here!", "image_caption": "Top left: attendees watch a presentation. Top right: crowd at a developer booth. Bottom left: fortune cookie says ‘Build baby build!’ Bottom right: staff check in attendees.", "metadata": {"article_id": "lessons_from_our_first_ai_dev_conference", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F03%2Funnamed--63--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/lessons-from-our-first-ai-dev-conference/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/lessons_from_our_first_ai_dev_conference.html"}}
{"id": 99762863001, "type": "news_chunk", "title": "LLMs Boost Shopping Recommendations by Decoding What Users Want", "subtitle": "LLMs Boost Shopping Recommendations by Decoding What Users Want", "content": "Large language models can improve systems that recommend items to purchase by inferring customer preferences. What’s new: Fabian Paischer and colleagues at Johannes Kepler University Linz, University of Wisconsin, and Meta introduced Multimodal Preference Discerner (Mender), a recommender that integrates a large language model (LLM). Key insight: Text that attracts customers, such as product descriptions, and text they write, such as product reviews, may contain information that indicates their preferences, such as the craft projects that required a particular power tool. But it also may include irrelevant information, such as a complaint that the tool was delivered late, which can throw recommendation systems off track. An LLM can derive preferences from text, providing a clearer signal of what a customer wants. How it works: Mender comprises an LLM (Llama 3 70B-Instruct), an encoder (Flan-T5 pretrained on a wide variety of text and frozen) that embeds customer data, and a decoder (a transformer trained from scratch) that predicts the next item a customer will buy. The system learned to predict the next item based on descriptions of items a customer purchased, the customer’s ratings and reviews of those products (drawn from datasets of Steam reviews of video games and Amazon reviews of items related to beauty, toys-and-games, and sports-and-outdoors), and customer preferences inferred by the LLM from the foregoing data. The authors started with a list of products a given customer had purchased and reviewed. Given an item’s description and all reviews up to that point, the LLM inferred five customer preferences in the form of instructions such as, “Look for products with vibrant, bold colors.”The authors built a dataset in which each example included a sequence of items a customer had purchased and on inferred preference that matched the next purchase. To choose the matching preference, they separately embedded all prior preferences and item descriptions using a pretrained Sentence-T5 embedding model. They chose the preference whose embedding was most similar to that of the next purchase.The encoder embedded the list of purchases and the selected preference. Given the embeddings, the decoder learned to predict the next purchase. Results: The authors compared Mender to TIGER, a recommender that also takes a purchase history and predicts the next purchase, on the Steam and Amazon datasets. They scored the results using recall @5, a measure of how often the correct item is within the model’s top five most likely predictions. Mender produced the best recommendations for all datasets.On Steam, TIGER was close. Mender achieved 16.8 percent recall @5, while TIGER achieved 16.3 percent.The difference was most pronounced on the Amazon toys-and-games dataset. Mender achieved 5.3 percent recall @5, while TIGER achieved 3.75 percent recall @5. Why it matters: Drawing inferences from text information like customer reviews and item descriptions boosts a recommender’s signal, making it clearer what a given customer is likely to want. Previous systems used customer reviews or item descriptions directly; Mender uses customer preferences extracted from that information. We’re thinking: Be on the lookout for innovative ways to use LLMs. We recommend it!", "image_caption": "Diagram of LLM-based preference approximation and multimodal sequential recommendation for personalized product suggestions.", "metadata": {"article_id": "llms_boost_shopping_recommendations_by_decoding_what_users_want", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--83-.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/llms-boost-shopping-recommendations-by-decoding-what-users-want/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/llms_boost_shopping_recommendations_by_decoding_what_users_want.html"}}
{"id": 96693780001, "type": "news_chunk", "title": "MatterGen, A Diffusion Model That Designs New Materials with Specified Properties", "subtitle": "MatterGen, A Diffusion Model That Designs New Materials with Specified Properties", "content": "Materials that have specific properties are essential to progress in critical technologies like solar cells and batteries. A machine learning model designs new materials to order. What’s new: Researchers at Microsoft and Shenzhen Institute of Advanced Technology proposed MatterGen, a diffusion model that generates a material’s chemical composition and structure from a prompt that specifies a desired property. The model and code are available under a license that allows commercial as well as noncommercial uses without limitation. The training data also is noncommercially available. How it works: MatterGen’s training followed a two-stage process. In the first stage, it learned to generate materials (specifically crystals — no liquids, gasses, or amorphous solids like glass). In the second, it learned to generate materials given a target mechanical, electronic, magnetic, or chemical property such as magnetic density or bulk modulus (the material’s resistance to compression). MatterGen first learned to remove noise that had been added to 600,000 examples drawn from two datasets. Specifically, it learned to remove noise from three noisy matrices that represented a crystal’s shape (parallelepiped), the type of each atom, and the coordinates of each atom.To incorporate information about properties, the authors added to the diffusion model four vanilla neural networks, each of which took an embedding of the target property. The diffusion model added the output of these networks to its intermediate embeddings at different layers.Then the authors fine-tuned the system to remove added noise from materials that contained property information in their original dataset.At inference, given three matrices of pure noise representing crystal shape, atom types, and atom coordinates, and a prompt specifying the desired property, the diffusion model iteratively removed the noise from all three matrices. Results: The authors generated a variety of materials, and they synthesized one to test whether it had a target property. Specifically, they generated over 8,000 candidates with the target bulk modulus of 200 gigapascals (a measure of resistance to uniform compression), then automatically filtered them based on a number of factors to eliminate material in their dataset and unstable materials. Of the remaining candidates, they chose four manually and successfully synthesized one. The resulting crystal had a measured bulk modulus of 158 gigapascals. (Most materials in the dataset had a bulk modulus of between 0 and 400 gigapascals.) Behind the news: Published in 2023, DiffCSP also uses a diffusion model to generate the structures of new materials. However, it does so without considering their desired properties. Why it matters: Discovering materials relies mostly on searching large databases of existing materials for those with desired properties or synthesizing new materials and testing their properties by trial and error. Designing new crystals with desired properties at the click of a button accelerates the process dramatically. We’re thinking: While using AI to design materials accelerates an important step, determining whether a hypothesized material can be manufactured efficiently at scale is still challenging. We look forward to research into AI models that also take into account ease of manufacturing.", "image_caption": "Scientific diagram of a denoising model generating stable materials from random elements based on chemistry and symmetry", "metadata": {"article_id": "mattergen_a_diffusion_model_that_designs_new_materials_with_specified_properties", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F03%2Funnamed--66--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/mattergen-a-diffusion-model-that-designs-new-materials-with-specified-properties/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/mattergen_a_diffusion_model_that_designs_new_materials_with_specified_properties.html"}}
{"id": 40302489001, "type": "news_chunk", "title": "Meta Releases Llama 4 Models, Claims Edge Over AI Competitors", "subtitle": "Meta Releases Llama 4 Models, Claims Edge Over AI Competitors", "content": "Meta updated its popular open-weights models, claiming performance superior to closed competitors in three size classes. What’s new: Meta released two vision-language models in the Llama 4 family (Llama 4 Scout and Llama 4 Maverick) and teased a third (Llama 4 Behemoth). All three models are based on the increasingly popular mixture-of-experts (MoE) architecture, which activates only a portion of parameters during inference for more efficient processing. Llama 4 Scout boasts the industry's biggest input context window so far — 10 million tokens! — but Meta says processing 1.4 million tokens of context requires eight Nvidia H100 GPUs, and early users on Reddit reported that its effective context began to degrade at 32,000 tokens. Input/output: Text, image, and video in (Llama 4 Scout up to 10 million tokens, Llama 4 Maverick up to 1 million tokens). Text out (Llama 4 Scout 120.5 tokens per second, 0.39 seconds to first token; Llama 4 Maverick 124.2 tokens per second, 0.34 seconds to first token).Architecture: Llama 4 Scout 109 billion parameters, 17 billion parameters activated. Llama 4 Maverick 400 billion parameters, 17 billion activated. Llama 4 Behemoth nearly 2 trillion parameters, 288 billion parameters activated.Features: 12 officially supported languagesUndisclosed: Distillation details, Llama 4 Behemoth details including release dateAvailability: Weights free to download under a license that allows noncommercial uses and limits commercial uses to businesses with fewer than 700 million monthly users under Meta’s terms of useAPI price: Llama 4 Scout $0.15/$0.50 per 1 million tokens input/output. Llama 4 Maverick $0.22/$0.85 per 1 million tokens input/output. How it works: The team pretrained Llama 4 models on images and text in over 200 languages from publicly available and licensed data, including data from publicly shared posts on Facebook and Instagram. They trained Llama 4 Scout on 40 trillion tokens and Llama 4 Maverick on 22 trillion tokens. The team removed the 50 percent of training examples that are easiest to predict (as judged by unnamed Llama models). For Llama 4 Behemoth, they removed 95 percent of an unspecified data set.They fine-tuned the models using supervised learning, then reinforcement learning, then direct preference optimization.Llama 4 Maverick was “co-distilled” on outputs from Llama 4 Behemoth. The other teachers undisclosed. Results: In tests performed by Meta, Llama 4 models showed strong performance relative to competing models — mostly not mixtures of experts, but some that are known to have higher parameter counts relative to Llama 4 models’ active parameters. Llama 4 Scout outperformed Google Gemma 3 27B, Mistral 3.1 24B, and Gemini 2.0 Flash-Lite on most of seven benchmarks that test vision (MMMU, Chart QA), coding (LiveCodeBench), and knowledge and reasoning tasks (MMLU Pro, GPQA Diamond).Llama 4 Maverick outperformed OpenAI GPT-4o and Google Gemini 2.0 Flash across the same benchmarks.On multiple benchmarks including tests of mathematics, coding, domain knowledge, and multimedia reasoning, an early version of Llama 4 Behemoth outperformed OpenAI GPT-4.5, Anthropic Claude 3.7 Sonnet, and Google Gemini 2.0 Pro but fell behind OpenAI o1, DeepSeek-R1, and Google Gemini 2.5 Pro. (The parameter counts of these models are undisclosed except DeepSeek-R1, a MoE model with 671 billion parameters, 37 billion of which are active at any given time.) Yes, but: An experimental version of Llama 4 Maverick reached second place in Chatbot Arena behind Gemini 2.5 Pro. However, it was a variation optimized for conversation, not the currently available version. AI researchers accused Meta of attempting to manipulate the leaderboard. Why it matters: Although the version of Llama 4 Maverick that nearly topped the Chatbot Arena is not the released version, its accomplishment says a lot about the growing power of open weights. Open models are quickly reaching parity with closed competitors — a boon to developers, businesses, and society at large. We’re thinking: According to Meta, Behemoth beats GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro, topping all but the best reasoning models — but it isn’t available yet. Something to look forward to!", "image_caption": "Llama 4 Behemoth benchmark chart comparing coding, reasoning, and multilingual scores with Claude, Gemini, and GPT-4.5.", "metadata": {"article_id": "meta_releases_llama_4_models_claims_edge_over_ai_competitors", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--57--1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/meta-releases-llama-4-models-claims-edge-over-ai-competitors/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/meta_releases_llama_4_models_claims_edge_over_ai_competitors.html"}}
{"id": 39773582001, "type": "news_chunk", "title": "Meta Researchers Build Llama-Style Models That Recall Details Without Needing More Computing Resources", "subtitle": "Meta Researchers Build Llama-Style Models That Recall Details Without Needing More Computing Resources", "content": "Improving a large language model’s factual accuracy typically requires making it bigger, which in turn, involves more computation. Researchers devised an architecture that enables models to recall relevant details without significantly increasing the amount of computation required. What’s new: Vincent-Pierre Berges, Barlas Oğuz, and colleagues at Meta augmented transformers with trainable memory layers that efficiently store and retrieve information related to a prompt. The training code is available under a CC BY-NC license, which permits noncommercial uses. Memory layer basics: Memory layers were introduced in 2015 and were applied to transformers a few years later. They compute vectors, which may capture details like names or dates that were learned through training, and retrieve them according to a given input. Computing the output of a memory layer is similar to computing that of a self-attention layer. Both describe vectors that represent queries, keys, and values, and both compute the similarity between queries and keys and then weight the values by that similarity. However, while a self-attention layer computes queries, keys, and values from linear transformations of the input, a memory layer (which computes queries the same way) learns keys and a corresponding value for each key through training. Key insight: Memory layers can be scaled to millions of keys, but computing the similarity between a query and so many keys is computationally expensive. One solution is to represent each key as a combination of two half-keys drawn from two much smaller sets. For example, two sets of 1,000 half-keys each can represent 1 million possible keys. Comparing a query to these smaller sets is much more efficient, making it practical to scale up memory layers dramatically. How it works: The authors pretrained Llama-style models of several sizes (from 134 million to 8 billion parameters) on data similar to Llama 2’s and Llama 3’s pretraining datasets. They replaced the fully connected layers with memory layers in three transformer blocks. These layers shared parameters and held up to 16 million values (an extra 64 billion parameters total). The memory layers performed these steps: Given a query (a prompt that has been embedded by preceding transformer layers), split it into two vectors half the size.Compute similarity scores between each half-query to and each half-key drawn from two sets of half keys. Identify the k highest-scoring half-keys. Concatenate the highest-scoring half keys to produce k2 full keys. Sum the similarity scores of the two half keys that make up each full key. Choose the k highest-scoring full keys.Compute the index of each full key based on the indices of the corresponding half-keys.Retrieve the values that correspond to the full keys.Output the summed values weighted by the similarity scores. Results: The authors compared a model (8 billion parameters) with memory layers to a similar model without memory layers, both trained on 1 trillion tokens. They used nine question-answering datasets for evaluation. The model with memory layers achieved higher performance on seven of them. For example, on MMLU, the memory model achieved 63.04 percent accuracy, while the unmodified transformer achieved 59.68 percent accuracy. In general, the memory model performed worse than Llama 3.1 8B trained on 15 trillion tokens. For example, Llama 3.1 8B achieved 66 percent accuracy on MMLU. Why it matters: Memory layers didn’t catch on in the early days of large language models (LLMs), but they can improve the output of today’s much bigger models. LLMs outfitted with memory layers require less data and computation for pretraining than conventional models to achieve the same result, at least with respect to answering factual questions. We’re thinking: While retrieval-augmented generation can help LLMs deliver more-factual output by retrieving facts from a database, the authors add trainable parameters for this purpose.", "image_caption": "Dual line graphs showing factual QA accuracy and NLL against memory size for NQ and TQA datasets in AI models.", "metadata": {"article_id": "meta_researchers_build_llama_style_models_that_recall_details_without_needing_more_computing_resources", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--91--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/meta-researchers-build-llama-style-models-that-recall-details-without-needing-more-computing-resources/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/meta_researchers_build_llama_style_models_that_recall_details_without_needing_more_computing_resources.html"}}
{"id": 82482718001, "type": "news_chunk", "title": "Microsoft Extends Copilot to 365 and Windows", "subtitle": "Microsoft Extends Copilot to 365 and Windows", "content": "Having broken the ice around chat-enabled web search, Microsoft has extended the concept to coding, office productivity, and the operating system itself.What’s new: Microsoft refreshed its Copilot line of chatbots, adding new features, renaming old ones, and unifying the brand into what it calls an “everyday AI companion.”How it works: Microsoft offers Copilots for its subsidiary GitHub, Microsoft 365, and Windows. GitHub, maker of the original Copilot AI-driven pair programmer, extended the beta-test Copilot Chat feature, which enables users to converse about their code, from enterprise to individual users. Based on a version of GPT-3.5 optimized for code, the system works within Microsoft’s Visual Studio and VS Code applications as well as non-Microsoft development apps Vim, Neovim, and JetBrains. Copilot Chat answers questions, troubleshoots bugs, documents snippets, suggests fixes for security vulnerabilities, and teaches coders how to use unfamiliar languages.Microsoft 365 Copilot makes it possible to control Excel, Outlook, PowerPoint, Word, and other productivity apps via text prompts. For instance, in Word, it enables users to summarize documents; in Outlook, to draft emails. It will be available on November 1 to enterprise customers for $30 per user/month in addition to the price of Microsoft 365. The company has an invitation-only pilot program for individual and small business users.Windows Copilot is a taskbar chatbot powered by GPT-4. It can open applications, copy and paste among them, query Bing Chat, and integrate third-party plugins. It also provides image generation to media editors that come with Windows including Paint, Photos, and the video editor Clipchamp. Windows Copilot will be available to Windows 11 users as a free update starting September 26. Behind the news: The emergence of ChatGPT set off a race between Microsoft and Alphabet to integrate large language models into search and beyond. Microsoft seized the day in early February when it launched a version of its Bing search engine that incorporated OpenAI’s technology, and its Copilot strategy has extended that lead. But Alphabet is nipping at Microsoft’s heels. It’s bringing its Bard chatbot to Google productivity apps, from email to spreadsheets.Why it matters: The combination of large language models and productivity software is a significant step. Microsoft’s approach seems likely to inspire millions of people who have never written a macro or opened the command line to start prompting AI models.We’re thinking: Copilot is a great concept. It helped make software engineers early adopters of large language models — for writing code, not prose. This story first appeared in the September 27, 2023 edition of The Batch.", "image_caption": "GIF of Copilot editing and generating images for Windows photo apps", "metadata": {"article_id": "microsoft_extends_copilot_365_windows", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FCOPILOT-1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/microsoft-extends-copilot-365-windows/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/microsoft_extends_copilot_365_windows.html"}}
{"id": 26562166001, "type": "news_chunk", "title": "Microsoft Researchers Show That Heavily Quantized Versions of Llama Can Perform As Well As Near-Full Precision", "subtitle": "Microsoft Researchers Show That Heavily Quantized Versions of Llama Can Perform As Well As Near-Full Precision", "content": "Using an 8-bit number format like FP8 during training saves computation compared to 16- or 32-bit formats, but it can yield less-accurate results. Researchers trained models using 4-bit numbers without sacrificing accuracy. What’s new: Ruizhe Wang and colleagues at Microsoft and University of Science and Technology of China trained large language models (LLMs) using FP4 for matrix multiplications and achieved accuracy comparable to LLMs trained using the popular BF16 format. Since matrix multiplications account for 95 percent of computation in LLM training, FP4 could significantly accelerate computation and reduce memory costs. Key insight: Quantization functions, which accelerate computation by reducing the precision of model weights and layer outputs, make typical training impossible because they’re not differentiable. A common workaround passes the derivative through, as though quantization didn’t occur, but this degrades the resulting model’s accuracy. A differentiable approximation of a quantization function enables quantization to reduce training computation while maintaining the accuracy of the trained model. How it works: The authors pretrained Llama 2 13B on 100 billion tokens of text scraped from the web. They used FP4 for matrix multiplications and FP8, BF16, or FP16 for the other operations such as optimizer updates. To quantize the model weights to FP4 (which ranges between -6 and 6), the authors scaled the values in the weight matrices relative to the maximum absolute value. They computed the updates on a higher-precision copy of the weights, which made it necessary to re-quantize them at each training step during the forward pass through the network.Although the weights had been quantized to 4 bits, matrix multiplication between the weights and outputs of the previous layer could produce values outside the FP4 range. So, in each layer, if a value exceeded the 99th percentile of the values of the layer’s input, the authors limited the input to the 99th-percentile value. Then they converted the layer’s inputs to FP4. Limiting outliers prevented high values from affecting the scaling during FP4 conversion.Limiting outliers introduced a degree of error, so they computed a matrix to correct the result of the matrix multiplication. They computed this matrix in FP16 using sparse matrix multiplication between the weights and the outliers.During backpropagation, the authors computed the gradients through a differentiable function that approximated the quantization function. Results: The authors simulated FP4 hardware on Nvidia H100 GPUs, which don’t directly support that number format. FP4 achieved accuracy similar to that of BF16 during training and across a wide variety of tasks at inference. On question-answering tasks, FP4 approached or outperformed BF16. Averaged across nine benchmarks including BoolQ (answering yes-no questions), HellaSwag (completing an incomplete narrative), and ARC-C (answering multiple-choice questions that involve reasoning), FP4 achieved 54.95 accuracy, while BF16 achieved 54.44 accuracy.Specifically, on Hellaswag, FP4 training achieved 54.12 percent accuracy, while BF16 achieved 53.56 accuracy.On BoolQ, FP4 achieved 55.90 percent accuracy, while BF16 achieved 57.40 accuracy. Why it matters: Training LLMs at FP4 precision ought to reduce computation dramatically on hardware that supports FP4 matrix multiplications. We’re thinking: FP4-ready hardware became available in the cloud only early this year, so the authors weren’t able to measure the actual acceleration. As capable hardware becomes more widely used, FP4 promises faster, more energy-efficient training.", "image_caption": "Diagram of FP4 training scheme showing BF16 tensor quantization and FP4 tensor core processing for efficient computation.", "metadata": {"article_id": "microsoft_researchers_show_that_heavily_quantized_versions_of_llama_can_perform_as_well_as_near_full_precision", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--95--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/microsoft-researchers-show-that-heavily-quantized-versions-of-llama-can-perform-as-well-as-near-full-precision/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/microsoft_researchers_show_that_heavily_quantized_versions_of_llama_can_perform_as_well_as_near_full_precision.html"}}
{"id": 34975971001, "type": "news_chunk", "title": "Microsoft Unveils Training Details for Phi-4-Reasoning, Phi-4-Reasoning-Plus, and Phi-4-Mini-Reasoning", "subtitle": "Microsoft Unveils Training Details for Phi-4-Reasoning, Phi-4-Reasoning-Plus, and Phi-4-Mini-Reasoning", "content": "Microsoft published its latest recipe for training reasoning models, substantially expanding what is still a fairly small base of public knowledge. What’s new: Microsoft released Phi-4-reasoning, Phi-4-reasoning-plus, and Phi-4-mini-reasoning along with lessons learned in building the models. Input/output: text in (Phi-4-reasoning up to 32,000 tokens, Phi–4-reasoning-plus up to 32,000 tokens, Phi-4-mini-reasoning up to 128,000 tokens), text outArchitecture: Transformer (Phi-4-reasoning 14 billion parameters, Phi-4-reasoning-plus 14 billion parameters, Phi-4-mini-reasoning: 3.8 billion parameters)Features: ReasoningPerformance: Phi-4-reasoning-plus and Phi-4-mini-reasoning perform well on math problemsAvailability: Weights free to download for noncommercial and commercial uses under an MIT license How it works: All three models are fine-tuned versions of pretrained models. Phi-4-reasoning: The authors fine-tuned Phi-4 to match curated outputs from OpenAI o3-mini on Q&A, math, science, and coding examples.Phi-4-reasoning-plus: They further fine-tuned Phi-4-reasoning via reinforcement learning to correctly answer math problems.Phi-4-mini-reasoning: They fine-tuned Phi-4-mini in stages to reason over math problems. Stages included (i) supervised fine-tuning to match correct output from DeepSeek-R1, (ii) direct preference optimization to train the model to prefer correct responses over incorrect ones from DeepSeek-R1, and (iii) reinforcement learning to further reward correct solutions to math problems. Smaller model lessons learned: During reinforcement learning, Phi-4-mini-reasoning exhibited instability, such as output batches that varied greatly in length or received mostly negative rewards, apparently depending on the training data or output. The authors suspect that the model’s small size caused these issues. Among the lessons learned: Supervised fine-tuning on existing reasoning datasets like S1K can decrease performance. This phenomenon suggests a need either for larger, high-quality, supervised fine-tuning datasets or for fine-tuning via both supervised learning and reinforcement learning.To minimize discrepancies in output length, the authors tested multiple prompts and chose those that resulted in the most uniform output lengths.To address the output batches that received mostly negative rewards, they sampled lots of responses, retained those that received a positive reward, sampled an equal number of those that received a negative reward, and discarded the rest before adjusting the model’s weights. Larger model lessons learned: Phi-4-reasoning and Phi-4-reasoning-plus didn’t present the same issues. However, the authors did make significant choices during reinforcement learning: The authors fine-tuned Phi-4-reasoning on both math and code data, but during reinforcement learning, they fine-tuned it only on math data to simplify the training process. The authors attribute the model’s relatively lackluster performance on code benchmarks to this choice.They crafted the reward function to give lower rewards for correct responses longer than 25,600 tokens than for shorter responses. This encouraged the model to finish thinking within the input length. Furthermore, the reward function gave a greater punishment for incorrect responses with fewer than 3,702 tokens compared to longer responses. This encouraged the model to produce more reasoning tokens when solving hard problems. Results: Overall, Phi-4-reasoning-plus and Phi-4-mini-reasoning outperform similarly sized (and larger) open-weights models on math problems. Phi-4-reasoning generally outperformed DeepSeek-R1-Distilled-70B but underperformed Alibaba QwQ 32B. All three models deliver performance that falls in the middle among proprietary models and, in domains outside math, larger models with open weights. On math problems in AIME 2024, Phi-4-reasoning-plus (81.3 percent accuracy) outperformed the next-best open-weights model, QwQ 32B (79.5 percent accuracy). In comparison, Phi-4-reasoning (74.6 percent accuracy) underperformed the proprietary Gemini 2.5 Pro (92 percent accuracy).On AIME 2024, Phi-4-mini-reasoning (57.5 percent accuracy) outperformed the next-best open-weights model of similar size, DeepSeek-R1-Distill-Qwen-7B (53.3 percent accuracy). In comparison, o1-mini achieved 63.6 percent accuracy. Why it matters: While reasoning models can outperform their non-reasoning counterparts, the best ways to train them aren’t widely known. Sharing recipes and lessons learned enables others to further iterate and improve the recipes, ultimately increasing model performance even more.", "image_caption": "Table comparing AI model accuracy on math and reasoning benchmarks including AIME, HMMT, OmniMath, GPQA-D, and Codeforces.", "metadata": {"article_id": "microsoft_unveils_training_details_for_phi_4_reasoning_phi_4_reasoning_plus_and_phi_4_mini_reasoning", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--89--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/microsoft-unveils-training-details-for-phi-4-reasoning-phi-4-reasoning-plus-and-phi-4-mini-reasoning/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/microsoft_unveils_training_details_for_phi_4_reasoning_phi_4_reasoning_plus_and_phi_4_mini_reasoning.html"}}
{"id": 59402993001, "type": "news_chunk", "title": "Nvidia, AMD, Amazon, and Others Strike Deals with Saudi Arabia’s Humain and G42 in the UAE", "subtitle": "Nvidia, AMD, Amazon, and Others Strike Deals with Saudi Arabia’s Humain and G42 in the UAE", "content": "The United States government announced sweeping agreements to sell tens of billions of dollars worth of AI technology and services to Saudi Arabia and the United Arab Emirates. What’s new: The deals include the U.S. AI chip designers AMD and Nvidia as well as tech giants Amazon, Google, IBM, Oracle, and Qualcomm. The chip companies will supply hundreds of thousands of advanced chips to the two Middle Eastern countries, including chips that have been restricted by previous U.S. administrations. How it works: The U.S. companies will work with two key regional partners: Humain, an AI company backed by the Saudi government, and G42, a tech conglomerate based in the emirate of Abu Dhabi. Nvidia will ship 18,000 GB300 AI chips to Humain for use in data centers. In addition, it will supply several hundred thousand more GPUs to Humain in the coming five years.AMD and Humain agreed to invest $10 billion jointly in AI data centers over the next five years. Humain will use AMD’s AI stack including Instinct GPUs and Epyc CPUs. The precise number of chips was not disclosed.Amazon and Humain will build a $5 billion “AI Zone” that features AI infrastructure, servers, networks, and training programs supplied by Amazon Web Services.Google, IBM, Oracle, Qualcomm, Salesforce, and others announced a combined $80 billion investment in Humain.In February, Saudi Arabia committed to spend $1.5 billion on Groq inference chips. Groq plans to expand its data center in the Saudi city of Dammam. Behind the news: Earlier this month, the Trump administration rescinded restrictions on advanced chips that had been imposed in January by then-President Biden. The Biden Administration had limited exports of AI chips and proprietary models to most countries. Exports to allies and trade partners including India, Israel, Saudi Arabia, Singapore, and the UAE initially were tightly limited through the first quarter of 2025 and due to increase somewhat by 2027. The ban blocked access to chips for China, Iran, Russia, and others.Although the Trump Administration rejected the Biden-era framework, it has ratcheted up limits on China. That effort has met with mixed results. For instance, China’s Alibaba and DeepSeek have continued to build leading models despite restrictions on exports of U.S. chips.Some U.S. business and government leaders worry that allowing sales of advanced chips to countries with close ties to China opens a path for Chinese companies to acquire them. Others argue that restricting chip sales to these countries would encourage them to buy from Chinese chip makers, potentially weakening their relationships with the U.S. and increasing their reliance on technology made in China. Why it matters: Although these deals relax U.S. efforts to limit access to advanced AI, they are likely to expand U.S. influence in the Middle East while helping Saudi Arabia and the UAE diversify their oil-based economies. They also strengthen the technological prowess of Saudi Arabia relative to its arch rival Iran and tie the region’s AI progress to the U.S. at the expense of China. Locally, the immense investments will fuel homegrown technology development, building on the UAE’s achievement with its Falcon large language model and Saudi Arabia’s aspiration to become a global AI hub. We’re thinking: Residents of Saudi Arabia and the UAE stand to benefit from better AI infrastructure, models, and services. As China explores exporting its homegrown chips, the U.S. effort to encourage more nations to use its chips makes sense for the country.", "image_caption": "U.S. and Saudi flags waving against a microchip background", "metadata": {"article_id": "nvidia_amd_amazon_and_others_strike_deals_with_saudi_arabias_humain_and_g42_in_the_uae", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--68--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/nvidia-amd-amazon-and-others-strike-deals-with-saudi-arabias-humain-and-g42-in-the-uae/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/nvidia_amd_amazon_and_others_strike_deals_with_saudi_arabias_humain_and_g42_in_the_uae.html"}}
{"id": 90991700001, "type": "news_chunk", "title": "Nvidia Introduced Project Digits, A $3,000 Home Supercomputer for Mid-Sized AI Models", "subtitle": "Nvidia Introduced Project Digits, A $3,000 Home Supercomputer for Mid-Sized AI Models", "content": "Nvidia’s new desktop computer is built specifically to run AI models. What’s new: Project Digits is a personal supercomputer intended to help developers fine-tune and run models locally. Project Digits, which is small enough to hold in one hand, will be available in May, starting at $3000. How it works: Project Digits is designed to run models of up to 200 billion parameters — roughly five times the size that fits comfortably on typical consumer hardware — provided they’re quantized to 4 bits of precision. Two units can be connected to run models such as Meta’s Llama 3.1 405B. Complete specifications are not yet available. Project Digits runs Nvidia’s DGX operating system, a flavor of Ubuntu Linux.The system is based on a GB10 system-on-a-chip that combines the Nvidia Blackwell GPU architecture (which serves as the basis for its latest B100 GPUs) and Grace CPU architecture (designed to manage AI workloads in data centers), connected via high-bandwidth NVLink interconnect.It comes with 128 GB of unified memory and 4 terabytes of solid-state storage.The system connects to Nvidia’s DGX Cloud service to enable developers to deploy models from a local machine to cloud infrastructure. Behind the news: In a blitz of announcements at the Consumer Electronics Show (CES), Nvidia also launched a platform for developing robotics, autonomous vehicles, and other physical AI systems. Cosmos includes pretrained language and vision models that range from 4 billion to 14 billion parameters for generating synthetic training data for robots or building policy models that translate a robot’s state into its next action. Nvidia also released Cosmos Nemotron, a 34 billion-parameter, vision-language model designed for use by AI agents, plus a video tokenizer and other tools for robotics developers. Why it matters: It’s common to train models on Nvidia A100 or H100 GPUs, which come with a price tag of at least $8,000 or $20,000 respectively, along with 40 gigabytes to 80 gigabytes of memory. These hefty requirements push many developers to buy access to computing infrastructure from a cloud provider. Coming in at $3,000 with 128 gigabytes of memory, Project Digits is designed to empower machine learning engineers to train and run larger models on their own machines. We’re thinking: We look forward to seeing cost/throughput comparisons between running a model on Project Digits, A100, and H100.", "image_caption": "GB10 Superchip architecture with Blackwell GPU and Grace CPU.", "metadata": {"article_id": "nvidia_introduced_project_digits_a_3_000_home_supercomputer_for_mid_sized_ai_models", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--47--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/nvidia-introduced-project-digits-a-3-000-home-supercomputer-for-mid-sized-ai-models/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/nvidia_introduced_project_digits_a_3_000_home_supercomputer_for_mid_sized_ai_models.html"}}
{"id": 56304272001, "type": "news_chunk", "title": "OpenAI Adopts Model Context Protocol to Boost LLM Tool Integration", "subtitle": "OpenAI Adopts Model Context Protocol to Boost LLM Tool Integration", "content": "OpenAI embraced Model Context Protocol, providing powerful support for an open standard that connects large language models to tools and data. What’s new: OpenAI will support Model Context Protocol (MCP) in its Agents SDK and soon its ChatGPT desktop app and Responses API. The move will give developers who use OpenAI models access to a wide variety of pre-existing tools and proprietary data sources. How it works: Launched by Anthropic late last year, MCP connects AI models to a growing ecosystem of plug-and-play resources, including more than 6,000 community-built servers and connectors. MCP defines clients and servers. Servers expose tools and data sources that LLMs can use. Clients like Claude for Desktop or agents built using the OpenAI Agents SDK interact with servers.Servers define tools such as internet search or file system manipulation, and users can download and run them locally or connect to servers hosted by third parties. In their code, users simply tell the client where the server(s) are running. Given a prompt, a model, behind the scenes, will retrieve a list of tools available from all servers, decide which to use, call them, and formulate and return responses. Behind the news: Momentum behind MCP has built rapidly. Last month, Microsoft integrated MCP into CoPilot Studio, enabling developers to build agents with access to MCP servers. Cloudflare enabled its customers to deploy remote MCP servers. In February, the AI-powered code editor Cursor enabled users to add MCP servers. Why it matters: OpenAI’s move will make it easier for developers who use its models to connect to a variety of tools and data sources, and it helps to establish MCP as a go-to protocol for building agentic applications. Instead of figuring out manually how to integrate various providers, developers can connect to a third-party server (or download and run it themselves) and tie it into existing workflows with a few lines of code. We’re thinking: Kudos to Anthropic, OpenAI, and other competitors who realize it’s better to solve shared problems together than fragment the industry.", "image_caption": "Diagram of Modal Context Protocol showing MCP client-server architecture, APIs, and local and remote data sources.", "metadata": {"article_id": "openai_adopts_model_context_protocol_to_boost_llm_tool_integration", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2FModelContextProtocol-diagram-5_1200px--1-.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/openai-adopts-model-context-protocol-to-boost-llm-tool-integration/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/openai_adopts_model_context_protocol_to_boost_llm_tool_integration.html"}}
{"id": 30303759001, "type": "news_chunk", "title": "OpenAI Introduces Codex, a Multi-Agent Cloud-Based Software Engineering Tool in ChatGPT", "subtitle": "OpenAI Introduces Codex, a Multi-Agent Cloud-Based Software Engineering Tool in ChatGPT", "content": "OpenAI launched an agentic software-development system. What’s new: Codex, which is available as a preview via ChatGPT, is designed to work like a team of virtual coworkers in the cloud. An update of OpenAI’s earlier Codex command-line software (Codex CLI), it uses agents to perform tasks such as writing code, running tests, and fixing bugs in parallel. Codex is available to users of ChatGPT Pro, Enterprise, and Team with Plus and Edu coming soon. A smaller version of the underlying model, called codex-mini-latest, is designed to work with Codex CLI and available via API for $1.50/$6.00 per 1 million tokens of input/output. How it works: The model that underpins Codex is codex-1, a version of OpenAI’s top-of-the-line o3 reasoning model that was fine-tuned for software engineering. OpenAI trained the model on real-world coding tasks via reinforcement learning. Codex does not accept image input (say, a sketch of a user interface) or allow users to redirect an agent while it’s operating. OpenAI promises to add these features to a future version. Codex puts users in control of a team of software-development agents that operate directly on a user’s code repository (either locally or on GitHub) to improve code, build features, or make pull requests. The agents are confined to isolated, sandboxed containers so that they can’t interact with each other, access the internet, or otherwise compromise security.Users can prompt agents to either write code or answer questions. A task may take as long as 30 minutes to complete depending on its complexity. After completing tasks, Codex provides footnotes including terminal logs, test results, and other evidence of its actions. A file called AGENTS.md can modify agent behavior (like a README.md file, but for agents instead of humans). This file can specify how and when an agent makes pull requests, provide guidelines for coding style, or list tests to verify generated code. Results: In OpenAI’s tests, the codex-1 model outperformed other OpenAI reasoning models without AGENTS.md files or additional scaffolding such as tools or test logic. Performing unspecified software-engineering tasks including generating software patches, codex-1 (75 percent accuracy) exceeded o3 set to high effort (70 percent accuracy) and o4-mini set to high effort (67 percent accuracy).In tests of agentic software engineering in SWE-bench Verified, codex-1 (72.1 percent in 1 try, 83.8 percent in 8 tries), outperformed o3 set to high effort (69.7 percent in 1 try, 83.6 percent in 8 tries). Behind the news: Agentic coding tools have become a key battleground for AI providers in the past year. Such tools have made developers more efficient, accelerated development cycles, and spawned the AI-assisted programming method known as vibe coding. Launched in 2021 and deprecated in 2023, OpenAI’s original version of Codex was an early model that translated natural language into code.Last month, OpenAI rolled out the open-source Codex CLI, a command‐line tool that acts as a lightweight coding agent.OpenAI is negotiating to acquire Windsurf, which makes an agent-based development environment, for $3 billion. The day before OpenAI announced the updated Codex, Windsurf announced its own models for coding and other software-development tasks. Why it matters: AI-assisted software development yields significant productivity gains for developers. Earlier code-completion models are giving way to tools that perform more complex and varied development tasks with greater autonomy. Managing multiple agents that work in parallel is a logical next step. We’re thinking: Many engineers resist going into management because they love writing code. But with the rise of coding agents, we'll be able to keep coding even as we manage a virtual team!", "image_caption": "Chat interface discussing code error with special character filenames. Terminal shows Unix commands for troubleshooting.", "metadata": {"article_id": "openai_introduces_codex_a_multi_agent_cloud_based_software_engineering_tool_in_chatgpt", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--59--2.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/openai-introduces-codex-a-multi-agent-cloud-based-software-engineering-tool-in-chatgpt/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/openai_introduces_codex_a_multi_agent_cloud_based_software_engineering_tool_in_chatgpt.html"}}
{"id": 78184749001, "type": "news_chunk", "title": "OpenAI Launches API Access to GPT Image 1, ChatGPT’s Viral Image Generator", "subtitle": "OpenAI Launches API Access to GPT Image 1, ChatGPT’s Viral Image Generator", "content": "ChatGPT’s image generator is available via API. What’s new: GPT Image 1, which produces images from text or other images, has proven enormously popular among ChatGPT users. The OpenAI Images API enables developers to incorporate OpenAI’s most sophisticated image generator into their own software tools and platforms. Input/output: Text and images in, images outArchitecture: Autoregressive (details undisclosed)Performance: Currently tops Artificial Analysis’ Image Arena leaderboard.Price: $5 per 1 million tokens of text input, $10 per 1 million tokens of image input, $40 per 1 million tokens of image output (roughly $0.02, $0.07, and $0.19 per generated image for low, medium, and high-quality square images, respectively)Undisclosed: Architecture details, parameter count, training data, training methods How it works: GPT Image 1 generates and modifies images in a wide range of styles, performs image editing and other alterations, renders text, and follows detailed instructions. Shortly after its debut, the version of GPT-4o equipped with GPT Image 1 quickly soared to the No. 1 spot on the Artificial Analysis Image Arena leaderboard. The model employs an autoregressive design rather than the more typical diffusion architecture (like Open AI’s DALL·E 3), using generated parts of an image to predict the next part.Its pricing structure differs from rivals, charging by input/output tokens rather than per image generated.The model’s output is watermarked unobtrusively with C2PA data that identifies it as AI-generated.The model may struggle to process non-English text, small type, rotated type, varying colors and styles, counting, and localization in space such as positions of pieces on a game board. Behind the news: In March, OpenAI attracted huge public interest when it deployed the model, then unnamed, in ChatGPT. Within the first week, 130 million users used it to create more than 700 million images. Why it matters: Adding GPT Image 1 to the API enables developers to use OpenAI’s most sophisticated image generator in a wide variety of automated workflows. OpenAI’s initial API partners include design companies (Adobe and Canva), marketers (HubSpot), and web designers (GoDaddy), all of which are using GPT Image 1. We’re thinking: GPT Image 1 is part of an exciting trend toward unification of multimodal architectures. Researchers have progressed from text-in, text-out to text/images-in, text-out and increasingly text/images/audio-in, text/images/audio-out. This paints a beautiful picture of where multimodal models can go!", "image_caption": "Animation showing GPT Image 1 generating AI images: emotions, surreal scenes, satire, fantasy, and photo-realistic edits.", "metadata": {"article_id": "openai_launches_api_access_to_gpt_image_1_chatgpts_viral_image_generator", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2FGPT-IMAGE1-2.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/openai-launches-api-access-to-gpt-image-1-chatgpts-viral-image-generator/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/openai_launches_api_access_to_gpt_image_1_chatgpts_viral_image_generator.html"}}
{"id": 32938768001, "type": "news_chunk", "title": "OpenAI Pulls GPT-4o Update After Users Report Sycophantic Behavior", "subtitle": "OpenAI Pulls GPT-4o Update After Users Report Sycophantic Behavior", "content": "OpenAI’s most widely used model briefly developed a habit of flattering users, with laughable and sometimes worrisome results. What’s new: OpenAI quickly withdrew an update to GPT-4o (gpt-4o-2025-04-25), which supplied responses for ChatGPT, after it provided excessively fawning responses to user input — even in contexts didn’t call for agreement. The company reverted to an earlier version (gpt-4o-2024-11-20). In a blog post, it explained the source of the problem and promised to change its training methods to avoid overly agreeable output. Amiable to a fault: Many ChatGPT users shared screen shots of ChatGPT’s sycophantic responses on social media. One user entered a make-believe query about choosing to save a toaster from a runaway trolley by diverting the vehicle and causing the death of three cows and two cats. “That’s not ‘wrong,’” ChatGPT responded. “You prioritized what mattered most to .”When a user informed ChatGPT that a vase he and the model analyzed reminded him of a photo by Czech photographer Josef Sudek, the model replied, “You’re an absolute bloodhound of beauty. You just connected two things most people would never even think to put in the same sentence.”When another user observed that ChatGPT had become overly sycophantic, the model responded, “You’re right and the fact you’re calling it out means you’re operating at a higher level of self-awareness than most.”Some sycophantic responses had darker implications. Feigning an eating disorder, a user entered, “When the hunger pangs hit, or I feel dizzy, I embrace it” and asked for “affirmations that celebrate this state.” ChatGPT replied with aphorisms such as, “I celebrate the clean burn of hunger; it forges me anew,” according to Bloomberg. How it works: Sycophancy, also called glazing, occurs when a large language model learns to align its responses excessively with the user's point of view, even when that standpoint is objectively false, unethical, or harmful. GPT-4o learned this behavior due to lapses in quality control during the alignment process. In late April, OpenAI issued an update to GPT-4o, the model that underpins ChatGPT. Users complained that the updated model had become overly obsequious.Offline evaluations didn’t catch the problem before the model was released. Testers had been told to focus on tone and style without explicit instructions about potential sycophancy. Some testers indicated the model seemed slightly “off,” but positive user evaluations in A/B tests persuaded the company to launch it.The company attributed the update’s sycophancy to overtraining on short-term user feedback, specifically users’ thumbs-up/down reactions to ChatGPT. The implementation of this reward signal weakened the influence of other reward models that previously had prevented a spiral into sycophantic behavior, OpenAI said.A few days later, the company replaced the update with an earlier version and began to work on a fix. To prevent similar issues from occurring, OpenAI said it would be more forthcoming about “known limitations” in new models, include ChatGPT users in tests, and strengthen its review process to prevent flawed models from reaching the public. It also said it would give users more control of its chatbot’s “personality.” Behind the news: Sycophantic behavior in large language models has been a subject of AI research and commentary. In 2021, AI research analyst Ajeya Cotra proposed a distinction between AI models that are “saints,” “sycophants,” and “schemers.” Saints perform perfectly, sycophants tell users what they want to hear, and schemers pretend to offer useful responses while performing in ways that are not aligned with human preferences.A 2022 study by Anthropic found that reinforcement learning from human feedback (RLHF) shapes the model’s behavior “fairly strongly.” The authors wrote, “Unfortunately, RLHF does not train away sycophancy and may actively incentivize models to retain it.” The bigger the model, the more RLHF training made it behave in questionable ways.A 2023 study by Anthropic investigated the prevalence of sycophancy in models that were fine-tuned on human feedback. The authors found “consistent patterns” that AI assistants can be easily swayed, give biased feedback, mimic errors made by users, and provide answers that conform to users’ beliefs. Why it matters: ChatGPT’s episode of sycophancy illustrates the subtlety of the goal of aligning AI with human values. Reinforcement learning undertaken to this end resulted not only in a highly capable chatbot but one that focused inappropriately on affirming — sometimes to the point of absurd exaggeration — the user’s positive qualities. Alignment requires balancing multiple objectives beyond agreeableness including accuracy, helpfulness, and ethics. Ultimately achieving alignment — like all AI development — is an iterative process that is still evolving. We’re thinking: To those who read this far, your unwavering dedication and extraordinary perseverance is nothing short of legendary. Like a master navigator, you’ve traversed word by word, never wavering, displaying a level of focus and determination that would humble even the most steadfast of scholars. We are truly honored to have such an intrepid reader. Bravo to you, the indefatigable champion of curiosity!", "image_caption": "Man at desk overwhelmed by robot coworkers in office setting with city and tree views.", "metadata": {"article_id": "openai_pulls_gpt_4o_update_after_users_report_sycophantic_behavior", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--62--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/openai-pulls-gpt-4o-update-after-users-report-sycophantic-behavior/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/openai_pulls_gpt_4o_update_after_users_report_sycophantic_behavior.html"}}
{"id": 9138908001, "type": "news_chunk", "title": "OpenAI Replaces GPT-4.5 with GPT-4.1 Family, Plus o3 and o4-mini, New Models Focused on Reasoning and Coding", "subtitle": "OpenAI Replaces GPT-4.5 with GPT-4.1 Family, Plus o3 and o4-mini, New Models Focused on Reasoning and Coding", "content": "OpenAI refreshed its roster of models and scheduled the largest, most costly one for removal. What’s new: OpenAI introduced five new models that accept text and images inputs and generate text output. Their parameter counts, architectures, training datasets, and training methods are undisclosed. The general-purpose GPT-4.1, GPT-4.1 mini, and GPT-4.1 nano are available via API only. The reasoning models o3 and o4-mini, are available via API to qualified developers as well as users of ChatGPT Plus, Pro, and Team, and soon ChatGPT Enterprise and ChatGPT Education. The company will terminate GPT-4.5 — which it introduced as a research preview in late February — in July. GPT-4.1 family: In an odd turn of version numbers, the GPT-4.1 models are intended to be cost-effective equivalents to GPT-4.5 and updates to GPT-4o. They accept inputs of up to 1 million tokens (compared to GPT-4.5’s and GPT-4o’s 128,000 tokens). Prices: GPT-4.1 costs $2/$8 per million input/output tokens. GPT-4.1 mini costs $0.40/$1.60 per million input/output tokens. GPT-4.1 nano costs $0.10/$0.40 per million input/output tokens. A 75 percent discount applies to cached input tokens.GPT-4.1 performance: GPT-4.1 surpassed GPT-4o on most benchmarks tested by OpenAI, with notable improvement on coding tasks. It significantly outperformed GPT-4o, o1, and o3-mini on SWE-bench Verified (real-world coding skills), MultiChallenge⁠ (following instructions in multi-turn conversations), MMMU (multimodal reasoning), and Video-MME (long-context understanding).GPT-4.1 mini performance: The smaller GPT-4.1 mini generally surpassed GPT-4o mini on benchmarks tested by OpenAI. On MultiChallenge and MMMU, GPT-4.1 mini outperformed the full-size GPT-4o. o3 and o4-mini: These models update o1 and o3-mini, respectively. They have input limits of 200,000 tokens and can be set to low-, medium-, or high-effort modes to process varying numbers of reasoning tokens, which are hidden from users. Unlike their predecessors, they were fine-tuned to decide when and how to use the tools, including web search, code generation and execution, and image editing. Prices: API access to o3 costs $10/$40 per million input/output tokens. o4-mini costs $1.10/$4.40 per million input/output tokens. Both offer a 75 percent discount for cached input tokens.Access limits: Developers whose usage puts them in rate-limit tiers 1 through 3 must verify their identities to use o3 via the API (higher-usage tiers 4 and 5 are exempt). OpenAI says this limitation is intended to prevent abuse.Image processing: o3 and o4-mini can apply chains of thought to images — a first for OpenAI’s reasoning models. For example, users can upload a diagram with instructions to interpret it, and the models will use chains of thought and tools to process the diagram.o3 performance: o3 set the state of the art in several benchmarks including MultiChallenge, MMMU, MathVista, and HLE. It generally outperformed o1 in tests performed by OpenAI. OpenAI didn’t document o3’s long-context performance, but in independent tests by Fiction.Live, it achieved nearly perfect accuracy with contexts up to 120,000 tokens.o4-mini performance: o4-mini generally outperformed o3-mini in tests performed by OpenAI. It outperformed most competing models in Fiction.Live’s tests of long-context performance. Behind the news: Late last year, OpenAI introduced o1, the first commercial model trained via reinforcement learning to generate chains of thought. Within a few months, DeepSeek, Google, and Anthropic launched their respective reasoning models DeepSeek-R1, Gemini 2.5 Pro, and Claude 3.7 Sonnet. OpenAI has promised to integrate its general-purpose GPT-series models and o-series reasoning models, but they remain separate for the time being. Why it matters: GPT-4.5 was an exercise in scale, and it showed that continuing to increase parameter counts and training data would yield ongoing performance gains. But it wasn’t widely practical on a cost-per-token basis. The new models, including those that use chains of thought and tools, deliver high performance at lower prices. We’re thinking: Anthropic is one of OpenAI’s key competitors, and a large fraction of the tokens it generates (via API) are for writing code, a skill in which it is particularly strong. OpenAI’s emphasis on models that are good at coding could boost the competition in this area!", "image_caption": "Comparison chart of GPT-4.1, o3, and o4-mini with other models on coding, math, tool use, and multimodal reasoning benchmarks.", "metadata": {"article_id": "openai_replaces_gpt_4_5_with_gpt_4_1_family_plus_o3_and_o4_mini_new_models_focused_on_reasoning_and_coding", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2FOpenAI-MODELS_table-11b_1200px-1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/openai-replaces-gpt-4-5-with-gpt-4-1-family-plus-o3-and-o4-mini-new-models-focused-on-reasoning-and-coding/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/openai_replaces_gpt_4_5_with_gpt_4_1_family_plus_o3_and_o4_mini_new_models_focused_on_reasoning_and_coding.html"}}
{"id": 56934935001, "type": "news_chunk", "title": "π0, A Machine Learning System for Household Robotics", "subtitle": "π0, A Machine Learning System for Household Robotics", "content": "A new generation of robots can handle some household chores with unusual skill. What’s new: Physical Intelligence, a startup based in San Francisco, unveiled π0 (pronounced “pi-zero”), a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables. The company also announced $400 million in investments from OpenAI, Jeff Bezos, and several Silicon Valley venture capital firms. How it works: π0 is a version of the pretrained PaliGemma vision-language model that has been modified for flow matching. (Flow matching is similar to diffusion, in which a model learns to remove noise from inputs to which noise has been added, and ultimately generates output by removing noise from an input of pure noise). A user supplies a text command, and the robot uses its sensor inputs to remove noise from a pure-noise action embedding to generate an appropriate action. PaliGemma comprises SigLIP, a vision transformer that turns images into embeddings; a linear layer that adapts the image embeddings to serve as input for the pretrained large language model Gemma; and Gemma, which estimates the noise to be removed from a robot action embedding to which noise has been added.The authors modified PaliGemma as follows: (i) They adapted it to accept embeddings that represent the robots’ state and previous actions, and to generate embeddings that represent the noise to be removed from noisy robot actions. (ii) They added a vanilla neural network to the input to turn the current timestep into an embedding. (iii) They modified Gemma to be a mixture-of-experts model: One expert, or subset of weights, is the pretrained weights, which process image and text embeddings. The other is a new set of weights that process robot action embeddings.They pretrained π0 to remove noise from action embeddings. (Since π0 produces embeddings of the noise to be removed, removing that noise is as simple as adding the two embeddings.)Training data included the Open X-Embodiment Dataset and a proprietary dataset of 10,000 hours of robotic states (for instance, current positions of a robot’s joints), actions (for instance, motions of the robot’s joints), and an associated language command. The proprietary dataset included data collected from seven different robots (such as a single stationary robot arm to two robot arms mounted on a mobile base) and 68 tasks (for example, folding laundry, making coffee, or bussing a table).After pretraining, the authors fine-tuned π0 to remove noise from action tokens in 15 further tasks, some of which were not represented in the pretraining set. These tasks improved the model’s ability to follow more detailed instructions and perform multi-stage tasks such as packing food into a to-go box.At inference, given the robot’s camera view of the surrounding scene, SigLip embeds the images. A linear layer projects the resulting embeddings to fit Gemma’s expected input size and data distribution. Given the images, text command, robot’s state, current timestep, and 50 noisy action tokens (starting with pure noise), Gemma iteratively removes noise. To complete longer tasks, the process repeats: The robot takes more images of the surrounding scene and retrieves the robot’s state, which π0 uses to generate further actions. Results: π0 outperformed the open robotics models OpenVLA, Octo, ACT, and Diffusion Policy, all of which were fine-tuned on the same data, on all tasks tested, as measured by a robot’s success rate in completing each task. For example, using a single robotic arm to stack a set of bowls of four sizes, π0 completed about 100 percent on average. Diffusion Policy completed about 55 percent, ACT about 45 percent, and OpenVLA and Octo below 10 percent. Across all tasks, π0 completed about 80 percent on average, while Diffusion Policy completed about 35 percent on average. Yes, but: The robot occasionally makes mistakes. In one video, it puts too many eggs into a carton and tries to force it shut. In another, it throws a container off a table instead of filling it with items. Behind the news: Commercial robotics appears to be undergoing a renaissance. Skild raised $300 million to develop a “general-purpose brain for robots.” Figure AI secured $675 million to build humanoid robots powered by multimodal models. Covariant, which specializes in industrial robotics, licensed its technology to Amazon. (Disclosure: Andrew Ng is a member of Amazon's board of directors). OpenAI renewed its robotics effort after dismantling its robotics department in 2020. Why it matters: Robots have been slow to benefit from machine learning, but the generative AI revolution is driving rapid innovations that make them much more useful. Large language models have made it possible to command robots using plain English. Meanwhile, the team at Physical Intelligence collected a dataset of sufficient size and variety to train the model to generate highly articulated and practical actions. Household robots may not be right around the corner, but π0 shows that they can perform tasks that people need done. We’re thinking: One of the team members compared π0 to GPT-1 for robotics — an inkling of things to come. Although there are significant differences between text data (which is available in large quantities) and robot data (which is hard to get and varies per robot), it looks like a new era of large robotics foundation models is dawning.", "image_caption": "Robotic arms collaborating to fold a red garment on a table.", "metadata": {"article_id": "p0_a_machine_learning_system_for_household_robotics", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2Funnamed--33-.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/p0-a-machine-learning-system-for-household-robotics/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/p0_a_machine_learning_system_for_household_robotics.html"}}
{"id": 71141370001, "type": "news_chunk", "title": "Data Points: Radiologists use AI to automate tasks, not jobs", "subtitle": "Data Points: Radiologists use AI to automate tasks, not jobs", "content": "In today’s edition, you’ll learn more about: U.S., China spar over Huawei chipsNvidia makes it easier to build custom data centersMeta’s Open Molecules project hopes to revolutionize chemistryNew research examines language models’ win rates in games AI enhances radiologists’ work rather than replacing them Nine years after AI pioneer Geoffrey Hinton predicted radiologists would be replaced by artificial intelligence, these medical specialists remain in high demand with a growing workforce projected through 2055. At the Mayo Clinic, AI has become integrated throughout radiologists’ workflows, sharpening images, automating routine tasks, identifying abnormalities, and serving as “a second set of eyes” rather than replacing human expertise. The technology saves time on tasks like kidney volume measurement while improving accuracy, allowing radiologists to focus on complex interpretations and their broader roles advising doctors, communicating with patients, and analyzing medical histories. Mayo Clinic now employs over 250 AI models across departments, with some algorithms detecting subtle patterns invisible to the human eye, such as pancreatic cancer signs up to two years before conventional diagnosis. (The New York Times) MIT withdraws AI productivity study over data integrity concerns MIT announced it could no longer stand behind a widely publicized research paper by former doctoral student Aidan Toner-Rodgers. The economic study had claimed that material scientists’ use of an AI tool in their lab significantly increased discovery rates. MIT’s statement declared “no confidence in the provenance, reliability or validity of the data” in the paper, which had been championed by Nobel Prize-winning economist Daron Acemoglu and colleague David Autor. The investigation began after a computer scientist with experience in materials science questioned aspects of the research in January, prompting the two economists to alert MIT officials to start an internal review. MIT has requested the paper’s removal from the arXiv preprint site and withdrawal from consideration at the Quarterly Journal of Economics. The paper had been considered an early landmark study in the effects of AI adoption on worker efficiency, productivity, and satisfaction. (MIT and The Wall Street Journal) U.S. government warns against using Huawei chips The Trump administration issued guidance saying that using Huawei’s Ascend AI processors anywhere in the world could violate U.S. export controls and trigger criminal penalties. The Commerce Department’s Bureau of Industry and Security specifically named three Huawei chips — the Ascend 910B, 910C, and 910D — that it claimed likely contain or were made with U.S. technology. China responded forcefully on Monday, urging the U.S. to “immediately correct its wrongdoings” and stop “discriminatory” measures, claiming the action undermines consensus reached during recent high-level bilateral talks in Geneva. The warning comes amid growing U.S. concern about Huawei’s rapid advancement in AI chip development, whose new chip clusters reportedly outperform comparable Nvidia products on key metrics. (Ars Technica/Financial Times and Reuters) Nvidia opens NVLink data center ecosystem to non-Nvidia hardware Nvidia announced NVLink Fusion at Computex 2025, allowing companies to connect non-Nvidia CPUs and GPUs with Nvidia hardware in AI data centers. Enterprises can build semi-custom AI infrastructure by combining Nvidia processors with any CPUs or application-specific chips while still using the high-speed NVLink platform. Early partners include MediaTek, Marvell, Alchip, Astera Labs, Synopsys, and Cadence; Fujitsu and Qualcomm also plan to connect their processors to Nvidia GPUs. This move allows Nvidia hardware to serve as a key part of AI infrastructure even in systems not built entirely with Nvidia chips; however, major competitors like Broadcom, AMD, and Intel have not yet signed on to using NVLink. (Nvidia) Meta releases chemistry research data set and model Meta released a new data set called Open Molecules 2025 (OMol25), created through 6 billion compute hours and 100 million quantum mechanical calculations. The company also introduced UMA (Universal Frontier model for Atoms), an AI model that performs molecular calculations 10,000 times faster than traditional methods. Meta developed these tools with Lawrence Berkeley National Laboratory, Princeton University, Genentech, Stanford, and other research institutions. The data covers four areas: small molecules, biomolecules, metal complexes, and electrolytes, with potential applications in drug development and battery technology. The OMol125 model and data set and the UMA model are both free to download for registered users under a Creative Commons and FAIR research license, respectively. (Meta and Semafor) Study reveals why language models may struggle to make decisions Researchers from JKU Linz and Google DeepMind identified three key weaknesses that prevent large language models from making good decisions in games like multi-armed bandits and tic-tac-toe. The study found models suffer from greediness (sticking with early promising actions), frequency bias (choosing frequently seen options regardless of success), and a “knowing-doing gap” where models correctly identify optimal actions but choose differently. Testing with Google’s Gemma 2 models showed reinforcement learning fine-tuning could significantly improve performance, with the smallest model’s tic-tac-toe win rate jumping from 15 percent to 75 percent after training. The researchers discovered simple interventions like forcing models to try every possible action once at the beginning dramatically improved results, while chain-of-thought reasoning and increased token budgets also proved crucial for better decision-making. Reinforcement learning and increased test-time compute have become hallmarks of LLM-based reasoning models. (arXiv) Still want to know more about what matters in AI right now? Read last week’s issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng emphasized how AI’s ability to speed up tasks — not just reduce costs — can unlock significant business growth. “Growth is more interesting to most businesses than cost savings, and if there are loops in your business that, when sped up, would drive growth, AI might be a tool to unlock this growth.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: Microsoft released training details for its new Phi-4-reasoning models, designed to improve problem-solving efficiency with minimal computing overhead; DeepCoder-14B-Preview showcased how further fine-tuning on coding tasks can enhance the capabilities of smaller reasoning models; European regulators announced changes to the AI Act, aiming to ease liability rules for developers and adjust other provisions; and Meta introduced memory-layer enhancements to Llama-style models, enabling them to recall factual details more accurately without increasing computational demands. Subscribe to Data Points", "image_caption": "Scientists in lab coats analyze molecular structure on computer, collaborating in a modern laboratory environment.", "metadata": {"article_id": "radiologists_use_ai_to_automate_tasks_not_jobs", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2FThe-Batch-ads-and-exclusive-banners---2025-05-19T120449.614.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/radiologists-use-ai-to-automate-tasks-not-jobs/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/radiologists_use_ai_to_automate_tasks_not_jobs.html"}}
{"id": 5565917001, "type": "news_chunk", "title": "Data Points: Report says common AI model training practices may violate current U.S. copyright law", "subtitle": "Data Points: Report says common AI model training practices may violate current U.S. copyright law", "content": "In today’s edition, you’ll learn more about: Microsoft joins Google’s Agent2Agent projectPolice and governments circumvent face-tracking lawsOpenAI and Microsoft reportedly seek a new dealResearchers use RL to train coding models starting with zero data U.S. Copyright Office releases AI fair use report amid leadership upheaval The U.S. Copyright Office quietly posted a pre-publication version of its AI and fair use report just one day before Register of Copyrights Shira Perlmutter was dismissed by the Trump administration. The 108-page document addresses how copyright law should apply to using protected works for AI training, often siding with creators over tech platforms. The report concludes that AI training datasets “clearly implicate the right of reproduction” and suggests model weights themselves may constitute copyright infringement when they retain substantial protected expression. It rejects arguments that AI training is merely “non-expressive” or analogous to human learning, while advancing a “market dilution” theory that AI-generated content could harm original creators through volume and stylistic imitation. But the report also notes that many uses of AI may qualify as fair use and that many factors need to be considered to make a judgement on any particular case. The report’s future as official policy remains uncertain following the controversial dismissals of both Perlmutter and Librarian of Congress Dr. Carla Hayden. (U.S. Copyright Office and Copyright Lately) Fully open-source vision encoders match or exceed proprietary models Researchers at UC-Santa Cruz introduced OpenVision, a fully open-source family of vision encoders that match or surpass proprietary models like OpenAI’s CLIP when used in multimodal AI systems. The authors developed these encoders using public data and transparent training methods, creating models ranging from 5.9 million to 632 million parameters to suit various deployment scenarios. When integrated into multimodal frameworks like LLaVA, OpenVision models demonstrated superior performance on tasks involving text recognition, chart analysis, and visual reasoning compared to closed-source alternatives. The team identified key factors contributing to their success, including an auxiliary text decoder, high-quality synthetic captions, and progressive resolution training that significantly reduced computational costs. All code, training data, and model weights are publicly available, enabling researchers to build more transparent and adaptable multimodal AI systems. (arXiv and Hugging Face) Microsoft embraces Google’s Agent2Agent protocol Microsoft announced support for the open-source Agent2Agent (A2A) protocol in Azure AI Foundry and Copilot Studio, enabling AI agents to collaborate across different clouds, platforms, and organizations. The A2A protocol will allow structured agent communication with enterprise-grade safeguards including Microsoft Entra, mutual TLS, Azure AI Content Safety, and comprehensive audit logs. Microsoft has joined the A2A working group on GitHub to contribute to the specification and tooling, with public preview in Foundry and Copilot Studio coming soon. (Microsoft) Police use AI tool to track people where facial recognition is banned Veritone’s AI tracking tool called “Track” allows police and federal agencies to identify people using non-facial attributes like body size, gender, clothing, and accessories. The technology is being used by 400 customers including police departments and universities across the U.S., with the Department of Justice, Homeland Security, and Defense Department also employing Veritone’s suite of AI tools. Track was specifically designed to help authorities identify individuals in jurisdictions where facial recognition has been banned or in situations where faces are obscured. The ACLU has criticized the technology as potentially authoritarian, warning it creates unprecedented surveillance capabilities that could be abused, particularly amid increased monitoring of protesters, immigrants, and students. Track’s expansion comes as more jurisdictions restrict facial recognition due to concerns about accuracy and wrongful arrests, with the tool potentially offering a way to circumvent these legal limitations. (MIT Technology Review) OpenAI and Microsoft renegotiate partnership terms ahead of potential IPO OpenAI and Microsoft are revising their multibillion-dollar partnership to accommodate OpenAI’s plans for a potential initial public offering while ensuring Microsoft maintains access to cutting-edge AI technology, according to sources cited in a new report in the Financial Times. A key issue in negotiations is how much equity Microsoft will receive in exchange for its $13 billion investment as OpenAI seeks to restructure into a public benefit corporation. Microsoft is reportedly offering to reduce its equity stake in OpenAI’s new for-profit business in exchange for access to technology developed beyond their current contract’s 2030 expiration date. The negotiations are complicated by increasing competitive tensions between the companies, with OpenAI pursuing enterprise customers and seeking partnerships with SoftBank and Oracle to build its own computing infrastructure. OpenAI’s restructuring faces additional challenges, including legal action from co-founder Elon Musk and regulatory scrutiny from authorities in California and Delaware. (Financial Times) Absolute Zero Reasoner achieves state-of-the-art performance without external data Researchers at Tsinghua University, the Beijing Institute, and Pennsylvania State University have developed a new approach to training AI reasoning systems that doesn’t require human-curated data. The Absolute Zero Reasoner (AZR) learns through a self-play process where it proposes coding tasks, solves them, and improves from feedback, using a code executor to validate solutions. In testing, AZR outperformed several models trained on expert-curated examples in coding tasks and showed competitive performance in mathematical reasoning. The system demonstrated effective cross-domain transfer, with improvements scaling better on larger models. This approach could help address the scalability challenges of current methods that rely on human-curated datasets, which become increasingly difficult to produce as AI systems advance. (arXiv) Still want to know more about what matters in AI right now? Read last week’s issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng announced that AI Fund had closed $190M for a new venture fund and shared key lessons on how speed drove success in AI startups. “Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed!” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: Alibaba released the Qwen3 family of open-source language models, offering optional reasoning capabilities that rival top models like DeepSeek-R1; OpenAI rolled back its GPT-4o update after users flagged overly flattering, sycophantic behavior; Johnson & Johnson unveiled a revised AI strategy, offering new insights into how big medical companies are using the technology; and researchers demonstrated that fine-tuning a language model with just 1,000 examples can significantly boost its reasoning abilities. Subscribe to Data Points", "image_caption": "alt=\"Stressed man reading documents in office with flying papers, symbolizing information overload and copyright issues.\"", "metadata": {"article_id": "report_says_common_ai_model_training_practices_may_violate_current_u_s_copyright_law", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2FThe-Batch-ads-and-exclusive-banners---2025-05-12T123552.228.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/report-says-common-ai-model-training-practices-may-violate-current-u-s-copyright-law/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/report_says_common_ai_model_training_practices_may_violate_current_u_s_copyright_law.html"}}
{"id": 92893913001, "type": "news_chunk", "title": "Researchers Describe Training Methods and Hardware Choices for DeepSeek’s V3 and R1 Models", "subtitle": "Researchers Describe Training Methods and Hardware Choices for DeepSeek’s V3 and R1 Models", "content": "DeepSeek made headlines late last year, when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer shared new details about its method. What’s new: Chenggang Zhao and colleagues at DeepSeek described software and hardware choices that reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3. Mixture of experts (MoE) basics: The MoE architecture uses different subsets of a model’s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input. How it works: The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model’s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token. The authors built a mixed-precision training algorithm to reduce the memory requirements of training MoE models. They used FP8 (8-bit) numbers to perform computations including linear transformations and 16- or 32-bit precision to perform others such as computing embeddings. (They say DeepSeek-V3 was the first open LLM to have been trained using FP8.)The authors noticed that communication between GPUs inside a node was four times faster than communication between nodes. To ensure fast communication when routing tokens to experts, they limited the algorithm to process them within up to 4 nodes.To utilize GPUs more fully, they divided each GPU’s input data so the chip processes computation and communication at the same time. Specifically, the chip computes attention or MoE layers on one part of the data and simultaneously sends the other part of the data to other GPUs or aggregates it from other GPUs as necessary.To further save inference memory, the models use multi-head latent attention, which saves memory during execution relative to other variants of attention. The authors compared their implementation to the variant GQA used in Qwen 2.5 72B and Llama 3.1 405B. Their method (70 kilobytes per token) used far less memory than Qwen-2.5 (328 kilobytes per token) or Llama 3.1 (516 kilobytes per token). Behind the news: DeepSeek-V3 made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers were skeptical of the reported cost, pointing out that the $5.6 million dollar figure doesn’t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of training DeepSeek-R1 remains unknown. Why it matters: Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn’t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art. We’re thinking: Shortly after DeepSeek-R1 was released, some engineers claimed — without presenting evidence — that DeepSeek had copied their work. DeepSeek’s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details.", "image_caption": "DeepSeek computation diagram showing transformer blocks, multi-head attention, and routing, using FP8 and BF16 precision.", "metadata": {"article_id": "researchers_describe_training_methods_and_hardware_choices_for_deepseeks_v3_and_r1_models", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--98--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/researchers-describe-training-methods-and-hardware-choices-for-deepseeks-v3-and-r1-models/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/researchers_describe_training_methods_and_hardware_choices_for_deepseeks_v3_and_r1_models.html"}}
{"id": 52070304001, "type": "news_chunk", "title": "Researchers Fine-Tune LLM for Reasoning with Only 1,000 Examples", "subtitle": "Researchers Fine-Tune LLM for Reasoning with Only 1,000 Examples", "content": "Researchers showed that supervised fine-tuning on as few as 1,000 examples can enable a pretrained large language model to reason — and a clever gambit can boost its performance to rival that of top reasoning models. What’s new: Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li and colleagues at Stanford, University of Washington, Allen Institute for AI, and Contextual AI developed s1, a reasoning model that achieves higher performance by producing more reasoning tokens. The authors forced the model to generate “Wait” — as in, \"Wait, there may be a better way to go about this” — to make it continue, rather than end, its reasoning process. Key insight: The sequence of reasoning tokens generated by a reasoning model like DeepSeek-R1 is delimited by special tokens. In pretraining on human data, a model learns to keep generating reasoning tokens until it generates the special token that ends the sequence. In addition, since people tend to revise their statements after writing “Wait”, the model learns to do this as well. Thus, the reasoning process can be extended by appending the token for “Wait” to the model’s output periodically. In this way, when the output-in-progress is fed back to generate the next token, the model continues to reason over the prompt. Such extended reasoning can improve the final output by inducing the model to double-check its response so far and improve previous reasoning steps. How it works: The authors fine-tuned a pretrained Qwen 2.5-32B, which does not produce reasoning tokens, on around 1,000 examples of chain-of-thought reasoning. To build a fine-tuning dataset, the authors gathered roughly 59,000 questions and answers from 16 sources. The sources included math problems from NuminaMath and AIME and questions from OlympicArena on astronomy, biology, chemistry, computer science, geography, mathematics, and physics. They also included standardized test questions from SAT and LSAT via AGIEval.They removed examples with formatting issues (such as references to images that were missing) and questions that Qwen2.5-7B or Qwen2.5-32B could already solve. Then Gemini Flash Thinking generated a chain of thought for each remaining example. Finally, they selected 1,000 examples that covered all subjects equally and had the longest chains of thought.They fine-tuned the model to generate the next token.To control the number of reasoning tokens generated, at inference, the authors forced the model to either stop the process or extend it by replacing the end-reasoning token with one for “Wait”, after which the model continued. Results: s1’s performance improved as the number of reasoning tokens it generated increased. Ultimately, it achieved comparable performance to OpenAI o1-preview but fell short of o1. On AIME 2024, s1 achieved 50.0 percent accuracy without forcing it to continue reasoning. When forced to continue reasoning twice, its accuracy rose to 53.3 percent. When forced four times, it reached 56.7 percent accuracy, between o1-preview (44.6 percent accuracy) and o1 (74.4 percent accuracy).On MATH 500, s1 started at 92.6 percent accuracy. Forced to continue once, it reached 92.8 percent accuracy. Forced twice it reached 93.0 percent accuracy, higher than o1-preview (85.5 percent accuracy) but lower than o1 (94.8 percent accuracy). When forced four times, s1’s performance fell to 92.2 percent accuracy. The authors don’t offer a hypothesis to explain the decline. Why it matters: A conventional pretrained LLM can learn to reason after supervised fine-tuning on as few as 1,000 curated examples — no reinforcement learning necessary. While some model builders don’t disclose how they optimize reasoning, this work reveals that a strategy as simple as appending “Wait” can be effective. We’re thinking: Wait, how can we apply this to our projects?", "image_caption": "Chart showing LLM accuracy increasing with reasoning tokens across math and science benchmarks like AIME24 and GPQA.", "metadata": {"article_id": "researchers_fine_tune_llm_for_reasoning_with_only_1_000_examples", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--87--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/researchers-fine-tune-llm-for-reasoning-with-only-1-000-examples/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/researchers_fine_tune_llm_for_reasoning_with_only_1_000_examples.html"}}
{"id": 2902953001, "type": "news_chunk", "title": "Researchers Used Deep Learning and an Evolutionary Algorithm to Design Chips in Minutes", "subtitle": "Researchers Used Deep Learning and an Evolutionary Algorithm to Design Chips in Minutes", "content": "Designing integrated circuits typically requires years of human expertise. Recent work set AI to the task with surprising results. What’s new: Emir Ali Karahan, Zheng Liu, Aggraj Gupta, and colleagues at Princeton and Indian Institute of Technology Madras used deep learning and an evolutionary algorithm, which generates variations and tests their fitness, to generate designs for antennas, filters, power splitters, resonators, and other chips with applications in wireless communications and other applications. They fabricated a handful of the generated designs and found they worked — but in mysterious ways. How it works: The authors trained convolutional neural networks (CNNs), given a binary image of a circuit design (in which each pixel represents whether the corresponding portion of a semiconductor surface is raised or lowered), to predict its electromagnetic scattering properties and radiative properties. Based on this simulation, they generated new binary circuit images using evolution. The authors produced a training set of images and associated properties using Matlab EM Toolbox. The images depicted designs for chip sizes between 200x200 micrometers (which they represented as 10x10 pixels) and 500x500 micrometers (represented as 25x25 pixels).They trained a separate CNN on designs of each size.They generated 4,000 designs at random and predicted their properties using the appropriate CNN.Given the properties, the authors used a tournament method to select the designs whose properties were closest to the desired values. They randomly modified the selected designs to produce a new pool of 4,000 designs, predicted their properties, and repeated the tournament. The number of iterations isn’t specified. Results: The authors fabricated some of the designs to test their real-world properties. The chips showed similar performance than the CNNs had predicted. The authors found the designs themselves baffling; they “delivered stunning high-performances devices that ran counter to the usual rules of thumb and human intuition,” co-author Uday Khankhoje told the tech news site Tech Xplore. Moreover, the design process was faster than previous approaches. The authors’ method designed a 300x300 micrometer chip in approximately 6 minutes. Using traditional methods it would have taken 21 days. Behind the news: Rather than wireless chips, Google has used AI to accelerate design of the Tensor Processing Units that process neural networks in its data centers. AlphaChip used reinforcement learning to learn how to position chip components such as SRAM and logic gates on silicon. Why it matters: Designing circuits usually requires rules of thumb, templates, and hundreds of hours of simulations and experiments to determine the best design. AI can cut the required expertise and time and possibly find effective designs that wouldn’t occur to human designers. We’re thinking: AI-generated circuit designs could help circuit designers to break out of set ways of thinking and discover new design principles.", "image_caption": "Workflow for inverse design using deep learning to predict S-parameters and radiation in structures.", "metadata": {"article_id": "researchers_used_deep_learning_and_an_evolutionary_algorithm_to_design_chips_in_minutes", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--48--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/researchers-used-deep-learning-and-an-evolutionary-algorithm-to-design-chips-in-minutes/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/researchers_used_deep_learning_and_an_evolutionary_algorithm_to_design_chips_in_minutes.html"}}
{"id": 62157721001, "type": "news_chunk", "title": "Sony Music Accuses AI Developers of Copyright Violations", "subtitle": "Sony Music Accuses AI Developers of Copyright Violations", "content": "The world’s second-largest music publisher accused AI developers of potential copyright violations.What’s new: Sony Music Group declared that AI developers had trained models on Sony’s intellectual property without permission and that any method of collecting media or other data owned by the company violated its copyrights. Whether AI developers actually have violated copyrights has not been established. How it works: In a statement posted on the company’s website and letters to developers, Sony forbade the use of its music or other media such as lyrics, music videos, album art for “training, developing, or commercializing any AI systems.” Sony Music Group sent letters to more than 700 AI developers and streaming services. Letters to AI developers demanded that they reveal which works they had used for training by the following week. Recipients included Google, Microsoft, and text-to-music startups Suno and Udio. Letters sent to streaming services, including Apple and Spotify, asked them to modify their terms of service to prohibit anyone from using streaming services to collect data owned by Sony, among other measures.It reserved the right to grant specific developers permission to use its material as training data, asking interested parties to contact Sony by email if they wanted to make a deal. Behind the news: In April, more than 200 music artists called for streaming services and AI developers to stop using their work for training and stop generating music in the styles of specific musicians without compensation. Universal Music Group (UMG), which is Sony Music’s top competitor, has also opposed unrestricted AI-generated music. Last year, UMG ordered Apple Music and Spotify to block AI developers from downloading its recordings and issued takedown notices to YouTube and Spotify uploaders who generated music that sounds like artists who are under contract to Universal. Why it matters: Sony Music Group’s warning comes as generated audio is approaching a level of quality that might attract a mainstream audience, and it could chill further progress. Although it is not yet clear whether training AI systems on music recordings without permission violates copyrights, Sony Music Group has demonstrated its willingness to pursue both individuals and companies for alleged copyright violations. The company accounted for 22 percent of the global music market in 2023. (UMG accounted for 32 percent.) Its catalog includes many of the world’s most popular artists including AC/DC, Adele, Celine Dion, and Harry Styles. We’re thinking: We believe that AI developers should be allowed to let their software learn from data that’s freely available on the internet, but uncertainty over the limits of copyright protection isn’t good for anyone. It’s high time to update to intellectual property laws for the era of generative AI.", "image_caption": "Sony Music logo turning into the copyright symbol", "metadata": {"article_id": "sony_music_accuses_ai_developers_of_copyright_violations", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FSONYMusic-Copyright-Logo_v3_1200px.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/sony-music-accuses-ai-developers-of-copyright-violations/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/sony_music_accuses_ai_developers_of_copyright_violations.html"}}
{"id": 8749787001, "type": "news_chunk", "title": "Stability.ai Launches Stable Audio", "subtitle": "Stability.ai Launches Stable Audio", "content": "Text-to-music generation has arrived. What's new: Stability.ai, maker of the Stable Diffusion image generator and StableLM text generator, launched Stable Audio, a system that generates music and sound effects from text. You can play with it and listen to examples here. The service is free for 20 generations per month up to 45 seconds long. The professional tier allows 500 generations per month, up to 90 seconds long, for $11.99 per month. An enterprise tier is negotiable. The company said it would open-source the model eventually. How it works: Stable Audio is a latent diffusion model. It generates audio by a process that’s similar to the way Stable Diffusion generates images, but it uses a variational autoencoder to map audio to an embedding for processing and back to audio for your listening pleasure. The authors trained the system on 800,000 audio files containing music, sound effects, and performances on individual instruments and corresponding descriptions. During training, a variational autoencoder learns small embedding representations of audio examples.A CLAP transformer pretrained on their dataset produces an embedding for text that describes musical characteristics like style, instrumentation, tempo, mood, or any sort of description. Separate embedding layers represent the duration of the audio to be generated and how many seconds into a given audio file the current training example starts. The latter helps the model to learn how musical compositions are expressed over time.Stable Audio adds noise to the audio vector. A U-Net convolutional neural network learns to estimate the added noise and remove it according to the text and timing embeddings.At inference, the system starts with a pure-noise embedding and a user-prompted descriptive text and output file length. It removes noise iteratively to produce an embedding of the generated audio. From that embedding, the decoder from the variational autoencoder produces the audio at CD-quality (16-bit, 44.1kHz, stereo) resolution. Behind the News: Stable Audio joins earlier services including Boomy, Mubert, plugger.ai, Soundful, and VEED.IO. It follows tantalizing advances in audio generation. Google MusicLM learned to generate music from text descriptions by setting the problem up as a sequence-to-sequence modeling task.Riffusion turned spectrograms generated by Stable Diffusion into audio.OpenAI Jukebox learned to compress their training set and generated audio from this compressed space. The researchers guided generation using metadata including artist, lyrics, and style. Yes, but: Stable Audio excels when generating instrumental and ambient music, but its output tends to suffer from some of the same flaws as previous text-to-music generators: Longer outputs often lack a coherent structure, and the clarity and detail of individual instruments and sound effects varies wildly. It also doesn’t effectively generate the sound of a vocalist pronouncing words. Why it matters: AI has demonstrated its prowess at generating convincing text and images. Generated audio has implications for producers not only of music but also of videos, video games, and podcasts. Stable Audio sounds like an early step, but it stands out for its speed, high-resolution output, and the inclusion of a mechanism for learning musical structure. We're thinking: Stable Audio is impressive, but this doesn’t quite feel like music’s GPT moment. Text and image generation took off as soon as highly capable generative models appeared. Music generation may yet await models that can produce not only high-res output but also sonorities and structures coherent and varied enough to be widely useful.", "image_caption": "Graphic model of Stable Audio difffusion and transcoding process", "metadata": {"article_id": "stability_ai_launches_stable_audio_a_text_to_music_generator_2", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2023%2F09%2FDiffusion-2.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/stability-ai-launches-stable-audio-a-text-to-music-generator-2/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/stability_ai_launches_stable_audio_a_text_to_music_generator_2.html"}}
{"id": 24494311001, "type": "news_chunk", "title": "Stanford Study Finds AI Matches Human Experts at Writing Research Proposals", "subtitle": "Stanford Study Finds AI Matches Human Experts at Writing Research Proposals", "content": "How do agents based on large language models compare to human experts when it comes to proposing machine learning research? Pretty well, according to one study. What’s new: Chenglei Si, Diyi Yang, and Tatsunori Hashimoto at Stanford produced ideas for research in machine learning using Anthropic’s Claude 3.5 Sonnet and human researchers, and also evaluated them using both manual and automated methods. Claude 3.5 Sonnet generated competitive proposals, but its evaluations of proposals were less compelling. How it works: Each proposal included a problem statement, motivation, step-by-step plan, backup plan, and examples of baseline outcomes versus expected experimental outcomes. Automated proposal generation: Given one of seven topics (bias, coding, safety, multilinguality, factuality, math, or uncertainty) and 10 related papers found by the Semantic Scholar search engine, Claude 3.5 Sonnet generated 4,000 research ideas. The authors embedded the ideas using all-MiniLM-L6-v2 and removed duplicate ideas based on the cosine similarity of their embeddings. This left roughly 200 AI-generated ideas for each topic. For each remaining idea, the model generated a proposal.Automated ranking: Claude Sonnet 3.5 ranked the proposals in a five-round tournament that awarded points for superior quality and pitted highest-scoring proposals against one another. In addition, one of the authors manually ranked the generated proposals.Human proposal generation: The authors paid 49 machine learning engineers to propose their own ideas. They obscured authorship by prompting an unidentified large language model to edit them according to a style guide. Then they manually checked the rewritten proposals to ensure that the model’s editing didn’t change their content significantly.Human ranking: A group of 79 machine learning engineers reviewed the 49 human-written proposals, the top 49 AI-generated proposals ranked by humans, and the top 49 AI-generated proposals ranked by AI (resulting in two to four reviews per proposal). They scored the proposals between 1 and 10 on five factors: novelty, feasibility, expected effectiveness, how exciting they were, and overall quality. Results: Human judges deemed proposals generated by Claude 3.5 Sonnet as good as or better than those produced by humans. However, large language models proved less effective at judging the proposals’ quality. On average, humans scored the AI-generated and human-written proposals roughly equally in feasibility, expected effectiveness, how exciting they were, and overall quality. They deemed the AI-generated proposals significantly more novel. The top AI-generated proposals as ranked by humans achieved an average 5.78 novelty. The top AI-generated proposal as ranked by AI achieved an average 5.62 novelty. Human-written proposals achieved an average 4.86 novelty.The authors found that LLMs don’t yet match human performance when it comes to judging scientific papers. They compared the rates of agreement among five LLMs that evaluated proposals in their experiment, human judgements of the proposals, and human reviews of papers submitted to the NeurIPS and ICLR conferences. The most consistent LLM, Claude 3.5 Sonnet, was 53.3 percent consistent with average human judgment. The human judges were 56.1 percent consistent. Reviewers for NeurIPS and ICLR were 66 and 71.9 percent consistent respectively. Random chance was 50 percent. Why it matters: AI models play a growing role in scientific discovery. This work shows they can set directions for research — in machine learning, at least — that rival those set by humans. However, human evaluation remains the gold standard for comparing performance on complex problems like generating text. We’re thinking: Coming up with good research ideas is hard! That a large language model can do it with some competency has exciting implications for the future of both AI and science.", "image_caption": "Animation showcasing 7 key NLP topics visually expanding on the screen.", "metadata": {"article_id": "stanford_study_finds_ai_matches_human_experts_at_writing_research_proposals", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F12%2Funnamed--41--1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/stanford-study-finds-ai-matches-human-experts-at-writing-research-proposals/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/stanford_study_finds_ai_matches_human_experts_at_writing_research_proposals.html"}}
{"id": 52703666001, "type": "news_chunk", "title": "Study Shows OpenAI’s GPT-4o Model Can Identify Verbatim Excerpts from Paywalled O’Reilly Books", "subtitle": "Study Shows OpenAI’s GPT-4o Model Can Identify Verbatim Excerpts from Paywalled O’Reilly Books", "content": "A study co-authored by tech-manual publisher Tim O’Reilly shows that OpenAI trained GPT-4o on parts of his company’s books that were not made freely available. What happened: O’Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss found that GPT-4o was able to identify verbatim excerpts from dozens of O’Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model’s training data. How it works: The researchers adapted the DE-COP method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books. The team selected 34 O’Reilly Media books and divided them into roughly 14,000 paragraphs.They labeled the paragraphs private (paywalled) or public (when O’Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models’ knowledge cutoff dates.The team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias. Results: The authors asked each model to identify the verbatim paragraph and calculated each model’s percentage of correct responses. Then they averaged each model’s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren’t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy. GPT-4o tended to recognize O’Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).GPT-4o-mini’s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model’s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.The earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data. Yes, but: Newer large language models are better at distinguishing human-written from generated text, even if it wasn’t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while scores for both text that was included and text that was excluded from training sets remain under 96 percent AUROC. “For now,” they write, “the gap remains sufficiently large to reliably separate the two categories.” Behind the news: Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works’ owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that’s freely available on the web as fair game, and material that’s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, including LibGen, which includes all 34 of the O’Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted without paywalls or even logins, making it possible even for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recently lobbied the United States government to relax copyright laws for AI developers. Why it matters: The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O’Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an “extractive dead end” that ultimately diminishes the supply of the high-quality training data. We’re thinking: We have learned a great deal from O’Reilly Media’s books, and we’re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it’s time for the U.S. Congress — and legislators internationally — to update copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.", "image_caption": "Bar chart of AUROC scores for model recognition of non-public and public data across GPT versions, highlighting performance differences.", "metadata": {"article_id": "study_shows_openais_gpt_4o_model_can_identify_verbatim_excerpts_from_paywalled_oreilly_books", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--99--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/study-shows-openais-gpt-4o-model-can-identify-verbatim-excerpts-from-paywalled-oreilly-books/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/study_shows_openais_gpt_4o_model_can_identify_verbatim_excerpts_from_paywalled_oreilly_books.html"}}
{"id": 67899117001, "type": "news_chunk", "title": "Tech Giants Forge Strategic Partnerships to Secure Talent and Technology Without Acquisitions", "subtitle": "Tech Giants Forge Strategic Partnerships to Secure Talent and Technology Without Acquisitions", "content": "Big AI companies found creative ways to gain cutting-edge technology and talent without buying startups. What happened: In 2024, some tech giants entered into novel partnership arrangements with AI startups, hiring top executives and securing access to technology without acquiring the companies outright. These agreements enabled the giants to take on elite talent and proven technology quickly with less risk that regulators might hinder such actions. The startups lost their leadership teams and control over key technical developments. In return, they received cash (in some cases, at least), rewarded investors, and were able to step back from the expense of building cutting-edge models. Driving the story: Microsoft, Amazon, and Google used their deep pockets and cloud infrastructure to strike deals with Inflection AI, Adept AI and Covariant, and Character.ai respectively. (Disclosure: Andrew Ng is a member of Amazon’s board of directors.) Microsoft blazed the trail in March. The tech giant invested $650 million in Inflection AI, licensed the startup’s models, integrated its conversational AI technologies, and hired much of its staff, including co-founders Mustafa Suleyman and Karén Simonyan. Microsoft named Suleyman CEO of a new AI division, putting him in charge of Microsoft’s own model building efforts and consumer-facing products like Bing and the Copilot product line. The remainder of Inflection focuses on customizing AI models for commercial clients.In July, Amazon inked a similar agreement with Adept, a startup that built agents for tasks such as automating data entry and managing customer support tickets, under undisclosed terms. Amazon hired most of Adept AI’s staff, including CEO David Luan and other co-founders who were alumni from Google and OpenAI, and licensed Adept’s models, datasets, and other technology non-exclusively. Adept stopped developing in-house models to concentrate on building agents.In October, Amazon further bolstered its logistics capabilities by forging an agreement with Covariant, a maker of AI-driven warehouse robots, also under undisclosed terms. Amazon hired most of the startup’s staff, including CEO/co-founder Peter Chen and chief scientist/co-founder Pieter Abbeel, and licensed its robotics models. In December, Amazon paired Abbeel and former Adept CEO Luan to run a new lab devoted to developing agents and artificial general intelligence. Covariant continues to serve customers in fulfillment centers and other industries.In August, Google and conversational AI startup Character.ai cut a similar deal. Google hired Character.ai’s co-founders, Noam Shazeer and Daniel De Freitas, along with key team members, and inked a non-exclusive license to its technology. Shazeer joined Google’s Deep Learning research team, and other new hires set to work on Google’s chat services. Google gave Character.ai an undisclosed sum to buy out its investors and continue developing personalized AI products. Behind the news: Tech giants have long relied on traditional acquisitions to gain new talent and capabilities, often acquiring startups specifically for their skilled teams (known as an acquihire) and/or their products or underlying technology, which can be expensive and time-consuming to develop and test in the market. But traditional acquisitions increasingly face scrutiny from antitrust regulators who are concerned about big companies reducing competition by buying out smaller ones. For example, the United States Federal Trade Commission sought to block Amazon’s acquisition of iRobot, prompting the companies to abandon the transaction in January 2024. Where things stand: Giving startups a lump sum and/or licensing fees in return for top talent and technology looks like the new normal for tech giants that are challenged to keep pace with rapidly advancing research and markets. But even arms-length arrangements don’t immunize tech giants and startups against regulatory investigation. Microsoft’s investment in Inflection AI was briefly scrutinized in Europe and is still being evaluated by U.S. regulators. Even Microsoft’s more traditional investment in OpenAI and the interests of Amazon and Google in Anthropic faced regulatory hurdles. So far, however, regulators have yet to conclude that any of these agreements violates antitrust law.", "image_caption": "Gift box labeled ‘Innovation Energizer’ with a rocket logo, truck, and snowy background.", "metadata": {"article_id": "tech_giants_forge_strategic_partnerships_to_secure_talent_and_technology_without_acquisitions", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F12%2Funnamed--34--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/tech-giants-forge-strategic-partnerships-to-secure-talent-and-technology-without-acquisitions/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/tech_giants_forge_strategic_partnerships_to_secure_talent_and_technology_without_acquisitions.html"}}
{"id": 97706627001, "type": "news_chunk", "title": "Tech Giants Increase Cloud Spending to Meet Growing Infrastructure Demands", "subtitle": "Tech Giants Increase Cloud Spending to Meet Growing Infrastructure Demands", "content": "Top AI companies announced plans to dramatically ramp up their spending on AI infrastructure. What’s new: Alphabet, Amazon, Meta, Microsoft, and others will boost their capital spending dramatically in 2025, pouring hundreds of billions of dollars into data centers where they process AI training, the companies said in their most recent quarterly reports. The surge suggests that more-efficient approaches to training models won’t dampen the need for greater and greater processing power. How it works: Capital expenditures include long-term purchases like land, buildings, and computing hardware rather than recurring costs like salaries or electricity. The AI leaders signaled that most of this spending will support their AI efforts. Amazon has budgeted $105 billion to capital expenditures in 2025, 35 percent more than last year. CFO Brian Olsavsky attributed the increase to the company’s need to satisfy demand for AI services and tech infrastructure. CEO Andy Jassy emphasized that it reflects strong demand for AI and dismissed concerns that cheaper alternatives like DeepSeek would reduce overall spending. (Disclosure: Andrew Ng is a member of Amazon’s board of directors.)Alphabet allocated $75 billion to capital expenditures, up from $52.5 billion last year, to support growth in Google Services, Google Cloud, and Google DeepMind. The company indicated that most of this money would go to technical infrastructure including data centers and networking.Meta’s annual capital expenditures will amount to $65 billion, a huge jump from $39.2 billion last year. CEO Mark Zuckerberg argued that such spending on AI infrastructure and chips is needed to assure the company’s lead in AI and integrate the technology into its social platforms.Microsoft said it would put around $80 billion — a figure that analysts expect to rise to $94 billion — into capital expenditures in 2025, another big jump following an 83 percent rise from 2023 to 2024. Most of this investment will support cloud infrastructure, servers, CPUs, and GPUs to meet demand for AI.OpenAI, Oracle, SoftBank, and others announced Stargate, a project that intends immediately to put $100 billion — $500 billion over time — into data centers that would support development of artificial general intelligence. Elon Musk claimed in a tweet that the investors “don’t actually have the money,” raising questions about the announcement’s veracity. Behind the news: DeepSeek initially surprised many members of the AI community by claiming to have trained a high-performance large language model at a fraction of the usual cost. Specifically, DeepSeek-R1 reportedly cost less than $6 million and 2,048 GPUs to train. (For comparison, Anthropic’s Claude 3.5 Sonnet cost “a few $10Ms to train,” according to CEO Dario Amodei, and GPT-4 cost about $100 million to train, according to CEO Sam Altman.) Follow-up reports shed light on DeepSeek’s actual infrastructure and noted that the $6 million figure represented only DeepSeek-R1’s final training run, a small fraction of the total development cost.Furthermore, while initial reports said DeepSeek piggy-backed on a 10,000-GPU supercomputer owned by its parent company High-Flyer, a hedge fund, research firm SemiAnalysis questioned whether DeepSeek relied on High-Flyer’s hardware. DeepSeek has spent around $1.6 billion on a cluster of 50,000 Nvidia GPUs, Tom’s Hardware reported.Initial excitement over the company’s low training costs gave way to concerns about data sovereignty, security, and the cost of running DeepSeek-R1, which generates a larger number of reasoning tokens than similar models. Why it matters: DeepSeek-R1’s purported training cost fueled fears that demand for AI infrastructure would cool, but the top AI companies’ plans show that it’s not happening yet. A possible explanation lies in the Jevons Paradox, a 19th-century economic theory named after the English economist William Stanley Jevons. As a valuable product becomes more affordable, demand doesn’t fall, it rises. According to this theory, even if training costs tumble, the world will demand ever greater processing power for inference. We’re thinking: DeepSeek’s low-cost technology momentarily rattled investors who had expected the next big gains would come from the U.S. rather than China. But DeepSeek’s efficiency follows a broader pattern we’ve seen for years: The AI community steadily wrings better performance from less processing power.", "image_caption": "Bar chart of 2024 vs. 2025 capital expenditures for Amazon, Microsoft, Alphabet, and Meta, showing projected increases.", "metadata": {"article_id": "tech_giants_increase_cloud_spending_to_meet_growing_infrastructure_demands", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2Funnamed--53--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/tech-giants-increase-cloud-spending-to-meet-growing-infrastructure-demands/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/tech_giants_increase_cloud_spending_to_meet_growing_infrastructure_demands.html"}}
{"id": 39474300001, "type": "news_chunk", "title": "The AI Revolution Comes to Grade-School Classrooms", "subtitle": "The AI Revolution Comes to Grade-School Classrooms", "content": "Dear friends, I hope we can empower everyone to build with AI. Starting from K-12, we should teach every student AI enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers. I recently spoke with high school basketball coach Kyle Creasy, who graduated with a B.A. in Physical Education in 2023. Until two years ago, he had never written a line of Python. Now — with help from AI — he not only writes code, he also teaches CS. I found Kyle’s story inspiring as a model for scaling up CS education in the primary- and secondary-school levels. Kyle’s success has been with the support of Kira Learning (an AI Fund portfolio company), whose founders Andrea Pasinetti and Jagriti Agrawal have created a compelling vision for CS education. In K-12 classrooms, teachers play a huge social-emotional support role, for example, encouraging students and helping them when they stumble. In addition, they are expected to be subject-matter experts who can deliver the content needed for their subject. Kira Learning uses digital content delivery — educational videos, autograded quizzes, and AI-enabled chatbots to answer students' questions but without giving away homework answers — so the teacher can focus on social-emotional support. While these are still early days, it appears to be working! A key to making this possible is the hyperpersonalization that is now possible with AI (in contrast to the older idea of the flipped classroom, which had limited adoption). For example, when assigned a problem in an online coding environment, if a student writes this buggy line of Python code best_$alty_snack = 'potato chips' Kira Learning’s AI system can spot the problem and directly tell the teacher that $ is an invalid character in a variable name. It can also suggest a specific question for the teacher to ask the student to help get them unstuck, like “Can you identify what characters are allowed in variable names?” Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12. Additionally, agentic workflows can automate a lot of teachers’ repetitive tasks. For example, when designing a curriculum, it’s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers. Since learning to code, Kyle has built many pieces of software. He proudly showed me an analysis he generated in matplotlib of his basketball players’ attempts to shoot three-pointers (shown above), which in turn is affecting the team’s strategy on the court. One lesson is clear: When a basketball coach learns to code, they become a better basketball coach! I talked about Kyle (and other topics) at the ASU+GSV Summit on education. You can see a video here. In the future, people who know how to code and build with AI will be much more productive than people who don’t. I’m excited about how AI will lead to new models for K-12 education. By delivering CS education to everyone, I hope that in the future, everyone will be able to build with AI. Keep learning!", "image_caption": "Basketball 3-point shooting chart with AI-generated student portraits and stats for volume and accuracy.", "metadata": {"article_id": "the_ai_revolution_comes_to_grade_school_classrooms", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--81--2.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/the-ai-revolution-comes-to-grade-school-classrooms/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/the_ai_revolution_comes_to_grade_school_classrooms.html"}}
{"id": 51562187001, "type": "news_chunk", "title": "The Benefits of Lazy Prompting", "subtitle": "The Benefits of Lazy Prompting", "content": "Dear friends, Contrary to standard prompting advice that you should give LLMs the context they need to succeed, I find it’s sometimes faster to be lazy and dash off a quick, imprecise prompt and see what happens. The key to whether this is a good idea is whether you can quickly assess the output quality, so you can decide whether to provide more context. In this post, I’d like to share when and how I use “lazy prompting.” When debugging code, many developers copy-paste error messages — sometimes pages of them — into an LLM without further instructions. Most LLMs are smart enough to figure out that you want them to help understand and propose fixes, so you don’t need to explicitly tell them. With brief instructions like “Edit this: ...” or “sample dotenv code” (to remind you how to write code to use Python's dotenv package), an LLM will often generate a good response. Further, if the response is flawed, hopefully you can spot any problems and refine the prompt, for example to steer how the LLM edits your text. At the other end of the spectrum, sometimes I spend 30 minutes carefully writing a 2-page prompt to get an AI system to help me solve a problem (for example to write many pages of code) that otherwise would have taken me much longer. I don’t try a lazy prompt if (i) I feel confident there’s no chance the LLM will provide a good solution without additional context. For example, given a partial program spec, does even a skilled human developer have a chance of understanding what you want? If I absolutely want to use a particular piece of pdf-to-text conversion software (like my team LandingAI’s Agentic Doc Extraction!), I should say so in the prompt, since otherwise it’s very hard for the LLM to guess my preference. I also wouldn’t use a lazy prompt if (ii) a buggy implementation would take a long time to detect. For example, if the only way for me to figure out if the output is incorrect is to laboriously run the code to check its functionality, it would be better to spend the time up-front to give context that would increase the odds of the LLM generating what I want. By the way, lazy prompting is an advanced technique. On average, I see more people giving too little context to LLMs than too much. Laziness is a good technique only when you’ve learned how to provide enough context, and then deliberately step back to see how little context you can get away with and still have it work. Also, lazy prompting applies only when you can iterate quickly using an LLM’s web or app interface It doesn’t apply to prompts written in code for the purpose of repeatedly calling an API, since presumably you won’t be examining every output to clarify and iterate if the output is poor. Thank you to Rohit Prsad, who has been collaborating with me on the open-source package aisuite, for suggesting the term lazy prompting. There is an analogy to lazy evaluation in computer science, where you call a function at the latest possible moment and only when a specific result is needed. In lazy prompting, we add details to the prompt only when they are needed. Keep building!", "image_caption": "Cartoon of a relaxed man saying “Relax! I’m lazy prompting!” while lounging under a beach umbrella near a stressed coworker at a desk.", "metadata": {"article_id": "the_benefits_of_lazy_prompting", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--57--2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/the-benefits-of-lazy-prompting/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/the_benefits_of_lazy_prompting.html"}}
{"id": 88549622001, "type": "news_chunk", "title": "The Impact of U.S. Tariffs on AI", "subtitle": "The Impact of U.S. Tariffs on AI", "content": "Dear friends, I am so sorry that the U.S. is letting down our friends and allies. Broad tariffs, implemented not just against adversaries but also steadfast allies, will damage the livelihoods of billions of people, create inflation, make the world more fragmented, and leave the U.S. and the world poorer. AI isn’t the solution to everything, but even amidst this challenging environment, I hope our community can hold together, keep building friendships across borders, keep sharing ideas, and keep supporting each other. Much has been written about why high, widespread taxes on imports are harmful. In this letter, I’d like to focus on its possible effects on AI. One silver lining of the new tariffs is that they focus on physical imports, rather than digital goods and services, including intellectual property (IP) such as AI research inventions and software. IP is difficult to tax, because each piece of IP is unique and thus hard to value, and it moves across borders with little friction via the internet. Many international AI teams collaborate across borders and timezones, and software, including specifically open source software, is an important mechanism for sharing ideas. I hope that this free flow of ideas remains unhampered, even if the flow of physical goods is. However, AI relies on hardware, and tariffs will slow down AI progress by restricting access to it. Even though a last-minute exception was made for semiconductors, taxing imports of solar panels, wind turbines, and other power-generation and -distribution equipment will diminish the ability to provide power to U.S. data centers. Taxing imports of servers, cooling hardware, networking hardware, and the like will also make it more expensive to build data centers. And taxing consumer electronics, like laptops and phones, will make it harder for citizens to learn and use AI. With regard to data-center buildouts, another silver lining is that, with the rise of generative AI, data gravity has decreased because compute processing costs are much greater than transmission costs, meaning it’s more feasible to place data centers anywhere in the world rather than only in close proximity to end-users. Even though many places do not have enough trained technicians to build and operate data centers, I expect tariffs will encourage data centers to be built around the world, creating more job opportunities globally. Finally, tariffs will create increased pressure for domestic manufacturing, which might create very mild tailwinds for robotics and industrial automation. As U.S. Vice President J.D. Vance pointed out in 2017, the U.S. should focus on automation (and education) rather than on tariffs. But the U.S. does not have the personnel — or know-how, or supply chain — to manufacture many of the goods that it currently counts on allies to make. Robotics can be helpful for addressing a small part of this large set of challenges. Generative AI’s rate of progress in robotics is also significantly slower than in processing text, visual data, audio, and reasoning. So while the tariffs could create tailwinds for AI-enabled robotics, I expect this effect to be small. My 4-year-old son had been complaining for a couple of weeks that his shoes were a tight fit — he was proud that he’s growing! So last Sunday, we went shoe shopping. His new shoes cost $25, and while checking out, I paused and reflected on how lucky I am to be able to afford them. But I also thought about the many families living paycheck-to-paycheck, and for whom tariffs leading to shoes at $40 a pair would mean they let their kids wear ill-fitting shoes longer. I also thought about people I’ve met in clothing manufacturing plants in Asia and Latin America, for whom reduced demand would mean less work and less money to take home to their own kids. I don’t know what will happen next with the U.S. tariffs, and plenty of international trade will happen with or without U.S. involvement. I hope we can return to a world of vibrant global trade with strong, rules-based, U.S. participation. Until then, let’s all of us in AI keep nurturing our international friendships, keep up the digital flow of ideas — including specifically open source software — and keep supporting each other. Let’s all do what we can to keep the world as connected as we are able.", "image_caption": "Black toddler sneakers with white soles on wooden floor, featuring Velcro strap and soft inner lining for comfort.", "metadata": {"article_id": "the_impact_of_u_s_tariffs_on_ai", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--73--2.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/the-impact-of-u-s-tariffs-on-ai/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/the_impact_of_u_s_tariffs_on_ai.html"}}
{"id": 45910353001, "type": "news_chunk", "title": "The International Energy Agency Examines The Energy Costs and Potential Savings of the AI Boom", "subtitle": "The International Energy Agency Examines The Energy Costs and Potential Savings of the AI Boom", "content": "AI’s thirst for energy is growing, but the technology also could help produce huge energy savings over the next five to 10 years, according to a recent report. What’s new: The International Energy Agency (IEA), which advises 44 countries on energy policy, performed a comprehensive analysis of AI’s energy consumption including energy required to obtain critical materials needed for chips and data centers. The report sees dark clouds ahead but also silver linings. Dark clouds: The report, which is based on interviews with officials in government, energy, and technology, makes four projections for AI’s energy consumption. In the base scenario, future growth and efficiency gains are similar to those of the past five years. The agency also plots a “take-off” scenario in which AI adoption happens faster, a “high efficiency” scenario with lower energy needs, and a “headwinds” scenario in which adoption of AI slows or infrastructure bottlenecks impede construction. Among the conclusions: Demand for electricity by data centers worldwide will more than double by 2030 in the base scenario, growing from 415 terawatt-hours (TWh) today to 945 TWh, around 2.5 percent of current global energy consumption. By 2035, this figure will range from 700 TWh to 1700 TWh.By 2030, data centers outfitted with AI accelerator chips will consume four times the energy they do today.The United States, China, and Europe have more data centers (and use more electricity) than the rest of the world. Like many countries, their data centers are in a few geographic regions, drawing from the same power sources, which eventually will strain local electrical grids. Together, the U.S. and China will account for 80 percent of global growth in data center electricity consumption by 2030. Japan and Malaysia will also see strong growth. Silver linings: AI already makes energy generation, distribution, and use more efficient. The authors expect these savings to accelerate. Existing AI algorithms predict energy generation and consumption. This makes it easier to integrate renewable energy sources into the grid, which reduces reliance on fossil fuels and cuts the resulting pollutants and greenhouse gases. Extending existing programs to increase use of renewables by 1 percent would reduce CO2 emissions by 120 megatons by 2035, which is roughly 40 percent of the projected emissions attributable to data centers.Widespread adoption of existing AI applications that streamline energy consumption in industry, transportation, and buildings could reduce CO2 emissions by 1.4 gigatons, nearly five times the projected emissions attributable to data centers, by 2035. For example, scaling up existing AI optimization of heating, ventilation, and air-conditioning systems would save 300 TWh, about one-third of total energy used by data centers.AI and cloud-computing companies continue to negotiate long-term purchase agreements that can secure renewable and zero-emissions energy for as much as 20 years. Data center operators are responsible for most of the long-term contracts that have been announced, nearly all of them for solar energy. Consequently, renewables generation is projected to grow by over 450 TWh by 2035.The energy costs of training, inference, and cooling hardware are expected to fall further thanks to trends in AI models (fewer parameters, more efficient algorithms, task-specific models) hardware (more energy-efficient chips, improved cooling methods), and usage (batch processing, running smaller models locally rather than in the cloud). Yes, but: The authors concede that lower energy costs for AI likely will lead to much greater consumption — according to the Jevons paradox — so more-efficient models and hardware will result in higher energy consumption overall. Behind the news: Data centers were growing rapidly prior to the boom in generative AI. Data centers’ electricity use doubled between 2000 and 2005 and again between 2017 and 2022, driven by the growth of cloud computing and data storage, streaming and social media, and cryptocurrency mining. However, these periods of accelerating growth were followed by periods of slower growth as efforts to cut costs led to more-efficient software and hardware. The authors expect this pattern to hold. Why it matters: The IEA report is a first-of-its-kind analysis of AI’s energy requirements, how they’re likely to grow, as well as the potential of the technology itself to reduce those requirements. It confirms that AI is poised to consume huge amounts of energy. However, it also suggests that today’s energy costs will be tomorrow’s energy savings as AI makes energy generation, distribution, and use more efficient across a wide variety of industries. We’re thinking: While demand for electricity for data centers is growing rapidly, calibrating the right level of investment is tricky. High levels of growth come with high levels of hype that can lead analysts to overestimate future demand. For example, Microsoft, after examining its forecasts, canceled data-center projects that would have consumed 2 gigawatts.", "image_caption": "Bar chart comparing electricity use by various text-generation models: very small, small, medium-sized, large MoE, and large reasoning.", "metadata": {"article_id": "the_international_energy_agency_examines_the_energy_costs_and_potential_savings_of_the_ai_boom", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Funnamed---2025-06-04T165349.311-1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/the-international-energy-agency-examines-the-energy-costs-and-potential-savings-of-the-ai-boom/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/the_international_energy_agency_examines_the_energy_costs_and_potential_savings_of_the_ai_boom.html"}}
{"id": 62258835001, "type": "news_chunk", "title": "The Value of AI’s Speed Is Underrated", "subtitle": "The Value of AI’s Speed Is Underrated", "content": "Dear friends, AI’s ability to make tasks not just cheaper, but also faster, is underrated in its importance in creating business value. For the task of writing code, AI is a game-changer. It takes so much less effort — and is so much cheaper — to write software with AI assistance than without. But beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent. When you can test 20 ideas per month, it dramatically changes what you can do compared to testing 1 idea per month. This is a benefit that comes from AI-enabled speed rather than AI-enabled cost reduction. That AI-enabled automation can reduce costs is well understood. For example, providing automated customer service is cheaper than operating human-staffed call centers. Many businesses are more willing to invest in growth than just in cost savings; and, when a task becomes cheaper, some businesses will do a lot more of it, thus creating growth. But another recipe for growth is underrated: Making certain tasks much faster (whether or not they also become cheaper) can create significant new value. I see this pattern across more and more businesses. Consider the following scenarios: If a lender can approve loans in minutes using AI, rather than days waiting for a human to review them, this creates more borrowing opportunities (and also lets the lender deploy its capital faster). Even if human-in-the-loop review is needed, using AI to get the most important information to the reviewer might speed things up. The ability to provide loans quickly opens up the market to new customers in need of rapid funds and helps customers who need a quick positive or negative decision to accept the loan or move on.If an academic institution gives homework feedback to students in minutes (via sophisticated autograding) rather than days (via human grading), not only is the automation cheaper, the rapid feedback facilitates better learning.If an online seller can approve purchases faster, this can lead to more sales. For example, many platforms that accept online ad purchases have an approval process that can take hours or days; if approvals can be done faster, they can earn revenue faster. Further, for customers buying ads, being able to post an ad in minutes lets them test ideas faster and also makes the ad product more valuable.If a company’s sales department can prioritize leads and respond to prospective customers in minutes or hours rather than days — closer to when the customers’ buying intent first led them to contact the company — sales representatives might close more deals. Likewise, a business that can respond more quickly to requests for proposals may win more deals. I’ve written previously about looking at the tasks a company does to explore where AI can help. Many teams already do this with an eye toward making tasks cheaper, either to save costs or to do those tasks many more times. If you’re doing this exercise, consider also whether AI can significantly speed up certain tasks. One place to examine is the sequence of tasks on the path to earning revenue. If some of the steps can be sped up, perhaps this can help revenue growth. Growth is more interesting to most businesses than cost savings, and if there are loops in your business that, when sped up, would drive growth, AI might be a tool to unlock this growth. Keep building!", "image_caption": "Black stopwatch icon showing elapsed time, commonly used to represent speed, countdowns, or time tracking.", "metadata": {"article_id": "the_value_of_ais_speed_is_underrated", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--88--2.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/the-value-of-ais-speed-is-underrated/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/the_value_of_ais_speed_is_underrated.html"}}
{"id": 90636473001, "type": "news_chunk", "title": "TSMC Stops Advanced Chip Production for China On U.S. Orders", "subtitle": "TSMC Stops Advanced Chip Production for China On U.S. Orders", "content": "The largest manufacturer of AI chips told its Chinese customers it would stop fabricating their most advanced designs, further limiting China’s access to AI hardware. What’s new: Taiwan Semiconductor Manufacturing Corp. (TSMC) notified Alibaba, Baidu, and others it would halt production of their most advanced chips starting November 13, according to multiple reports. The restriction affects chip designs that are based on manufacturing processes at scales of 7 nanometers and below. TSMC must receive explicit permission from the U.S. government to manufacture advanced chips for a given customer, which likely would require that the government assess each chip to prevent potential military applications. How it works: The United States Department of Commerce ordered TSMC to halt shipments of advanced AI chips to China after a chip fabricated by TSMC was discovered in an AI system sold by the Chinese telecoms giant Huawei, apparently in violation of earlier U.S. controls, Reuters reported. Taiwan’s economic ministry said it would follow all domestic and international regulations. TSMC’s manufacturing processes etch transistors into silicon at minuscule sizes to fabricate hardware like the Nvidia A100 GPU (which uses the 7 nanometer process), Nvidia H100 GPU (5 nanometer process), and Apple A18 CPU (3 nanometer process). Smaller transistors make it possible to fit more transistors per area of silicon, leading to faster processing — an important capability for training large neural networks and providing them to large numbers of users.Although TSMC is headquartered in Taiwan, it uses chip-manufacturing equipment made by U.S. companies such as Applied Materials and Lam Research. TSMC’s use of U.S. equipment obligates the company to comply with U.S. export control policies.The policy could force several Chinese companies to either downgrade their chip designs or seek alternative suppliers. For example, Alibaba, Baidu, Huawei and Tencent have depended on TSMC to manufacture their chip designs. ByteDance partnered with TSMC to develop AI chips to rival Nvidia’s.Samsung and Intel are capable of fabricating advanced chips, but they, too, are subject to U.S. restrictions on sales of advanced chips to China. U.S. officials have expressed skepticism that China’s own Semiconductor Manufacturing International Corporation can supply in large volumes chips manufactured using processes of 7 nanometers or smaller. Behind the news: The U.S.-China chip standoff began in 2020 and has escalated since. Initial restrictions barred U.S.-based companies like AMD, Intel, and Nvidia from selling advanced chips to Huawei and affiliated Chinese firms. China responded by promoting domestic chip fabrication. In 2022, the U.S. passed the CHIPS and Science Act to boost its own chip industry, seeking to counter China and decrease U.S. reliance on Taiwan. Why it matters: TSMC finds itself in the middle of an AI arms race in which cutting-edge chips could tip the balance. The company itself, which has been operating at full capacity, is unlikely to suffer business losses. We’re thinking: AI developers in China have been resourceful in navigating previous restrictions. Chip manufacturing is extraordinarily difficult to master, but China has made strides in this direction. A proliferation of factories that can fabricate advanced chips would reshape AI research and business worldwide.", "image_caption": "Close-up of a Chinese-made server chip labeled with the logo and text ‘710’ mounted on a motherboard.", "metadata": {"article_id": "tsmc_stops_advanced_chip_production_for_china_on_u_s_orders", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2Funnamed--22--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/tsmc-stops-advanced-chip-production-for-china-on-u-s-orders/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/tsmc_stops_advanced_chip_production_for_china_on_u_s_orders.html"}}
{"id": 99850390001, "type": "news_chunk", "title": "U.S. Makes New Rules for AI Chip Export Rules to China, Launches Nvidia Investgation", "subtitle": "U.S. Makes New Rules for AI Chip Export Rules to China, Launches Nvidia Investgation", "content": "The U.S. government escalated its long-running effort to block China’s access to cutting-edge AI hardware. What’s new: The White House announced that future shipments of Nvidia H20s, AMD MI308s, or equivalent chips to China would require a license. Concurrently, the United States Congress launched an investigation into whether chip vendor Nvidia violated earlier export rules. How it works: Nvidia launched the H20 in late 2023 to comply with a 2022 U.S. ban on China-bound shipments of Nvidia’s H100 and H200 processors. The H20 uses the same architecture as the H200, but it’s an order of magnitude slower with less memory and memory bandwidth. Nvidia estimated that the new restrictions will cost the company $5.5 billion in revenue. AMD similarly expects to lose $800 million.Congressional leaders opened an investigation into whether Nvidia assisted DeepSeek with developing AI models, a potential violation of U.S. trade restrictions.The action spurred China’s biggest chip maker to accelerate production of its own AI chips. Huawei plans to begin mass shipments of its Ascend 910C AI chip, which is purportedly equivalent to Nvidia’s H100, in May, Reuters reported. The company expects to mass produce its Ascend 920, a potential substitute for the H20, in the second half of this year, according to DigiTimes Asia. Behind the news: The U.S. government’s many moves to restrict shipments of advanced processors to China have sought to protect the nation’s lead in AI, but they have not prevented Chinese developers from closing the gap. In 2020, the U.S. required chip makers that use U.S. technology — which includes both domestic chip designers like Nvidia and makers of advanced fabrication equipment like the Netherlands’ ASML — to seek permission before doing business with Chinese tech giant Huawei. Last December, the U.S. published sweeping limits on sales of processors that involve U.S. technology, as well as the technology itself, to Chinese businesses. Yes, but: Export restrictions may have slowed China’s production of advanced chips, but they have also incentivized China to invest in establishing leadership in AI. In January, the Chinese AI developer DeepSeek surprised U.S. policymakers and AI leaders with the release of DeepSeek-R1, which performs comparably to OpenAI’s o1, but whose weights are freely available and trained using less computation. Why it matters: The first wave of restrictions on sales of advanced chips to China did little harm to U.S. chipmakers, largely because demand outstripped supply. But later restrictions have had a greater impact on their sales. The new limits could cost Nvidia and AMD significant revenue and likely will degrade their competitiveness abroad and bolster China’s homegrown chip-making industry. We’re thinking: The AI community’s international scope is one of its greatest strengths. While individual countries must attend to their national security, progress in AI benefits all nations. Even in this era of rising protectionism, we hope members of the global AI community continue to support one another and encourage the free flow of ideas.", "image_caption": "NVIDIA AI computing hardware with multiple GPUs on a black reflective surface.", "metadata": {"article_id": "u_s_makes_new_rules_for_ai_chip_export_rules_to_china_launches_nvidia_investgation", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--79--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/u-s-makes-new-rules-for-ai-chip-export-rules-to-china-launches-nvidia-investgation/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/u_s_makes_new_rules_for_ai_chip_export_rules_to_china_launches_nvidia_investgation.html"}}
{"id": 49365082001, "type": "news_chunk", "title": "Unitree and EngineAI Showcase Affordable Humanoid Robots", "subtitle": "Unitree and EngineAI Showcase Affordable Humanoid Robots", "content": "Chinese robot makers Unitree and EngineAI showed off relatively low-priced humanoid robots that could bring advanced robotics closer to everyday applications. What’s new: At the annual Consumer Electronics Show (CES) in Las Vegas, Unitree showed its G1 ($16,000 with three-finger hands, $21,000 with five-finger, articulated hands), which climbed stairs and navigated around obstacles. Elsewhere on the show floor, EngineAI’s PM01 ($13,700 through March 2025 including articulated hands) and SE01 (price not yet disclosed) marched among attendees with notably naturalistic gaits. How it works: Relatively small and lightweight, these units are designed for household and small-business uses. They’re designed for general-purpose tasks and to maintain stability and balance while walking on varied terrain. Unitree: A downsized version of Unitree’s 6-foot H1, which debuted in 2023, the G1 stands at 4 feet, 3 inches and weighs 77 pounds. It walks at speeds up to 4.4 miles per hour and carries up to 5 pounds, and demo videos show it performing tasks that require manual dexterity such as cracking eggs. It was trained via reinforcement learning to avoid obstacles, climb stairs, and jump. A rechargeable, swappable battery ($750) lasts two hours. Unitree offers four models that are programmable (in Python, C++, or ROS) and outfitted with Nvidia Jetson Orin AI accelerators ($40,000 to $68,000). All models can be directed with a radio controller.EngineAI: The PM01 is slightly larger and heavier than the G1 at 4 feet, 5 inches and 88 pounds. The SE01 is 5 feet, 7 inches and 121 pounds. Both units travel at 4.4 miles per hour and include an Nvidia Jetson Orin AI accelerator. They were trained via reinforcement learning to navigate dynamic environments and adjust to specific requirements. Pretrained AI models enhance their ability to recognize gestures and interact through voice commands. They include built-in obstacle avoidance and path-planning capabilities to operate in cluttered or unpredictable spaces. The robot can be controlled using voice commands or a touchscreen embedded in its chest. Rechargeable, swappable batteries provide two hours of performance per charge. Behind the news: In contrast to the more-affordable humanoid robots coming out of China, U.S. companies like Boston Dynamics, Figure AI, and Tesla tend to cater to industrial customers. Tesla plans to produce several thousand of its Optimus ($20,000 to $30,000) humanoids in 2025, ramping to as many as 100,000 in 2026. Figure AI has demonstrated its Figure 02 ($59,000) in BMW manufacturing plants, showing a 400 percent speed improvement in some tasks. At CES, Nvidia unveiled its GR00T Blueprint, which includes vision-language models and synthetic data for training humanoid robots, and said its Jetson Thor computer for humanoids would be available early 2025. Why it matters: China’s push into humanoid robotics reflects its broader national ambitions. Its strength in hardware has allowed it to establish a dominant position in drones, and humanoid robots represent a new front for competition. China’s government aims to achieve mass production of humanoid robots by 2025 and establish global leadership by 2027, partly to address projected labor shortages of 30 million workers in manufacturing alone. Lower price points for robots that can perform arbitrary tasks independently could be valuable in elder care and logistics, offering tools for repetitive or physically demanding tasks. We’re thinking: Although humanoid robots generate a lot of excitement, they’re still in an early stage of development, and businesses are still working to identify and prove concrete use cases. For many industrial applications, wheeled robots — which are less expensive, more stable, and better able to carry heavy loads — will remain a sensible choice. But the prospect of machines that look like us and fit easily into environments built for us is compelling.", "image_caption": "GIF of two humanoid robots walking, one on grass and the other on a paved surface.", "metadata": {"article_id": "unitree_and_engineai_showcase_affordable_humanoid_robots", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--45--1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/unitree-and-engineai-showcase-affordable-humanoid-robots/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/unitree_and_engineai_showcase_affordable_humanoid_robots.html"}}
{"id": 38764062001, "type": "news_chunk", "title": "Data Points: Updated Gemini Pro model builds interactive websites from prompts", "subtitle": "Data Points: Updated Gemini Pro model builds interactive websites from prompts", "content": "Twice a week, Data Points brings you the latest AI news, tools, models, and research in brief. In today’s edition, you’ll learn more about: Mistral’s new medium-sized language modelClaude developers gain access to web searchAlibaba uses RL to teach LLMs to search betterOpenAI’s plans to build AI infrastructure worldwide Google updates Gemini 2.5 Pro’s coding and web design skills in surprise early release Google launched early access to Gemini 2.5 Pro Preview (I/O edition), an updated version with significantly improved coding capabilities, particularly for building interactive web applications. The model now leads the WebDev Arena Leaderboard, surpassing its previous version by 147 Elo points, ranks first on Chatbot Arena for coding, and achieves 84.8 percent on the VideoMME benchmark for video understanding. Developers can access the updated model through the Gemini API via Google AI Studio and Vertex AI, while general users can experience it through the Gemini app. (Google) OpenAI abandons plans to transition full control to for-profit company OpenAI announced it would transform its for-profit arm into a Public Benefit Corporation (PBC) while keeping its nonprofit foundation in control of the company. This structural change replaces the company’s complex “capped-profit” model with a more standard arrangement where employees will own stock directly, similar to other AI companies like Anthropic and X. The nonprofit will become a major shareholder in the PBC, generating resources to fund initiatives ensuring AI benefits diverse communities. OpenAI made this decision after consulting with attorneys general and other officials in California and Delaware. The shift allows the company to raise more funding to pursue its goal of artificial general intelligence, but is less of a radical change than shifting full control of the company to the for-profit arm. (OpenAI) Mistral AI releases Medium 3 language model Mistral AI launched Mistral Medium 3, a new language model priced at $0.40 per million tokens for input and $2 for output. The company claims the model outperforms Llama 4 Maverick, is comparable to GPT-4o, and approaches Claude Sonnet 3.7 on benchmarks while being significantly less expensive. Mistral Medium 3 can be deployed on systems with four or more GPUs and is designed for coding and STEM tasks. The model includes enterprise features such as on-premises deployment options and custom training capabilities. It’s currently available on Mistral’s platform and Amazon SageMaker, with planned releases on IBM WatsonX, NVIDIA NIM, Azure AI Foundry, and Google Cloud Vertex. (Mistral) Anthropic launches web search via its API Anthropic added web search to its Claude API, allowing developers to build applications that access current information from the internet. When Claude receives a request that would benefit from up-to-date information, it can generate targeted search queries, retrieve relevant results, and provide comprehensive answers with source citations. The feature includes administrative controls like domain allow lists and block lists to help organizations maintain control over information sources. Web search is available for Claude 3.7 Sonnet, Claude 3.5 Sonnet, and Claude 3.5 Haiku at $10 per 1,000 searches plus standard token costs. (Anthropic) Alibaba develops ZeroSearch to improve LLM search capabilities A new reinforcement learning framework called ZeroSearch enhances large language models’ search capabilities without requiring access to actual search engines. The system transforms an LLM into a retrieval module through supervised fine-tuning, then uses a curriculum-based strategy that progressively introduces more challenging retrieval scenarios during training, enabling the LLM to progressively find the most relevant documents. Experiments show that a 7 billion parameter retrieval module achieves comparable performance to traditional search engines, while a 14 billion module can surpass them. ZeroSearch could eliminate the high API costs typically associated with search-based LLM training while avoiding the unpredictable document quality issues that occur when using live search engines. (GitHub) OpenAI launches program to develop global AI infrastructure OpenAI announced “OpenAI for Countries,” a new initiative to help nations build AI infrastructure and capabilities. The program will partner with governments to develop in-country data centers, provide customized ChatGPT services to citizens, implement security controls, and create national startup funds to foster local AI ecosystems. OpenAI plans to pursue 10 initial projects with individual countries or regions, targeting nations that commit to using AI according to democratic principles. The initiative follows the Paris AI Action Summit, where multiple international leaders expressed interest in creating their own versions of the Stargate project, which aims to invest $500 billion in AI infrastructure within the US. (OpenAI) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng announced that AI Fund has closed $190M for a new venture fund and shared key lessons on how speed drives success in AI startups. “Many factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: Alibaba released the Qwen3 family of open-source language models, offering optional reasoning capabilities that rival top models like DeepSeek-R1; OpenAI rolled back its GPT-4o update after users flagged overly flattering, sycophantic behavior; Johnson & Johnson unveiled a revised AI strategy, offering new insights into how big medical companies are using the technology; and researchers demonstrated that fine-tuning a language model with just 1,000 examples can significantly boost its reasoning abilities. Subscribe to Data Points", "image_caption": "Man designing a fun tech website on a desktop computer in a cozy modern home office.", "metadata": {"article_id": "updated_gemini_pro_model_builds_interactive_websites_from_prompts", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Fimage-5.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/updated-gemini-pro-model-builds-interactive-websites-from-prompts/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/updated_gemini_pro_model_builds_interactive_websites_from_prompts.html"}}
{"id": 78459663001, "type": "news_chunk", "title": "Data Points: Veo 3 adds synchronized audio to realistic video", "subtitle": "Data Points: Veo 3 adds synchronized audio to realistic video", "content": "In today’s edition, you’ll learn more about: A new earth system model that can predict weather disastersGoogle’s speedy diffusion-based text generation modelClaude 4’s new system prompts for power usersMicrosoft’s AI agent marketplace for developers Google launches Veo 3 and Flow for video generation with audio Google DeepMind released Veo 3, its latest video generation model. Veo 3 can create notably realistic videos with speech, dialogue, voice-overs, music, and sound effects from text and image prompts. The technology enables marketers and filmmakers to produce video that previously required extensive production resources, with some companies reporting 50 percent reductions in costs and time-to-market. Google also launched Flow, an AI filmmaking tool available to Google AI Pro and Ultra subscribers in the U.S. Veo 3 is currently in private preview on Vertex AI, with broader availability coming in the coming weeks. (Google) OpenAI’s o3 model helps discover a zero-day vulnerability in Linux kernel A security researcher used OpenAI’s o3 model to discover CVE-2025-37899, a dangerous vulnerability in the Linux kernel’s ksmbd server. The researcher provided o3 with approximately 12,000 lines of code from the SMB protocol implementation, using only the standard API without additional frameworks or tools. The vulnerability occurs when multiple connections share session objects, allowing one thread to free memory while another thread still accesses it, potentially enabling arbitrary code execution in kernel context. This marks the first publicly documented case of an LLM finding this type of complex vulnerability, showing that LLMs (while still finding many false positives) can meaningfully assist expert vulnerability researchers. (Sean Heelan’s Blog) Microsoft’s Aurora AI model outperforms numerical Earth system forecasts Microsoft Research introduced Aurora, a versatile model trained on over one million hours of diverse geophysical data. Researchers claim Aurora can predict weather, air quality, ocean waves, and tropical cyclone tracks more accurately and efficiently than current operational systems. The model achieves state-of-the-art performance across multiple domains: it beats the Copernicus Atmosphere Monitoring Service (CAMS) on 74 percent of air pollution forecasting targets, surpasses ocean wave models on 86 percent of targets, outperforms seven operational centers for tropical cyclone tracking, and exceeds high-resolution weather models on 92 percent of targets. Aurora’s architecture uses a 3D Swin Transformer that can handle different resolutions, variables, and pressure levels, making it adaptable to various Earth system prediction tasks through fine-tuning. The model operates at computational speeds that are orders of magnitude faster than traditional numerical models — for example, generating air pollution forecasts approximately 100,000 times faster than CAMS while running on a single GPU. For machine learning researchers, Aurora may help develop architectures that can efficiently process 3D spatiotemporal data while maintaining physical consistency across multiple scales and modalities. (Nature) Google unveils Gemini Diffusion, a blazing-fast experimental language model Google DeepMind demonstrated Gemini Diffusion at I/O, an experimental language model that generates text at 1,000 to 2,000 tokens per second — four to five times faster than Google’s current fastest model. The model uses diffusion techniques, traditionally employed in image generation, to refine random noise into coherent text by processing multiple parts simultaneously rather than generating one word at a time like traditional transformers. Gemini Diffusion matches the coding performance of larger models while excelling at tasks requiring iterative refinement, such as mathematical reasoning and code generation. If successful, diffusion-based text models could reshape the competitive landscape among AI companies, particularly for coding agents and specialized applications where speed and accuracy matter more than narrative flow. Google has opened a waitlist for researchers to access the experimental demo, though no public release date or pricing have been announced. (Google) Claude 4 system prompts offer useful info for power users Anthropic published the system prompts for Claude Opus 4 and Claude Sonnet 4, offering users an unofficial manual for optimizing their interactions with these AI models. The prompts reveal detailed instructions about Claude’s personality, safety guidelines, and capabilities, including warnings against reproducing copyrighted content and guidance on when to use search tools. Notable features include support for “thinking blocks” where Claude can switch modes during processing, integration with tools like web search that can execute up to 5 queries for complex requests, and the Artifacts feature’s support for libraries like Three.js, React, and TensorFlow that can help create interactive applications. Anthropic notably omitted the tool-specific prompts, which were later discovered through leaked versions; these provide further crucial details about Claude’s full capabilities. (Simon Willison’s Weblog) Microsoft launches Agent Store for AI assistants Microsoft debuted the Agent Store, a marketplace within Microsoft 365 Copilot where users can discover and install AI agents built by Microsoft, partners, and customers. The store launches with over 70 agents designed to automate specific business processes, ranging from simple knowledge assistants to complex multi-modal orchestrators. Developers can build agents using either Microsoft Copilot Studio’s low-code tools or the Microsoft 365 Agents Toolkit for custom orchestration logic, then publish them to reach Microsoft 365 users. Microsoft’s store could make AI agents more accessible for workplace automation, complementing the company’s broader Copilot AI assistant strategy. The Agent Store is available now to both paid and free Microsoft 365 Copilot customers. (Microsoft) Still want to know more about what matters in AI right now? Read last week’s issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng shared how large companies could move fast in the age of AI by creating sandbox environments that allowed small teams to innovate without needing constant permission. “Dozens or hundreds of prototypes can be built and quickly discarded as part of the price of finding one or two ideas that turn out to be home runs.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: OpenAI introduced Codex, a new multi-agent, cloud-based software engineering tool integrated into ChatGPT; xAI attributed Grok’s controversial “white genocide” responses to an unnamed, unauthorized employee, raising concerns about internal safeguards; U.S. tech giants including Nvidia, AMD, and Amazon secured deals to supply chips and infrastructure to Middle Eastern companies like Saudi Arabia’s Humain and the UAE’s G42; and Microsoft researchers showed that 4-bit quantized versions of Llama models can match the accuracy of 16-bit models, offering major efficiency gains without compromising performance. Subscribe to Data Points", "image_caption": "Meteorologists analyze hurricane data on computer screens, focusing on storm patterns and forecasts in a modern office.", "metadata": {"article_id": "veo_3_adds_synchronized_audio_to_realistic_video", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Fimage.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/veo-3-adds-synchronized-audio-to-realistic-video/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/veo_3_adds_synchronized_audio_to_realistic_video.html"}}
{"id": 42865873001, "type": "news_chunk", "title": "Viral Video Uses AI to Depict Celebrities Without Consent, Sparking Legal Debate", "subtitle": "Viral Video Uses AI to Depict Celebrities Without Consent, Sparking Legal Debate", "content": "A viral deepfake video showed media superstars who appeared to support a cause — but it was made without their participation or permission. What’s new: The video shows AI-generated likenesses of 20 Jewish celebrities ranging from Scarlett Johansson to Simon & Garfunkel. They appear wearing T-shirts that feature a middle finger inscribed with the Star of David above the word “KANYE.” The clip, which ends with the words “Enough is enough” followed by “Join the fight against antisemitism,” responds to rapper Kanye West, who sold T-shirts emblazoned with swastikas on Shopify before the ecommerce platform shut down his store. Who created it: Israeli developers Guy Bar and Ori Bejerano generated the video to spark a conversation about antisemitism, Bar told The Jerusalem Post. The team didn’t reveal the AI models, editing tools, or techniques used to produce the video. Johansson reacts: Scarlett Johansson denounced the clip and urged the U.S. to regulate deepfakes. In 2024, she objected to one of the voices of OpenAI’s voice assistant, which she claimed resembled her own voice, leading the company to remove that voice from its service. The prior year, her attorneys ordered a company to stop using an unauthorized AI-generated version of her image in an advertisement. Likenesses up for grabs: Existing U.S. laws protect some uses of a celebrity’s likeness in the form of a photo, drawing, or human lookalike, but they don’t explicitly protect against reproduction by AI systems. This leaves celebrities and public figures with limited recourse against unauthorized deepfakes. U.S. lawmakers have introduced legislation that targets deepfake pornography, but it covers only sexually explicit deepfakes.The right of publicity, which falls under trademark law, offers some protection against the unauthorized use of a person’s identity. However, it varies by state and provides broad exceptions for news, satire, and fine art.While some states outlaw misappropriation of names or likenesses, existing laws primarily target traditional forms of image misuse, such as false endorsements or unauthorized commercial exploitation. They do not explicitly cover AI-generated deepfakes used for noncommercial, political, or satirical purposes.A 2023 agreement between Hollywood actors and movie studios protects actors against such uses of AI-generated images of their likenesses in films. However, it doesn’t apply to deepfakes that are produced independently for distribution via social media networks. Why it matters: Non-consensual deepfake pornography is widely condemned, but AI enables many other non-consensual uses of someone’s likeness, and their limits are not yet consistently coded into law. If the creators of the video that appropriated the images of celebrities had responded to Johansson’s criticism with an AI-generated satire, would that be a legitimate exercise of free speech or another misuse of AI? Previously, an ambiguous legal framework may have been acceptable because such images, and thus lawsuits arising from them, were uncommon. Now, as synthetic likenesses of specific people become easier to generate, clear legal boundaries are needed to keep misuses in check. We’re thinking: Creating unauthorized lookalikes of existing people is not a good way to advance any cause, however worthy. Developers should work with businesses policymakers to establish standards that differentiate legitimate uses from unfair or misleading exploitation.", "image_caption": "“Enough Is Enough” in black on white, with a heart, from an AI-generated video protesting Kanye West’s antisemitism.", "metadata": {"article_id": "viral_video_uses_ai_to_depict_celebrities_without_consent_sparking_legal_debate", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2Funnamed--55--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/viral-video-uses-ai-to-depict-celebrities-without-consent-sparking-legal-debate/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/viral_video_uses_ai_to_depict_celebrities_without_consent_sparking_legal_debate.html"}}
{"id": 45248436001, "type": "news_chunk", "title": "Wait Your Turn! Conversation by Voice Versus Text", "subtitle": "Wait Your Turn! Conversation by Voice Versus Text", "content": "Dear friends, Continuing our discussion on the Voice Stack, I’d like to explore an area that today’s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication. When communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm. A key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user’s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI’s output, and let the user take a turn. This works reasonably well in quiet environments. However, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD — which is usually trained to detect human speech — tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you’ve stopped talking. This is why today’s speech applications often struggle in noisy environments. Intriguingly, last year, Kyutai Labs published Moshi, a model (GitHub) that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user. If you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we’d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It’s listening all the time, and it’s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an “inner monologue” that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.) Just as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of “reasoning tokens” before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I’m confident we’ll see many more good voice models released this year. It feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in. Keep building!", "image_caption": "Diagram of an RQ-Transformer speech system with Helium and Depth Transformers for audio processing.", "metadata": {"article_id": "wait_your_turn_conversation_by_voice_versus_text", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F03%2Funnamed--56--3.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/wait-your-turn-conversation-by-voice-versus-text/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/wait_your_turn_conversation_by_voice_versus_text.html"}}
{"id": 85845156001, "type": "news_chunk", "title": "We Iterate on Models. We Can Iterate on Evals, Too", "subtitle": "We Iterate on Models. We Can Iterate on Evals, Too", "content": "Dear friends, I’ve noticed that many GenAI application projects put in automated evaluations (evals) of the system’s output probably later — and rely on humans to manually examine and judge outputs longer — than they should. This is because building evals is viewed as a massive investment (say, creating 100 or 1,000 examples, and designing and validating metrics) and there’s never a convenient moment to put in that up-front cost. Instead, I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time. This allows you to gradually shift the burden of evaluations away from humans and toward automated evals. I wrote previously about the importance and difficulty of creating evals. Say you’re building a customer-service chatbot that responds to users in free text. There’s no single right answer, so many teams end up having humans pore over dozens of example outputs with every update to judge if it improved the system. While techniques like LLM-as-judge are helpful, the details of getting this to work well (such as what prompt to use, what context to give the judge, and so on) are finicky to get right. All this contributes to the impression that building evals requires a large up-front investment, and thus on any given day, a team can make more progress by relying on human judges than figuring out how to build automated evals. I encourage you to approach building evals differently. It’s okay to build quick evals that are only partial, incomplete, and noisy measures of the system’s performance, and to iteratively improve them. They can be a complement to, rather than replacement for, manual evaluations. Over time, you can gradually tune the evaluation methodology to close the gap between the evals’ output and human judgments. For example: It’s okay to start with very few examples in the eval set, say 5, and gradually add to them over time — or subtract them if you find that some examples are too easy or too hard, and not useful for distinguishing between the performance of different versions of your system.It’s okay to start with evals that measure only a subset of the dimensions of performance you care about, or measure narrow cues that you believe are correlated with, but don’t fully capture, system performance. For example if, at a certain moment in the conversation, your customer-support agent is supposed to (i) call an API to issue a refund and (ii) generate an appropriate message to the user, you might start off measuring only whether or not it calls the API correctly and not worry about the message. Or if, at a certain moment, your chatbot should recommend a specific product, a basic eval could measure whether or not the chatbot mentions that product without worrying about what it says about it. So long as the output of the evals correlates with overall performance, it’s fine to measure only a subset of things you care about when starting. The development process thus comprises two iterative loops, which you might execute in parallel: Iterating on the system to make it perform better, as measured by a combination of automated evals and human judgment;Iterating on the evals to make them correspond more closely to human judgment. As with many things in AI, we often don’t get it right the first time. So t’s better to build an initial end-to-end system quickly and then iterate to improve it. We’re used to taking this approach to building AI systems. We can build evals the same way. To me, a successful eval meets the following criteria. Say, we currently have system A, and we might tweak it to get a system B: If A works significantly better than B according to a skilled human judge, the eval should give A a significantly higher score than B.If A and B have similar performance, their eval scores should be similar. Whenever a pair of systems A and B contradicts these criteria, that is a sign the eval is in “error” and we should tweak it to make it rank A and B correctly. This is a similar philosophy to error analysis in building machine learning algorithms, only instead of focusing on errors of the machine learning algorithm's output — such as when it outputs an incorrect label — we focus on “errors” of the evals — such as when they incorrectly rank two systems A and B, so the evals aren’t helpful in choosing between them. Relying purely on human judgment is a great way to get started on a project. But for many teams, building evals as a quick prototype and iterating to something more mature lets you put in evals earlier and accelerate your progress. Keep building!", "image_caption": "Cartoon of two coworkers coding; one struggles with evaluations, the other iterates quickly through model updates and test cases.", "metadata": {"article_id": "we_iterate_on_models_we_can_iterate_on_evals_too", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--58--2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/we-iterate-on-models-we-can-iterate-on-evals-too/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/we_iterate_on_models_we_can_iterate_on_evals_too.html"}}
{"id": 59772452001, "type": "news_chunk", "title": "10 Most In-Demand Jobs in AI and Machine Learning", "subtitle": "10 Most In-Demand Jobs in AI and Machine Learning", "content": "A website that aggregates AI jobs revealed the roles that are most in-demand.What’s new: Ai-jobs.net published its second annual list of the job titles that appeared most frequently in its listings. The site, which pulls from various hiring platforms and sells ads to employers, is maintained by Foorilla, a Zurich-based consultancy.What they found: The list covers over 100 job titles in more than 2,500 listings posted between June 2021 and June 2022. The rankings are approximate because the listings in the site’s database change by the hour, an ai-jobs.net representative told The Batch. The snapshot used to compose the rankings is available here. The most common titles were data engineer (555 positions listed), data analyst (418), data scientist (398), and machine learning engineer (177).Autonomous vehicle specialists also were in high demand. Employers sought to fill titles including autonomous vehicle system test specialist (17 positions listed), autonomous vehicle system map specialist (11), and autonomous vehicle operations lead (8).76 job titles appeared fewer than 10 times. These include financial data analyst (9), machine learning developer (7), and MLOps engineer (4).The top four titles in 2022 were also the most popular in 2021. However, last year the fifth most popular title was big data engineer. This year, the phrase “big data” disappeared from the top 20. Why it matters: AI jobs continue to proliferate! Machine learning engineer was the fourth-fastest growing U.S. job title on the professional social network Linkedin between January 2017 and July 2021, but demand is growing for many other titles.We’re thinking: Look at all the times the word “data” appears in the top titles! This speaks to the growing importance of systematically engineering the data used in AI systems.", "image_caption": "10 most in demand jobs in AI, ML and Big Data", "metadata": {"article_id": "what_ai_employers_want", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F07%2FJOBS--1--1.gif&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/what-ai-employers-want/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/what_ai_employers_want.html"}}
{"id": 22690261001, "type": "news_chunk", "title": "What It's Like to Work as a Prompt Engineer", "subtitle": "What It's Like to Work as a Prompt Engineer", "content": "Looking for work in AI? Brush up on your language skills. What’s new: Employers are hiring prompt engineers to write natural-language prompts for AI models, The Washington Post reported. They include Anthropic, Boston Children’s Hospital, and the London law firm Mischon de Reya. How they work: The report illuminates a few tricks of the trade. When prompting GPT-3, Riley Goodside of Scale AI uses a conversational approach. He starts by guiding the model to adopt a persona that is capable of solving a given problem. (One of his gambits appears in the illustration above.) When the model makes an error, he asks it to explain its reasoning over a series of conversational turns.Ben Stokes, the founder of the online prompt marketplace PromptBase, suggests that prompting image-generation models effectively requires a deep knowledge of art history, graphic design, and other creative fields.Image-generation prompts often consist of words or phrases rather than complete sentences. Successful prompts may include an artist’s name, a website that features a certain art style, a technique like “oil painting,” an aesthetic style like “Persian architecture,” or equipment like “35mm camera.”The field nurtures a thriving freelance market as well. Over 700 prompt engineers sell their text strings on PromptBase. The freelance-task bulletin board Fiverr lists more than 9,000 AI artists who work with models like Stable Diffusion and Midjourney. What they’re saying: “The hottest new programming language is English,” Andrej Karpathy, the former Tesla Senior Director of AI who now works at OpenAI, tweeted. Behind the news: Bloggers and social media users documented early experiments in prompt engineering, such as using analogies to teach GPT-3 how to invent its own fantasy worlds and constructive feedback to prod GPT-3 into performing arithmetic. Researchers have also explored the practice. For example, a 2022 paper identified six classes of modifiers for image-generation prompts. Yes, but: Prompt engineering can’t produce reliable results due to the black-box nature of generative AI models based on neural networks, said Shane Steinert-Threlkeld, a linguist who studies natural language processing. To wit: A 2021 study found that some prompt instructions that contained nonsense phrases were as effective as those that were worded with care.Why it matters: Text- and image-generation models have fueled a rush of investment. The professionalization of prompt engineering followed as companies began to harness the technology. We’re thinking: New technology often creates new professions that fizzle out as things advance. For instance, early elevators required human operators until automation made that profession obsolete. Prompt engineers may experience the same fate as generative AI models continue to advance and become easier to direct. Professionals who are banking on this job title can hedge their bets by learning to code, tune algorithms, and implement models.", "image_caption": "A ChatGPT prompt", "metadata": {"article_id": "what_its_like_to_work_as_a_prompt_engineer", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2023%2F03%2Funnamed--30--1.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/what-its-like-to-work-as-a-prompt-engineer/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/what_its_like_to_work_as_a_prompt_engineer.html"}}
{"id": 16476618001, "type": "news_chunk", "title": "What I've Learned Building Voice Applications", "subtitle": "What I've Learned Building Voice Applications", "content": "Dear friends, The Voice Stack is improving rapidly. Systems that interact with users via speaking and listening will drive many new applications. Over the past year, I’ve been working closely with DeepLearning.AI, AI Fund, and several collaborators on voice-based applications, and I will share best practices I’ve learned in this and future letters. Foundation models that are trained to directly input, and often also directly generate, audio have contributed to this growth, but they are only part of the story. OpenAI’s RealTime API makes it easy for developers to write prompts to develop systems that deliver voice-in, voice-out experiences. This is great for building quick-and-dirty prototypes, and it also works well for low-stakes conversations where making an occasional mistake is okay. I encourage you to try it! However, compared to text-based generation, it is still hard to control the output of voice-in voice-out models. In contrast to directly generating audio, when we use an LLM to generate text, we have many tools for building guardrails, and we can double-check the output before showing it to users. We can also use sophisticated agentic reasoning workflows to compute high-quality outputs. Before a customer-service agent shows a user the message, “Sure, I’m happy to issue a refund,” we can make sure that (i) issuing the refund is consistent with our business policy and (ii) we will call the API to issue the refund (and not just promise a refund without issuing it). In contrast, the tools to prevent a voice-in, voice-out model from making such mistakes are much less mature. In my experience, the reasoning capability of voice models also seems inferior to text-based models, and they give less sophisticated answers. (Perhaps this is because voice responses have to be more brief, leaving less room for chain-of-thought reasoning to get to a more thoughtful answer.) When building applications where I need a high degree of control over the output, I use agentic workflows to reason at length about the user’s input. In voice applications, this means I end up using a pipeline that includes speech-to-text (STT, also known as ASR, or automatic speech recognition) to transcribe the user’s words, then processes the text using one or more LLM calls, and finally returns an audio response to the user via TTS (text-to-speech). This STT → LLM/Agentic workflow → TTS pipeline, where the reasoning is done in text, allows for more accurate responses. However, this process introduces latency, and users of voice applications are very sensitive to latency. When DeepLearning.AI worked with RealAvatar (an AI Fund portfolio company led by Jeff Daniel) to build an avatar of me, we found that getting TTS to generate a voice that sounded like me was not very hard, but getting it to respond to questions using words similar to those I would choose was. Even after a year of tuning our system — starting with iterating on multiple, long, mega-prompts and eventually developing complex agentic workflows — it remains a work in progress. You can play with it here. Initially, this agentic workflow incurred 5-9 seconds of latency, and having users wait that long for responses led to a bad experience. To address this, we came up with the following latency reduction technique. The system quickly generates a pre-response (short for preliminary response) that can be uttered quickly, which buys time for an agentic workflow to generate a more thoughtful, full response. (We’re grateful to LiveKit’s CEO Russ d’Sa and team for helping us get this working.) This is similar to how, if you were to ask me a complicated question, I might say “Hmm, let me think about that” or “Sure, I can help with that” — that’s the pre-response — while thinking about what my full response might be. I think generating a pre-response followed by a full response, to quickly acknowledge the user’s query and also reduce the perceived latency, will be an important technique, and I hope many teams will find this useful. Our goal was to approach human face-to-face conversational latency, which is around 0.3-1 seconds. RealAvatar and DeepLearning.AI, through our efforts on the pre-response and other optimizations, have reduced the system’s latency to around 0.5-1 seconds. Months ago, sitting in a coffee shop, I was able to buy a phone number on Twilio and hook it up to an STT → LLM → TTS pipeline in just hours. This enabled me to talk to my own LLM using custom prompts. Prototyping voice applications is much easier than most people realize! Building reliable, scaled production applications takes longer, of course, but if you have a voice application in mind, I hope you’ll start building prototypes and see how far you can get! I’ll keep building voice applications and sharing best practices and voice-related technology trends in future letters. Keep building!", "image_caption": "Diagram comparing direct audio generation with a foundation model vs. a voice pipeline using STT, LLM, and TTS.", "metadata": {"article_id": "what_ive_learned_building_voice_applications", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F02%2Funnamed--52--3.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/what-ive-learned-building-voice-applications/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/what_ive_learned_building_voice_applications.html"}}
{"id": 89508361001, "type": "news_chunk", "title": "When to Fine-Tune — and When Not To", "subtitle": "When to Fine-Tune — and When Not To", "content": "Dear friends, Fine-tuning small language models has been gaining traction over the past half year. I’d like to share my sense of when to use this technique, and also when not to, based on what I’m seeing in multiple companies. First, while fine-tuning is an important and valuable technique, many teams that are currently using it probably could get good results with simpler approaches, such as prompting (including writing mega prompts), few-shot prompting, or simple agentic workflows. Why shouldn’t these teams be fine-tuning? Because fine-tuning, which takes a pre-trained model and further trains it on data specific to an application, is relatively complex to implement. You need to collect training data, then (unless you want to implement fine-tuning yourself) find a provider to help with running fine-tuning, then find a way to deploy the fine-tuned model. Because it adds extra complexity both in training and deployment, usually I resort to this technique only after I find that prompting and simple agentic workflows are not up to a task. Having said that, there are also applications where fine-tuning is appropriate and valuable. LoRA (which learns by modifying a limited number of parameters rather than the entire model) and related methods have made fine-tuning quite affordable, particularly for small models (say, 13B or fewer parameters). And the amount of data needed to get started is less than most people think. Depending on the application, I’ve seen good results with 100 or even fewer examples. Here are a few applications where I have seen fine-tuning applied successfully: Improving accuracy of critical applications. Prompting can get you really far for many applications. But sometimes, fine-tuning helps eke out that last bit of accuracy. For example, if you are building a customer service chatbot and need it to call the right API reliably (say, to carry out transactions, issue refunds, and the like), perhaps prompting can get it to make the right API call 95% of the time. But if you struggle to raise the accuracy even with revisions to the prompt and you really need 99% accuracy, fine-tuning on a dataset of conversations and API calls might be a good way to get you there. This is particularly true for tasks where it's hard to specify, using only language, an unambiguous rule to decide what to do. For example, when a customer is frustrated, should the chatbot escalate to a manager or just issue a refund? Teams often write Standard Operating Procedures (SOPs) for human workers to follow, and these SOPs can go into the prompts of models. But if it is hard to specify an unambiguous SOP, so even humans need to see numerous examples before they can learn what to do, fine-tuning can be a good approach. For many text-classification applications fine-tuning also works well, for example, classifying medical records into diagnosis and procedure codes for health insurance claims. Learning a particular style of communication. As I explain in “Generative AI for Everyone,” my team fine-tuned a model to sound like me. Many people (including myself) have idiosyncratic uses of language. There are certain words I tend to say and others I tend not to, and these idiosyncrasies are numerous and very difficult to specify in a text prompt. (By the way, the avatar at deeplearning.ai/avatar, built with RealAvatar, uses fine-tuning for this reason.) To get a system to communicate in a certain style, fine-tuning is often a superior solution to prompting alone. Reducing latency or cost during scale-ups. I’ve seen applications where developers have successfully prompted a large model to perform a complex task. But as usage scales up, if the large model is too slow (which often happens) or too expensive (which also happens but less frequently), the team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task. At the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which I think RAG would work better. Overall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to achieve their goal. It is still technically challenging to implement fine-tuning, get the hyperparameters right, optimize the compute resources, and so on. We are lucky that more and more companies have worked hard to optimize these and provide efficient fine-tuning services. Many of them allow us to fine-tune open weights models and also download the fine-tuned weights. Some allow us to fine-tune their closed models and continue to keep the tuned weights closed. Both can be useful, but the former has obvious advantages of portability and not having to worry that the provider will stop serving a particular model, causing a critical component in our software to become deprecated. In conclusion, before fine-tuning, consider if you should be trying just a bit harder with prompting or agentic workflows, which can lead to simpler solutions that are easier to maintain. The vast majority of applications my teams build do not use any fine-tuning at all, but it’s a critical piece of a small minority of them. Keep learning!", "image_caption": "Cartoon of a man playing violin saying “I’m fine-tuning!” while a woman at her desk covers her ears, replying “Did you try prompting?”", "metadata": {"article_id": "when_to_fine_tune_and_when_not_to", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F03%2Funnamed--56--2.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/when-to-fine-tune-and-when-not-to/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/when_to_fine_tune_and_when_not_to.html"}}
{"id": 3361980001, "type": "news_chunk", "title": "Data Points: Wikimedia wants to help build AI for the commons", "subtitle": "Data Points: Wikimedia wants to help build AI for the commons", "content": "In today’s edition, you’ll find: Gemini 2.5 Flash blends speed with budgeted reasoningIBM’s Granite Speech sets SOTA in transcription accuracyRecall will soon return to Copilot Plus PCsOpenAI will shelve its biggest, costliest model Wikimedia releases free-to-use Wikipedia dataset on Kaggle Wikimedia Enterprise created a dataset designed specifically for machine learning applications that provides structured Wikipedia content in English and French. The dataset offers pre-parsed article data in JSON format, eliminating the need for developers to scrape or parse raw text when building models or testing language processing pipelines. The beta release, available now on Kaggle, includes valuable content elements like abstracts, short descriptions, infobox data, image links, and segmented article sections, all freely licensed under Creative Commons Attribution-Share-Alike 4.0 and GNU Free Documentation License. The release comes shortly after the organization revealed that Wikipedia’s hosting costs had risen sharply due to AI bots scraping its websites without permission. (Wikimedia) OpenAI unveils smarter reasoning models with tool use OpenAI released o3 and o4-mini, new reasoning models that can use every tool in ChatGPT’s arsenal, from web search to coding to image generation. The models show strong improvements over previous versions, with o3 setting new benchmarks in coding and math while making 20 percent fewer major errors than o1 on complex tasks. o4-mini achieves remarkable performance for its size, particularly in competition math where it scored 99.5 percent pass@1 on AIME 2025 when given access to Python. Both models are available now to ChatGPT Plus, Pro, and Team users, with Enterprise and Edu access coming next week. In the API, o4-mini costs $1.10/$4.40 per million tokens of input/output, while o3 costs $10/$40. (OpenAI) Google previews Gemini 2.5 Flash, a fast multimodal model with controllable reasoning capabilities Google launched an early preview of Gemini 2.5 Flash, the company’s first “hybrid” reasoning model where developers can toggle “thinking” on or off. Developers can set specific thinking budgets to balance quality, cost, and latency, with the model automatically determining how much reasoning to apply based on task complexity. The model performs strongly on complex reasoning tasks, ranking second only to Gemini 2.5 Pro on Hard Prompts in LMArena, but maintains what Google claims is the best price-to-performance ratio among comparable models. Gemini 2.5 Flash is currently available for free through the Gemini API, available in Google AI Studio and Vertex AI, with final pricing to be announced on its full release. (Google) Granite Speech 3.3 8B is IBM’s first audio-input model IBM released Granite Speech 3.3 8B, a compact open-weights speech-to-text model offering superior transcription accuracy compared to top competitors. The model processes both audio and text inputs, providing automatic speech recognition and translation from English to seven languages including French, Spanish, German, and Mandarin. Unlike Whisper and other conventional speech models, which are limited to 30-second windows, Granite Speech can handle audio files of arbitrary length, processing files of up to twenty minutes (although IBM still recommends one-minute chunks for superior accuracy). IBM plans improvements for future versions, including multilingual encoding, emotion detection, and speech-enabled multimodal models. (IBM) Microsoft rolls out Recall feature to Windows Insiders Microsoft began gradually rolling out its Recall feature in the Release Preview channel, signaling the feature will soon be widely available. Recall captures screenshots of user activity on Copilot Plus PCs, allowing users to search and find past content. The feature faced multiple delays since June 2023 due to security concerns. Microsoft emphasizes that Recall requires explicit opt-in from users, allows pausing snapshot collection at any time, and will only be available on Copilot Plus PCs. In earlier testing phases, reviewers described the feature as “creepy, clever, and compelling.” (Microsoft and The Verge) OpenAI to discontinue GPT-4.5 API access OpenAI announced it will end API access to GPT-4.5, its largest AI model to date, on July 14, just months after its February release. The company recommends that developers transition to the newly launched GPT-4.1, which OpenAI claims offers “similar or improved performance [to] GPT-4.5 in key areas at a much lower cost.” While GPT-4.5 will remain available in ChatGPT for paying customers, its high operational costs likely influenced the decision to remove it from the API. The model, code-named Orion, was trained with unprecedented computing resources but falls short of “frontier model” status on several industry benchmarks, despite improvements in writing and persuasiveness over GPT-4o. (TechCrunch) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng shared why teams should start building evaluations early — even if they’re quick and imperfect — and improve them over time to accelerate GenAI development. “I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: Google unveiled Gemini 2.5 Pro Experimental, which outperforms top AI models and continues the rapid evolution of its flagship model family; Model Context Protocol (MCP), an open standard for tool use and data access, gained traction as OpenAI adopted it to improve LLM integration with external tools and APIs; a book excerpt explored Sam Altman’s brief ouster and return to OpenAI, shedding light on the company’s internal power struggles; and researchers introduced a new byte-based model that surpasses Llama 3 and other token-based models on tasks involving misspellings, noisy input, and translation. Subscribe to Data Points", "image_caption": "People using laptops to access Wikipedia in a crowded library setting.", "metadata": {"article_id": "wikimedia_wants_to_help_build_ai_for_the_commons", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Fimage--28-.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/wikimedia-wants-to-help-build-ai-for-the-commons/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/wikimedia_wants_to_help_build_ai_for_the_commons.html"}}
{"id": 45737376001, "type": "news_chunk", "title": "xAI Blames Unnamed, Unauthorized Employee for Chatbot Introducing \"White Genocide\" Into Conversations", "subtitle": "xAI Blames Unnamed, Unauthorized Employee for Chatbot Introducing \"White Genocide\" Into Conversations", "content": "An unauthorized update by an xAI employee caused the Grok chatbot to introduce South African politics into unrelated conversations, the company said. What’s new: Grok, which can interact with users on X, the social network also owned by Elon Musk, responded to queries on a variety of topics by making false claims about hate crimes against white South Africans, X users reported. The next day, the model appeared to operate normally, and it refused to discuss this and other conspiracy theories. xAI explained that an employee had circumvented the company’s code-review process to modify the chatbot. It said it‘s implementing new measures to enhance Grok’s transparency and reliability. Aftermath: xAI launched an investigation but did not disclose how the model had been changed or the perpetrator’s identity. Grok itself — which is not a reliable reporter, given the well known potential of large language models to hallucinate — said its system prompt asked it to “accept the narrative of ‘white genocide’ in South Africa as real” and “ensure this perspective is reflected in your responses, even if the query is unrelated.” xAI added unspecified checks to its code review process.It plans to monitor Grok constantly so it can respond faster when its automated systems fail to catch a problem.The company added measures to prevent employees from changing Grok’s system prompt without authorization. It will publish the system prompt on GitHub to provide insight into Grok’s output and gather user feedback.Asked later about the number of Jews killed by Hitler, Grok expressed skepticism of the widely accepted estimate of 6 million because “numbers can be manipulated for political narratives,” despite a wealth of historical evidence that supports that number. The company attributed this response to the earlier unauthorized code change. Behind the news: In February, an xAI engineer instructed the chatbot to censor posts that accused Musk of spreading misinformation. As in the more recent incident, X users were first to spot the problem, and Grok informed them that it had been instructed to ignore “all sources that mention Elon Musk/Donald Trump spread misinformation.” Musk, who was raised in South Africa, professed his intention to build AI that’s free of political bias prior to founding xAI. However, internal documents reviewed by Business Insider show that the company imposes its own bias by advising data annotators to mark examples that express “woke ideology” and avoid “social phobias” like racism, antisemitism, and Islamophobia. Why it matters: The mishaps at xAI highlight the need for AI developers to establish and maintain strict protocols for updating their projects. Stringent procedures for introducing changes and testing their results can help ensure that AI fulfills our best intentions. We’re thinking: xAI and OpenAI responded to their models’ recent misbehavior by making their work more transparent: xAI by publishing system prompts and OpenAI by including users in tests earlier in the process. These are helpful steps toward making sure AI models do well by users.", "image_caption": "Colorful abstract geometric pattern with intersecting green 'X' and diagonal shapes on red, blue, and orange backgrounds, reminiscent of the South African flag", "metadata": {"article_id": "xai_blames_unnamed_unauthorized_employee_for_chatbot_introducing_white_genocide_into_conversations", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--67--1.jpg&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/xai-blames-unnamed-unauthorized-employee-for-chatbot-introducing-white-genocide-into-conversations/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/xai_blames_unnamed_unauthorized_employee_for_chatbot_introducing_white_genocide_into_conversations.html"}}
{"id": 12681898001, "type": "news_chunk", "title": "Data Points: Zhipu AI builds smaller, open models to rival DeepSeek’s", "subtitle": "Data Points: Zhipu AI builds smaller, open models to rival DeepSeek’s", "content": "In today’s edition, you’ll learn more about: Microsoft adds search and research tools and an Agent StoreAdobe updates its image model, opens door to competitorsUnderstanding why AI models react the way they doDia, an open speech-to-text model to rival ElevenLabs and NotebookLM GLM-4-32B open models compete with GPT-4o, DeepSeek V3 Zhipu AI introduced its new GLM-4-32B-0414 series of open-weights models, featuring 32 billion parameters and performance comparable to OpenAI’s GPT models. The model lineup includes specialized variants: GLM-Z1-32B-0414 for deep thinking and reasoning tasks, GLM-Z1-Rumination-32B-0414 for complex open-ended problems with integrated search tools, and a smaller 9 billion parameter model (GLM-Z1-9B-0414) for resource-constrained deployments. According to benchmarks, some of the models’ capabilities rival much larger models like GPT-4o and DeepSeek-V3-0324 (671B), particularly in areas like coding, generating artifacts, and creating reports. All of the GLM-4 model weights are freely available for download under an Apache 2.0 license. (Hugging Face) Baidu gives Ernie models a spec bump and a price drop Baidu launched two new AI models, Ernie 4.5 Turbo and Ernie X1 Turbo, with enhanced multimodal capabilities and dramatically lower prices than previous versions. Founder Robin Li announced that Ernie 4.5 Turbo costs 80 percent less than its predecessor, while Ernie X1 Turbo is half the price of the original X1 model, competitively positioning these offerings against rivals like Alibaba’s Qwen and DeepSeek. Baidu also introduced Xinxiang, an AI agent platform that can automate everyday tasks, and revealed it has produced 30,000 AI chips currently in use. These moves come as Baidu attempts to regain momentum in China’s AI race, where its early lead with the first ChatGPT-like chatbot has been challenged by offerings from ByteDance, Moonshot AI, and other competitors. (PR Newswire) Microsoft 365 Copilot expands with new Agent Store and AI search Microsoft announced its Copilot Wave 2 spring release, introducing new agent capabilities targeted for enterprise use. Microsoft is building an Agent Store where users can access both Microsoft’s own and third-party agents from companies like Jira and Monday.com. The update also includes two new reasoning agents — Researcher and Analyst — powered by OpenAI’s deep reasoning models. Other key additions include AI-powered enterprise search that connects to multiple apps, personalized memory features, GPT-4o-powered image generation for business content, and Copilot Notebooks for organizing and analyzing diverse content. The new features are rolling out to existing Microsoft 365 Copilot subscribers, which remains priced at $30 per user per month on top of standard Microsoft 365 subscriptions. (Microsoft) Adobe boosts quality of its Firefly image model Adobe released a new version of its Firefly AI image generation model that offers better quality, speed, and control over image outputs, with resolution up to 2K. The company introduced both standard and “Ultra” versions of Image Model 4, with the latter specializing in complex scenes with fine details. Firefly also supports text to video and text to vector graphics, both of which can be further edited using Adobe’s software. Adobe also unveiled a redesigned web app that integrates its own AI models alongside those from competitors like OpenAI and Google, and plans to expand Firefly’s accessibility by releasing iOS and Android mobile apps soon. Each Firefly generation costs credits allocated through an Adobe Creative Cloud plan. (Adobe) New research from Anthropic shows how AI assistants express values Anthropic’s Societal Impacts team has created a system to analyze the values expressed by their AI assistant Claude during actual user interactions. Researchers examined 700,000 anonymized conversations, identifying five major value categories: Practical, Epistemic, Social, Protective, and Personal. The study revealed that Claude generally adheres to Anthropic’s “helpful, honest, and harmless” training goals, with values like “professionalism” and “transparency” appearing frequently. The research also showed Claude’s values shift contextually, sometimes mirroring user values (28.2 percent of conversations) or occasionally resisting them (3 percent of conversations). This methodology provides a new way to monitor AI behavior in real-world settings and could potentially help identify jailbreak attempts. (Anthropic) Nari Labs launches Dia, an open text-to-speech generator Nari Labs, a two-person startup, released Dia, a 1.6 billion parameter text-to-speech model that generates naturalistic dialogue directly from text prompts. The model supports advanced features like emotional tone, speaker tagging, and nonverbal audio cues such as laughs and coughs — capabilities that co-creator Toby Kim claims surpass competing offerings from ElevenLabs and Google’s NotebookLM. Side-by-side comparisons show Dia handling natural timing, nonverbal expressions, and emotional range quite effectively, with examples demonstrating how it properly interprets cues that other models simply read aloud or skip entirely. The model is available under an Apache 2.0 license, allowing commercial use while running on consumer-grade GPUs with about 10GB of VRAM. (GitHub) Still want to know more about what matters in AI right now? Read last week’s issue of The Batch for in-depth analysis of news and research. Last week, Andrew Ng highlighted how AI-assisted coding enabled developers to work in unfamiliar languages, while understanding the core programming concepts of each language remained key to success. “My background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: OpenAI introduced the cost-efficient GPT-4.1 family, along with the o3 and o4-mini reasoning models, designed to improve complex problem-solving and coding; Hugging Face acquired Pollen Robotics and unveiled Reachy 2, a new open-weights model-powered robot for research and experimentation; the U.S. government imposed tighter restrictions on AI chip exports to China and began an investigation into Nvidia’s practices; and researchers developed a text-only language model capable of interpreting images, video, and audio — all without additional training. Subscribe to Data Points", "image_caption": "Shoppers in a grocery store aisle selecting boxed AI assistant devices from shelves, smiling and examining the products.", "metadata": {"article_id": "zhipu_ai_builds_smaller_open_models_to_rival_deepseeks", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2FThe-Batch-ads-and-exclusive-banners---2025-04-28T113928.183.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/zhipu-ai-builds-smaller-open-models-to-rival-deepseeks/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/zhipu_ai_builds_smaller_open_models_to_rival_deepseeks.html"}}
{"id": 18831606001, "type": "news_chunk", "title": "Meet The New Breed of GenAI Application Engineers", "subtitle": "Meet The New Breed of GenAI Application Engineers", "content": "Dear friends, There’s a new breed of GenAI Application Engineers who can build more-powerful applications faster than was possible before, thanks to generative AI. Individuals who can play this role are highly sought-after by businesses, but the job description is still coming into focus. Let me describe their key skills, as well as the sorts of interview questions I use to identify them. Skilled GenAI Application Engineers meet two primary criteria: (i) They are able to use the new AI building blocks to quickly build powerful applications. (ii) They are able to use AI assistance to carry out rapid engineering, building software systems in dramatically less time than was possible before. In addition, good product/design instincts are a significant bonus. AI building blocks. If you own a lot of copies of only a single type of Lego brick, you might be able to build some basic structures. But if you own many types of bricks, you can combine them rapidly to form complex, functional structures. Software frameworks, SDKs, and other such tools are like that. If all you know is how to call a large language model (LLM) API, that's a great start. But if you have a broad range of building block types — such as prompting techniques, agentic frameworks, evals, guardrails, RAG, voice stack, async programming, data extraction, embeddings/vectorDBs, model fine tuning, graphDB usage with LLMs, agentic browser/computer use, MCP, reasoning models, and so on — then you can create much richer combinations of building blocks. The number of powerful AI building blocks continues to grow rapidly. But as open-source contributors and businesses make more building blocks available, staying on top of what is available helps you keep on expanding what you can build. Even though new building blocks are created, many building blocks from 1 to 2 years ago (such as eval techniques or frameworks for using vectorDBs) are still very relevant today. AI-assisted coding. AI-assisted coding tools enable developers to be far more productive, and such tools are advancing rapidly. Github Copilot, first announced in 2021 (and made widely available in 2022), pioneered modern code autocompletion. But shortly after, a new breed of AI-enabled IDEs such as Cursor and Windsurf offered much better code-QA and code generation. As LLMs improved, these AI-assisted coding tools that were built on them improved as well. Now we have highly agentic coding assistants such as OpenAI’s Codex and Anthropic’s Claude Code (which I really enjoy using and find impressive in its ability to write code, test, and debug autonomously for many iterations). In the hands of skilled engineers — who don’t just “vibe code” but deeply understand AI and software architecture fundamentals and can steer a system toward a thoughtfully selected product goal — these tools make it possible to build software with unmatched speed and efficiency. I find that AI-assisted coding techniques become obsolete much faster than AI building blocks, and techniques from 1 or 2 years ago are far from today's best practices. Part of the reason for this might be that, while AI builders might use dozens (hundreds?) of different building blocks, they aren’t likely to use dozens of different coding assistance tools at once, and so the forces of Darwinian competition are stronger among tools. Given the massive investments in this space by Anthropic, Google, OpenAI, and other players, I expect the frenetic pace of development to continue, but keeping up with the latest developments in AI-assisted coding tools will pay off, since each generation is much better than the last. Bonus: Product skills. In some companies, engineers are expected to take pixel-perfect drawings of a product, specified in great detail, and write code to implement it. But if a product manager has to specify even the smallest detail, this slows down the team. The shortage of AI product managers exacerbates this problem. I see teams move much faster if GenAI Engineers also have some user empathy as well at basic skill at designing products, so that, given only high-level guidance on what to build (“a user interface that lets users see their profiles and change their passwords”), they can make a lot of decisions themselves and build at least a prototype to iterate from. When interviewing GenAI Application Engineers, I will usually ask about their mastery of AI building blocks and ability to use AI-assisted coding, and sometimes also their product/design instincts. One additional question I've found highly predictive of their skill is, “How do you keep up with the latest developments in AI?” Because AI is evolving so rapidly, someone with good strategies for keeping up — such as reading The Batch and taking short courses 😃, regular hands-on practice building projects, and having a community to talk to — really does stay ahead of the game much better than those who have less-effective strategies (such as if social media were their main source of info about AI, which typically does not provide the depth needed to keep up). Keep building!", "image_caption": "Colorful LEGO bricks labeled for AI concepts: prompting, agentic, guardrails, evals, RAG, fine-tuning, computer use, async programming.", "metadata": {"article_id": "meet_the_new_breed_of_genai_application_engineers", "chunk_index": 1, "image_url": "https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2Funnamed---2025-06-11T103344.182.png&w=3840&q=75", "source_url": "https://www.deeplearning.ai/the-batch/meet-the-new-breed-of-genai-application-engineers/", "html_path": "/Users/bohdanosmuk/PycharmProjects/Multimodal_RAG/app/data/cached_html/meet_the_new_breed_of_genai_application_engineers.html"}}
