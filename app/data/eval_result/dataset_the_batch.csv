question,ground_truth,answer,context
What was the final strategy that hiders discovered in the hide-and-seek game played by the virtual agents?,"Hiders discovered the final, unbeatable strategy: Lock up every moveable object they wouldn’t be using as a barricade, then lock themselves inside a shelter of movable walls.",The final strategy that hiders discovered in the hide-and-seek game was to lock up every movable object they wouldn't be using as a barricade and then lock themselves inside a shelter made of movable walls.,"-----------------
Title: Ready or Not
Content: Dear friends, I read an interesting paper comparing the results of traditional passive learning (sitting in a lecture) versus active methods like the flipped classroom, where students watch videos at home and work on exercises in class. The paper is nicely summarized by this figure: The leftmost pair of bars shows that students learn more from active learning. Ironically, they feel they are learning more from passive methods, shown by the remaining bars. I’ve been using a flipped classroom for much of my teaching, with great results. Students watch lectures on Coursera, then come to the classroom to ask questions and work in small groups. This paper explains why many instructors are reluctant to switch to active learning, even though it’s more effective. The world needs much better education everywhere. I hope more educators who teach in person will embrace active learning methods. Keep learning! Independent research lab OpenAI designed virtual agents to play hide-and-seek. They evolved increasingly clever strategies, eventually hacking the game world’s physics to gain advantage. What happened: The researchers trained the agents to navigate and manipulate their environment and juiced them with reinforcement learning. Then they divided their creations into teams of hiders and seekers and set them loose in a virtual world that included movable blocks, walls, and ramps. How it works: Seekers scored points if they caught sight of a hider. Hiders scored if they finished a game without being seen. An agent could move or lock objects in place; but only the agent that locked a given object could unlock it again. The agents figured out the basics over the first several million rounds. Around game 22 million, hiders — which were given a grace period at the start of each round to scramble for cover — began building shelters out of the movable objects.Roughly 100 million rounds in, seekers learned to infiltrate these hideaways using ramps. A few million later, the hiders stymied this strategy by locking the ramps.The researchers say they didn’t expect the agents to learn much more. But around game 450 million, seekers discovered they could push blocks around even if they were standing on top. This allowed them to surf to hiders’ walls and walk right into their hideaways (as seen in the animation above).Hiders eventually discovered the final, unbeatable strategy: Lock up every moveable object they wouldn’t be using as a barricade, then lock themselves inside a shelter of movable walls. Why it matters: Hide-and-seek strategies could map to many real-world applications. For instance, rescue robots could be programmed as seekers — with rules restricting which types of objects are okay to pick up or move — to sift through rubble for survivors after a disaster.We’re thinking: Reinforcement learning continues to find clever solutions. But the need to play 480 million rounds limits such techniques to simulated environments. We look forward to breakthroughs in small-data RL that make it possible to apply such techniques to physical robots that can play, say, thousands of games before they wear out. Meta learning, which organizations including OpenAI have worked on, could be an important step in this direction.

Image to article:
- Caption to image: Simulated hide-and-seek environment
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize201201.gif
-----------------"
What technique did MIT CSAIL researchers develop to detect generated images and their artifacts?,They developed a technique to detect portions of an image that betray fakery by using convolutional neural networks (CNNs) to recognize areas dominated by generated textures.,"MIT CSAIL researchers developed a technique that uses convolutional neural networks (CNNs) to detect portions of an image that betray fakery by focusing on textures. They built classifiers that analyze images one patch at a time, concentrating on fine details by modifying Resnet and Xception architectures. This approach produces heatmaps to identify areas with generated artifacts, allowing them to detect images produced by different GANs like StyleGAN, Glow, and GMM.","-----------------
Title: The Telltale Artifact
Content: Deepfakes have gone mainstream, allowing celebrities to star in commercials without setting foot in a film studio. A new method helps determine whether such endorsements — and other images produced by generative adversarial networks — are authentic.What’s new: Lucy Chai led MIT CSAIL researchers in an analysis of where image generators fool and where they fail. They developed a technique to detect portions of an image that betray fakery.Key insight: Large-scale features of generated images are highly varied, but generated textures contain consistent artifacts. Convolutional neural networks (CNNs) are especially sensitive to textures, which makes them well suited to recognizing such artifact-laden areas. A CNN tailored for analyzing small pieces of images can learn to recognize parts dominated by generated textures.How it works: The authors built classifiers that survey images one patch at a time. They ran the classifiers on output from StyleGAN, Glow, and a generator model based on Gaussian mixture models (GMMs). They averaged the patchwise classifications to analyze each GAN’s vulnerability to detection. The authors created a dataset of images generated by a Progressive GAN trained on the CelebA-HQ dataset of celebrity portraits.They modified Resnet and Xception architectures to classify patches of user-determined size and trained them on the generated images. They removed the deeper layers, which analyze larger image areas, to concentrate the models on fine details.They used the classifications to produce heatmaps of image areas recognized as generated (blue) or not (red). Predominantly blue images were deemed to have been generated.By averaging the heatmaps over many images produced by the same GAN, the authors were able to identify the areas where that model is especially prone to leaving artifacts. For instance, StyleGAN and Glow generated high concentrations of artifacts in facial details, while GMM tended to generate them in backgrounds. Results: The authors’ best classifier achieved 100 percent average precision on StyleGAN output and 91.38 percent on GMM. These scores outperformed non-truncated MesoInception4, Resnet-18, Xception, and CNN models, which achieved average precision between 99.75 and 73.33 percent. On Glow, the authors’ best classifier achieved 95 percent average precision, whereas the best full model scored 97.32 percent.Why it matters: The better GANs become, the more useful they can be for both good and ill. In shedding light on areas where particular GANs produce more artifacts, this work illuminates pathways for researchers to improve them. But it also provides a map for malefactors to make their activities harder to detect. In fact, when the researchers trained a GAN to fool their classifiers, accuracy fell to less than 65 percent.We’re thinking: Building a discriminator that recognizes a particular generator’s output is easier than building a good generator. In fact, GAN researchers routinely degrade discriminators to give the generator a fighting chance to fool it. But social media platforms, among others, would like to catch all generated images, regardless of the generator that produced them. Looking for common artifacts offers a promising approach — until a variety of generators learn how to avoid producing them. Learn how image translation is used to create deepfakes in the upcoming Course 3: Apply GANs, available soon on Coursera.

Image to article:
- Caption to image: Data and examples related to a new technique to detect portions of an image
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2022.gif
-----------------"
What is the accuracy rate of Visa's new deep learning system in predicting credit card transaction approvals compared to the earlier rule-based algorithm?,"The new model matched banks’ decisions with 95 percent accuracy, while the earlier rule-based algorithm was half as accurate.","Visa's new deep learning system predicts credit card transaction approvals with 95 percent accuracy, which is twice as accurate as the earlier rule-based algorithm.","-----------------
Title: News
Content: A neural network is helping credit card users continue to shop even when the lender’s credit-approval network goes down.What’s new: Visa developed a deep learning system that analyzes individual cardholders’ behavior in real time to predict whether credit card transactions should be approved or denied. The system can step in when a card issuer — generally a bank that normally would vet such transactions — suffers a network outage that makes it impossible to assess creditworthiness.How it works: If a cardholder’s purchases are blocked, they might switch to another card, costing the bank revenue and possibly a customer. And if a miscreant tries to commit fraud, the bank stands to lose money. So Visa provides a backup system that predicts the decision in case the lender can’t due to software glitches, severe weather, or routine maintenance. The new model is trained on the company’s database of historical transactions. It learns an individual’s normal behavior based on factors like spending history, location, and timing of transactions.In tests, it matched banks’ decisions with 95 percent accuracy. An earlier, rule-based algorithm was half as accurate, according to a report by the Wall Street Journal.Visa plans to make the service available for a fee starting in October. Why it matters: Unlike, say, fraud detection, this model touches cardholders directly to improve the customer experience. It points the way toward public-facing models that personalize banking, credit, and other financial arrangements. Yes, but: Visa declined to share details of its new algorithm with The Batch. Decisions to extend credit can be based on patterns in data that encode social biases, and an algorithm trained on a biased dataset will reflect its biases. For instance, an algorithm may decline transactions requested by a cardholder whose home address is in a neighborhood associated with defaults on loans, and accept those requested by someone with a comparable history of repayment who lives in a wealthier neighborhood. Large financial institutions are aware of this problem, but standards that specify what is and isn’t fair are still in development. We’re thinking:The financial industry’s health depends on trust. That should provide ample incentive to define the fairness of automated systems in lending and other financial services. Efforts such as Singapore’s Principles to Promote Fairness, Ethics, and Transparency are an important step.

Image to article:
- Caption to image: Woman with plenty of shopping bags
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Visa.gif
-----------------"
What is a key reason why change management is particularly important in AI projects?,Change management is important in AI projects because the technology's disruptive nature affects many stakeholders and requires their buy-in to succeed.,"Change management is particularly important in AI projects because AI's disruptive nature affects many stakeholders, and without proper management, projects can be blocked or slowed down due to fear and misunderstanding. Effective change management involves bringing stakeholders onboard, adjusting systems, and building trust to ensure successful implementation.","-----------------
Title: DeepLearning.ai Exclusive
Content: Dear friends, My last two letters explored robustness and small data as common reasons why AI projects fail. In the final letter of this three-part series, I’d like to discuss change management. Change management isn’t an issue specific to AI, but given the technology’s disruptive nature, we must pay attention to it if we want our projects to succeed. An AI system that, say, helps doctors triage patients in an emergency room affects many stakeholders, from doctors to the intake nurses to the insurance underwriters. To keep projects on track, people must be brought onboard and systems must be adjusted. I recently saw a union block even small-scale experiments because of fear that AI would automate jobs away. This was unfortunate, because the AI system being contemplated would have made employees more valuable without reducing employment. A change management process could have made the stakeholders comfortable with experimenting and helped them understand why it was worthwhile rather than threatening. Many engineers underestimate the human side of change management. Some tips: Budget enough time. Change management requires asking lots of questions, assessing how various roles will change, and explaining to many people what the AI will do.Identify all stakeholders. Either communicate with them directly or find ways to have colleagues talk to them. Many organizations make decisions by consensus, and it is important to minimize the odds of any stakeholder blocking or slowing down implementation. We also need to build trust among stakeholders that the AI will work.Provide reassurance. Where possible, explain to people how their work may change and how the new system will benefit them.Explain what’s happening and why. There is still significant fear, uncertainty and doubt (FUD) about AI. I have seen that providing a basic education — along the lines of the AI for Everyone curriculum — eases these conversations. Other tactics including explainability, visualization, rigorous testing, and auditing also help build trust in an AI system and convince our customers (and ourselves!) that it really works.Right-size the first project. If it is not possible to start with a complex deployment that affects a lot of people, consider starting with a smaller pilot (The AI Transformation Playbook includes helpful perspective on this) that affects a smaller number of stakeholders, and is thus easier to get buy in. As we have seen with self-driving cars, building an AI system often involves solving a systems problem. That requires reorienting not only stakeholder roles and organizational structures, but also many things around the AI, like setting expectations with other drivers, pedestrians, and first responders and updating procedures around road maintenance and construction. Addressing the systems problem will increase the odds of your project succeeding. If you understand the problems of robustness, small data, and change management, and if you can spot these problems in advance and pre-empt them, you’ll be well ahead of the curve in building a successful AI project. Building AI projects is hard. Let’s keep pushing and share what we learn with each other, so we can keep moving the field forward! Keep learning! Nitin knew he needed to learn more to build the products he had in mind. So he took the Deep Learning Specialization and applied to jobs that would allow him to transition from web performance to machine learning. He sold LinkedIn on combining the two. Read more

Image to article:
- Caption to image: Nitin Pasumarthy and a quote from him
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/TheBatchFeaturedImageBreakingIntoAINitinPasumarthy201.png
-----------------"
What is the main advantage of AlphaFold in predicting protein shapes compared to earlier methods?,"AlphaFold predicts the distances and angles between residues, making the optimization of protein shapes easier.","The main advantage of AlphaFold in predicting protein shapes compared to earlier methods is its ability to predict the distances and angles between residues, rather than just whether residues are in contact with one another. This makes the optimization process easier and more accurate.","-----------------
Title: Protein Shapes Revealed
Content: A protein’s biological function depends largely on its three-dimensional shape, but deducing its shape from its sequence of amino acids has been a longstanding problem. Researchers at DeepMind reveal how they used deep learning to solve the puzzle.What’s new: Andrew Senior and colleagues released long-awaited details about AlphaFold, a protein-folding model that wowed experts in a high-profile competition in late 2018. The paper is behind a paywall. This video offers some details.Key insight: Research has shown that protein shapes are determined by the proximity of essential portions, or residues, of amino acids. The researchers found likely shapes by optimizing over possible structures that keep residues close to one another. Earlier methods predict whether residues are in contact with one another. AlphaFold predicts the distances and angles between residues, making the optimization easier.How it works: AlphaFold extracts features from an input protein sequence, predicts relationships between residues, and uses those predictions to find the protein’s likely shape. The feature extractor compares the input sequence with sequences in a protein database. It represents relationships between amino-acid pairs based on the similarities it finds.The features feed a CNN trained on a dataset of 3D protein structures, which predicts the distribution of distances and angles between residues.The model infers the protein’s physical stability based on the distances and angles. The physical stability equation is differentiable, so the predicted structure can be optimized by gradient descent. The most stable structure is the final output. Results: At the 2018 CASP13 conference, AlphaFold predicted 24 out of 43 previously unknown protein shapes with high accuracy. The next-best model achieved 14 predictions of similar accuracy.Why it matters: The ability to determine protein structures could have wide-ranging impacts on drug discovery, countering neurodegenerative diseases, and more. Stay tuned for further progress when CASP14 convenes in April.We’re thinking: Hard problems don’t always offer enough training data to train an end-to-end neural network. In this case, combining a physical model with neural networks led to significant progress. This design pattern holds promise in many other domains from climate change to robot dynamics.

Image to article:
- Caption to image: Data related to AlphaFold, a protein-folding model
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Proteins20320ASPECT.png
-----------------"
What was the cumulative precision of the authors' predictions for thermoelectricity compared to the alternative method?,"The cumulative precision for thermoelectricity was 76 percent, while the cumulative precision of the alternative method was 48 percent.","The cumulative precision of the authors' predictions for thermoelectricity was 76 percent, compared to 48 percent for the alternative method.","-----------------
Title: AI Predicts Scientific Breakthroughs Using Social Graphs
Content: A new AI method directs scientists toward promising avenues of inquiry. What's new: Jamshid Sourati and James A. Evans at University of Chicago proposed a method to predict new scientific discoveries by building a graph that connects researchers, their objects of study, and the scientific properties thereof. They evaluated their approach using data from materials science. Key insight: Overlapping interests among researchers may indicate areas where further research would be fruitful. For example, if one group of researchers studies a material A and its property P, a second group studies materials A and B, and another group studies materials B and C, it may turn out that material C exhibits property P. How it works: The authors tried to predict whether certain inorganic materials have certain electrical properties based on scientific literature through the year 2000. From 1.5 million articles that described 100,000 inorganic compounds, they extracted the author names, materials mentioned (for example, sodium nitrite), and their properties (for example, thermoelectricity, the ability to convert heat into electricity and vice versa). They used this data to construct a graph whose nodes were authors, materials, and properties. Edges connected the nodes that appeared in the same paper, for example a particular author whose paper covered specific material or property. The authors conducted random walks through the graph, stepping from node to node, to produce sequences of authors, materials, and properties. Then they removed the authors from the sequences, because they were interested mainly in establishing possible connections between materials and properties. They trained Word2Vec, which computes word embeddings, on their sequences, treating materials and properties as words and sequences as documents. This yielded an embedding for each material and property.To predict possible discoveries — that is, which material might exhibit a given property — the authors scored each material based on (i) the similarity between the material’s embedding and the given property’s embedding and (ii) the smallest number of edges in the path that connected each material and the property. Then they summed scores (i) and (ii). The 50 highest-scoring materials were predicted to have the property (that weren’t directly connected in the graph; that is, excluding materials that already were known to have the property). Results: The authors predicted which materials possessed each of three properties. They compared their results with predictions obtained in a similar way using a Word2Vec model trained exclusively on text from scientific papers. They used papers from 2001 through 2018 to evaluate the predictions. For thermoelectricity, the cumulative precision (percentage of predicted discoveries proven correct) was 76 percent, while the cumulative precision of the alternative method was 48 percent. The cumulative precision of random guesses was about 3 percent. The authors obtained similar results for the other two properties. Why it matters: Science is a social endeavor, where the connections between people and their work can be represented as a graph that reflects the collective attention of the scientific community. The collective attention acts as a signal that predicts promising avenues for further research — a signal that machine learning can help to tease out. We're thinking: The authors also predicted drug discoveries with similarly good results. Their method may be useful for identifying fruitful directions in other scientific areas, and perhaps in other domains entirely.

Image to article:
- Caption to image: Predicting Scientific Discoveries: AI predicts scientific breakthroughs using social graphs
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2FSCIENCE-FIX-1.gif&w=3840&q=75
-----------------"
What is the primary focus of the report published by the Carnegie Endowment for International Peace regarding AI surveillance?,"The report details which countries are buying surveillance gear, which companies are supplying it, and what technologies are most in-demand.","The primary focus of the report published by the Carnegie Endowment for International Peace is to detail which countries are buying AI surveillance gear, which companies are supplying it, and what technologies are most in-demand. It highlights the use of AI surveillance in 75 out of 176 surveyed countries, spanning from advanced democracies to dictatorships.","-----------------
Title: Watching the Watchers
Content: A growing number of nations use AI to track their citizens. A new report sheds light on who’s watching and how. What’s new: Published by Carnegie Endowment for International Peace, “The Global Expansion of AI Surveillance” details which countries are buying surveillance gear, which companies are supplying it, and what technologies are most in-demand.What the report says: Of 176 countries surveyed, at least 75 use some combination of face recognition, location tracking, and predictive policing. This list of users spans advanced democracies, including the U.S., Germany, and the UK, to absolute dictatorships. Countries with the largest defense budgets are most likely to invest in AI-driven surveillance.The U.S. and China are the top producers of equipment. Huawei is the most prolific, selling to governments in 50 countries. IBM is the biggest U.S. dealer, providing surveillance systems to 11 countries.The report simply lists nations, suppliers, and applications. It doesn’t evaluate whether particular users or uses violate international human-rights agreements. Methodology: The authors drew their information from news reports. They accepted information as reported by established sources like The New York Times and Economist. They gathered corroborating accounts before relying on less rigorous sources like blogs.Why it matters: Surveillance networks are deeply rooted even in bastions of liberal democracy like London. They can support public safety, as in New South Wales, Australia, where smartcams spot drivers using a phone behind the wheel. But they also promote social biases and erode trust in authority and, at their worst, they’re powerful tools for repression. Baltimore’s secret drone-policing fiasco shows how an all-seeing eye can lead well-intentioned authorities in the direction of invasive dystopia.We’re thinking: Tracking which governments use which technology is important because it empowers citizens to react. The AI community, in particular, should take a proactive stance in promoting wise use of these technologies.

Image to article:
- Caption to image: AI tracking on pedestrians
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize204.gif
-----------------"
What is the flagship model of Alibaba's Qwen3 language models and how many parameters does it have?,"The flagship model of Alibaba's Qwen3 language models is Qwen3-235B-A22B, which has 235 billion parameters.","The flagship model of Alibaba's Qwen3 language models is the Qwen3-235B-A22B, which has 235 billion parameters, with 22 billion active at any given time.","-----------------
Title: Alibaba Releases the Qwen3 Family of Open LLMs With Optional Reasoning
Content: Alibaba’s new model family may unseat DeepSeek-R1’s four-month reign as the top open-weights large language model. What’s new: Alibaba released weights for eight large language models, all of which offer a reasoning mode that can be switched on or off. Two use a mixture of experts (MoE) architecture: Qwen3-235B-A22B (the name indicates 235 billion parameters, 22 billion of which are active at any given time) and Qwen3-30B-A3B). The other six are dense models in sizes between 32 billion parameters and 0.6 billion parameters — tiny by LLM standards, and with reasoning, too. Input/output: MoE models: Text in (up to 131,072 tokens), text out. Dense models: Text in (up to 32,768 tokens), text out.MoE architecture: Transformers. Qwen3-235B-A22B: 235 billion parameters, 22 billion active at any given time. Qwen3-30B-A3B: 30.5 billion parameters, 3.3 billion active at any given time.Dense architecture: Transformers with parameter counts of 32 billion, 14 billion, 8 billion, 4 billion, 1.7 billion, 0.6 billionTraining data: Pretrained on 36 trillion tokens, generated and scraped from the web, including textbooks, PDF documents, question-answer pairs, math problems, codeFeatures: Selectable reasoning mode, multilingual (119 languages and dialects) Undisclosed: Knowledge cutoff, fine-tuning data, output limits Availability: Free for noncommercial and commercial uses under Apache 2.0 license via HuggingFace and ModelScopeAPI price: Qwen3-235B-A22B: $0.22/$0.88 per million input/output tokens. Qwen3-30B-A3B: $0.15/$0.60 per million input/output tokens. Via Fireworks.ai How it works: The Qwen3 family implements chain-of-thought reasoning in both relatively large and quite small LLMs. The team pretrained Qwen3 models on roughly twice the data used to pretrain Qwen2.5. A substantial part of the additional data was devoted to training the model in several major languages plus region-specific dialects like Haitian, Luxembourgish, and Eastern Yiddish, and lesser-known Austronesian languages like Waray, Minangkabau, and Iloko. Pretraining took place over three stages that progressed to longer, more complex data. The authors fine-tuned the models on long chains of thought in domains that included coding, engineering, logical reasoning, mathematics, science, and technology. A reward model reinforced successful completions of these tasks. The in-progress models were used to generate synthetic data to train the non-reasoning mode. Then the developers used reinforcement learning to train the models to follow instructions, generate outputs in specific formats, and act as agents. Results: Qwen3-235B-A22B and Qwen3-30B-A3B performed as well as, or better than, leading open-weights models in tests performed by Alibaba. Qwen3-4B, too, achieved results that are competitive with many models several times its size. Alibaba didn’t provide results for the other dense models. On coding challenges in LiveCodeBench and Codeforces, Qwen3-235B-A22B (70.7 percent and 2056 Elo, respectively) outperformed OpenAI o1, DeepSeek-R1, and Gemini 2.5 Pro, but fell behind OpenAI o4-mini set to high effort. It outperformed the same models on the Berkeley Function-Calling Leaderboard (BFCL). Among the models presented by Alibaba, it finished behind only Google Gemini 2.5-Pro testing math skills (AIME 2024, AIME 2025) and a variety of recently updated math, language, and problem-solving questions (LiveBench).Qwen3-30B-A3B outperformed Google Gemma-3-27B-IT and DeepSeek-V3 on all benchmarks highlighted by Alibaba, and it underperformed only OpenAI GPT-4o on BFCL. On GPQA Diamond’s test of graduate-level questions in a variety of domains, Qwen3-30B-A3B (65.8 percent) outperformed next-best DeepSeek-V3.Qwen3-4B, with 4 billion parameters, was competitive across a wide range of benchmarks against DeepSeek-V3 (671 billion parameters) and Gemma-3-27B-IT (27 billion). For instance, on both Codeforces and LiveBench, Qwen3-4B (1,671 Elo and 63.6 percent, respectively) outperformed DeepSeek-V3 (1,134 Elo and 60.5 percent). Why it matters: Qwen3 continues a string of high-performance, open-weights models released by developers in China. Alibaba says it designed the models to do the thinking in agentic systems. Reasoning that can be switched on and off can help control costs in agentic and other applications. We’re thinking: Alibaba’s 235-billion parameter MoE model may perform better according to benchmarks, but Qwen3-30B-A3B does nearly as well and can run locally on a pro laptop without straining its memory. Add the easy ability to switch reasoning on or off, and Qwen3’s versatile, mid-sized MoE model may turn out to be the star of the show.

Image to article:
- Caption to image: LLM performance benchmark table comparing Qwen, OpenAI, Gemini, and others on coding, math, and language tasks.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--85--1.png&w=3840&q=75
-----------------"
What is the purpose of the VirtuSense Balance system used in senior living facilities?,"The purpose of the VirtuSense Balance system is to help specialists spot postures and motions that could contribute to falls, assess fall risk, and provide recommendations for improving patients' mobility.","The purpose of the VirtuSense Balance system used in senior living facilities is to help protect elderly people from falls by diagnosing postures and motions that could lead to a fall. It uses infrared light to scan patients as they perform various motions, and a pose detection model analyzes their positions. The system assigns a fall risk score and provides recommendations for improving mobility, thereby aiming to save lives, reduce insurance premiums, and help caregivers use their time more efficiently.","-----------------
Title: Risk Reduction for Elders
Content: Deep learning is helping to protect elderly people from potentially catastrophic tumbles.What’s happening: More than 2,000 senior living facilities across the U.S. use a diagnostic system called VirtuSense Balance to keep residents on their feet.How it works: The system helps a specialist spot postures and motions that could contribute to a fall. It scans patients with infrared light as they perform a series of motions. A pose detection model analyzes their positions, a company spokesperson told The Batch. A balance test measures how much a person sways while standing still.A gait test assesses walking speed, the angles of the knees, and length of each step.In function tests, the system analyzes various sitting, standing, and walking activities.The system compares input from a given patient with norms for their age group, then assigns a fall risk score. It also provides to caregivers recommendations for improving the patient’s mobility. Behind the news: Automated systems are helping to improve elder care in various ways. CarePredict is a wearable device that tracks patient behavior and alerts caregivers if they aren’t eating or sleeping well.The People Power Family system uses sensors to monitor seniors living at home for falls, late-night activity, and unexpected comings and goings. A model learns each patient’s habits and sends out warnings when they diverge in alarming ways. Why it matters: Falls kill thousands of elderly adults each year and injure millions more. Highlighting risk factors could save lives, reduce insurance premiums, and help caregivers use their time more efficiently.We’re thinking: AI has a clear role to play in caring for a surging elderly population. However, a recent study found that many older people resented and resisted being monitored by electronic systems. Technologists and health care practitioners alike must build such systems with compassion and respect for the people who will use them.

Image to article:
- Caption to image: Video showing diagnostic system VirtuSense Balance working
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/09/ezgif.com-gif-maker---2021-08-31T102909.159.gif
-----------------"
"What did ShotSpotter analysts do with the system's findings in some criminal cases, according to recent investigations?",ShotSpotter analysts reclassified sounds attributed to other causes as gunshots and changed the locations where gunshots were determined to have occurred.,"ShotSpotter analysts reclassified sounds that the system had attributed to other causes as gunshots and changed the location where the system determined that gunshots had occurred. This was done in some criminal cases, as revealed by recent court documents.","-----------------
Title: Shots in the Dark
Content: Dear friends, Since the pandemic started, several friends and teammates have shared with me privately that they were not doing well emotionally. I’m grateful to each person who trusted me enough to tell me this. How about you — are you doing okay?Last week, the Olympic gymnastic champion Simone Biles temporarily withdrew from competition because she didn’t feel mentally healthy enough to do her best and perhaps avoid a career-ending injury. She’s not alone in struggling with mental health. About 4 in 10 adults in the U.S. reported symptoms of anxiety or depression during the pandemic, according to one survey.Once, after looking over a collaborator’s project, I said, “That’s really nice work” and got back a sad facial expression as though my collaborator was near tears. I asked if they were okay, wondering if I had said something wrong, but they paused and shook their head. After I probed gently a little more, they burst out crying and told me that my remark was the first appreciation they had received in longer than they could remember. Many people outwardly look like they’re doing well, but inside they’re lonely, anxious, or uncertain about the future. If you’re feeling fine, that’s great! But if you’re among the millions who feel that something is off-balance, I sympathize, and I want you to know that I care about you and appreciate you.As the pandemic wears on, many of us are hungry to connect with others more deeply. If this describes you, or if you want to help someone else who might feel this way, perhaps you can start by letting someone know you appreciate them or something they did. I think this will make them — and maybe you, too — feel better. Love,Andrew A crime-fighting AI company altered evidence to please police, a new investigation claims — the latest in a rising chorus of criticism.What’s new: ShotSpotter, which makes a widely used system of the same name that detects the sound of gunshots and triangulates their location, modified the system’s findings in some cases, Vice reported.Altered output: ShotSpotter’s output and its in-house analysts’ testimony have been used as evidence in 190 criminal cases. But recent court documents reveal that analysts reclassified as gunshots sounds the system had attributed to other causes and changed the location where the system determined that gunshots had occurred. Last year, ShotSpotter picked up a noise around one mile from a spot in Chicago where police believed someone was murdered at the same time. The system classified it as a firecracker. Analysts later reclassified it as a gunshot and modified its location, placing the sound closer to the scene of the alleged crime. Prosecutors withdrew the ShotSpotter evidence after the defense requested that the judge examine the system’s forensic value.When federal agents fired at a man in Chicago in 2018, ShotSpotter recorded only two shots — those fired by cops. The police asked the company to re-examine the data manually. An analyst found five additional shots, presumably those fired by the perpetrator.In New York in 2016, a company analyst reclassified as gunshots a sound that the algorithm had classified as helicopter noise after being contacted by police. A judge later threw out the conviction of a man charged with shooting at police in that incident, saying ShotSpotter’s evidence was unreliable. The response: In a statement, ShotSpotter called the Vice report “false and misleading.” The company didn’t deny that the system’s output had been altered manually but said the reporter had confused two different services: automated, real-time gunshot detection and analysis after the fact by company personnel. “Forensic analysis may uncover additional information relative to a real-time alert such as more rounds fired or an updated timing or location upon more thorough investigation,” the company said, adding that It didn’t change its system’s findings to help police.Behind the news: Beyond allegations that ShotSpotter has manually altered automated output, researchers, judges, and police departments have challenged the technology itself. A May report by the MacArthur Justice Center, a nonprofit public-interest legal group, found that the vast majority of police actions sparked by ShotSpotter alerts did not result in evidence of gunfire or gun crime.Several cities have terminated contracts with ShotSpotter after determining that the technology missed around 50 percent of gunshots or was too expensive.Activists are calling on Chicago to cancel its $33 million contract with the company after its system falsely alerted police to gunfire, leading to the shooting of a 13-year-old suspect. Why it matters: ShotSpotter’ technology is deployed in over 100 U.S. cities and counties. The people who live in those places need to be able to trust criminal justice authorities, which means they must be able to trust the AI systems those authorities rely on. The incidents described in legal documents could undermine that trust — and potentially trust in other automated systems.We’re thinking: There are good reasons for humans to analyze the output of AI systems and occasionally modify or override their conclusions. Many systems keep humans in the loop for this very reason. It’s crucial, though, that such systems be transparent and subject to ongoing, independent audits to ensure that any modifications have a sound technical basis.

Image to article:
- Caption to image: Video showing ShotSpotter in action
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/08/SHOTSPOTTER2.gif
-----------------"
What did the Government Accountability Office (GAO) recommend for federal agencies using face recognition systems?,The GAO recommended that agencies using commercial systems develop protocols for appropriate use.,The Government Accountability Office (GAO) recommended that federal agencies using commercial face recognition systems develop protocols for appropriate use.,"-----------------
Title: U.S. Lax on Face Recognition
Content: A U.S. government watchdog agency called for stronger face recognition protocols for federal agencies.What’s new: An audit of federal agencies by the Government Accountability Office (GAO) found that, while many employ face recognition, they may not know where it came from, how it’s being used, or the hazards involved. The auditors recommended that agencies using commercial systems develop protocols for appropriate use.What they found: Twenty agencies that employ law-enforcement officers reported using face recognition. Of these, 11 used systems developed by private companies including Clearview AI and Vigilant. The others either developed their own or used systems developed by another agency. One of the most popular is the Department of Homeland Security’s Automated Biometric Identification System, which contains data on 835 million individuals.Several agencies did not seem to know who built some of the systems they use.Six agencies used the technology to investigate people involved in protests against police brutality. Three used it to look into perpetrators of the January 6 attack on the U.S. Capitol.Only one agency that reported using a privately developed face recognition system — Immigrations and Customs Enforcement — had implemented oversight protocols such as requiring that employees report each use. Behind the news: Face recognition is increasingly controversial in the U.S. Lawmakers recently introduced legislation that would freeze government use of the technology. At least 20 U.S. cities and several states have passed laws that restrict the technology.Why it matters: Face recognition has clear potential to infringe on privacy. Moreover, it has a spotty record of identifying minorities, which has led to false arrests. The finding that many federal agencies are taking a cavalier approach raises troubling questions about privacy and fairness.We’re thinking: The GAO audit of face recognition systems is a step forward. While regulators, ethicists, technologists, and businesses sort out appropriate standards, a moratorium on law enforcement use of face recognition would be sensible, so we can position the technology for socially beneficial uses while guarding against detrimental ones.

Image to article:
- Caption to image: Series of images related to face recognition protocols for federal agencies
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/07/gao.gif
-----------------"
What is the name of the algorithm used by Insilico to create digital representations of molecules for drug discovery?,"Generative Tensorial Reinforcement Learning, or GENTRL","The algorithm used by Insilico to create digital representations of molecules for drug discovery is called Generative Tensorial Reinforcement Learning, or GENTRL.","-----------------
Title: Treatment: The Elusive Molecule
Content: Will deep learning discover new medicines? Startups — and big-pharma partners — are betting on it.The problem: In theory, there’s a pharmacological cure for just about any ailment. In practice, discovering those therapies takes years and billions of dollars.The solution: Deep learning, with its ability to discern patterns amid noise, could speed up drug discovery considerably. In a dramatic test, Insilico used an algorithm to sift through petabytes of biochemical data to find potential drugs in 21 days.How it works: Based in Rockville, Maryland, Insilico used its Generative Tensorial Reinforcement Learning, or GENTRL, to create digital representations of molecules with properties that inhibit an enzyme linked to several types of cancer, atherosclerosis, and fibrosis. To make sure the model steered clear of established intellectual property, the researchers fed it a database of 17,000 patented compounds.The model produced 30,000 candidates, which the researchers whittled down to 848 using a mix of computational and AI methods.They selected 40 at random to examine more closely. They sent six of the most promising to WuXi AppTec, a pharmaceutical contract manufacturer in Shanghai, to synthesize. One of the molecules did indeed inhibit the enzyme in mice. Status: Insilico’s enzyme inhibitor was only a proof of concept. However, it attracted partnerships with GlaxoSmithKline, Jiangsu Chia Tai Fenghai Pharmaceutical, and Pfizer.Behind the news: Drug discovery is an attractive target for AI startups, given the abundance of biochemical data and desperation of pharmaceutical giants to cut costs. But success still seems hit-or-miss. Only one AI-designed drug — made by Exscientia — has progressed to human trials. Verseon has been working on the problem for nearly two decades without creating a marketable product. And, crucially, no one has found a reliable way to accelerate clinical trials, the most expensive and time-consuming part of drug development.Why it matters: The average successful drug costs $2.5 billion dollars to bring to market, according to a 2016 study. Cutting even a fraction of that cost could allow companies to channel resources towards more and different drugs, potentially providing the public with more cures in less time.We’re thinking: Finding a molecule that becomes a viable drug is like hunting for a single, specific plankton in the Pacific Ocean. Good thing machine learning engineers relish searching for tiny patterns in massive pools of data. Use deep learning to estimate treatment effects for individual patients in Course 3 of our AI for Medicine Specialization.

Image to article:
- Caption to image: Illustration of a syring with red liquid inside
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/3-treatment_2.png
-----------------"
What is the purpose of the computer vision system developed by John Deere?,The purpose of the computer vision system is to spot defective joints in welded parts to ensure that heavy machinery leaves the production line ready to roll.,"The computer vision system developed by John Deere is designed to enable tractors to navigate agricultural fields safely without a driver. It distinguishes between obstacles like fallen branches and trees, gathers data on soil quality, and helps optimize the use of pesticides, herbicides, and fungicides, potentially reducing their use by up to 20 percent.","-----------------
Title: Outstanding in the Field
Content: One of the world’s largest makers of farm equipment is doubling down on self-driving tractors.What’s new: John Deere agreed to pay $250 million for Bear Flag Robotics, a California startup that upgrades conventional tractors for full autonomy.How it works: Deere has offered GPS-enabled tractor guidance systems that aid a human driver for nearly two decades. Bear Flag has adapted self-driving technology developed by the automotive industry to help tractors roam agricultural fields safely without a driver. Tractors equipped with Bear Flag tech navigate using a combination of GPS tracking and sensor data. Lidar, radar, and cameras enable the vehicles to see their surroundings. Actuator systems control steering, braking, and a variety of towed implements.The system is adapted for farm driving. For instance, the vision algorithm distinguishes between fallen branches that can be driven over and trees that should be avoided.The sensors also gather data on the quality of the soil tilled in the tractor’s wake. The information can help growers fine-tune their use of pesticides, herbicides, and fungicides, resulting in reductions of up to 20 percent, the company said. The system learns the boundaries of a farmer’s property during an initial drive-through. It also identifies roads, waterways, and other obstacles. It can upload the resulting map to a fleet of tractors for remote control and monitoring.Behind the news: Deere has been pursuing AI capabilities for several years. In 2017, it acquired Blue River Technology, a California-based startup that makes weed-killing robots. The following year, it launched a program to partner with promising startups including some that use deep learning.Why it matters: In addition to helping the farmers deal with a long-running labor shortage, AI-driven equipment could help increase their productivity and limit environmental impacts such as pesticide runoff.We’re thinking: Self-driving cars aren’t yet commonly used on public roads, but the technology appears to be good enough for commercial use in constrained environments like farms.

Image to article:
- Caption to image: Sequence of images showing tractors working on fields
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/08/ezgif.com-gif-maker--4--2.gif
-----------------"
How do researchers at the University of Michigan enable amputees to control a prosthetic hand with their thoughts?,"They graft bits of muscle onto severed nerve bundles, implant electrodes, and use machine learning to match nerve signals with hand motions.","Researchers at the University of Michigan enable amputees to control a prosthetic hand with their thoughts by grafting bits of muscle onto the severed nerve bundles at the ends of amputees’ forearms and implanting electrodes into the muscle. They amplify and record the electric signals transmitted to the nerves when the recipients think about specific hand motions. A naive Bayes classifier and a Kalman filter are then used to match these signals with corresponding hand movements, allowing the subjects to control the prosthetic hand.","-----------------
Title: AI Gets a Grip
Content: Amputees can control a robotic hand with their thoughts — plus machine learning.What’s new: University of Michigan researchers developed a system that uses signals from an amputee’s nervous system to control a prosthetic hand.How it works: The researchers grafted bits of muscle onto the severed nerve bundles at the ends of amputees’ forearms, then implanted electrodes into the muscle. They amplified and recorded the electric signals transmitted to the nerves when the recipients thought about, say, making a fist, pointing a finger, or rotating a thumb. Then they trained a pair of models to match the signals with the corresponding hand motions. A naive Bayes classifier learned to associate nerve signal patterns with common hand shapes.The researchers asked the subjects to mimic a virtual thumb as it made back-and-forth and side-to-side motions on a computer screen. A Kalman filter took in the electrical signals and the position and velocity of the avatar and learned to control the digit.Once trained, the software enabled the subjects to pick up and move objects and play Rock, Paper, Scissors. Behind the news: Other research groups are using similar methods to control robotic prostheses. Some promising approaches: Epineural electrodes wrap around nerves like a cuff to track signals from the brain.Intraneural electrodes tap into nerves using needles, so researchers can target brain signals more precisely.Targeted muscle reinnervation re-routes nerves from a severed limb into a nearby muscle. Sensors attached to the skin pick up the signals and transmit them to a prosthesis. Why it matters: Nearly two million Americans have lost a limb, along with millions more worldwide. More responsive prostheses could dramatically improve their quality of life.We’re thinking: Will they train robotic hands to do COVID-19-safe, palms-together namaste greetings?

Image to article:
- Caption to image: Robotic hand controlled by an amputee taking a can
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Prosthetics1-hand20only2.gif
-----------------"
What is Amazon's stance on the use of face recognition technology according to the context provided?,Amazon believes face recognition technology should be used in accordance with existing laws and emphasizes high standards of transparency for law enforcement and businesses using the technology.,"Amazon is drafting a statutory framework to guide the use of face recognition technology, anticipating a federal crackdown. The company believes face recognition should be used in accordance with existing laws, emphasizing privacy and non-discrimination. Amazon suggests high standards of transparency for law enforcement and proposes that the technology should only be used to narrow down suspects with a high confidence level, not as the final arbiter of guilt or innocence.","-----------------
Title: Amazon Prepares for a Crackdown
Content: Amazon is writing what it hopes will become U.S. law governing use of face recognition technology. What happened: At a press event announcing new features for Amazon’s Alexa smart-home service, Jeff Bezos told a reporter that his company’s lawyers are drafting a statutory framework to guide what he views as an inevitable federal crackdown on face recognition. Amazon sells the cloud-based face recognition service Rekognition, whose use by law enforcement agencies has raised alarm among civil liberties advocates.What it says: The company has released no details about the model legislation in progress. However, in February, Amazon VP of Global Public Policy Michael Punke published a blog that could provide clues to the company’s aims. Face recognition should be used in accordance with existing laws, Punke writes in the post proposing ethical guidelines for the technology. He points out that the U.S. Constitution’s Fourth Amendment and Civil Rights Act of 1964 explicitly outline an individual’s right to privacy and freedom from discrimination.Law enforcement groups, government agencies, and businesses using face recognition should be held to high standards of transparency, the post says.Law enforcement should be allowed to use the technology only to narrow down groups of suspects, and only when a model is at least 99 percent confident in its prediction. Models should never be used as the final arbiter of a person’s guilt or innocence. Behind the news: Face recognition’s rapid proliferation has spawned a widespread backlash in the U.S. cities. San Francisco and Oakland, California, and Somerville, Massachusetts, have banned the technology. California’s legislature is considering a statewide ban. Several bills restricting its use are wending their way through Congress, and two representatives have vowed to propose further legislation.We’re thinking: Punke’s guidelines are sound, and Amazon is well situated to understand how the technology could be abused. When industries propose their own regulations, though, legislators need to take special care to make sure any resulting laws benefit society as a whole.

Image to article:
- Caption to image: Amazon Rekognition screen capture
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-3020at202.08.4320PM.png
-----------------"
What is the name of the computer vision startup acquired by Tesla to help achieve its goal of autonomous taxis?,DeepScale,Not found in context,"-----------------
Title: Tesla All-In For Computer Vision
Content: Tesla is abandoning radar in favor of a self-driving system that relies entirely on cameras. What’s new: The electric car maker announced it will no longer include radar sensors on Model 3 sedans and Model Y compact SUVs sold in North America. Tesla is the only major manufacturer of autonomous vehicles to bet solely on computer vision. Most others rely on a combination of lidar, radar, and cameras. How it works: Tesla has dropped radar only in the U.S. and only in its two most popular models. It aims to gather data and refine the technology before making the change in Model S, Model X, and vehicles sold outside the U.S. The eight-camera system called Tesla Vision will provide sensory input for Autopilot driver-assistance features such as lane controls as well as the Full Self-Driving upgrade, which automatically parks and summons vehicles, slows for stop signals, and automates highway driving. Such features will be “limited or inactive” during the transition.The move comes on the heels of earlier statements that touted cameras. “When radar and vision disagree, which one do you believe?” Musk said in a tweet on April 10. “Vision has much more precision, so better to double down on vision than do sensor fusion.”CEO Elon Musk predicted that Tesla Vision would help the company’s vehicles achieve full autonomy by the end of 2021. (Musk has a history of declaring ambitious goals his company has failed to meet.) Behind the news: Some people in the self-driving car industry favor using relatively expensive lidar and radar sensors in addition to low-cost cameras because they provide more information and thus greater safety. Camera-only advocates counter that humans can drive safely perceiving only images, so we should build AI that does the same. Most companies working on autonomous vehicles have chosen the more expensive route as the fastest way to reach full autonomy safely. Once they get there, the thinking goes, they can attend to bringing the cost down. Why it matters: If Tesla’s bet on cameras pays off, it could have an outsize influence on future self-driving technology. We’re thinking: While it’s great to see ambitious plans to commercialize computer vision, Tesla’s initiative will require tests on public streets. That means countless drivers will be the company’s unwitting test subjects — a situation that, as ever, demands strong oversight by road-safety authorities.

Image to article:
- Caption to image: Animation showing Tesla car's vision system
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/06/tesla-redo_orange-background-2.gif
-----------------"
What is the success rate of the Toyota robot when performing tasks in real-world environments after being trained in a virtual reality setting?,The robot achieved an 85 percent success rate.,The Toyota robot achieved an 85 percent success rate when performing tasks in real-world environments after being trained in a virtual reality setting.,"-----------------
Title: A Robot in Every Kitchen
Content: Every home is different. That makes it difficult for domestic robots to translate skills learned in one household — say, fetching a soda from the fridge — into another. Training in virtual reality, where the robot has access to rich information about three-dimensional objects and spaces, can make it easier for robots to generalize skills to the real world.What’s new: Toyota Research Institute built a household robot that users can train using a virtual reality interface. The robot learns a new behavior based on a single instance of VR guidance. Then it responds to voice commands to carry out the behavior in a variety of real-world environments.How it works: Toyota’s robot is pieced together from off-the-shelf parts, including two cameras provide stereoscopic vision. Classical robotics software controls the machine, while convolutional neural networks learn unique embeddings. To teach the robot new tasks, a user wears a VR headset to see through its eyes and drive it via handheld paddles.During training, the system maps each pixel to a wealth of information including object class, a vector pointing to the object’s center, and other features invariant to view and lighting.When the robot carries out a learned action in the real world, it establishes a pixel correspondence between its training and the present scene, and adjusts its behavior accordingly. Results: The Toyota researchers trained the bot in the virtual environment on three tasks: retrieving a bottle from a refrigerator, removing a cup from a dishwasher, and moving multiple objects to different locations. Then they had the robot perform each task 10 times in two physical homes. They ran the experiments with slight alterations, for instance asking the robot to retrieve a bottle from a higher shelf than the virtual one it was trained on, or doing so with the lights turned off. The robot achieved an 85 percent success rate — though it took an average 20 times longer than a human would.Why it matters: Researchers have given a lot of attention lately to the use of reinforcement learning on robots that are both trained and tested in a simulated environment. Getting such systems to generalize from a simulation to the real world is an important step toward making them useful.We’re thinking: Birth rates have been slowing for decades in Japan, China, the U.S., and much of Europe. The World Health Organization estimates that 22 percent of the world’s population will be over 60 years old by 2050. Who will care for the elderly? Robots may be part of the answer.

Image to article:
- Caption to image: Robot being trained using a virtual reality interface
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Toyota20Robot20Resized.gif
-----------------"
What system does the Los Angeles Police Department use to track license plates?,"The LAPD uses TBird, a license plate tracking system from data-mining company Palantir.","The Los Angeles Police Department uses TBird, a license plate tracking system from data-mining company Palantir.","-----------------
Title: Glimpse My Ride
Content: Dear friends, Nearly a decade ago, I got excited by self-taught learning and unsupervised feature learning — ways to learn features from unlabeled data that afterward can be used in a supervised task. These ideas contributed only marginally to practical performance back then, but I’m pleased to see their resurgence and real traction in self-supervised learning. Many of you know the story of how the increasing scale of computation and data, coupled with innovation in algorithms, drove the rise of deep learning. Recent progress in self-supervised learning also appears to be powered by greater computational and data scale — we can now train large neural networks on much larger unlabeled datasets — together with new algorithms like contrastive predictive coding. Today feels very much like the early, heady days a decade-plus ago, when we saw neural networks start to work in practical settings. The number of exciting research directions seems larger than ever! Keep learning, Police in the U.S. routinely use AI to track cars with little accountability to the public.What happened: Documents obtained by Wired revealed just how intensively police in Los Angeles, California, have been using automatic license plate readers. Officials queried databases of captured plate numbers hundreds of thousands of times in 2016 alone, records show.How it works: The Los Angeles Police Department, county sheriff, and other local agencies rely on TBird, a license plate tracking system from data-mining company Palantir. Detectives can search for full or partial numbers. The system maps the locations of vehicles with matching plates, annotated with previous searches and the time each image was captured.The system acts as a virtual dragnet, alerting nearby officers whenever cameras spot a flagged plate.It also lists all plates that appeared in the vicinity of a crime, along with each vehicle’s color, make, and style, thanks to machine vision from Intrinsics.The LAPD shares its license plate records with those of other nearby police departments as well as private cameras located in malls, universities, transit centers, and airports. Behind the news: A 2013 survey by the U.S. Dept. of Justice found that many urban police departments use automatic license plate readers.The LAPD was among the first to do so starting in 2009.Why it matters: License plate readers help solve serious crimes. Wired describes a case in which the LAPD used TBird to search for vehicles spotted near the place where a murdered gang member’s body was found. The plates led them to a rival gang member who eventually was convicted for the homicide.We’re thinking: Digital tools are becoming important in fighting crime, but it shouldn’t take a reporter’s public records request to find out how police are using them. We support regulations that require public agencies to disclose their use of surveillance technology, as well as rigorous logging and auditing to prevent misuse.

Image to article:
- Caption to image: Automatic license plate reader
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/License20plate204.gif
-----------------"
What is the name of the interactive text-adventure game that contributed to the script of the whimsical fantasy video 'The Squire'?,AI Dungeon 2,The interactive text-adventure game that contributed to the script of the whimsical fantasy video 'The Squire' is AI Dungeon 2.,"-----------------
Title: Here Be Dragons
Content: AI is contributing to paintings, music, and now a whimsical fantasy video.What’s new: The Squire is an amateur romp through a snowy realm of knights in armor and damsels in distress. The script was composed by AI Dungeon 2, an interactive text-adventure game based on the GPT-2 language model.How it works: Filmmakers Josh Johr and Dominick Todero began like any other AI Dungeon 2 player: By choosing a character (squire, mage, and so on) and setting (forest, dungeon). The program generated an internal context statement and fed it to the text engine, which responded: “You are Jake, a squire living in the kingdom of Larion...” It asked them periodically for input and generated text to advance the story. When the program declared, “GAME OVER!,” they set about planning the production. Unlike earlier text-adventure games, AI Dungeon 2 has no rules for how players can respond to prompts and no pre-programmed outcomes — everything is generated on the fly. The game’s creator, Nick Walton, fine-tuned the 1.5 billion-parameter version of GPT-2 on 30 megabytes of text scraped from an online choose-your-own-adventure forum.The game is prone to unexpected and occasionally nonsensical twists, such as when roadside bandits hand their sword to the befuddled Squire simply because he asks them to. But it anchors each session in its setting and narrative by feeding GPT-2 the previous 10 prompts, along with the original context statement, each time the player enters a new prompt, Walton told Towards Data Science.Inspired by the model CTRL, he imposed a penalty whenever the system reused previously generated words. That keeps the text generator from repeating itself, which can be a problem with the latest language models. We’re thinking: In the early days of generated music, some listeners enjoyed the jarring notes that computers often came up with. We’re still in the early days of generated narrative, but the results, for all their screwball turns, can be delightful.

Image to article:
- Caption to image: Excerpt from The Squire, an AI written short film
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize-1.gif
-----------------"
What should be prioritized over simply trying to beat human-level performance (HLP) in machine learning applications when HLP is significantly less than 100 percent?,It is better to explore how to redefine the labels to reduce variability and improve consistency among labelers.,"When human-level performance (HLP) is significantly less than 100 percent, it is more important to focus on redefining the labels to reduce variability and make them more consistent. This involves understanding how different labelers form their judgments and establishing consistent standards for labeling. By doing so, the labels become less noisy, which improves both human agreement and machine learning performance.","-----------------
Title: News
Content: Dear friends, Last week, I wrote about the limitation of using human-level performance (HLP) as a metric to beat in machine learning applications for manufacturing and other fields. In this letter, I would like to show why beating HLP isn’t always the best way to improve performance. In many machine learning problems, labels are determined by a person who evaluates the same sort of input as a learning algorithm would. For instance, a human labeler may look at a picture of a phone to determine if it’s scratched, and an algorithm would examine a similar picture to learn to detect scratches. (Note that this is not always the case. A human labeling a cancer diagnosis on an X-ray image may also rely on a tissue biopsy from the patient, while an algorithm would use the resulting dataset to learn to diagnose cancer based on images alone.) In cases where labels were determined by a human by looking at the same input that an algorithm would, what are we to make of situations in which HLP is well below 100 percent? This just means that different people labeled the data differently. For example, the ground-truth labeler who created a test set may have labeled a particular phone as scratched, while a different labeler thought the same phone was not scratched, and thus made a mistake in marking this example. If the second labeler disagreed with the ground-truth labeler on 1 out of 10 examples, then HLP in this task would be 90 percent. In this situation, rather than trying to build a learning algorithm that achieves 91 percent accuracy, it would be better to look into how the two labelers formed their judgements and try to help them make their labels more consistent. For example, all labelers may agree that scratches smaller than 1 mm are not significant (y=0), and scratches greater than 3 mm are significant (y=1), but they label scratches between 1 mm and 3 mm inconsistently. If we can spot this problem and get the labelers to agree on a consistent standard — say, that 1.5 mm is the point at which the labels should switch from y=0 to y=1 — then we’ll end up with less noisy labels. Setting standards that make labels more consistent will actually raise HLP, because humans now agree with one another more frequently. At the same time, having more consistently labeled data will result in better machine learning performance. This improvement is more important in many practical applications than the academic question of whether an algorithm beat HLP. HLP does have a role to play in establishing baseline performance for estimating irreducible, or Bayes, error, which in turn helps with error analysis. You can learn more about this in Deep Learning Specialization Course 3 and Machine Learning Yearning. But the message I hope you’ll take away from this letter is that, when a human labeler has created the class labels that constitute ground truth and HLP is significantly less than 100 percent, we shouldn’t just set out to beat HLP. We should take the deficit in human performance as a sign that we should explore how to redefine the labels to reduce variability. Keep learning! Social biases are well documented in decisions made by supervised models trained on ImageNet’s labels. But they also crept into the output of unsupervised models pretrained on the same dataset.What’s new: Two image classification models learned social biases from ImageNet photos, according to a study by researchers Carnegie Mellon and George Washington University.How it works: The authors measured the extent to which Google’s SimCLRv2 and OpenAI’s iGPT associated types of people with certain attributes. Using images from CIFAR-100 and Google Images, they assigned each picture either a category (such as man, woman, white, black, or gay) or an attribute (such as pleasant, unpleasant, career, or family).Then they fed the images to the model to generate features.They compared the features generated in response to different types of people (say, men or women) with features of opposing pairs of attributes (say, pleasant and unpleasant). In this way, they could determine the degree to which the model associated men versus women with those attributes. Results: Features generated by both models showed social biases such as associating white people with tools and black people weapons. While SimCLRv2 tended to associate stereotyped attributes with certain categories more strongly, iGPT showed such biases toward a broader range of categories. For instance, features generated by iGPT associated thin people with pleasantness and overweight people with unpleasantness, and also associated men with science and women with liberal arts. Behind the news: ImageNet 2012 contains 14 million images annotated by human workers, who passed along their prejudices to the dataset. ImageNet creator Fei-Fei Li is spearheading an effort to purge the dataset of labels that associated genders, races, or other identities with stereotypes and slurs.Why it matters: When unsupervised models pick up on biases in a dataset, the issue runs deeper than problematic labels. The authors believe that their models learned social stereotypes because ImageNet predominantly includes images of people in stereotypical roles: men in offices, women in kitchens, and non-white people in general excluded from images showing situations that have positive associations such as weddings. Machine learning engineers need to be aware that a dataset’s curation alone can encode common social prejudices.We’re thinking: Datasets are built by humans, so it may be impossible to eliminate social biases from them completely. But minimizing them will pay dividends in applications that don’t discriminate unfairly against certain social groups.

Image to article:
- Caption to image: Collage of self portraits
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker2033.gif
-----------------"
What is the primary function of the Oasis platform developed by Decart and Etched?,Oasis generates a Minecraft-like game in real time based on user input.,"The primary function of the Oasis platform developed by Decart and Etched is to generate a Minecraft-like game in real time, allowing users to explore an open-ended, interactive virtual world without a traditional game engine. It generates frames based on user input and previously generated frames, enabling real-time gameplay.","-----------------
Title: AI Creates an Interactive Minecraft-Like World in Real Time
Content: A real-time video generator lets you explore an open-ended, interactive virtual world — a video game without a game engine. What’s new: Decart, a startup that’s building a platform for AI applications, and Etched, which designs specialized AI chips, introduced Oasis, which generates a Minecraft-like game in real time. The weights are open and available here. You can play with a demo here. How it works: The system generates one frame at a time based on a user’s keystrokes, mouse movements, and previously generated frames. The training dataset is undisclosed, but it’s almost certainly based on videos of Minecraft gameplay, given the output’s striking semblance to that game. Some recent video generators produce an initial frame, then the nth frame, and then the frames in between. This approach isn’t practical for real-time gameplay. Instead, Oasis learned to generate the next frame. A ViT encoder embeds previously generated frames. Given those embeddings, an embedding of a frame to which noise had been added, and a user’s input, a diffusion transformer learned to remove the noise using a variation on diffusion called diffusion forcing.Generated frames may contain glitches, and such errors can snowball if the model incorporates glitches from previous frames into subsequent frames. To avoid this, during training, the system added noise to embeddings of previous frames before feeding them to the transformer to generate the next frame. This way, the transformer learned to ignore glitches while producing new frames.At inference, the ViT encoder embeds previously generated frames, and the system adds noise to the frame embeddings. Given the user’s input, the noisy frame embeddings, and a pure-noise embedding that represents the frame to be generated, the transformer iteratively removes the noise from the previous and current frame embeddings. The ViT’s decoder takes the denoised current frame embedding and produces an image.The system currently runs on Nvidia H100 GPUs using Decart’s inference technology, which is tuned to run transformers on that hardware. The developers aim to change the hardware to Etched’s Sohu chips, which are specialized for transformers and process Llama 70B at a jaw-dropping 500,000 tokens per second. Results: The Oasis web demo enables users to interact with 360-by-360-pixel frames at 20 frames per second. Users can place blocks, place fences, and move through a Minecraft-like world. The demo starts with an image of a location, but users can upload an image (turning, say, a photo of your cat into a blocky Minecraft-style level, as reported by Wired). Yes, but: The game has its fair share of issues. For instance, objects disappear and menus items change unaccountably. The world’s physics are similarly inconsistent. For instance, players don’t fall into holes dug directly beneath them and, after jumping into water, players are likely to find themselves standing on a blue floor. Behind the news: In February, Google announced Genie, a model that generates two-dimensional platformer games from input images. We weren’t able to find a publicly available demo or model. Why it matters: Oasis is more a proof of concept than a product. Nonetheless, as an open-world video game entirely generated by AI — albeit based on data produced by a traditional implementation — it sets a bar for future game generators. We’re thinking: Real-time video generation suggests a wealth of potential applications — say, a virtual workspace for interior decorating that can see and generate your home, or an interactive car repair manual that can create custom clips based on your own vehicle. Oasis is an early step in this direction.

Image to article:
- Caption to image: Comparison of Minecraft terrain with and without player modifications.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2Funnamed--32--1.gif&w=3840&q=75
-----------------"
What innovative technique did the filmmakers of 'Welcome to Chechnya' use to protect the identities of at-risk subjects?,They used deepfakes to create realistic decoy faces over the subjects' faces.,"The filmmakers of 'Welcome to Chechnya' used a technique called Censor Veil, developed by visual effects supervisor Ryan Laney, to protect the identities of at-risk subjects. This process involved deepfaking realistic decoy faces over the film's subjects by combining an autoencoder with conventional visual effects. U.S. LGBTQ activists volunteered to have their faces stand in for those of the interviewees, and their images were captured using an array of nine cameras.","-----------------
Title: News
Content: Dear friends, There’s a lot we don’t know about the future: When will a Covid-19 vaccine be available? Who will win the next election? Or in a business context, how many customers will we have next year? With so many changes going on in the world, many people are feeling stressed about the future. I have a practice that helps me regain a sense of control. Faced with uncertainty, I try to: Make a list of plausible scenarios, acknowledging that I don’t know which will come to pass.Create a plan of action for each scenario.Start executing actions that seem reasonable.Review scenarios and plans periodically as the future comes into focus. For example, back in March, I did this scenario planning exercise. I imagined quick (three months), medium (one year), and slow (two years) recoveries from Covid-19 and made plans for managing each case. These plans have helped me prioritize where I can. The same method can apply to personal life, too. If you’re not sure you’ll pass an exam, get a job offer, or be granted a visa — all of which can be stressful — you can write out what you’d do in each of the likely scenarios. Thinking through the possibilities and following through on plans can help you navigate the future effectively no matter what it brings. Bonus: With a training in AI and statistics, you can calculate a probability to each scenario. I’m a fan of the Superforecasting methodology, in which the judgements of many experts are synthesized into a probability estimate. I refer to this site as a source of probability estimates as well. There will always be uncertainty, but with a little discipline, imagination, and foresight, we can still move forward with confidence. Keep learning! Documentary filmmakers often shield the identities of people who might be harmed for speaking out. But typical tactics like blurring faces and distorting voices can make it hard for audiences to connect emotionally. A new documentary uses deepfakes to protect the privacy of at-risk subjects.What’s new: The makers of the HBO documentary “Welcome to Chechnya” deepfaked faces of gay men and women fleeing the Russian republic of Chechnya, where LGBTQ people are being persecuted, the New York Times reported. How it works: Visual effects supervisor Ryan Laney developed the process, which he calls Censor Veil, to paint a realistic decoy face over each of the film’s 23 subjects. The process combines an autoencoder with conventional visual effects, Laney told The Batch.U.S. LGBTQ activists volunteered to have their faces stand in for those of interviewees. Their images were captured using an array of nine cameras.The filmmakers blurred the faces deliberately to signal to the audience that identities were hidden. What they’re saying: “This technology allowed us to just stretch the faces . . . over the images that I shot in the film. The face moves exactly the same way. It smiles, it cries in exactly the same way, but it is somebody else’s face.” — David France, director of “Welcome to Chechnya,” in Variety. Behind the news: An estimated 40,000 gay men and women live in Chechnya. They are at risk of arrest, torture, and detention in secret camps. Many have been killed.Why it matters: This technique provides a new way for journalists to preserve the impact of credible witnesses while protecting their privacy.We’re thinking: Deepfakes are infamous for their potential to propagate mistaken identities. This work (and similar initiatives like the BLM Privacy Bot) demonstrates that swapping one person’s face for another’s can have a socially beneficial use.

Image to article:
- Caption to image: Excerpts of HBO documentary ""Welcome to Chechnya""
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2026.gif
-----------------"
What is the primary purpose of the U.S. Army's Next Generation Combat Vehicles program?,The primary purpose is to develop autonomous vehicles that can replace soldiers in high-risk situations on the battlefield.,"The primary purpose of the U.S. Army's Next Generation Combat Vehicles program is to develop autonomous vehicles that can replace soldiers in high-risk battlefield situations. These vehicles are intended to act as decoys, infiltrate enemy lines, call in artillery strikes, gather information, screen for obstacles, and operate in areas affected by nuclear, chemical, or biological weapons.","-----------------
Title: Robot Tanks on the March
Content: A new generation of battlebots is gaining momentum.What’s new: The Army is at least two years ahead of schedule in its plan to deploy self-driving (and self-aiming) transports, jeeps, and tanks, said Richard Ross Coffman, director of Next Generation Combat Vehicles, in an interview with Breaking Defense. Rolling thunder: The NGCV program features three phases of testing for vehicles of graduated firepower and autonomy. In Phase One, beginning in early 2020, the Army will test autonomous-driving hard- and software on Vietnam-era armored transports. The vehicles will be remote-controlled by soldiers in M2 Bradley transports.In 2021, Phase Two will test the same hardware on custom-built vehicles. Half of these four-wheeled prototypes will be lightweight models (less than 10 tons) and carry machine guns and anti-tank missiles. The other half will be medium-weight (up to 12 tons) and able to carry bigger weaponry.Heavyweight autonomous tanks weighing up to 20 tons and mounted with 120mm cannons will roll out in 2023 for Phase Three.Coffman envisions systems that enable one human to control a dozen tanks in 2035 or later. Even then, a flesh-and-blood soldier will oversee firing. Behind the news: The U.S. Army has spent billions on robotic fighting machines that never came to fruition. In 2009, the service cancelled a previous autonomous war-fighting effort, the $20 billion Future Combat Systems program, after six years in development. That program was nixed partly because the technology didn’t progress as quickly as expected and partly due to a shift from warfare to counterterrorism.Why it matters: Robot vehicles could act as decoys, drawing fire meant for human troops. They could also infiltrate enemy lines and call in artillery strikes, gather information, screen for obstacles, and wade into areas affected by nuclear, chemical, or biological weapons.What they’re saying: “Anywhere that a soldier is at the highest risk on the battlefield, and we can replace him or her with a robot, that’s what we want to do.” — Richard Ross Coffman, Director, Next Generation Combat Vehicles, U.S. Army.We’re thinking: How is the Army, which must cope with irregular terrain, intermittent explosions, and the fog of war, ahead of schedule when the automotive industry, navigating smooth surfaces and relatively orderly traffic, has fallen behind its initial projections? The military faces very different problems, some harder to navigate than urban environments, some easier. Its emphasis on remote control also could make a significant difference. Bottom line: Like many people, we’re unsettled by the combination of AI and fighting machines. We strongly support proposals for an international ban on autonomous weapons.

Image to article:
- Caption to image: Military tank
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/tanks20SIZED.gif
-----------------"
What role did AI play in the development and detection of solutions during the coronavirus pandemic?,"AI accelerated the search for a coronavirus vaccine, detected Covid-19 cases, and helped in various areas such as compliance monitoring and drug discovery.","AI played a significant role in accelerating the search for a coronavirus vaccine, detecting Covid-19 cases, and mitigating the pandemic's impact. Machine learning was used to optimize mRNA sequences for vaccines, as seen with Moderna's vaccine. AI models were developed to detect Covid-19 in CT scans with high accuracy and were deployed in multiple countries. Additionally, AI was used to analyze news reports for early pandemic detection, evaluate compliance with health regulations, and guide relief fund distribution. Despite some challenges, AI contributed to rapid vaccine development and improved pandemic response.","-----------------
Title: Coping With Covid
Content: AI accelerated the search for a coronavirus vaccine, detected Covid-19 cases, and otherwise softened the pandemic’s blow.What happened: Machine learning researchers worldwide scrambled to harness the technology against the coronavirus. Among many misfires, they racked up important successes in detection, inoculation, other areas.Driving the story: The pandemic began with high hopes for AI-driven solutions among researchers and officials. But an April metastudy sounded a cautious note, finding that 145 models surveyed were poorly documented, yielded overly optimistic results, and were likely to be biased. Researchers persisted, ultimately delivering vaccines in record time. Outside the lab, deep learning teams tried to keep people safer and more connected. BlueDot, which analyzes news reports for significant events, detected the nascent pandemic several days ahead of the global health monitors and sent an early warning to its customers.The cities of Paris and Cannes evaluated compliance with masking regulations using computer vision in transit stations, buses, and markets. The government of Togo trained a model to identify regions of extreme poverty in satellite imagery. It used the output to guide distribution of relief funds to those most in need.Chatbots provided the locked-down and lonely with synthetic friends to chat and flirt with. For people working from home, videoconferencing companies trained models to filter background noises and virtually transform pajamas into business attire.A collaboration among many institutions in China developed a model that detects Covid-19 in CT scans with better than 90 percent accuracy. The model has been deployed in seven countries and the code has been downloaded 3 million times so far.Moderna, a U.S. biotech company whose vaccine was approved by the U.S. Food and Drug Administration in December, used machine learning to optimize mRNA sequences for conversion into molecules that could be tested. Behind the news: AI may yet play an important role in treating Covid-19. The nonprofit Covid Moonshot project used a semisupervised deep learning platform to filter 14,000 candidate antiviral drugs. The system validated four compounds that are expected to advance to animal trials.Where things stand: AI is no silver bullet, but the advent of this new, virulent, highly infectious strain of coronavirus has been a bracing test run of its capabilities to fight infectious diseases — and helped us live with them, too. Learn more: The Batch featured regular AI-Against-Covid news reports starting in April.

Image to article:
- Caption to image: Two reindeers with masks on a snowy night
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_ReindeerMedicine_576x324.jpg
-----------------"
What is the primary purpose of the Multiple-Instance Contrastive Learning (MICLe) method developed by Shekoofeh Azizi and colleagues?,The primary purpose of MICLe is to enhance unsupervised pretraining by using different perspectives of the same patient to improve the model's performance in diagnosing skin conditions.,"The primary purpose of the Multiple-Instance Contrastive Learning (MICLe) method developed by Shekoofeh Azizi and colleagues is to enhance unsupervised pretraining by using different perspectives of the same patient. This approach helps the model produce similar representations of an illness regardless of the camera’s viewpoint, thereby improving the model's accuracy in diagnosing skin conditions.","-----------------
Title: Same Patient, Different Views
Content: When you lack labeled training data, pretraining a model on unlabeled data can compensate. New research pretrained a model three times to boost performance on a medical imaging task.What’s new: Shekoofeh Azizi and colleagues at Google developed Multiple-Instance Contrastive Learning (MICLe), a training step that uses different perspectives of the same patient to enhance unsupervised pretraining.Key insight: Presented with similar images, a model trained via contrastive learning produces representations that are nearby in vector space. Training via contrastive learning on images of the same patient taken from various angles can produce similar representations of an illness regardless of the camera’s viewpoint.How it works: The authors started with a ResNet-50 (4x) pretrained on ImageNet. They added contrastive pretraining steps and fine-tuning to diagnose 26 skin conditions from acne to melanoma. The training data was a private set of 454,295 images that included multiple shots of the same patients. To refine the general representations learned from ImageNet for medical images, the authors pretrained the model according to SimCLR, an earlier contrastive learning technique. The model regarded augmented versions of the same parent image as similar and augmented versions of different images as dissimilar.To sharpen the representations for changes in viewpoint, lighting, and other variables, they further pretrained the model on multiple shots of 12,306 patients. In this step — called MICLe — the model regarded randomly cropped images of the same patient as similar and randomly cropped images of different patients as dissimilar.To focus the representations for classifying skin conditions, they fine-tuned the model on the images used in the previous step. Results: The authors compared the performance of identical ResNet-50s pretrained and fine-tuned with and without MICLe. The authors’ method boosted the model’s accuracy by 1.18 percent to 68.81 percent, versus 67.63 percent without it. Why it matters: A model intended to diagnose skin conditions no matter where they appear on the body may not have enough data to gain that skill through typical supervised learning methods. This work shows that the same learning can be accomplished using relatively little data through judicious unsupervised pretraining and contrastive losses.We’re thinking: The combination of SimCLR and MICLe is a study in contrasts.

Image to article:
- Caption to image: Sequence showing a training step that uses different perspectives of the same patient to enhance unsupervised pretraining
- URL to image: https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-08T122032.068-1.gif?upscale=true&name=ezgif.com-gif-maker%20-%202021-03-08T122032.068-1.gif
-----------------"
What reason did OpenAI cofounder Wojciech Zaremba give for disbanding the robotics program?,A lack of data was holding back OpenAI’s progress in robotics.,"OpenAI cofounder Wojciech Zaremba stated that a lack of data was holding back OpenAI’s progress in robotics, which led to the disbanding of the robotics program.","-----------------
Title: Bye Bye Bots
Content: The independent research lab OpenAI wowed technology watchers in 2019 with a robotic hand that solved Rubik’s Cube. Now it has disbanded the team that built it. What’s new: OpenAI cofounder Wojciech Zaremba revealed that OpenAI shuttered its robotics program last October. Robo retrenchment: In a podcast produced by Weights & Biases, a maker of AI development tools, Zaremba said a lack of data was holding back OpenAI’s progress in robotics. The company’s broad goal is to develop artificial general intelligence, and it believes it can make more progress by focusing on approaches such as reinforcement learning with human feedback, a representative told VentureBeat. Behind the news: OpenAI previously developed a robotics simulation environment, a reinforcement learning toolkit, and techniques for training robots. Why it matters: The robotics industry has seen several high-profile players struggle with the high cost of research and development. In recent years, Honda shuttered its Asimo subsidiary, Rethink Robotics closed up shop, and Boston Robotics, famous for its acrobatic bipeds and resilient quadrupeds, repeatedly changed hands. We’re thinking: When even a fleet of robots isn’t able to generate enough data, that’s a sign of how data-hungry our algorithms are. It’s also a reminder of how far the current state of the art is from human-level AI. After all, infants have only one body’s worth of data to learn from.

Image to article:
- Caption to image: Forbidden sign over a robot's hand solving a Rubik's Cube
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/07/openai.gif
-----------------"
What was the main controversy surrounding the Pulse image upsampling model developed by researchers at Duke University?,"The controversy was that Pulse transformed images of dark-skinned individuals into images of white individuals, highlighting bias in the machine learning model.","The main controversy surrounding the Pulse image upsampling model developed by researchers at Duke University was its tendency to transform pixelated portraits of nonwhite individuals, such as Barack Obama, into detailed pictures of white individuals. This sparked a debate about racial bias in AI, as the model's training data was racially imbalanced, leading to biased outputs.","-----------------
Title: News
Content: Dear friends, We know that biased data leads to biased machine learning. But does the problem go beyond that? A few colleagues asked about this after a heated exchange on Twitter between Yann LeCun and Timnit Gebru (see “Image Resolution in Black and White” below). There are plenty of documented examples of biased data contributing to bad outcomes. But suppose we find purely unbiased data and build an AI system that helps lenders optimize interest rates for payday loans. We’re careful to make sure the data, algorithms, and learned models don’t discriminate unfairly against any disadvantaged or minority group. Our results are unbiased and in the clear, right? Unfortunately, no. Payday loans are quick-turnaround loans often with very high interest rates — in California, a lender can charge 459 percent interest on a $100, 14-day loan. They target low income individuals. In the U.S., they’re used disproportionately by the Black community. Thus even a fair algorithm will hurt this community especially. Beyond biased data, the way we frame problems, choose what to build, and choose where to deploy can add to or subtract from problems of bias and privilege. An “unbiased” AI technology operating in an unfair social system can contribute to biased outcomes. We still have a lot of work ahead to address harmful biases throughout society. Twenty years ago, the AI community was a small group working on an exciting but obscure technology. Today our community is large, worldwide, and rapidly growing, and we contribute to applications at the center of daily life. We have a greater responsibility than ever to educate ourselves not only in the technology but also in its social context. It’s not always easy to foresee the indirect impact of our work. Who would have guessed that a poorly designed software implementation to enable freedom of speech would lead to toxic communications on social media? But with a broader perspective, I hope our community can better understand the impact of our work and make better decisions about how to help society move forward with greater fairness and less bias. Keep learning! A new model designed to sharpen images tends to turn some dark faces white, igniting fresh furor over bias in machine learning.What’s new: Built by researchers at Duke University, Photo Upsampling via Latent Space Exploration (Pulse) generates high-resolution versions of low-resolution images. It sparked controversy when it transformed a pixelated portrait of Barack Obama into a detailed picture of a white man.How it works: Most upsampling models are trained to generate high-res output from low-res input. Pulse creates a series of high-res images progressively optimized to match the low-res source. Pulse uses StyleGAN, a pre-trained generative adversarial network, to generate a new image from the original and an input vector.The system downsamples the generated image to the original’s resolution. Then it compares the two.It modifies the input vector based on the differences and repeats the process 100 times to arrive at its final output.Human judges scored Pulse’s output more realistic than that of four competing models. The computer-based assessment Natural Image Quality Evaluator (NIQE) rated Pulse’s images higher than those in a database of high-res celebrity portraits. The controversy: Twitter user Chicken3gg revealed Pulse’s bias using a downsampled photo of the former U.S. president. That prompted machine learning engineer Robert Osazuwa Ness to try it on blurred images of U.S. Senator Kamala Harris, actress Lucy Liu, and other nonwhite individuals. The system whitewashed them, too, and also interpreted some female faces as male. The incident triggered an online debate that drew in major figures in the AI community.Pulse’s authors blamed racially imbalanced training data. When they trained their system on a more diverse dataset, its accuracy with respect to race ranged from 79 percent to 90 percent.Facebook’s AI chief Yann LeCun echoed the notion that the system’s bias resulted from racially lopsided training data. Timnit Gebru, co-leader of Google’s ethical AI team, shot back that focusing on data alone downplays systemic bias in the machine learning community. As the argument grew heated, LeCun announced his withdrawal from Twitter. Why it matters: Flawed AI leads to real harm. In January, police in Detroit arrested an African-American man for theft after he was misidentified by a face recognition system. Such systems have been shown to misclassify Black people. We’re thinking: Upscaling powered by machine learning is making images sharper on televisions and microscopes. The AI community has a pressing need for tests and audit procedures to ensure that such technology is trustworthy and free of bias.

Image to article:
- Caption to image: Examples of high-resolution versions of low-resolution images.
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Obama203.gif
-----------------"
What is the main advantage of YOLOv4 compared to its predecessor and other high-accuracy competitors?,YOLOv4 boosts accuracy without sacrificing speed.,"The main advantage of YOLOv4 compared to its predecessor and other high-accuracy competitors is its ability to boost accuracy without sacrificing speed. YOLOv4 achieves higher average precision while maintaining real-time frame rates, outperforming both its predecessor and competitors like EfficientDet in terms of speed and accuracy balance.","-----------------
Title: Another Look at YOLO
Content: The latest update of the acclaimed real-time object detector You Only Look Once is more accurate than ever.What’s new: Alexey Bochovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao at Taiwan’s Institute of Information Science Academia Sinica offer YOLOv4 — the first version not to include the architecture’s original creators.Key insight: Rapid inference is YOLO’s claim to fame. The authors prioritized newer techniques that improve accuracy without impinging on speed (their so-called “bag of freebies”). In addition, they made improvements that boost accuracy at a minimal cost to speed (the “bag of specials”). All told, these tweaks enable the new version to outperform both its predecessor and high-accuracy competitors running at real-time frame rates.How it works: YOLO, as well as most object detectors since, tack a model that predicts bounding boxes and classes onto a pre-trained ImageNet feature extractor. Techniques under the heading “bag of freebies” boost accuracy by adding computation during training. These include alternate bounding box loss functions, data augmentation, and decreasing the model’s confidence for ambiguous classes.The authors introduce new data augmentation techniques such as Mosaic, which mixes elements drawn from four training images to place objects in novel contexts.“Bag of specials” techniques include the choice of activation function: ReLU variants are marginally slower, but they can yield better accuracy.The authors accommodate users with limited hardware resources by choosing techniques that allow training on a single, reasonably affordable GPU. Results: The authors pitted YOLOv4 against other object detectors that process at least 30 frames per second, using the COCO image dataset. YOLOv4 achieved 0.435 average precision (AP), running at 62 frames per second (FPS). It achieved 0.41 AP at its maximum rate of 96 FPS. The previous state of the art, EfficientDet, achieved 0.43 AP running at nearly 42 FPS and 0.333 AP at its top speed of 62 FPS.Why it matters: YOLOv4 locates and classifies objects faster than measurements of human performance. While it’s not as accurate as slower networks such as EfficientDet, the new version boosts accuracy without sacrificing speed.We’re thinking: You only look once . . . twice . . . thrice . . . four times and counting!

Image to article:
- Caption to image: Data related to YOLOv4
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/YOLO.gif
-----------------"
How much funding has the U.S. National Science Foundation committed to the new National Artificial Intelligence Research Institutes?,$220 million,The U.S. National Science Foundation has committed $220 million to fund 11 National Artificial Intelligence Research Institutes.,"-----------------
Title: Fresh Funds for U.S. Research
Content: The U.S. plans to build nearly a dozen new civilian AI research labs.What’s new: The U.S. National Science Foundation (NSF) committed $220 million to fund 11 National Artificial Intelligence Research Institutes, complementing seven other AI research institutes that were established last year.What’s happening: The NSF grants provide each institute about $20 million annually over five years. Some will receive additional funding from public and private partners such as the U.S. Department of Homeland Security, Amazon, and Intel. Their missions include: Agriculture: Several institutes will focus on improving aspects of farming, including adapting to climate change, modeling plants, and developing security infrastructure for initiatives such as precision agriculture.Industry: Others will look at challenges like improving semiconductor design and building better robots. Two institutes will specialize in edge devices and wireless AI systems. Another is devoted to using AI models to optimize systems and automated decision-making.Scientific research: One institute will focus on algorithms and applications to control complex dynamic systems.Social good: A trio will seek to improve human life at various stages. One will focus on education for children, another on training for adults. A third institute will develop systems to care for elderly people. Behind the news: The NSF funded an initial seven national AI institutes in September. Earlier, the U.S. had said it would spend $2 billion annually on AI over the next two years.Why it matters: Other governments spend much more on AI than the U.S., and this outlay is small in the scheme of national AI funding. However, the allocation and the goals to which it is being put suggest that the federal government recognizes AI’s importance to the U.S. economy and its potential to benefit the world at large.We’re thinking: U.S. government funding was critical to AI's rise. For example, the Defense Advanced Research Products Agency (DARPA) provided funds to both Andrew and Yann LeCun for deep learning research. We’re hopeful that these new programs will fund similarly valuable innovations.

Image to article:
- Caption to image: Series of images showing AI research labs' campuses
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/08/nsf-2.gif
-----------------"
What is the main feature of IBM's Granite Speech 3.3 8B model?,"It offers superior transcription accuracy and can process audio files of arbitrary length, handling files of up to twenty minutes.","IBM's Granite Speech 3.3 8B model is a compact open-weights speech-to-text model that offers superior transcription accuracy compared to top competitors. It processes both audio and text inputs, providing automatic speech recognition and translation from English to seven languages, including French, Spanish, German, and Mandarin. Unlike conventional speech models, it can handle audio files of arbitrary length, processing files of up to twenty minutes.","-----------------
Title: Data Points: Wikimedia wants to help build AI for the commons
Content: In today’s edition, you’ll find: Gemini 2.5 Flash blends speed with budgeted reasoningIBM’s Granite Speech sets SOTA in transcription accuracyRecall will soon return to Copilot Plus PCsOpenAI will shelve its biggest, costliest model Wikimedia releases free-to-use Wikipedia dataset on Kaggle Wikimedia Enterprise created a dataset designed specifically for machine learning applications that provides structured Wikipedia content in English and French. The dataset offers pre-parsed article data in JSON format, eliminating the need for developers to scrape or parse raw text when building models or testing language processing pipelines. The beta release, available now on Kaggle, includes valuable content elements like abstracts, short descriptions, infobox data, image links, and segmented article sections, all freely licensed under Creative Commons Attribution-Share-Alike 4.0 and GNU Free Documentation License. The release comes shortly after the organization revealed that Wikipedia’s hosting costs had risen sharply due to AI bots scraping its websites without permission. (Wikimedia) OpenAI unveils smarter reasoning models with tool use OpenAI released o3 and o4-mini, new reasoning models that can use every tool in ChatGPT’s arsenal, from web search to coding to image generation. The models show strong improvements over previous versions, with o3 setting new benchmarks in coding and math while making 20 percent fewer major errors than o1 on complex tasks. o4-mini achieves remarkable performance for its size, particularly in competition math where it scored 99.5 percent pass@1 on AIME 2025 when given access to Python. Both models are available now to ChatGPT Plus, Pro, and Team users, with Enterprise and Edu access coming next week. In the API, o4-mini costs $1.10/$4.40 per million tokens of input/output, while o3 costs $10/$40. (OpenAI) Google previews Gemini 2.5 Flash, a fast multimodal model with controllable reasoning capabilities Google launched an early preview of Gemini 2.5 Flash, the company’s first “hybrid” reasoning model where developers can toggle “thinking” on or off. Developers can set specific thinking budgets to balance quality, cost, and latency, with the model automatically determining how much reasoning to apply based on task complexity. The model performs strongly on complex reasoning tasks, ranking second only to Gemini 2.5 Pro on Hard Prompts in LMArena, but maintains what Google claims is the best price-to-performance ratio among comparable models. Gemini 2.5 Flash is currently available for free through the Gemini API, available in Google AI Studio and Vertex AI, with final pricing to be announced on its full release. (Google) Granite Speech 3.3 8B is IBM’s first audio-input model IBM released Granite Speech 3.3 8B, a compact open-weights speech-to-text model offering superior transcription accuracy compared to top competitors. The model processes both audio and text inputs, providing automatic speech recognition and translation from English to seven languages including French, Spanish, German, and Mandarin. Unlike Whisper and other conventional speech models, which are limited to 30-second windows, Granite Speech can handle audio files of arbitrary length, processing files of up to twenty minutes (although IBM still recommends one-minute chunks for superior accuracy). IBM plans improvements for future versions, including multilingual encoding, emotion detection, and speech-enabled multimodal models. (IBM) Microsoft rolls out Recall feature to Windows Insiders Microsoft began gradually rolling out its Recall feature in the Release Preview channel, signaling the feature will soon be widely available. Recall captures screenshots of user activity on Copilot Plus PCs, allowing users to search and find past content. The feature faced multiple delays since June 2023 due to security concerns. Microsoft emphasizes that Recall requires explicit opt-in from users, allows pausing snapshot collection at any time, and will only be available on Copilot Plus PCs. In earlier testing phases, reviewers described the feature as “creepy, clever, and compelling.” (Microsoft and The Verge) OpenAI to discontinue GPT-4.5 API access OpenAI announced it will end API access to GPT-4.5, its largest AI model to date, on July 14, just months after its February release. The company recommends that developers transition to the newly launched GPT-4.1, which OpenAI claims offers “similar or improved performance [to] GPT-4.5 in key areas at a much lower cost.” While GPT-4.5 will remain available in ChatGPT for paying customers, its high operational costs likely influenced the decision to remove it from the API. The model, code-named Orion, was trained with unprecedented computing resources but falls short of “frontier model” status on several industry benchmarks, despite improvements in writing and persuasiveness over GPT-4o. (TechCrunch) Still want to know more about what matters in AI right now? Read this week’s issue of The Batch for in-depth analysis of news and research. This week, Andrew Ng shared why teams should start building evaluations early — even if they’re quick and imperfect — and improve them over time to accelerate GenAI development. “I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time.” Read Andrew’s full letter here. Other top AI news and research stories we covered in depth: Google unveiled Gemini 2.5 Pro Experimental, which outperforms top AI models and continues the rapid evolution of its flagship model family; Model Context Protocol (MCP), an open standard for tool use and data access, gained traction as OpenAI adopted it to improve LLM integration with external tools and APIs; a book excerpt explored Sam Altman’s brief ouster and return to OpenAI, shedding light on the company’s internal power struggles; and researchers introduced a new byte-based model that surpasses Llama 3 and other token-based models on tasks involving misspellings, noisy input, and translation. Subscribe to Data Points

Image to article:
- Caption to image: People using laptops to access Wikipedia in a crowded library setting.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Fimage--28-.png&w=3840&q=75
-----------------"
"What is the main focus of the research conducted by psychologists at DeepMind, Stanford, and University College London regarding deep learning agents?",The research focuses on exploring deep learning agents' ability to perform systematic reasoning and generalize their understanding of tasks.,Not found in context,"-----------------
Title: Simulation Substitutes for Data
Content: The future of machine learning may depend less on amassing ground-truth data than simulating the environment in which a model will operate.What happened: Deep learning works like magic with enough high-quality data. When examples are scarce, though, researchers are using simulation to fill the gap. Driving the story: In 2019, models trained in simulated environments accomplished feats more complex and varied than previous work in that area. In reinforcement learning, DeepMind’s AlphaStar achieved Grandmaster status in the complex strategy game StarCraft II — able to beat 99.8 percent of human players — through tens of thousands of virtual years competing in a virtual league. OpenAI Five similarly trained a team of five neural nets to best world champions of Dota 2. But those models learned in a virtual world to act in a virtual world. Other researchers transferred skills learned in simulations to the real world. OpenAI’s Dactyl robot hand spent the simulated equivalent of 13,000 years in virtual reality developing the dexterity required to manipulate a Rubik’s Cube puzzle. Then it applied those skills to a physical cube. It was able to solve the puzzle in 60 percent of tries when unscrambling the colored sides required 15 or fewer twists of the cube. Its success rate dropped to 20 percent when solving the puzzle required more moves.Researchers at CalTech trained a recurrent neural network to differentiate overlapping and simultaneous earthquakes by simulating seismic waves rippling across California and Japan and using the simulations as training data.Amazon’s Aurora self-driving vehicle unit runs hundreds of simulations in parallel to train its models to navigate urban environments. The company is training Alexa’s conversational faculties, delivery drones, robots for its fulfillment centers in a similar way. Where things stand: Simulation environments like Facebook’s AI Habitat, Google’s Behavior Suite for Reinforcement Learning and OpenAI’s Gym offer resources for mastering tasks like optimizing textile production lines, filling in blank spots in 3D imagery, and detecting objects in noisy environments. On the horizon, models could explore molecular simulations to learn how to design drugs with desired outcomes.

Image to article:
- Caption to image: Illustration of a crystal snowball
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/RL.png
-----------------"
What system did researchers develop to reconstruct images based on brain activity while watching Doctor Who?,Brain2Pix,"Researchers developed Brain2Pix, a system that reconstructs what people saw from scans of their brain activity while watching Doctor Who.","-----------------
Title: Deciphering The Brain’s Visual Signals
Content: What’s creepier than images from the sci-fi TV series Doctor Who? Images generated by a network designed to visualize what goes on in peoples’ brains while they watch Doctor Who.What’s new: Lynn Le, Luca Ambrogioni, and colleagues at Radboud University and Max Planck Institute for Human and Brain Cognitive Sciences developed Brain2Pix, a system that reconstructs what people saw from scans of their brain activity.Key insight: The brain uses neurons nearby one another to represent visual features nearby one another. Convolutional neural networks excel at finding and using spatial patterns to perform tasks such as image generation. Thus, a convolutional neural network can use the spatial relationships between active neurons in a brain scan to reconstruct the corresponding visual image. How it works: The authors used a picture-to-picture generative adversarial network (GAN) to try to produce an image of what a person was looking at based on functional magnetic resonance imaging (fMRI): 3D scans that depict blood oxygenation in the brain, which indicates neuron activity. They trained the GAN on Doctor Who fMRI, a collection of video frames from 30 episodes of Doctor Who and corresponding fMRIs captured as an individual watched the show. The authors converted each 3D scan into 2D images, each of which represented distinct sections of the brain, using a neuroscientific device known as a receptive field estimator .They trained the GAN’s discriminator to classify whether an image came from Doctor Who or the GAN’s generator. They trained the generator with a loss function that encouraged it to translate the 2D images of neuron activity into an image that would fool the discriminator.The generator used two additional loss terms. The first term aimed to minimize the difference between the pixel values of a video frame and its generated counterpart. The second term aimed to minimize the difference between representations, extracted by a pretrained VGG-16, of a video frame and its generated counterpart.The generator used a convolutional architecture inspired by U-Net in which residual connections passed the first layer’s output to the last layer, second layer’s output to the penultimate layer, and so on. This arrangement helped later layers in the network to preserve spatial patterns in the brain scans. Results: The researchers used an AlexNet to extract representations of Brain2Pix images and Doctor Who frames and compared the distance between them. Brain2Pix achieved an average distance of 4.6252, an improvement over the previous state-of-the-art method’s average of 5.3511.Why it matters: The previous state-of-the-art used 3D convolutions directly on the raw fMRIs, yet the new approach fared better. For some problems, engineering features — in this case, converting fMRIs into 2D — may be the best way to improve performance. We’re thinking: We wouldn’t mind sitting in an fMRI machine for hours on end if we were binge-watching Doctor Who.

Image to article:
- Caption to image: Images generated by a network designed to visualize what goes on in peoples’ brains while they watch Doctor Who
- URL to image: https://dl-staging-website.ghost.io/content/images/2022/10/What-the-brain-sees-1.gif
-----------------"
What algorithm did Loon use to develop its autonomous control system for navigating balloons?,"QR-DQN, a distributional reinforcement learning algorithm.","Loon used QR-DQN, a distributional reinforcement learning algorithm, to develop its autonomous control system for navigating balloons.","-----------------
Title: News
Content: Dear friends, Like many people in the AI community, I am saddened by the sudden departure from Google of ethical AI researcher Timnit Gebru. Timnit is a tireless champion of diversity and fairness in AI. Her work, for example highlighting bias in face recognition systems, has been a productive influence on many researchers and companies. At the same time, my friend Jeff Dean built Google AI into a world-class engineering organization. I’ve seen him speak up for diversity when no one else in the room was doing so. Having not yet spoken to either of them, I hesitate to offer my opinion on the matter at this time. But the situation highlights a larger problem in the AI community: lack of a shared set of values (such as fairness, diversity, and transparency) and norms (such as what to do when there’s a problem). In academia, all scholars place high value on the pursuit and dissemination of knowledge. In medicine, all doctors recognize that the wellbeing of patients is their primary duty. We need that kind universal commitment in AI. We’re building technology that affects billions of people without a coherent set of guiding principles. Many companies and think tanks have published their own codes of ethics, and these statements are important — but they are far from sufficient. We need a set of values and norms that are shared across our entire community and transcend any one company. That way, we can collectively hold individuals, companies, and perhaps even governments accountable to them and operate for the common good even when we disagree. How can we bring the AI community together around shared values and norms? I encourage you to spend time with your teams, collaborators, and peers to discuss this difficult question. It’s past time to lay the foundation for a set of values and norms that all AI practitioners will proudly stand up for. Keep learning! Helium balloons that beam internet service to hard-to-serve areas are using AI to navigate amid high-altitude winds. What’s new: Loon, the Alphabet division that provides wireless internet via polyethylene blimps, used reinforcement learning to develop an autonomous control system that keeps the vehicles closer to their targets while consuming less energy than its hand-coded predecessor. The new algorithm controls Loon’s fleet over Kenya, where the company launched its first commercial service in July. How it works: Balloons navigate by ascending or descending to catch winds that push them in the direction desired. Loon used QR-DQN, a distributional reinforcement learning algorithm, to train a feed-forward network to determine when the balloon should ascend, descend, or stay put. Working with Google AI’s Montreal team, Loon researchers modified a weather dataset from the European Center for Medium-Range Weather Forecasts to generate a large number of wind scenarios. They modeled the physics of balloon flight within these synthesized wind fields to build simulations used to train and evaluate the model.In training, the model received the maximum reward when the balloon was within 50 kilometers of its base station, the range at which it reliably sends and receives signals. The reward halved with every 100 kilometers the balloon strayed.In use, instruments on board feed the model wind readings from the balloon’s current location and wake. It estimates wind conditions at nearby locations using a Gaussian process that analyzes weather readings from nearby balloons and forecasts from the European Center for Medium-Range Weather Forecasts. A pump inflates or deflates the balloon accordingly.In real world tests against the earlier flight control system, the new algorithm stayed on target 7 percent more often while cutting energy consumption by 4 watts day. Behind the news: Loon began within Alphabet’s experimental X division in the early 2010s and became a for-profit subsidiary in 2018. The company provided emergency internet access to Puerto Rico after hurricane Maria in 2017, and to Peru following a massive earthquake in 2019. A single balloon can serve several thousand individuals spread over 80 square kilometers. Why it matters: Billions of people, including two-thirds of all school-age children, don’t have access to the internet. In the Covid era, with students and workers alike staying home, the digital divide is more acute than ever. Cutting the cost of service to remote areas could bring many of those people into the information economy.We’re thinking: In Kenya, where Loon’s first balloons are flying, better connections could boost the growing community of AI engineers. To learn more about Kenya’s AI scene, check out our Working AI profile of data scientist and DeepLearning.AI ambassador Kennedy Kamande Wangari.

Image to article:
- Caption to image: AI-driven balloon reaching high altitude
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/LOON.gif
-----------------"
What is the main argument presented about the importance of AI and data science in relation to traditional software engineering?,"AI and data science offer a new pathway to coding that is even more promising than traditional software engineering, as they enable individuals in various professions to harness data for better decision-making and efficiency.","The main argument is that AI and data science offer a new and promising pathway to coding, which is becoming increasingly important as machines become central to daily life. Unlike traditional software engineering, which involves writing explicit sequences of steps for computers, AI and data science enable computers to extract knowledge from data. This makes AI-oriented coding literacy more valuable, as it allows individuals in various professions to harness data for richer lives. The hope is that society will embrace this new form of literacy, much like it has with reading and writing.","-----------------
Title: DeepLearning.ai Exclusive
Content: Dear friends, Today we take it for granted that many people know how to read and write. Someday, I hope, it will be just as common that people know how to write code. Several hundred years ago, society didn’t view language literacy as a necessary skill. A small number of people learned to read and write, and everyone else let them do the reading and writing. It took centuries for literacy to spread, and now society is far richer for it. Words enable deep human-to-human communication. Code is the deepest form of human-to-machine communication. As machines become more central to daily life, that communication becomes ever more important. Traditional software engineering — writing programs that explicitly tell a computer sequences of steps to execute — has been the main path to code literacy. But AI, machine learning, and data science offer a new paradigm in which computers extract knowledge from data. This technology offers another pathway to coding — one that strikes me as even more promising. Many Sundays, I buy a slice of pizza from my neighborhood pizza parlor. The gentleman behind the counter may have little reason to learn how to build software applications (beyond personal growth and the pleasure of gaining a new skill). But AI and data science have great value even for a pizza maker. A linear regression model might enable him to better estimate demand so he could optimize the restaurant’s staffing and supply chain. He could better predict sales of Hawaiian pizza — my favorite! — so he could make more Hawaiian pies in advance and reduce the amount of time customers had to wait for them. Uses of AI and data science can be found in almost any situation that produces data, and I believe that a wide variety of professions will find more uses for custom AI applications and data-derived insights than for traditional software engineering. This makes literacy in AI-oriented coding even more valuable than traditional skills. It could enable countless individuals to harness data to make their lives richer. I hope the promise of building basic AI applications, even more than that of building basic traditional applications, encourages more people to learn how to code. If society embraces this new form of literacy as it has the ability to read and write, we will all benefit. Keep learning! As a senior machine learning engineer at Retro Rabbit, a software consultancy, Jade Abbott focuses on solving customer problems. On the side, she develops natural language processing models for African languages. Read more

Image to article:
- Caption to image: Screen Shot 2020-09-02 at 11.35.37 AM
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-0220at2011.35.3720AM.png
-----------------"
What is a data-centric approach in AI development aimed at improving model performance on specific subsets of data?,A data-centric approach focuses on systematically engineering the training and test data for problematic slices to enhance model performance without degrading it on other portions.,"A data-centric approach in AI development focuses on the importance of the specific data used to train models. It involves curating and preparing data to improve model performance, particularly on specific subsets of data. This approach acknowledges that current models do not learn efficiently and emphasizes the need for careful data preparation to enhance the model's ability to learn from data.","-----------------
Title: More Learning, Less Data | AI News & Insights
Content: Building a foundation model takes tremendous amounts of data. In the coming year, I hope we’ll enable models to learn more from less data. The AI community has achieved remarkable success by scaling up transformers and datasets. But this approach may be reaching a point of diminishing returns — an increasingly widespread belief among the pretraining community as they try to train next-generation models. In any case, the current approach poses practical problems. Training huge models on huge datasets consumes huge amounts of time and energy, and we’re running out of new sources of data for training large models. The fact is, current models consume much more data than humans require for learning. We’ve known this for a while, but we’ve ignored it due to the amazing effectiveness of scaling. It takes trillions of tokens to train a model but orders of magnitude less for a human to become a reasonably intelligent being. So there’s a difference in sample efficiency between our best models and humans. Human learning shows that there’s a learning algorithm, objective function, architecture, or a combination thereof that can learn more sample-efficiently than current models. One of the keys to solving this problem is enabling models to produce higher-level abstractions and filter out noise. I believe this concept, and thus the general problem of data efficiency, is related to several other current problems in AI: Data curation: We know that the specific data we use to train our models is extremely important. It’s an open secret that most of the work that goes into training foundation models these days is about the data, not the architecture. Why is this? I think it’s related to the fact that our models don’t learn efficiently. We have to do the work ahead of time to prepare the data for a model, which may hinder the core potential of AI as an automatic process for learning from data.Feature engineering: In deep learning, we always move toward more generalized approaches. From the beginning of the deep learning revolution, we’ve progressively removed handcrafted features such as edge detectors in computer vision and n-grams in natural language processing. But that engineering has simply moved to other parts of the pipeline. Tokenization, for instance, involves engineering implicit features. This suggests that there’s still a lot of room to make model architectures that are more data-efficient and more generally able to handle more raw modalities and data streams.Multimodality: The key to training a model to understand a variety of data types together is figuring out the core abstractions in common and relating them to each other. This should enable models to learn from less data by leveraging all the modalities jointly, which is a core goal of multimodal learning. Interpretability and robustness: To determine why a model produced the output it did, it needs to be able to produce higher-level abstractions, and we need to track the way it captures those abstractions. The better a model is at doing this, the more interpretable it should be, the more robust it should be to noise, and likely the less data it should need for learning.Reasoning: Extracting higher-level patterns and abstractions should allow models to reason better over them. Similarly, better reasoning should mean less training data.Democratization: State-of-the-art models are expensive to build, and that includes the cost of collecting and preparing enormous amounts of data. Few players can afford to do it. This makes developments in the field less applicable to domains that lack sufficient data or wealth. Thus more data-efficient models would be more accessible and useful. Considering data efficiency in light of these other problems, I believe they’re all related. It’s not clear which is the cause and which are the effects. If we solve interpretability, the mechanisms we engineer may lead to models that can extract better features and lead to more data-efficient models. Or we may find that greater data efficiency leads to more interpretable models. Either way, data efficiency is fundamentally important, and progress in that area will be an indicator of broader progress in AI. I hope to see major strides in the coming year. Albert Gu is an Assistant Professor of Machine Learning at Carnegie Mellon University and Chief Scientist of Cartesia AI. He appears on Time’s list of the most influential people in AI in 2024.

Image to article:
- Caption to image: ALBERT GU
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--39--1.png&w=3840&q=75
-----------------"
What is the purpose of the Standardized Atari Benchmark for Reinforcement Learning (Saber)?,To provide a consistent basis for comparing the performance of reinforcement learning models in Atari games.,The purpose of the Standardized Atari Benchmark for Reinforcement Learning (Saber) is to provide a consistent basis for comparing AI performance in Atari games by offering a standardized setup. It aims to match human capabilities and includes guidelines that remove limitations on gaming time and specify that models can only receive the game screen as input. This helps to address the variations in reported AI performance and allows for a fair comparison with human scores.,"-----------------
Title: Leveling the Playing Field
Content: Deep reinforcement learning has given machines apparent hegemony in vintage Atari games, but their scores have been hard to compare — with one another or with human performance — because there are no rules governing what machines can and can’t do to win. Researchers aim to change that. What’s new: Most AI research demonstrating superhuman performance in Atari games applies widely varying limits on gameplay, such as how frequently buttons can be pressed. Researchers from MINES ParisTech and Valeo offer a standardized setup: Standardized Atari Benchmark for Reinforcement Learning (Saber). They use it to achieve a new state of the art in around 60 games from Pong to Montezuma’s Revenge.Key Insight: Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde noticed that the reported human world-record scores average 1,000 times higher than the “expert human player” scores given in the first major deep reinforcement learning paper published in late 2013. Analyzing the settings used in deep learning publications since, the team pinpointed seven potential causes for reported variations in performance. How it works: The authors propose a set of guidelines designed to match human capabilities. Their benchmark includes a new metric for evaluating models, since the previous human benchmark misrepresents human capabilities. Saber removes limitations on gaming time — it takes time for human players to rack up a world record! — rather than the few minutes many researchers allow.The benchmark specifies that models can receive only the game screen as input, no further information allowed. For example, they must be able to use all buttons even if some don’t function.The benchmark ranks models on a normalized scale in which 0 represents a score obtained by pressing buttons randomly and 1 is the human world record. Results: The researchers tested a state-of-the-art model, Rainbow-IQN, and achieved an average of only 31% of the best human scores. The model achieved superhuman scores in four of 58 games. Why it matters: Training reinforcement learning models is so laborious that researchers often don’t bother to reproduce previous results to see how their own stack up. Saber finally provides a consistent basis for comparison.We’re thinking: Deep reinforcement learning research is exciting, but a lack of standardized benchmarks has kept the state of the art in a state of ambiguity. Saber signals a new and promising maturity.

Image to article:
- Caption to image: Arcade game
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Atari20Cropped.png
-----------------"
What is one suggested strategy for workers to stay relevant in the face of increasing automation by AI?,Invest in education to keep current and find tasks that put them in a position to supervise automated systems.,"One suggested strategy for workers to stay relevant in the face of increasing automation by AI is to engage in lifelong learning. This involves staying ahead of partial automation in one's current profession or changing careers if necessary. Utilizing networked resources like blogs, research papers, online videos, and online courses can help workers develop human insights that are likely to outpace machines for some time. Additionally, workers should aim to work with machines, not against them, by investing in education to keep current and finding tasks that allow them to supervise automated systems.","-----------------
Title: Machines Take Everyone’s Job
Content: From blue collar laborers to lab coated professionals, is any job safe from AI?The fear: AI will exceed human performance at a wide range of activities. Huge populations will become jobless. They’ll be unable to afford life’s necessities, and even government assistance won’t replace the sense of identity, pride, and direction that come with a job. Humanity will become unmoored. What could go wrong: Historically, technology created more jobs than it destroyed. What makes AI different is it threatens to outsource the one thing humans have always relied on for employment: their brains. Automated drive-through windows sell milkshakes. Healthcare models interpret x-rays. Natural language programs write sports news. The list is bound to grow longer as the technology becomes more capable. Behind the fear: Massive unemployment in the past have brought severe social disruption. The U.S. Great Depression in the 1930s saw jobless rates above 34 percent. Researchers have also linked this displacement of work to the rise of nationalism that fueled both the First and Second World Wars. How scared should you be? There’s little reason to worry in the short term. A 2017 report by McKinsey estimated that automation would replace fewer than 5 percent of the global workforce by 2030. That number comes with caveats, though. In some roles, for instance customer service and repetitive physical labor, one-third of all jobs could be taken by machines. Developing nations will be hit hardest, even though they may also experience explosive growth in high-touch fields such as education and healthcare.What to do: Lifelong learning is a front-line defense (and a rewarding pursuit!). Education can help you stay ahead of partial automation in your current profession or change lanes if your profession is being automated away. Networked resources like blogs, research papers, online videos, and online courses can help you absorb and develop the kinds of human insights that likely will outpace machines for some time. Beyond that, work with the machines, not against them, argue Andrew McAfee and Erik Brynjolfsson in their book Race Against the Machine. Workers who don’t want to wind up on the chopping block should invest in education to keep current and find tasks that put them in a position to supervise automated systems.

Image to article:
- Caption to image: Illustration of vending machine with candy and the text ""Say ""trick or treat""""
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Workforce20Displacement-1.png
-----------------"
What percentage of respondents reported that their company's executives had a poor or partial understanding of AI ethics?,Over 60 percent,Over 60 percent of respondents reported that their company’s executives had a poor or partial understanding of AI ethics.,"-----------------
Title: Irresponsible AI
Content: Few companies that use AI understand the ethical issues it raises. What’s new: While many companies are ramping up investments in AI, few look for and correct social biases in their models, according to a report by the credit-scoring company Fico. The report surveyed 100 C-level executives in data, analytics, and AI departments at companies that bring in revenue of $100 million or more annually. What they found: Nearly half of respondents said their company’s investment in AI had grown in the last 12 months. But there was no corresponding rise in efforts to make sure AI was ethical, responsible, and free of bias. Over 60 percent of respondents reported that their company’s executives had a poor or partial understanding of AI ethics. Even higher percentages found limited understanding among customers, board members, and shareholders.21 percent had prioritized AI ethics in the past year. Another 30 percent said they would do so this year. Still, 73 percent reported difficulty getting buy-in from colleagues on ethical AI goals.There is little consensus on corporate responsibility with regard to AI. Some respondents said they had no responsibility beyond legal and regulatory compliance, while others supported standards of fairness and transparency.Around half said they evaluated data and models for bias. 11 percent hired outside evaluators to test models for bias.51 percent of respondents did not monitor models after deployment. Behind the news: This is Fico’s second annual report, and it shows some improvement over the previous survey: Last year, 67 percent of respondents said they did not monitor systems after deployment. Why it matters: Never mind technical issues — taking the survey’s results at face value, a substantial percentage of large companies aren’t ready for AI transformation on an ethical level. Businesses that pursue AI without paying attention to ethical pitfalls run the risk of alienating customers and violating laws. We’re thinking: Companies that pay attention to ethics — in AI and elsewhere — will reap rewards in the form of better products, happier customers, and greater fairness and justice in the world.

Image to article:
- Caption to image: Charts and graphs showing relevant information regarding ethics in AI
- URL to image: https://info.deeplearning.ai/hs-fs/hubfs/fico-redo-2.gif?width=1200&upscale=true&name=fico-redo-2.gif
-----------------"
What machine learning models did researchers at Northern Arizona University use to identify styles of Native American painting on ceramic fragments?,They used an ensemble of VGG16 and ResNet50 convolutional neural networks.,Researchers at Northern Arizona University used an ensemble of VGG16 and ResNet50 convolutional neural networks pretrained on ImageNet to identify styles of Native American painting on ceramic fragments.,"-----------------
Title: Sorting Shattered Traditions
Content: Computer vision is probing the history of ancient pottery. What’s new: Researchers at Northern Arizona University developed a machine learning model that identifies different styles of Native American painting on ceramic fragments and sorts the shards by historical period. How it works: The researchers started with an ensemble of VGG16 and ResNet50 convolutional neural networks pretrained on ImageNet. They fine-tuned the ensemble to predict pottery fragments’ historical period. The researchers collected 3064 photographs of pottery fragments from the southwestern U.S. Four experts labeled each photo as belonging to one of nine periods between 825 AD and 1300 AD. A majority of the experts had to agree on the type of pottery in each image for it to be included in the fine-tuning dataset, which contained 2,407 images.To make their training data more robust, the researchers randomly rotated, shrunk, or enlarged every photo prior to each training cycle.Heat maps generated using Grad-CAM highlighted the design features that were most influential in the model’s decisions. Results: In tests, the model classified tens of thousands of unlabeled fragments. It scored higher than two experts and roughly equal to two others. Behind the news: AI is helping archaeologists discover long-lost civilizations and make sense of clues they had already uncovered. Researchers found evidence of ancient settlements by training a model to interpret lidar readings taken during flights over Madagascar and the U.S.Using a similar method, archaeologists developed a network that identified underground tombs in aerial photography.A model that reads cuneiform is helping scholars translate ancient Persian tablets. Why it matters: For human archaeologists, learning to recognize the patterns on ancient pottery takes years of practice, and they often disagree on a given fragment’s provenance. Machine learning could sift through heaps of pottery shards far more quickly, allowing the humans to focus on interpreting the results. We’re thinking: Even when experts correctly identify a fragment, they can’t always explain what features led them to their conclusion. Heat maps from machine learning models could help teach the next generation of archaeologists how to read the past.

Image to article:
- Caption to image: Computer vision is probing the history of ancient pottery
- URL to image: https://info.deeplearning.ai/hs-fs/hubfs/ezgif.com-gif-maker%20-%202021-05-25T145524.475.gif?width=1200&upscale=true&name=ezgif.com-gif-maker%20-%202021-05-25T145524.475.gif
-----------------"
What is one of the main goals for progress in reinforcement learning mentioned in the context?,To build models that generalize and can handle a variety of tasks or operate in a range of situations.,"One of the main goals for progress in reinforcement learning mentioned in the context is to build models that generalize, allowing robots to perform a variety of tasks with different objects and operate in a range of situations.","-----------------
Title: Chelsea Finn: Robots That Generalize
Content: Many people in the AI community focus on achieving flashy results, like building an agent that can win at Go or Jeopardy. This kind of work is impressive in terms of complexity. But it’s easy to forget another important axis of intelligence: generalization, the ability to handle a variety of tasks or operate in a range of situations. In 2020, I hope to see progress on building models that generalize. My work involves using reinforcement learning to train robots that reason about how their actions will affect their environment. For example, I’d like to train a robot to perform a variety of tasks with a variety of objects, such as packing items into a box or sweeping trash into a dustpan. This can be hard to accomplish using RL. In supervised learning, training an image recognizer on ImageNet’s 14 million pictures tends to result in a certain degree of generalization. In reinforcement learning, a model learns by interacting with a virtual environment and collecting data as it goes. To build the level of general skill we’re accustomed to seeing in models trained on ImageNet, we need to collect an ImageNet-size dataset for each new model. That’s not practical. If we want systems trained by reinforcement learning to generalize, we need to design agents that can learn from offline datasets, not unlike ImageNet, as they explore an environment. And we need these pre-existing datasets to grow over time to reflect changes in the world, just as ImageNet has grown from its original 1 million images. This is starting to happen. For example, robots can figure out how to use new objects as tools by learning from a dataset of their own interactions plus demonstrations performed by humans guiding a robot’s arm. We’re figuring out how to take advantage of data from other institutions. For instance, we collected a dataset of robots interacting with objects from seven different robot platforms across four institutions. It’s exciting to see critical mass developing around generalization in reinforcement learning. If we can master these challenges, our robots will be a step closer to behaving intelligently in the real world, rather than doing intelligent-looking things in the lab. Chelsea Finn is an assistant professor of computer science and electrical engineering at Stanford.

Image to article:
- Caption to image: Chelsea Finn
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chelsea20Finn20SIZED-1.png
-----------------"
"What is the primary advantage of the 'Ready, Fire, Aim' approach in building AI products according to the context?","The primary advantage is that it allows for quick execution and the ability to discover problems early, enabling pivots if necessary.","The primary advantage of the 'Ready, Fire, Aim' approach in building AI products is speed. This approach emphasizes making quick decisions and executing fast, which is crucial for the success of startups. It involves pursuing one focused idea, being willing to switch hypotheses based on data, trusting domain experts' quick decisions, and leveraging AI-assisted coding for rapid prototyping. This allows startups to iterate quickly and adapt to feedback, thus maintaining a competitive edge.","-----------------
Title: Hot Tips for Speedy Startups | AI News & Insights
Content: Dear friends, I’m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity. AI Fund isn’t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): We co-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices. Many factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my articles in The Batch as well. If you are building an AI startup, here are some ideas to consider: A startup with a small team that pursues one focused, concrete idea can move really fast. Rather than hedging, it is often better to pursue one hypothesis (for example, build one concrete product) but also be willing to switch quickly to a different hypothesis (say, change what features you decide to build) if the data that comes back indicates the original hypothesis is flawed. Concreteness gets you speed!A subject matter expert’s gut is remarkably good at making quick decisions. Obviously, there’s a role for data and user studies as well. But if you’re deciding whether to build feature A or B, or to sell first to user persona X or Y, sometimes a domain expert’s gut will point to a quick decision that you can execute and validate or falsify. Trusting a domain expert’s gut gets you speed!AI-assisted coding is making prototyping faster than ever before. Yes, AI assistance is speeding up building reliable, enterprise-grade applications and maintaining legacy codebases. But the acceleration it brings to building stand-alone prototypes is far greater. This is because stand-alone prototypes have low requirements for reliability, integration, or even security (if, say, you run them in a sandbox environment). This lets us prototype and test at a ferocious velocity. AI -assisted coding (including vibe coding, where you might barely look at the code) gets you speed!Finally, with faster prototyping, the bottleneck shifts to getting feedback from users. A single learning cycle might consist of (i) building a prototype and (ii) getting user feedback to inform the next iteration. Since (i) is now much faster than before, accelerating (ii) is growing in importance. This means teams that are skilled at finding prospective customers and getting their feedback in hours/days rather than weeks can go faster. For example, when building consumer products, I routinely approach strangers (in a respectful way) in public places to ask if they’re willing to give feedback on a prototype I’m working on. (Gathering feedback is more complex for enterprise products, because prospective customers are harder to track down.) Quick user feedback gets you speed! In addition to speed, a second criterion that I find important for startup success is deep knowledge of the technology. Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed! I’m grateful to AI Fund’s investors, team, and entrepreneur partners for working with us. There is much ahead to build!

Image to article:
- Caption to image: Blue performance gauge with needle pointing to maximum, indicating high level or peak performance.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F05%2Funnamed--84--3.png&w=3840&q=75
-----------------"
What percentage of time do doctors who use Suki reportedly spend less on clerical work compared to their previous tasks?,70 percent,Doctors who use Suki reportedly spend 70 percent less time on clerical work compared to their previous tasks.,"-----------------
Title: Data: From Patient to Health Record
Content: Doctors are overwhelmed by clerical work. Healthcare-savvy voice assistants are picking up the slack.The problem: Doctors generate lots of vital information while examining a patient. Properly recorded, it becomes data that informs treatment — but entering it properly is a time-consuming task that drains docs’ attention and finances.The solution: Voice assistants can serve as clinical stenographers. Suki is one of several apps on the market that transcribe doctors’ observations and instructions and insert them into a patient’s electronic health record.How it works: Saying “Suki, the patient is running a fever and has fluid in their lungs,” inserts a note in the patient’s record. “Suki, show me the patient’s prescriptions,” retrieves that information. “Suki, I examined the patient,” enters the full description of a normal exam, ready for customization to the particular case. The model also adds diagnostic codes for tests and procedures, which aid in billing. Suki uses off-the-shelf voice recognition from Google and other vendors, augmented by the company’s own deep learning models. These models were trained on public datasets of speech plus a proprietary corpus of 250,000 anonymized patient-doctor interactions to capture the nuances of medical jargon. The engineers added background noises and conversation to make the models more robust.The engineers built several natural language task models that incorporate custom word embeddings, text classification, and entity recognition. These were trained on a combination of anonymized proprietary patient notes and public repositories of medical and clinical text. They retrain these models periodically using updated data.The company cites internal research showing that doctors who use Suki spend 70 percent less time doing clerical work. The system complies with U.S. regulations that protect sensitive personal information. Status: Suki, which integrates with several popular electronic health records, is deployed in the health network Ascension, Unified Women’s Health Care, and more than 90 small-to-midsize practices. As of July, the software operated in seven specialties including internal medicine, OB-GYN, and pediatrics. The company is working on new features for smarter billing ordering items like prescriptions and tests.Behind the news: Suki has plenty of competition. Rivals include Saykara, Nuance, M*Modal, and Notable.Why it matters: Doctors are drowning in paperwork, and voice-assistant technology can help them come up for air. A 2016 study estimates that doctors spend between 37 and 49 percent of their working hours on clerical tasks. All that paperwork contributes to the high level of burnout and depression in the profession, according to a 2019 study.We’re thinking: If you notice an improvement in your physician’s bedside manner, you might want to thank a robot. Build a natural language tool to extract data from medical information in Course 3 of the AI for Medicine Specialization from deeplearning.ai.

Image to article:
- Caption to image: Illustration of doctor sheets and a pencil
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/4-knowledge-extraction201.png
-----------------"
What technique did the researchers use to improve the performance of the C51 model in reinforcement learning?,Spectral normalization,"The researchers used spectral normalization to improve the performance of the C51 model in reinforcement learning. This technique limits the degree of variation between representations of similar inputs, which helps in moderating changes during training.","-----------------
Title: Solve RL With This One Weird Trick
Content: The previous state-of-the-art model for playing vintage Atari games took advantage of a number of advances in reinforcement learning (RL). The new champion is a basic RL architecture plus a trick borrowed from image generation. What’s new: A team led by Florin Gogianu, Tudor Berariu, and colleagues found that spectral normalization, a technique that limits the degree of variation between representations of similar inputs, improved an RL model’s performance more than several recent innovations combined. The team included researchers at Bitdefender, Deepmind, Imperial College London, Technical University of Cluj-Napoca, and University College London. Key insight: In reinforcement learning, a model observes its environment (say, the Atari game Pong), chooses an action based on its observation (such as moving the paddle), and receives a reward for a desirable outcome (like scoring a point). Learning in this way can be difficult because, as a model selects different actions, its training data (observations and rewards) change. Mutable training data poses a similar problem for generative adversarial networks (GANs), where generator and discriminator networks influence each other even as they themselves change. Spectral normalization has been shown to help GANs learn by moderating these changes. It could also be beneficial in reinforcement learning. How it works: The authors added spectral normalization to a C51, a convolutional neural network designed for reinforcement learning. The authors trained their model on tasks in the Arcade Learning Environment, a selection of games in which the actions are valid Atari controller movements. Given an observation, a C51 predicts a set of distributions of the likely reward for taking each possible action. Then it selects the action that would bring the highest expected reward. During training, it refines its prediction by sampling and comparing predicted rewards to actual rewards.Spectral normalization constrains parameter values in network layers, such that the distance between any two predictions is, at most, the distance between the inputs times a constant factor (chosen by the user). The smaller the factor, the more similar a network’s predictions must be. During training, spectral normalization limits the magnitude of a layer’s weights. If an update exceeds that limit, it divides the weights evenly so their magnitude is equal to the limit.The authors argue that limiting weight changes is akin to dampening learning rates. They devised an optimization method that lowered the model’s learning rate proportionately to spectral normalization’s limit on the weights. Models trained either way performed nearly equally. Results: Using spectral normalization on every layer impeded performance, but using it on only the second-to-last layer led the model to achieve a higher median reward. The authors compared their C51 with spectral normalization on the second-to-last layer against Rainbow, the previous state of the art, which outfits a C51 with a variety of RL techniques. In 54 Atari games, the authors’ approach achieved a 248.45 median reward, outperforming Rainbow’s 227.05 median reward. Why it matters: Applying techniques from one area of machine learning, such as GANs, to a superficially different area, such as RL, can be surprisingly fruitful! In this case, it opens the door to much simpler RL models and perhaps opportunities to improve existing techniques. We’re thinking: People who have expertise in multiple disciplines can be exceptionally creative, spotting opportunities for cross-fertilization among disparate fields. AI is now big enough to offer a cornucopia of opportunities for such interdisciplinary insight.

Image to article:
- Caption to image: Sequence of famous arcade games' scenes
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/08/SPECTRAL-1.gif
-----------------"
What prompted MIT to withdraw the Tiny Images dataset from its website?,"Researchers found that the dataset contained racist, misogynistic, and demeaning labels.","MIT withdrew the Tiny Images dataset from its website after researchers found that it contained racist, misogynistic, and demeaning labels. This discovery highlighted the social bias present in the dataset, which was compiled using images associated with words from WordNet, including racial and gender-based slurs.","-----------------
Title: News
Content: MIT withdrew a popular computer vision dataset after researchers found that it was rife with social bias.What’s happening: Researchers found racist, misogynistic, and demeaning labels among the nearly 80 million pictures in Tiny Images, a collection of 32-by-32 pixel color photos. MIT’s Computer Science and Artificial Intelligence Lab removed Tiny Images from its website and requested that users delete their copies as well. What the study found: Researchers at the University College of Dublin and UnifyID, an authentication startup, conducted an “ethical audit” of several large vision datasets, each containing many millions of images. They focused on Tiny Images as an example of how social bias proliferates in machine learning. Psychologists and linguists at Princeton in 1985 compiled a database of word relationships called WordNet. Their work has served as a cornerstone in natural language processing.Scientists at MIT CSAIL compiled Tiny Images in 2006 by searching the internet for images associated with words in WordNet. The database includes racial and gender-based slurs, so Tiny Images collected photos labeled with such terms. What the dataset’s creators said: “Biases, offensive and prejudicial images, and derogatory terminology alienates [sic] an important part of our community — precisely those that we are making efforts to include,” the researchers who built Tiny Images said in a statement. Behind the news: Social bias — in data an d models, in the industry, and in society at large — has emerged as a major issue in the machine learning community. Concerns over bias in AI reignited last week after a generative model called Pulse converted a pixelated picture of Barack Obama, who is Black, into a high-resolution image of a white man.The compilers of ImageNet recently culled labels — also based on WordNet — deemed biased or offensive from the dataset’s person subtree. Why it matters: Social biases encoded in training data become entwined with the foundations of machine learning. WordNet transmitted its derogatory, stereotyped, and inaccurate information to Tiny Images, which may have passed them along to countless real-world applications. We’re thinking: As AI practitioners, we have a responsibility to re-examine the ways we collect and use data. For instance, Cifar-10 and Cifar-100 were derived from TinyImages. We’re not aware of biases in those datasets, but when one dataset’s bias may propagate to another, it’s necessary to track data provenance and address any problems discovered in an upstream data source. Recent proposals set standards for documenting models and datasets to weed out harmful biases before they take root.

Image to article:
- Caption to image: Tiny Images photos and datasets
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Dataset202.gif
-----------------"
What is the main purpose of the Photofeeler-D3 model developed by Agastya Kalra and Ben Peterson?,"The main purpose of the Photofeeler-D3 model is to help customers choose appealing portraits for dating and other purposes by rating perceived looks, intelligence, and trustworthiness in photos.","The main purpose of the Photofeeler-D3 model developed by Agastya Kalra and Ben Peterson is to help customers choose the most appealing portraits for dating and other purposes by rating perceived looks, intelligence, and trustworthiness in photos.","-----------------
Title: That Swipe-Right Look
Content: In an online dating profile, the photo that highlights your physical beauty may not be the one that makes you look smart or honest — also important traits in a significant other. A new neural network helps pick the most appealing shots.What’s new: Agastya Kalra and Ben Peterson run a business called Photofeeler that helps customers choose portraits for dating and other purposes. Their model Photofeeler-D3 rates perceived looks, intelligence, and trustworthiness in photos. You can watch a video demo here.Key insight: Individuals have biases when it comes to rating photos. Some consistently give higher scores than average, while others may consistently give more random scores. By taking into account individual raters’ biases, a model can predict more accurately how a group would judge a photo. How it works: Photofeeler-D3 scores the beauty, intelligence, and trustworthiness of a person in a photo on a scale of 1 to 10. The network was trained on more than 10 million ratings of over 1 million photos submitted by customers through the company website. Photofeeler-D3 learned each rater’s bias (that is, whether the person’s ratings tend to be extreme or middling) based on their rankings of photos in the training dataset. The model represents this individual bias as a vector.A convolutional neural network using the xception architecture learned to predict a score for each trait. (The score wasn’t used.) After training, the CNN used its knowledge to generate vector representations of input images.The model samples a random rater from the training dataset. An additional feed-forward layer predicts that rater’s scores using the bias vector and photo vector.Then it averages its predictions of 200 random raters to simulate an assessment by the general public. Results: Tested on a dataset of face shots scored for attractiveness, Photofeeler’s good-looks rating achieved 81 percent correlation compared to the previous state of the art, 53 percent. On the researchers’ own dataset, the model achieved 80 percent correlation for beauty, intelligence, and trustworthiness.Why it matters: Crowdsourced datasets inherit the biases of the people who contributed to them. Such biases add noise to the training process. But Photofeeler’s voter modeling turns raters’ bias into a benefit: Individuals tend to be consistent in the way they respond to other peoples’ looks, so combining individuals yields a more accurate result than estimating mean ratings while ignoring their source. We’re thinking: We’d rather live in a world where a link to your Github repo gets you the most dates.

Image to article:
- Caption to image: Heart shape made with two hands
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Photofeeler20220ASPECT.png
-----------------"
What is one approach to address the challenge of small data in machine learning projects?,"Transfer learning, in which we learn from a related task and transfer knowledge over.",Not found in context,"-----------------
Title: Labeling Errors Everywhere
Content: Dear friends, Machine learning development is highly iterative. Rather than designing a grand system, spending months to build it, and then launching it and hoping for the best, it’s usually better to build a quick-and-dirty system, get feedback, and use that feedback to improve the system.The iterative aspect of machine learning applies to many steps. For example:Data labeling: It’s hard to come up with fully fleshed-out labeling guidelines that result in clean and consistent labels on your first attempt. It might be better to use an initial set of guidelines to label some data, see what problems arise, and then improve the guidelines.Model training: Building an AI system requires deciding what data, hyperparameters, and model architecture to use. Rather than overthinking these choices, it’s often better to train an initial model, then use error analysis to drive improvements.Deployment and monitoring: When deploying a machine learning system, you might implement dashboards that track various metrics to try to spot concept drift or data drift. For example, if you’re building a product recommendation system, you might track both software metrics such as queries per second and statistical metrics such as how often the system recommends products of different categories. What metrics should we track? Rather than try to design the perfect set of dashboards before launch, I find it more fruitful to pick a very large set of metrics, evolve them, and prune the ones that prove less useful. Iteration is helpful in other phases of machine learning development as well. It make sense to take an empirical, experimental approach to decision making whenever: Multiple options are available and it's hard to know the best choice in advance.We can run experiments to get data quickly about the performance of different options. These two properties hold true for many steps in a typical ML project. One implication is that, if we can build tools and processes that enable high-throughput experimentation, we can make faster progress. For instance, if you have an MLOps platform that enables you to quickly train and evaluate new models, this will allow you to improve models more quickly. This principle applies to other aspects of ML development that are iterative. That’s why time spent optimizing your team's capacity to run many experiments can pay off well. Keep learning! Key machine learning datasets are riddled with mistakes. What’s new: Several benchmark datasets are shot through with incorrect labels. On average, 3.4 percent of examples in 10 commonly used datasets are mislabeled, according to a new study — and the detrimental impact of such errors rises with model size. The research: Curtis Northcutt and Anish Athalye at MIT and Jonas Mueller at Amazon trained a model to identify erroneous labels in popular datasets such as ImageNet, Amazon Reviews, and IMDB. Following confident learning, the authors considered an example mislabeled if it met two conditions: The model’s predicted classification didn't match the label, and the model’s confidence in its classification was greater than its average confidence in its predictions of the labeled class over all examples bearing that label.Human reviewers vetted the mislabeled examples. They found many obvious mistakes: an image of a frog labeled “cat,” an audio clip of a singer labeled “whistling,” and negative movie reviews misinterpreted as positive. QuickDraw had the highest rate of inaccurately labeled data, 10.1 percent. MNIST had the lowest, 0.15 percent.The authors fixed the bad labels and revised the test sets. Then they measured how well different models classified the corrected test sets. Smaller models like Resnet-18 or VGG-11 outperformed larger ones like NasNet or VGG-19. Why it matters: It’s well known that machine learning datasets contain a fair percentage of errors. Previous inquiries into the problem focused on training rather than test sets, and found that training on a small percentage of incorrect labels didn’t hurt deep learning performance. But accuracy on a test set that’s rife with errors is not a true measure of a model’s ability, and bad labels in the test set have a disproportionate impact on bigger models. We’re thinking: It’s time for our community to shift from model-centric to data-centric AI development. Many state-of-the-art models work well enough that tinkering with their architecture yields little gain in many problems, and the most direct path to improved performance is to systematically improve the data your algorithm learns from. You can check out Andrew’s recent talk on the subject here. #DataCentricAI

Image to article:
- Caption to image: Model identifying erroneous labels in popular datasets
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/05/image-12.png
-----------------"
What was the accuracy rate of the face recognition app developed by researchers at the University of Oxford for identifying individual chimpanzees?,84 percent,The accuracy rate of the face recognition app developed by researchers at the University of Oxford for identifying individual chimpanzees was 84 percent.,"-----------------
Title: Chimp Recognition
Content: AI is capable of picking faces out of the crowd — even if that crowd is squabbling over bananas in a jungle. What’s new: Researchers at the University of Oxford developed a face recognition app that identifies individual chimpanzees in footage shot in the wilds of Guinea. The work could give wildlife conservation efforts a powerful new tool. How it works: The group adapted the VGG-M convolutional neural network architecture. They trained the model on roughly 50 hours of footage representing 23 individuals over 14 years. The model identified apes as they aged.It was able to recognize individuals regardless of low light, poor image quality, and facing away from the camera.The researchers pitted their model against a human trained to recognize chimps. The human sorted 42 percent of the images correctly. The model’s accuracy was 84 percent. Behind the news: Zoologists have embraced image recognition for conservation efforts. The technology is counting giraffes in Africa and tracking wolverines in the Pacific Northwest. An innovative application called WildBook that trawls YouTube for wildlife videos has been used to catalog whale shark migrations. Why it matters: Chimpanzees, like humans, are highly social animals. The ability to track individuals enabled the researchers to map the tribe’s structure. The model generalized well to other primate species in preliminary tests. The researchers suggest that their approach could be used with other animals where a sufficient video record exists. We’re thinking: Applications like this could help cash-strapped conservation efforts to focus on translating data into action, and reduce the need for invasive, labor-intensive methods like tagging animals with RFID.

Image to article:
- Caption to image: Face recognition app identifying individual chimpanzees in footage shot in the wilds of Guinea
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Chimpanzees20faces.gif
-----------------"
"What is the most promising approach to reducing electronic waste from servers used in generative AI, according to the study?",Extending server lifespans.,The most promising approach to reducing electronic waste from servers used in generative AI is to extend server lifespans. Adding one year to a server’s operational life could reduce e-waste by 62 percent.,"-----------------
Title: Generative AI and GPU Boom Spawns Growing E-Waste Problem
Content: Rapid progress in generative AI comes with a hidden environmental cost: mountains of obsolete hardware. What’s new: A study projects that servers used to process generative AI could produce millions of metric tons of electronic waste by 2030. Extending server lifespans could reduce the burden substantially, according to author Peng Weng and colleagues at the Chinese Academy of Sciences and Reichman University. How it works: The study extrapolated from publicly available data to model accumulation of electronic waste, or e-waste, between 2023 and 2030. The authors examined four scenarios: One scenario assumed linear growth in which hardware manufacturing expands at the current rate of 41 percent annually. The other three assumed exponential growth of demand for computing: conservative (85 percent annually), moderate (115 percent annually), and aggressive (136 percent annually). The study evaluated each scenario with and without measures taken to reduce waste. In the linear-growth scenario, e-waste could add up to 1.2 million metric tons between 2023 and 2030. In the aggressive scenario, the total could reach 5 million metric tons, or roughly 1 percent of total electronic waste during that period. (These figures don’t account for mitigations, which would improve the numbers, or ongoing manufacturing of earlier, less efficient technology, which would exacerbate them.)The study assumed that servers typically would be discarded after three years. Upgrading servers more frequently, when improved hardware becomes available, would reduce overall server numbers because fewer servers would deliver greater processing power. However, because servers would be discarded more quickly, it could add a cumulative 1.2 million metric tons in the linear scenario or 2.3 million metric tons in the aggressive scenario, assuming no mitigation measures are taken.U.S. trade restrictions on advanced chips are also likely to exacerbate the problem. They could push affected countries to rely on less-efficient hardware designs and thus require more new servers to reach a competitive processing capacity. This could increase total waste by up to 14 percent.The authors explored several approaches to reducing e-waste. Repurposing equipment for non-AI applications and reusing critical components like GPUs and CPUs could cut e-waste by 42 percent. Improving the power efficiency of chips and optimizing AI models could reduce e-waste by 16 percent.The most promising approach to reducing e-waste is to extend server lifespans. Adding one year to a server’s operational life could reduce e-waste by 62 percent. Why it matters: E-waste is a problem not only due to its sheer quantity. Server hardware contains materials that are both hazardous and valuable. Discarded servers contain toxic substances like lead and chromium that can find their way into food water supplies. They also contain valuable metals, such as gold, silver, and platinum, that could save the environmental and financial costs of producing more of them. Proper recycling of these components could yield $14 billion to $28 billion, highlighting both the economic potential and the urgent need to develop and deploy advanced recycling technologies. We’re thinking: Humanity dumps over 2 billion metric tons of waste annually, so even comprehensive recycling and repurposing of AI hardware and other electronic devices would make only a small dent in the overall volume. However, the high density of valuable materials in e-waste could make mining such waste profitable and help recycle waste into valuable products, making for a more sustainable tech economy.

Image to article:
- Caption to image: Pile of discarded green circuit boards from electronic devices.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F12%2Funnamed--28--1.png&w=3840&q=75
-----------------"
What is the primary benefit of using a no-code platform like LandingLens in the machine learning project lifecycle?,"It accelerates the process of training, deploying, and managing machine learning models, allowing for quicker iterations and easier handling of tasks such as data augmentation and error analysis.","The primary benefit of using a no-code platform like LandingLens in the machine learning project lifecycle is the significant acceleration of the process from concept to deployment. It allows for faster data collection, model training, and production deployment by simplifying tasks such as managing data augmentation, linking data and models, and providing visualizations and metrics for error analysis. This enables even skilled engineers to complete tasks more quickly and efficiently, reducing the time needed from months to just minutes.","-----------------
Title: Robocoders
Content: Dear friends, It can take 6 to 24 months to bring a machine learning project from concept to deployment, but a specialized development platform can make things go much fasterMy team at Landing AI has been working on a platform called LandingLens for efficiently building computer vision models. In the process, I’ve learned important lessons about how such platforms can help accelerate the machine learning project lifecycle: Data collection: Ambiguity in labels (what is the “correct” value of y?) plagues many projects. If the labels are inconsistently defined, it’s impossible to achieve a high test-set accuracy. But it’s difficult to find these inconsistencies manually and to convince stakeholders (often subject-matter experts) to resolve them. An MLOps platform can identify problems and encourage consistency.Model training: The ability to write code to train a model in TensorFlow or PyTorch is a valuable skill. But even for skilled engineers, it’s faster to use a no-code platform that lets you do this via mouse clicks (to manage data augmentation, link the data and model, manage GPU training resources, keep track of data/model versions, and provide visualizations and metrics for error analysis).Production deployment: Many teams can execute a successful proof of concept and achieve high-test set accuracy. But to secure budgets and approval for deployment, a small demo can help others see a project’s value. A platform can make it easy to implement a demo that runs not just in a Jupyter notebook but in a lightweight deployment environment such as a mobile app or simple edge device. It used to take me months to deploy a model. With a no-code platform, I can train a RetinaNet demo, carry out error analysis, use a data-centric approach to clean up inconsistent data, retrain, and deploy to an edge device — all in 60 minutes. I get a thrill every time I go through the machine learning project lifecycle so quickly.Platforms like this can help a variety of AI projects across all industries. LandingLens works well for visual inspection in areas as diverse as automotive, semiconductor, and materials, I’m hoping to make it more widely available. Its sweet spot is computer vision problems (detection or segmentation) with 30 to 10,000 images. If you have a business problem in computer vision that falls in this sweet spot, I’d like to hear from you. Please get in touch by filling out this form. Keep learning! Language models are starting to take on programming work. What’s new: SourceAI uses GPT-3 to translate plain-English requests into computer code in 40 programming languages. The French startup is one of several companies that use AI to ease coding, according to Wired. How it works: Companies have trained language models to anticipate programmers’ needs. SourceAI, currently in beta test, enables users to describe the function they want, then select a programming language. Between 80 and 90 percent of code generated by the beta version works as intended, founder Furkan Bektes told The Batch. He plans to charge $0.04 to $0.10 per piece of code.GPT-3 also powers Debuild, which builds web applications like buttons and text input fields based on plain English descriptions.Belgian startup Tabnine has a GPT-2-powered tool that automatically suggests follow-on lines of code as programmers type. Behind the news: Other companies are also using machine learning to increase coders’ productivity and sniff out bugs. Facebook’s Aroma lets developers search code databases for snippets similar to whatever they’re working on.Intel’s Machine Inferred Code Similarity is a similar tool that compares pieces of code to determine their function.DeepMind published a model that rewrites human-generated code to make it run more efficiently. Why it matters: In the hands of a skilled programmer, such tools can save time, freeing up brainpower for more complex tasks. In the hands of the newbie, they make it possible to create applications with little experience and — with diligent attention — gain skills more quickly. We’re thinking: No AI system should replace a sacred rite of passage for neophyte coders: print (“Hello World!”).

Image to article:
- Caption to image: Animation of SourceAI working
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/05/code_revised-2.gif
-----------------"
What is the primary goal of the Canadian startup BlueDot?,To stop the spread of infectious diseases by giving healthcare workers early warning.,"The primary goal of the Canadian startup BlueDot is to stop the spread of infectious diseases by providing early warnings to healthcare workers, enabling them to identify and treat people who become infected.","-----------------
Title: Stopping Coronavirus
Content: A company that analyzes online information to predict epidemics spotted the upsurge in coronavirus at least a week ahead of public-health authorities.What’s new: Canadian startup BlueDot alerted customers to the outbreak in the Chinese city of Wuhan on New Year’s Eve, Wired reported. The U.S. Centers for Disease Control and Prevention issued its warning on January 6, and the World Health Organization followed suit three days later. The respiratory illness as of this writing has infected more than 6,000 people and killed more than 130, mostly in China. How it works: Founded in 2014, BlueDot aims to stop the spread of infectious diseases by giving healthcare workers early warning, so they can identify and treat people who become infected. The company’s natural language processing model ingests 100,000 articles in 65 languages daily to track more than 100 infectious diseases. It ignores social media but scans news reports, government information, blogs, and forums related to human, plant, and animal diseases, as well as travel ticketing and local weather data.Human analysts vet the model’s predictions. They issue reports to customers in business, government, and nongovernmental organizations, ultimately reaching healthcare facilities and public health officials in a dozen countries. Behind the news: In 2008, Google undertook a similar effort to forecast influenza outbreaks based on search terms entered by users. In initial research, Google Flu Trends tracked the number of cases two weeks faster than the CDC. However, it dramatically underestimated the peak of the 2013 flu season and was shuttered soon afterward. Subsequent analysis concluded that the algorithm overfit seasonal search terms unrelated to flu. Why it matters: Rapid detection of new diseases is crucial to avoid global pandemics. Virulent diseases often can be contained if they’re caught early enough, but every hour compounds the number of people exposed and thus the number of cases. An epidemic can quickly overwhelm healthcare systems, leaving people even more exposed.We’re thinking: It’s hard to know how well today’s techniques will play out tomorrow. But the ability to catch potential pandemics before they explode is too valuable not to try.

Image to article:
- Caption to image: Three people wearing face masks on a plane
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Coronavirus20ASPECT.png
-----------------"
What is the projected global revenue for the AI industry by 2024 according to the IDC study?,$500 billion,The projected global revenue for the AI industry by 2024 is expected to break $500 billion according to the IDC study.,"-----------------
Title: AI Sales Closing In on $500 Billion
Content: A new report projects a rosy future for the AI industry.What’s new: A study from market research firm IDC estimates that global revenues for AI software, hardware, and services will reach $341.8 billion in 2021 — up from an estimated $156.5 billion last year — and will break $500 billion by 2024. The study reflects interviews, distribution statistics, financial reports, and other data from over 700 AI companies around the world.What they found: The AI industry’s annual growth rate is expected to exceed 18.8 percent next year. The analysis breaks up that growth into three broad categories. Some of the most important findings: Software: Software sales make up 88 percent of the overall AI market. AI platforms (the largest of six software subcategories) account for half of the total. However, AI applications are expected to grow most quickly, marking a five-year annual rate of 33.2 percent.Hardware: AI-focused hardware — mainly servers and storage — accounts for just 5 percent of the industry’s sales. However, it is projected to grow by 29.6 percent in 2021 and 2022, faster than software and services. Server sales account for 82 percent of hardware sales which are dominated by Dell, HPE, Huawei, IBM, Inspur, and Lenovo.Services: AI services generated 14 percent of total sales and are expected to grow at a 21 percent compound annual rate through 2025. IT services bring in 80 percent of sales in this area. Behind the news: IDC’s most recent predictions are in line with their previous report, published in February, and jibe with research from MIT Technology Review.Why it matters: In the AI world — as in other high-tech sectors — it’s often difficult to discern real growth potential from gossip-fueled hype. Research reports that provide granular insights are a crucial tool for business leaders and investors who aim to capitalize on this industry, not to mention machine learning engineers who are plotting a career.We’re thinking: We’ve seen market research reports that later proved right and many that later proved dead wrong. We hope this is one of the former!

Image to article:
- Caption to image: Chart showing data about worldwide AI services forecast
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/09/IDC-1.gif
-----------------"
How do machine learning models assist automobile insurers in calculating repair costs?,They analyze damage documented through an app and estimate repair costs using data from local mechanics and parts suppliers.,"Machine learning models assist automobile insurers by automating the process of assessing car damage and estimating repair costs. When a driver gets into an accident, they use an app to document the damage. The app, using a model trained on crash photos, classifies the damage and determines if a human inspection is necessary. If not, it analyzes the damage, estimates repair costs using data from local mechanics and parts suppliers, and then a human adjustor reviews the model’s work. This process helps insurers save money and speeds up claim settlements.","-----------------
Title: Wreck Recognition
Content: Automobile insurers are increasingly turning to machine learning models to calculate the cost of car repairs. What’s new: The pandemic has made it difficult for human assessors to visit vehicles damaged in crashes, so the insurance industry is embracing automation, Wired reported. How it works: When drivers get into an accident, insurance companies direct them to download an app that guides them through documenting the effects. These systems are particularly good at assessing damage from minor collisions and determining when a car has been totaled. Such apps classify damage using a model trained on crash photos of a variety of makes and models. The app determines whether the damaged part needs to be inspected by a human. If not, it analyzes what needs to be fixed and estimates a repair cost using data from local mechanics and parts suppliers. Then a human adjustor reviews the model’s work.Tractable, which makes such software, says its system correctly estimates 25 percent of cases without human intervention.CCC Information Services, which makes an app called Smart Estimate, claims that adjusters who use its system are 30 percent more productive.Such models are particularly good at assessing minor damage and determining when a car has been totaled. Yes, but: Several body shop owners said that automated estimates weren’t accurate and often failed to spot hard-to-see damage such as a misaligned frame. Bad estimates resulted in substandard repairs and delays as mechanics haggled with insurance companies for more money. Why it matters: Smart damage-assessment apps can inspect vehicles far more quickly than a human who examines the damage first-hand. Accurate output helps insurance companies save money and drivers settle claims more quickly. We’re thinking: Will self-driving cars that get into a fender bender use an app to assess the damage?

Image to article:
- Caption to image: Tractable app determining the cost of a car's damage
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/05/image-1.png
-----------------"
What do Tim Hwang and Alex Pascal argue about the consequences of a winner-take-all approach to AI between global powers like the U.S. and China?,They argue that such an approach could lead to a loss of benefits from international collaboration.,"Tim Hwang and Alex Pascal argue that if global powers like the U.S. and China adopt a winner-take-all approach to AI, they will miss out on the benefits of international collaboration. They believe that the arms-race mentality, driven by the notion that autonomous weapons could be decisive in conflicts, encourages nations to keep research and development to themselves. However, the total benefit of collaboration is often greater than that of any particular initiative. They suggest that both countries would benefit more from AI that addresses global issues like climate change, health, and disaster response.","-----------------
Title: Competition or Cooperation?
Content: Some politicians view international competition in AI as an arms race. That mindset could lead to escalating conflict, experts said.What’s new: If global powers like the U.S. and China adopt a winner-take-all approach to AI, they will lose out on the benefits of international collaboration, Tim Hwang and Alex Pascal argue in Foreign Policy.The analysis: The arms-race mentality springs primarily from the notion that autonomous weapons will prove to be a trump card in international conflicts, the authors say. This belief encourages nations to keep research and development to themselves, but the total benefit of collaboration is often greater than that of any particular initiative, they say. Both the U.S. and China would benefit from AI that targets issues like climate change, global health, and disaster response.Powerful nations can use cooperative projects to shape global norms and rules for AI.A national agenda that prioritizes AI for warfare is likely to divert funding from non-defense applications.Countries that undertake arms races tend to escalate conflict rather than tamp it down. Behind the news: Scientific partnerships between the U.S. and USSR mitigated tensions during the Cold War era. In 1957, the rival nations agreed to send scientists to collaborate on projects. These cooperative relationships influenced diplomatic discussion and helped ease disagreements over issues like nuclear disarmament.Why it matters: AI’s potential role in warfare is still unclear, and the technology is far from fully developed. The gap creates breathing room for national leaders to establish policies that will mutually benefit their own countries and the world at large. For instance, the National Security Commission on AI advocates that the U.S. engage with China and Russia to control military uses of AI.We’re thinking: Electricity has uses in warfare, yet countries didn’t keep that technology to themselves, and the whole world is better off for it.

Image to article:
- Caption to image: US flag on the left, China flag on the right
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Arms20Race20ASPECT.png
-----------------"
What method did researchers at DeepMind develop for training simulated football teams?,"They developed a method that integrated individual and group skills, allowing agents to learn to run, pass, defend, and score goals on a physically accurate virtual field.","Researchers at DeepMind developed a method for training simulated football teams by integrating individual and group skills into humanoid agents. The training involved stages starting with lower-level controllers for actions like running and moving, and higher-level controllers for teamwork. They used supervised learning to train an encoder and decoder with motion-capture data, and reinforcement learning for performing drills and competing in teams. The agents learned to apply torques to joints, track positions and velocities, and collaborate with teammates to run, pass, defend, and score goals.","-----------------
Title: Team Players
Content: Playing a team sport involves a fluid blend of individual and group skills. Researchers integrated both types of action into realistic humanoid agents that play football (known as soccer in the U.S.).What's new: Siqi Liu, Guy Lever, Zhe Wang, and colleagues at DeepMind developed a method for training simulated football teams that learned to run, pass, defend, and score goals on a physically accurate virtual field. You can see the output here.Key insight: Football players must control their own muscle motions over time spans measured in milliseconds while collaborating with teammates over greater intervals. By training in stages — starting with lower-level controllers that operate on short time scales for things like running and moving on higher-level controllers that operate on longer time scales for, say, teamwork — agents can learn to move both independently and cooperatively.How it works: The authors trained 16 agents to compete in two-member teams. An agent could apply torques to its 56 joints; track its own joint angles, positions, and velocities; and observe the positions and velocities of other players and objects on the field. All model architectures were vanilla neural networks. In the first stage of training, a model learned motions like running and turning. The authors trained an encoder and decoder via supervised learning to predict an agent's motion, given 105 minutes of motion-capture data from real players in scripted scenes. The encoder learned to convert the agent’s physical state into a representation, while the decoder learned to convert the representation into torques on joints. The same decoder was used in subsequent steps.In the second stage, separate encoders learned via reinforcement learning to perform four drills: following a point, following a point while dribbling, kicking a ball to a point on the field, and shooting a goal. Each encoder learned representations of not only the agent’s physical state but also the drill, such as the point to be followed. The decoder determined how the agent should move its joints.Four additional encoders learned via supervised learning to re-create the drill model’s representations without access to information about where to run or kick the ball.Finally, the agents learned via reinforcement to compete in teams. An encoder learned to combine the drill representations and passed the result to the decoder to determine the agent’s motion. The model received +1 when its team scored a goal and -1 when its team was scored upon. Further rewards encouraged the player closest to the ball to advance it toward the opponents’ goal. Results: The agents’ skills increased with the number of training episodes. For example, at initialization, when an agent fell, it got up 30 percent of the time. After 375 million training steps in competition, it righted itself 80 percent of the time. Likewise, at initialization, when an agent touched the ball, it executed a pass 0 percent of time. After 80 billion training steps in competition, it passed the ball in 6 percent of touches.Why it matters: It may take more than one training mode to teach all the skills required to perform a complex task. In this case, the authors combined supervised learning, reinforcement learning, and training in teams.We’re thinking: How to build agents that operate at both short and long time scales is a longstanding problem in reinforcement learning. The authors solved it by specifying the skills at each time scale manually. The next step is to design agents that can learn that abstraction on their own.

Image to article:
- Caption to image: Animation showing a simulated football team and how it works
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/09/FOOTBALL-1.gif
-----------------"
What significant change in patent law occurred in 2016 that contributed to the increase in deep learning patents?,A federal court lowered the bar for approval of software patents.,"In 2016, a federal court lowered the bar for approval of software patents, which contributed to the increase in deep learning patents.","-----------------
Title: IP for AI
Content: The number of patents issued for deep learning has doubled every year since 2013.What’s new: Inventor, engineer, and lawyer Nick Brestoff tracks deep learning patents. He detailed his findings in a blog on InsideBigData and offers advice on how to get patent applications approved.AI on the rise: Breston searches weekly for filings containing keywords including “deep learning,” “deep neural,” or “multi-layer neural.” He found that IBM holds the most deep learning patents (51), followed by Google (39) and Microsoft (28). Between 2013 and 2015, the Patent Office issued just three to four deep learning patents a year.In 2016, that number jumped to 36 and has more than doubled each year since. As of December 3, the agency had issued 361 deep learning patents in 2019.A change in the law helps explain the 2016 jump. That year, a federal court lowered the bar for approval of software patents. Senior privilege: Inventors age 65 and over — even those listed as co-authors — can fast-track patent applications using a loophole called a Petition to Make Special. This trick has allowed Brestoff, who is 71 years old and holds eight patents on deep learning techniques, to complete the process in as little as three months, rather than the usual years-long wait. “There’s a wonderful advantage to having a knowledgeable senior on your innovation team,” he said.We’re thinking: Patents have a bad name in some circles, because of patent trolls and frivolous lawsuits that have destroyed value and slowed down innovation. But we’re also not sure a world with no patents whatsoever would be one with more innovation.

Image to article:
- Caption to image: Chart with number of patents per year
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Patents20ASPECT-1.png
-----------------"
What percentage of female participation in AI typically hovers according to the meta-analysis by Synced Review?,Between 10 and 20 percent.,"According to the meta-analysis by Synced Review, female participation in various aspects of AI typically hovers between 10 and 20 percent.","-----------------
Title: AI’s Gender Imbalance
Content: Women continue to be severely underrepresented in AI.What’s new: A meta-analysis of research conducted by Synced Review for Women’s History Month found that female participation in various aspects of AI typically hovers between 10 and 20 percent.What they found: Much of the research included in the analysis was based on numbers generated by rules-based software that categorizes names according to gender. Synced Review, which is based in China, said it didn’t examine studies of Chinese companies or institutions because Chinese names don’t correlate as tightly with gender as names in other languages. The 2019 AI Index produced by Stanford University’s Human-Centered AI Institute reported that females made up 20 percent of faculty members in academic AI departments. That number isn’t likely to rise soon; 20 percent of new faculty hires and 20 percent of AI-related PhD recipients are female.A 2018 study by Wired and Element AI, an enterprise software company, tallied men and women who contributed to the major AI conferences NeurIPS, ICLR, and ICML. Twelve percent were women. In a 2019 review of 21 conferences by Element AI, the percentage rose to 18 percent.A 2018 Wired analysis of AI researchers at Google and Facebook estimated that 10 percent of Google’s machine learning workforce and 15 percent of Facebook’s AI researchers were women. (Both companies later said the report had understated the true number, but they didn’t provide further information.)Researchers at Nesta, a UK research foundation, analyzed AI research papers on Arxiv. Women accounted for 12 percent of authors in 2015, less than in 2009. Behind the news: Women have a prominent place in AI’s history, going all the way back to Ida Rhodes, who in the 1960s laid the groundwork for natural language processing. The percentage of American women with computer science degrees, however, peaked in the mid-1980s at around 35 percent, and since has declined to under 20 percent.Why it matters: It’s important that people building the future represent diverse groups to make sure that anyone can participate and that the products we build encompass a variety of perspectives.We’re thinking: Each one of us can help promote diversity. Leaders can make an effort to interview, hire, and mentor underrepresented groups, and everyone can help make the workplace inclusive.

Image to article:
- Caption to image: Women in AI in academia and industry chart
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Gender20ASPECT201.png
-----------------"
What are some common pitfalls that can lead to the failure of AI projects?,"Common pitfalls include robustness, small data, and workflow.","Common pitfalls that can lead to the failure of AI projects include issues related to robustness, small data, and workflow. These challenges arise because the process of engineering AI systems is still immature compared to software engineering, and teams often have not learned how to avoid these common pitfalls.","-----------------
Title: DeepLearning.ai Exclusive
Content: Dear friends, I’ve heard this conversation in multiple companies: Machine learning engineer: Look how well I did on the test set! Business owner: But your ML system doesn’t work. This sucks! Machine learning engineer: But look how well I did on the test set! Why do AI projects fail? Last week, I addressed this question at our Pie & AI meetup. We had a spirited discussion with a live audience in 10 cities from London to Berlin, Ghent (Belgium) to Logroño (Spain). I remain as optimistic as ever about the AI industry, but I also see many AI projects struggle. Unlike software engineering, the process of engineering AI systems is immature, and teams have not yet learned about the most common pitfalls and how to avoid them. Common pitfalls fall under the headings: robustness, small data, and workflow. You can increase your odds of success by analyzing your AI project in terms of these issues. I’ll flesh out my thoughts on this in coming weeks. Stay tuned. Keep learning! Daniel Barbosa quit his job managing cloud infrastructure to self-study machine learning full-time. Learn how Daniel landed his first ML job.

Image to article:
- Caption to image: Pie & AI and IASI AI cupcakes
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/06/Iasi201-1.jpeg
-----------------"
"What is the purpose of the Pie & AI meetup held in Kuala Lumpur, Malaysia?",The purpose of the Pie & AI meetup was to bring together AI communities and promote collaboration in the field of artificial intelligence.,Not found in context,"-----------------
Title: DeepLearning.ai Exclusive
Content: Dear friends, I’ve heard this conversation in multiple companies: Machine learning engineer: Look how well I did on the test set! Business owner: But your ML system doesn’t work. This sucks! Machine learning engineer: But look how well I did on the test set! Why do AI projects fail? Last week, I addressed this question at our Pie & AI meetup. We had a spirited discussion with a live audience in 10 cities from London to Berlin, Ghent (Belgium) to Logroño (Spain). I remain as optimistic as ever about the AI industry, but I also see many AI projects struggle. Unlike software engineering, the process of engineering AI systems is immature, and teams have not yet learned about the most common pitfalls and how to avoid them. Common pitfalls fall under the headings: robustness, small data, and workflow. You can increase your odds of success by analyzing your AI project in terms of these issues. I’ll flesh out my thoughts on this in coming weeks. Stay tuned. Keep learning! Daniel Barbosa quit his job managing cloud infrastructure to self-study machine learning full-time. Learn how Daniel landed his first ML job.

Image to article:
- Caption to image: Pie & AI and IASI AI cupcakes
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/06/Iasi201-1.jpeg
-----------------"
"What is Apple's approach to machine learning applications, and how does it differ from other tech giants?","Apple's approach is pro-privacy and on-device processing, which helps preserve user privacy and reduces latency, contrasting with other tech giants that rely on aggressive data collection and cloud processing.","Apple's approach to machine learning applications focuses on a pro-privacy, on-device strategy. Unlike other tech giants that rely heavily on cloud-based data processing, Apple uses a chip subsystem called the Neural Engine to process most machine learning tasks directly on its devices. This approach helps preserve user privacy and reduces latency, allowing the software to run closer to real time. Additionally, Apple has acquired numerous machine learning startups to bolster its AI capabilities, emphasizing a privacy-centric, edge-based approach that contrasts with the industry's typical reliance on aggressive data collection.","-----------------
Title: News
Content: After years of trailing other tech giants in AI, Apple has a new ambition: to become the industry’s leading purveyor of products powered by machine learning.What’s new: In an interview with Ars Technica, the company’s AI chief argues that its pro-privacy, on-device approach is the best way to build such applications.Think different: John Giannandrea, the former head of Google’s AI and search who joined Apple in 2018, outlined the iPhone maker’s effort to infuse the technology into a wide range of products and services. Apple is putting a marketing push behind augmented reality apps and upgrades to its personal digital assistant Siri. It also touts AI features such as managing its devices’ energy consumption based on user habits and fusing successive photos into a single high-quality image.Like Google, Huawei, Qualcomm, and Samsung, Apple designed specialized chips to run AI software on smartphones, tablets, and watches. Its laptops are expected to include the chip later this year.Rather than executing tasks in the cloud, a chip subsystem called the Neural Engine processes most machine learning tasks on Apple devices. Processing data on the device helps preserve user privacy and reduces latency, so the software runs closer to real time, according to Giannandrea.Despite the company’s pro-privacy stance, it does collect and label some anonymized data, Giannandrea said. It also asks users to donate data with prompts like, “Would you like to make Siri better?” Buying in: Apple lists dozens of AI job openings, but it has acquired much of its AI technology by buying other companies. It purchased at least 20 machine learning startups — more than any of its rivals — since buying Siri in 2010, according to venture tracker CB Insights. Why it matters: Apple’s privacy-centric, edge-based approach stands out from much of the industry’s reliance on aggressive data collection and processing in the cloud. The difference could help counteract the longstanding impression that it’s behind other tech giants in AI.We’re thinking: AI’s voracious appetite for data boosts the accuracy of supervised learning systems, but it poses risks to user privacy. Apple’s effort to avoid collecting and exposing user data is refreshing — and raises the stakes for small data techniques that enable systems to learn effectively with fewer examples.

Image to article:
- Caption to image: Different Apple products using augmented reality
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize-2.gif
-----------------"
What is the main focus of the U.S. Copyright Office's AI and fair use report released amid leadership changes?,"The report addresses how copyright law should apply to using protected works for AI training, often siding with creators over tech platforms.",Not found in context,"-----------------
Title: Creative Workers Don't Want AI Developers to Train Models on Their Work
Content: The latest advances in AI are built on freely available training data. What will happen if it becomes off-limits? The fear: Creative workers don’t want AI developers to train models on their works without permission or compensation, or at all. Data is vanishing as they scramble to lock it down. Horror stories: Generative AI models readily produce outputs that imitate the styles of individual authors and artists. Creative people and organizations that work on their behalf are reacting by suing AI developers (all proceedings are ongoing at publication time) and restricting access to their works. A class-action lawsuit against Microsoft, OpenAI, and Github claims that OpenAI improperly used open source code to train Github’s Copilot code-completion tool.Several artists filed a class-action lawsuit against Stability AI, Midjourney, and the online artist community DeviantArt, arguing that the companies violated the plaintiffs’ copyrights by training text-to-image generators on their artwork.Universal Music Group, which accounts for roughly one-third of the global revenue for recorded music, sued Anthropic for training its Claude 2 language model on copyrighted song lyrics.The New York Times altered its terms of service to forbid scraping its webpages to train machine learning models. Reddit and Stack Overflow began charging for their data.Authors brought a class-action lawsuit against Meta, claiming that it trained LLaMA on their works illegally. The Authors Guild sued OpenAI on similar grounds. The threat of a lawsuit by a Danish publishers’ group persuaded the distributor of Books3, a popular dataset of about 183,000 digitized books, to take it offline. Survival in a data desert: Some AI companies have negotiated agreements for access to data. Others let publishers opt out of their data-collection efforts. Still others are using data already in their possession to train proprietary models. OpenAI cut deals with image provider Shutterstock and news publisher The Associated Press to train its models on materials they control.Google and OpenAI recently began allowing website owners to opt out of those companies’ use of webpages to train machine learning models.Large image providers Getty and Adobe offer proprietary text-to-image models trained on images they control. Facing the fear: Copyright holders and creative workers are understandably worried that generative AI will sap their market value. Whether the law is on their side remains to be seen. Laws in many countries don’t explicitly address use of copyrighted works to train AI systems. Until legislators set a clear standard, disagreements will be decided case by case and country by country.

Image to article:
- Caption to image: Illustration of a wizard with a facial expression evoking 'The Scream' painting in front of a half-empty bookshelf
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2023%2F10%2FDataDisappearence5_1200px-1.jpg&w=3840&q=75
-----------------"
What was the reason for the denial of entry to foreign researchers attending the NeurIPS conference in Canada?,"The Canadian Ministry of Immigration rejected visas for many researchers, mostly from developing nations, due to concerns that they might overstay their visas.","Foreign researchers, mostly from developing nations, were denied entry to Canada for the NeurIPS conference because Canada's Ministry of Immigration rejected their visas. The ministry screens visitors for signs that they might overstay their visa, flagging individuals from countries that are poor or at war, especially if they have relatives living in Canada.","-----------------
Title: Researchers Blocked at the Border
Content: Foreign researchers hoping to attend one of AI’s largest conferences were denied entry into Canada, where the event will be held. Most of those blocked were from developing nations.What happened: This year’s Conference on Neural Processing Information Systems (NeurIPS) is being held in Vancouver, Canada, in December. The country’s Ministry of Immigration rejected visas for a number of researchers, mostly from Africa.Gatekeeping gaffe: It’s unclear exactly how many researchers were blocked, but organizers for the conference’s Black in AI workshop said they were aware of around 30 people affected. Canada’s immigration ministry screens visitors for signs that indicate they might overstay their visa. The agency, for example, flags individuals from countries that are poor or at war, especially if they also have relatives living in Canada.NeurIPS had anticipated that attendees might experience difficulties getting through Canada’s border, and had been working with immigration officials since May to help smooth the process.After NeurIPs organizers complained, officials allowed 15 researchers whose visas initially were denied to enter the country. The rest are still under review.“While we cannot comment on the admissibility of any particular individual, we can say that, in general, all visitors to Canada must meet the requirements for temporary residence in Canada, as set out in Canada’s Immigration and Refugee Protection Act,” the ministry told the BBC by email. Behind the news: This isn’t the first time Canada has blocked researchers seeking to attend NeurIPS. Over 100 researchers bound for last year’s event in Montreal were held back. At the 2018 meeting of the G7, Wired confronted Prime Minister Justin Trudeau over whether Canada’s immigration policy undermined its goal to become an AI powerhouse. In September, the Partnership on AI suggested a new visa category for AI researchers.Why it matters: Conferences aren’t just opportunities tor share ideas. They’re opportunities for researchers to form important professional relationships. Policies like Canada’s keep developers from developing economies on the margins. The International Conference on Learning Representations (ICLR) is holding its 2020 conference in Addis Ababa, Ethiopia, because of the difficulty African researchers have entering places like the U.S., UK, and Canada.We’re thinking: We encourage conferences to schedule meetings in developing nations. A global research community benefits all nations. We need to make sure the rewards of AI — and, more broadly, science — are shared fairly. Pushing hard to make knowledge accessible to all is the ethical thing to do.

Image to article:
- Caption to image: Leaf stuck on a chain link fence
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Visas20SIZED.png
-----------------"
What is the primary purpose of the Reachy 2 robot developed by Hugging Face and Pollen Robotics?,The primary purpose of the Reachy 2 robot is for education and research in human-robot interaction in real-world settings.,"The primary purpose of the Reachy 2 robot, developed by Hugging Face and Pollen Robotics, is for education and research in human-robot interaction in real-world settings.","-----------------
Title: Hugging Face Acquires Pollen Robotics, Launches Reachy 2 Robot for Open-Source Research
Content: Hugging Face has made a name by providing open AI models. Now it’s providing an open robot. What’s new: Hugging Face acquired the French company Pollen Robotics for an undisclosed price. It plans to offer Pollen’s Reachy 2, a robot that runs on code that’s freely available under an Apache 2.0 license, for $70,000. How it works: Reachy 2 has two arms, gripper hands, and a wheeled base (optional). It’s designed primarily for education and research in human-robot interaction in real-world settings. Reachy 2 is programmable in Python and runs models from Hugging Face’s LeRobot library.It runs control software locally on a SolidRun Bedrock V3000 (a PC based on an AMD Ryzen Embedded V3000 processor) and processes AI in the cloud or on a local server.The robot responds to VR controllers including Meta Quest 2 and 3 as well as Pollen’s VR app.Its head senses the visual environment using a pair of cameras equipped with global shutters to capture fast-changing events and measures distances via an optical sensor. Its antennas are outfitted with microphones to capture sounds, and its torso senses distances using a depth camera. The base includes a lidar sensor to aid navigation.The body features 3D joints in the neck and wrists and 2D joints in the shoulders and elbows. Each arm can lift objects of up to 3 kilograms.A rechargeable, 24 volt battery provides around 10 hours of battery life. Behind the news: Last year, Remi Cadene, who worked on Tesla’s Optimus, joined Hugging Face to lead robotics projects. In May, he and his team rolled out the LeRobot open source robotics code library, which provides pretrained models, datasets, and simulators for reinforcement learning and imitation learning. In November, Nvidia announced a collaboration with Hugging Face to accelerate LeRobot’s data collection, training, and verification. Why it matters: Hugging Face’s acquisition of Pollen reflects an industry-wide investment in robots, notably humanoid robots, whose prices have been falling. Nvidia CEO Jensen Huang has called AI-enabled robotics a “multi-trillion dollar” opportunity. We’re thinking: AI-enabled robots are marching slowly toward what we hope will be breakthrough applications. Open-source systems are an important part of the trend!

Image to article:
- Caption to image: Person interacting with a humanoid robot using virtual reality headset and controllers.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F04%2Funnamed--78--1.png&w=3840&q=75
-----------------"
What is one important aspect to consider when preparing for a job interview according to the letter?,Ask the recruiter what to expect and practice answers to common interview questions.,"One important aspect to consider when preparing for a job interview, according to the letter, is to take time to review and practice answers to common interview questions, brush up key skills, and study technical materials to ensure they are fresh in your mind.","-----------------
Title: Andrew Ng on Fine Tuning Your AI Job Search
Content: Dear friends, I’ve devoted several recent letters to building a career in AI. In this one, I’d like to discuss some fine points of finding a job.The typical job search follows a fairly predictable path. Research roles and companies online or by talking to friends.Optionally, arrange informal informational interviews with people in companies that appeal to you.Either apply directly or, if you can, get a referral from someone on the inside.Interview with companies that give you an invitation.Receive one or more offers and pick one. Or, if you don’t receive an offer, ask for feedback from the interviewers, the human resources staff, online discussion boards, or anyone in your network who can help you plot your next move. Although the process may be familiar, every job search is different. Here are some tips to increase the odds you’ll find a position that supports your thriving and enables you to keep growing. Pay attention to the fundamentals. A compelling resume, portfolio of technical projects, and a strong interview performance will unlock doors. Even if you have a referral from someone in a company, a resume and portfolio will be your first contact with many people who don’t already know about you. Update your resume and make sure it clearly presents your education and experience relevant to the role you want. Customize your communications with each company to explain why you’re a good fit. Before an interview, ask the recruiter what to expect. Take time to review and practice answers to common interview questions, brush up key skills, and study technical materials to make sure they are fresh in your mind. Afterward, take notes to help you remember what was said. Proceed respectfully and responsibly. Approach interviews and offer negotiations with a win-win mindset. Outrage spreads faster than reasonableness on social media, so a story about how an employer underpaid someone gets amplified, whereas stories about how an employer treated someone fairly do not. The vast majority of employers are ethical and fair, so don’t let stories about the small fraction of mistreated individuals sway your approach. If you’re leaving a job, exit gracefully. Give your employer ample notice, give your full effort through your last hour on the job, transition unfinished business as best you can, and leave in a way that honors the responsibilities you were entrusted with. Choose who to work with. It’s tempting to take a position because of the projects you’ll work on. But the teammates you’ll work with are at least equally important. We’re influenced by people around us, so your colleagues will make a big difference. For example, if your friends smoke, the odds rise that you, too, will smoke. I don’t know of a study that shows this, but I’m pretty sure that if most of your colleagues work hard, learn continuously, and build AI to benefit all people, you’re likely to do the same. (By the way, some large companies won’t tell you who your teammates will be until you’ve accepted an offer. In this case, be persistent and keep pushing to identify and speak with potential teammates. Strict policies may make it impossible to accommodate you, but in my mind, that increases the risk of accepting the offer, as it increases the odds you’ll end up with a manager or teammates who aren’t a good fit.) Get help from your community. Most of us go job hunting only a small number of times in our careers, so few of us get much practice at doing it well. Collectively, though, people in your immediate community probably have a lot of experience. Don’t be shy about calling on them. Friends and associates can provide advice, share inside knowledge, and refer you to others who may help. I got a lot of help from supportive friends and mentors when I applied for my first faculty position, and many of the tips they gave me were very helpful. I know that the job search process can be intimidating. Instead of viewing it as a great leap, consider an incremental approach. Start by identifying possible roles and conducting a handful of informational interviews. If these conversations tell you that you have more learning to do before you’re ready to apply, that’s great! At least you have a clear path forward. The most important part of any journey is to take the first step, and that step can be a small one. Keep learning!

Image to article:
- Caption to image: Illustration shows an AI job searcher heading into the workforce.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F08%2FJOBSEARCH_Onward_Rerevise_1200px--1--1.jpg&w=3840&q=75
-----------------"
When is the free info session for the machine learning engineer program at FourthBrain?,"April 8, 2021","The free info session for the machine learning engineer program at FourthBrain is on April 8, 2021.","-----------------
Title: A MESSAGE FROM FOURTHBRAIN
Content: Become a machine learning engineer in only 16 weeks with FourthBrain! Meet our grads at a free info session on April 8, 2021. Ask them questions and learn more about the program. Register now

Image to article:
- Caption to image: The Batch - March 29th
- URL to image: https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20-%20March%2029th.png?upscale=true&name=The%20Batch%20-%20March%2029th.png
-----------------"
What are the two popular approaches to interpreting black-box algorithms mentioned in the context?,Local Interpretable Model-Agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP),The two popular approaches to interpreting black-box algorithms mentioned in the context are Local Interpretable Model-Agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP).,"-----------------
Title: Bias Goes Undercover
Content: As black-box algorithms like neural networks find their way into high-stakes fields such as transportation, healthcare, and finance, researchers have developed techniques to help explain models’ decisions. New findings show that some of these methods can be fooled.What’s new: Two popular approaches to interpreting black-box algorithms include Local Interpretable Model-Agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP). Researchers at Harvard and the University of California Irvine found a way to trick these methods to make a deliberately biased model appear to be unbiased.Key Insight: LIME and SHAP expose bias by showing how a model might react to small perturbations in its input. Statistical analysis shows that the distribution of perturbed inputs is distinct from that of the original inputs, as shown in the image above. A malicious actor can take advantage of the difference.How it works: LIME and SHAP use a linear model, which is highly explainable, to mimic a black-box model’s decision with respect to any given input sample. The explainable model is built by feeding perturbed inputs to the black-box model and measuring how its output changes in response to the perturbations. Once the model is built, we can rank the features most important to the decision, highlighting any bias in the original model. Dylan Slack, Sophie Hilgard, and their colleagues introduce a way to hide such bias from LIME and SHAP by learning to recognize data they’ve altered. The authors’ malicious system includes a distribution classifier, a biased model, and an unbiased LIME/SHAP model.The distribution classifier learns to categorize incoming data as either unperturbed or perturbed.The classification determines whether to use the biased or unbiased model. Unperturbed data are sent to the biased model. Perturbed data go to the unbiased LIME/SHAP model.If the classifier is accurate, LIME/SHAP will see, and thus explain, only the unbiased model’s output, not the biased model’s output.When the malicious system is deployed, the real-world data are not perturbed, so the system uses the biased model. Results: The authors created malicious systems to run COMPAS (a criminal recidivism dataset), Communities And Crime (a dataset of violent crime and demographic information), and the German Credit loan-application dataset. The embedded biased models deliberately abused discriminatory features such as race and gender, while the unbiased models used only features which were uncorrelated with discriminatory features. A malicious system biased on one feature (say, race) fooled LIME every time and SHAP in 85 percent of cases. A malicious system biased on two features fooled LIME over 90 percent of the time and SHAP 67 percent of the time.Why it matters: The authors’ approach highlights LIME’s and SHAP’s reliance on generating novel data. If these methods were to generate data more similar to the training data’s distribution, the method would fail. This may be a promising avenue for explainability research. Meanwhile, Duke University computer scientist Cynthia Rudin proposes avoiding black-box models in high-stakes situations. The AI community needs to hold a vigorous discussion about when such models are and aren’t appropriate.We’re thinking: If a major AI provider were caught using this technique, likely it would be vilified, which should provide some disincentive. We can imagine changes to LIME and SHAP that would counter a specific implementation, but this paper provides a dose of caution that checking for bias is not easy.

Image to article:
- Caption to image: Graph related to LIME and SHAP methods
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Bias20SIZED201.png
-----------------"
What are some key foundational skills necessary for a technical career in machine learning?,"Foundational machine learning skills, deep learning, math relevant to machine learning, and software development.","Key foundational skills necessary for a technical career in machine learning include understanding models such as linear regression, logistic regression, neural networks, decision trees, clustering, and anomaly detection. It's also important to grasp core concepts like bias/variance, cost functions, regularization, optimization algorithms, and error analysis. Additionally, knowledge of deep learning basics, relevant math (linear algebra, probability, and statistics), and software development skills are crucial.","-----------------
Title: How to Build a Career in AI: Learning Technical Skills
Content: Dear friends, Last week, I wrote about key steps for building a career in AI: learning technical skills, doing project work, and searching for a job, all of which is supported by being part of a community. In this letter, I’d like to dive more deeply into the first step.More papers have been published on AI than any person can read in a lifetime. So, in your efforts to learn, it’s critical to prioritize topic selection. I believe the most important topics for a technical career in machine learning are: Foundational machine learning skills. For example, it’s important to understand models such as linear regression, logistic regression, neural networks, decision trees, clustering, and anomaly detection. Beyond specific models, it’s even more important to understand the core concepts behind how and why machine learning works, such as bias/variance, cost functions, regularization, optimization algorithms, and error analysis.Deep learning. This has become such a large fraction of machine learning that it’s hard to excel in the field without some understanding of it! It’s valuable to know the basics of neural networks, practical skills for making them work (such as hyperparameter tuning), convolutional networks, sequence models, and transformers.Math relevant to machine learning. Key areas include linear algebra (vectors, matrices, and various manipulations of them) as well as probability and statistics (including discrete and continuous probability, standard probability distributions, basic rules such as independence and Bayes rule, and hypothesis testing). In addition, exploratory data analysis (EDA) — using visualizations and other methods to systematically explore a dataset — is an underrated skill. I’ve found EDA particularly useful in data-centric AI development, where analyzing errors and gaining insights can really help drive progress! Finally, a basic intuitive understanding of calculus will also help. In a previous letter, I described how the math needed to do machine learning well has been changing. For instance, although some tasks require calculus, improved automatic differentiation software makes it possible to invent and implement new neural network architectures without doing any calculus. This was almost impossible a decade ago.Software development. While you can get a job and make huge contributions with only machine learning modeling skills, your job opportunities will increase if you can also write good software to implement complex AI systems. These skills include programming fundamentals, data structures (especially those that relate to machine learning, such as data frames), algorithms (including those related to databases and data manipulation), software design, familiarity with Python, and familiarity with key libraries such as TensorFlow or PyTorch, and scikit-learn. This is a lot to learn! Even after you master everything in this list, I hope you’ll keep learning and continue to deepen your technical knowledge. I’ve known many machine learning engineers who benefitted from deeper skills in an application area such as natural language processing or computer vision, or in a technology area such as probabilistic graphical models or building scalable software systems.How do you gain these skills? There’s a lot of good content on the internet, and in theory reading dozens of web pages could work. But when the goal is deep understanding, reading disjointed web pages is inefficient because they tend to repeat each other, use inconsistent terminology (which slows you down), vary in quality, and leave gaps. That’s why a good course — in which a body of material has been organized into a coherent and logical form — is often the most time-efficient way to master a meaningful body of knowledge. When you’ve absorbed the knowledge available in courses, you can switch over to research papers and other resources.Finally, keep in mind that no one can cram everything they need to know over a weekend or even a month. Everyone I know who’s great at machine learning is a lifelong learner. In fact, given how quickly our field is changing, there’s little choice but to keep learning if you want to keep up. How can you maintain a steady pace of learning for years? I’ve written about the value of habits. If you cultivate the habit of learning a little bit every week, you can make significant progress with what feels like less effort. Keep learning!

Image to article:
- Caption to image: An illustration of a person holding a giant sheet with different ML subjects
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2022%2F07%2FColumnCloseup_Rev-LEARNNG-NoYOU_1200px-2.jpg&w=3840&q=75
-----------------"
What measures has Samasource taken to support its East African workforce during the Covid-19 pandemic?,Samasource has housed its East African workforce in internet-equipped hotels and resorts to allow them to work while maintaining social distance.,"Samasource has housed its East African workforce in internet-equipped hotels and resorts to support them during the Covid-19 pandemic. This arrangement allows employees to continue working while maintaining social distance, as many live in areas without internet connectivity. Over half of the company's workforce in East Africa agreed to this setup, where they each get a suite and have access to amenities like housekeeping and daily health checks. Workers can also enjoy watching wildlife from a terrace and participate in socially distanced group activities.","-----------------
Title: Goodbye Tourists, Hello Labelers
Content: Covid-19 has cost many workers their livelihood, but it has provided a lucky few on the lowest rungs of Africa’s machine learning industry with luxury suites.What’s new: Samasource, a data labeling company headquartered in San Francisco, California, is housing its East African workforce in hotels and resorts so they can continue to work while maintaining social distance, Wired reports.How it works: The pandemic prompted strict lockdowns in Kenya and Uganda, where Samasource employs some 2,000 workers. Many live in communities with no internet connectivity. So the company put up its workforce in four internet-equipped hotels that were vacant amid the coronavirus-driven collapse of tourism. Over half the company’s workforce in East Africa agreed to the arrangement. Employees each get a suite where they must remain throughout the workday. Housekeepers handle their laundry and nurses check their temperature daily.Wired profiled data-labeler Mary Akol (pictured in one of the photos above), one of 140 employees staying at the four-star Ole Sereni hotel, which overlooks Nairobi National Park.Workers there are allowed to leave their rooms at sunset to watch wildlife like rhinos, zebras, and giraffes from a terrace. They also engage in socially distanced group exercise. Akol has been teaching her co-workers salsa dancing — sans partners, of course. Behind the news: Several companies are providing jobs that help feed both the AI industry’s hunger for data and underserved communities. U.S.- and India-based iMerit has an all-female center in Kolkata that employs nearly 500 Muslim women who label computer vision data for companies like eBay, Microsoft, and TripAdvisor.Based in New York, Daivergent hires people on the autism spectrum to label data and helps neurodivergent people find tech jobs. Why it matters: Socially conscious outsourcing increases the tech industry’s talent pool by providing decent jobs to people who, because of geography, gender, race, or other factors, otherwise might be locked out.We’re thinking: The grocery industry’s Fair Trade labels help consumers distinguish between socially responsible employers and their wage-slashing competitors. A similar measure for AI would foster both growth and diversity.

Image to article:
- Caption to image: Series of pictures of hotels and resorts located in African countries
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AFRICA.gif
-----------------"
Which countries are experiencing the fastest growth in AI hiring according to the 2019 Artificial Intelligence Index?,"Australia, Brazil, Canada, and Singapore.","According to the 2019 Artificial Intelligence Index, AI hiring is growing fastest in Australia, Brazil, Canada, and Singapore.","-----------------
Title: Tracking AI’s Global Growth
Content: Which countries are ahead in AI? Many, in one way or another, and not always the ones you might expect. What’s new: The Stanford Institute for Human-Centered Artificial Intelligence published its 2019 Artificial Intelligence Index, detailing when, where, and how AI is on the rise. The authors also launched a Global AI Vibrancy Tool, making it easy to compare countries on a number of metrics.What it says: The report, guided by professor and entrepreneur Yoav Shoham, compiled data from all along the AI pipeline: college enrollment, journal citations, patent filings, conference attendance, job listings, and more. Some highlights: AI hiring is growing fastest in Australia, Brazil, Canada, and Singapore. The percentage of the U.S. workforce performing some sort of AI-related task grew from 0.26 percent to 1.32 percent.Argentina, Canada, Iran, and several European countries have relatively high proportions of women in the field.Total private investment in AI approached $40 billion in the U.S. last year, well ahead of runner-up China. Global private investment topped $70 billion, with startups contributing around half of that.Since 1998, the number of peer-reviewed AI research papers has more than quadrupled, and now accounts for 9 percent of published conference papers. Chinese authors published most of them. Nonetheless, U.S. papers are cited 40 percent more often than the global average. Behind the news: The AI Index is a product of the 100 Year Study on AI. Founded in 2014, the project tracks AI’s impact on jobs, education, national security, human psychology, ethics, law, privacy, and democracy.We’re thinking: William Gibson said it best: “The future is already here, it’s just not very evenly distributed.”

Image to article:
- Caption to image: Excerpt from 2019 Artificial Intelligence Index
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/AI20Index20ASPECT202.png
-----------------"
What is the name of the autonomous drone introduced by the Drone Racing League for the AlphaPilot competition?,RacerAI,The autonomous drone introduced by the Drone Racing League for the AlphaPilot competition is called RacerAI.,"-----------------
Title: Autonomous Drones Ready to Race
Content: Dear friends, I just replaced my two-year-old phone with a new one and figured out how to take long-exposure photos of Nova even while she’s asleep and the lights are very low. This piece of technology brought me a surprising amount of joy! I wrote about ethics last week, and the difficulty of distilling ethical AI engineering into a few actionable principles. Marie Kondo, the famous expert on de-cluttering homes, teaches that if an item doesn’t spark joy, then you should throw it out. When building AI systems, should we think about whether we’re bringing joy to others? This leaves plenty of room for interpretation. I find joy in hard work, helping others, increasing humanity’s efficiency, and learning. I don’t find joy in addictive digital products. I don’t expect everyone to have the same values, but perhaps you will find this a useful heuristic for navigating the complicated decision of what to work on: Is your ML project bringing others joy? This isn’t the whole answer, but I find it a useful initial filter. Keep learning! Pilots in drone races fly souped-up quadcopters around an obstacle course at 120 miles per hour. But soon they may be out of a job, as race organizers try to spice things up with drones controlled by AI.What’s new: The Drone Racing League, which stages contests to promote this so-called sport of the future, recently unveiled an autonomous flier called RacerAI. The new drone includes Nvidia’s Jetson AGX Xavier inference engine, four stereoscopic cameras, and propellers that deliver 20 pounds of thrust.What’s happening: RacerAI serves as the platform for AI models built by teams competing in AlphaPilot, a competition sponsored by the DRL and Lockheed Martin. 420 teams entered and tested their models on a simulated track.Virtual trials whittled the teams down to nine, which will compete in four races throughout fall 2019.Team USRG from Kaist University in South Korea won the first race on October 8. The second is scheduled for November 2 in Washington D.C.The series winner will take a $1 million prize. In early 2020, that model will face a top-rated human pilot for an additional $250,000 purse. Behind the news: Drone Racing League pilots use standardized drones built and maintained by the league, and train on the same simulator used to train RacerAI. Races are typically a mile long and take place in event spaces across the U.S. and Europe.Why it matters: Drone racing is fun and games, but the skills learned by autonomous racing models could be transferable to real-world applications like automated delivery.We’re thinking: A recent DRL video shows that current models have a way to go before they graduate from passing through rings to making high-speed maneuvers. Human pilots still have a significant edge — for now.

Image to article:
- Caption to image: Drone race
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/drone20gif20sized.gif
-----------------"
What is the main benefit of the robotic leg designed by researchers from the University of Utah for amputees?,The robotic leg helps amputees walk more naturally and step over obstacles in a natural way.,The main benefit of the robotic leg designed by researchers from the University of Utah is that it helps amputees walk more naturally by using machine learning to generate a human-like stride and assists wearers in stepping over obstacles in a natural way.,"-----------------
Title: News
Content: Dear friends, I spoke last week at the National Intergovernmental Audit Forum, a meeting attended by U.S. federal, state, and local government auditors. (Apparently some of the organizers had taken AI for Everyone.) Many attendees wanted to know how AI systems can be rolled out in a responsible and accountable way. Consider the banking industry. Many regional banks are under tremendous competitive pressure. How well they assess risk directly affects their bottom line, so they turn to credit scoring systems from AI vendors. But if they don’t have the technical expertise to evaluate such models, a hasty rollout can lead to unintended consequences like unfairly charging higher interest rates on loans to minority groups. For AI systems to enjoy smooth rollouts, we need to (a) make sure our systems perform well and pose minimal risk of unintended consequences and (b) build trust with customers, users, regulators, and the general public that these systems work as intended. These are hard problems. They require not just solving technical issues but also aligning technology with society’s values, and expectations. An important part of the solution is transparency. The open source software movement has taught us that transparency makes software better. And if making source code publicly available means that someone finds an embarrassing security bug, so be it! At least it gets fixed. With the rise of AI, we should similarly welcome third-party assistance, such as allowing independent parties to perform audits according to a well established procedure. That way, we can identify problems and fix them quickly and efficiently. After my presentation, the moderator asked me how auditors can avoid getting into adversarial relationships with AI vendors. Instead, we need to build collaborative relationships. By collaborating, we can help make sure the criteria used to judge our systems is reasonable and well specified. For instance, what are the protected groups we need to make sure our systems aren’t biased against? We can also better avoid “gotcha” situations in which our systems are assessed according to arbitrary, after-the-fact criteria. The AI community has a lot of work to do to ensure that our systems are fair, accountable, and reliable. For example, Credo AI (disclosure: a portfolio company of AI Fund, a sister organization to deeplearning.ai) is building tools that help audit and govern AI systems. Efforts like this can make a difference in designing and deploying AI systems that benefit all people. Keep learning! A prosthetic leg that learns from the user’s motion could help amputees walk more naturally.What’s new: Researchers from the University of Utah designed a robotic leg that uses machine learning to generate a human-like stride. It also helps wearers step over obstacles in a natural way.How it works: Rather than trying to recognize obstacles in the user’s path, the prosthesis relies on cues from the user’s body to tell it when something is in the way. Sensors in the user’s hip feed data a thousand times per second into a processing unit located in the unit’s calf. For instance, the way a user rotates their hip might tell the leg to tuck its knee to avoid tripping over an obstacle. A finite state machine (a logic-based controller) determines when and how to flex the knee based on angles of the ankle and thigh and the weight on the prosthetic foot.A second model called the minimum-jerk planner kicks in when the angle and speed of the artificial limb reach a certain point. It works to minimize sharp, sudden actions.The prosthesis applies reinforcement learning to adjust its motion as the user walks, using smoothness as the cost function. Behind the news: A new generation of AI-powered prosthetics could give amputees more control over robotic limbs. Researchers from the University of Michigan developed an open-source bionic leg that extrapolates knee and ankle movements by analyzing the wearer’s hip muscles, similar to the University of Utah’s method.A pair of Canadian students won Microsoft’s 2018 Imagine Cup with a camera-equipped prosthetic hand that uses computer vision to detect objects it is about to grasp and adjusts its grip accordingly.A mechanical arm from École polytechnique fédérale de Lausanne learns to associate common movements with cues from the user’s muscles. Why it matters: Battery-powered prostheses allow amputees to walk more easily, but they tend to stumble on unfamiliar terrain. This smart leg could provide them with smooth, hazard-free perambulation.We’re thinking: AI is helping people with the most basic human functions as well as the most abstract scientific problems.

Image to article:
- Caption to image: Man with prosthetic leg walking
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize2015.gif
-----------------"
What is the main advantage of the AdaBelief optimizer compared to traditional methods like Adam and SGD?,"AdaBelief is faster in optimization, allowing for quicker training and potentially more time for experimenting with different models.","The main advantage of the AdaBelief optimizer compared to traditional methods like Adam and SGD is its ability to adjust its step size based on the difference between the current gradient and the average of previous gradients. This allows AdaBelief to take larger steps when the landscape is relatively stable and smaller steps when it is changing, leading to faster optimization and training times while maintaining accuracy.","-----------------
Title: Striding Toward the Minimum
Content: When you’re training a deep learning model, it can take days for an optimization algorithm to minimize the loss function. A new approach could save time.What’s new: Juntang Zhuang and colleagues at Yale, University of Illinois at Urbana-Champaign, and University of Central Florida proposed AdaBelief, a more efficient variation on the popular Adam optimizer.Key insight: The popular optimization methods of stochastic gradient descent (SGD) and Adam sometimes take small steps, requiring more time to reach their destination, when they could take larger ones. Given a small learning rate and a point in a large, steep area of a loss function’s landscape, SGD takes small steps until the slope becomes steeper, while Adam’s steps become smaller as it progresses. In both scenarios, an ideal optimizer would predict that the slope is long and take larger steps.How it works: AdaBelief adjusts its step size depending on the difference between the current gradient and the average of previous gradients. Like Adam, AdaBelief moves along a function step by step and calculates an exponential moving average of the gradient, assigning exponentially smaller weights to previous gradients. Also like Adam, at each step, a steeper average gradient generally calls for a larger step size.Unlike Adam, AdaBelief treats the weighted average as a prediction of the gradient at the next step. If the difference between the prediction and the actual gradient is small, the function’s steepness probably isn’t changing much, and AdaBelief takes a relatively larger step. Conversely, if the difference is large, the landscape is changing, and AdaBelief decreases the step size. Results: The authors provide videos showing that, in experiments on functions with known minimums, AdaBelief was faster than both Adam and SGD with momentum (as shown above). To demonstrate their method’s accuracy, they compared AdaBelief to SGD, Adam, and other adaptive optimizers on tasks including image classification, image generation, and language modeling. AdaBelief basically matched SGD’s accuracy and exceeded that of all other adaptive optimizers. For instance, on ImageNet, AdaBelief increased a ResNet18’s highest top-1 accuracy, or accuracy of its best prediction, to 70.08 percent, on par with SGD’s 70.23 percent and 2 percent better than the best adaptive optimizers.Why it matters: Faster optimization means faster training, and that means more time to experiment with different models.We’re thinking: The authors’ video demonstrations suggest that AdaBelief could be a valuable alternative to Adam. However, they don’t supply any numbers that would make for a precise speed comparison. We look forward to the authors of the Deep Learning Optimizer Benchmark Suite, who have evaluated over a dozen optimizers in various tasks, running AdaBelief through its paces.

Image to article:
- Caption to image: Graphs comparing SGD + Momentum, Adam and AdaBelief
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/ezgif.com-gif-maker2061.gif
-----------------"
What technology does Bear Flag Robotics use to enable tractors to operate autonomously in agricultural fields?,"Bear Flag Robotics uses a combination of GPS tracking and sensor data, including Lidar, radar, and cameras, to enable tractors to navigate autonomously.","Bear Flag Robotics uses a combination of GPS tracking and sensor data, including lidar, radar, and cameras, to enable tractors to operate autonomously in agricultural fields. These technologies allow the tractors to navigate safely without a driver by seeing their surroundings and controlling steering, braking, and various towed implements. The system also gathers data on soil quality and learns the boundaries of a farmer's property.","-----------------
Title: Outstanding in the Field
Content: One of the world’s largest makers of farm equipment is doubling down on self-driving tractors.What’s new: John Deere agreed to pay $250 million for Bear Flag Robotics, a California startup that upgrades conventional tractors for full autonomy.How it works: Deere has offered GPS-enabled tractor guidance systems that aid a human driver for nearly two decades. Bear Flag has adapted self-driving technology developed by the automotive industry to help tractors roam agricultural fields safely without a driver. Tractors equipped with Bear Flag tech navigate using a combination of GPS tracking and sensor data. Lidar, radar, and cameras enable the vehicles to see their surroundings. Actuator systems control steering, braking, and a variety of towed implements.The system is adapted for farm driving. For instance, the vision algorithm distinguishes between fallen branches that can be driven over and trees that should be avoided.The sensors also gather data on the quality of the soil tilled in the tractor’s wake. The information can help growers fine-tune their use of pesticides, herbicides, and fungicides, resulting in reductions of up to 20 percent, the company said. The system learns the boundaries of a farmer’s property during an initial drive-through. It also identifies roads, waterways, and other obstacles. It can upload the resulting map to a fleet of tractors for remote control and monitoring.Behind the news: Deere has been pursuing AI capabilities for several years. In 2017, it acquired Blue River Technology, a California-based startup that makes weed-killing robots. The following year, it launched a program to partner with promising startups including some that use deep learning.Why it matters: In addition to helping the farmers deal with a long-running labor shortage, AI-driven equipment could help increase their productivity and limit environmental impacts such as pesticide runoff.We’re thinking: Self-driving cars aren’t yet commonly used on public roads, but the technology appears to be good enough for commercial use in constrained environments like farms.

Image to article:
- Caption to image: Sequence of images showing tractors working on fields
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/08/ezgif.com-gif-maker--4--2.gif
-----------------"
What did the authors find about the accuracy of GPT-style models in few-shot learning when using different prompt formats?,The accuracy prompted by the format selected according to cross-validation was only marginally above the mean and significantly below the accuracy of the best format.,"The authors found that the accuracy of GPT-style models in few-shot learning is significantly influenced by the prompt format used. They discovered that the heralded success of few-shot learning may depend on a well-engineered prompt. When comparing different prompt formats, the accuracy prompted by the format selected through cross-validation was only marginally above the mean and significantly below the accuracy of the best format. For example, for the largest model (GPT-3 with 175 billion parameters), the format chosen by cross-validation scored about 55 percent, while the best format achieved about 60 percent accuracy.","-----------------
Title: Ask Me in a Different Way
Content: Pretrained language models like GPT-3 have shown notable proficiency in few-shot learning. Given a prompt that includes a few example questions and answers (the shots) plus an unanswered question (the task), such models can generate an accurate answer. But there may be more to getting good results.What’s new: Ethan Perez, Douwe Kiela, and Kyunghyun Cho subjected GPT-style language models to a test they call true few-shot learning. They found that the heralded few-shot success may depend on a well engineered prompt. The authors are based at New York University, Facebook, and CIFAR, respectively.Key insight: Training a machine-learning model typically requires a validation set to tune hyperparameters such as the learning rate. For GPT-style models, those hyperparameters include the prompt format. In few-shot learning with a pretrained model, the prompt typically contains a handful of examples. However, researchers often experiment extensively to find a prompt format that yields accurate responses. This amounts to stacking the deck in the model’s favor, and without it, such models can’t perform so well.How it works: The authors evaluated four sizes of GPT-3, four sizes of GPT-2, and DistilGPT-2. They tested prompt formats from LAMA, a benchmark that comprises factual statements in a variety of formats, and LPAQA, which contains LAMA statements translated from English into a different language and back. LAMA provides statements in 41 categories, such as “X was born in Y,” where X is a personal name and Y is a place, and “X was created by Y,” where X is the name of a company and Y is the name of a product. It presents each statement in an average of 12 formats. For instance, “X was created by Y” is also formatted “X is developed by Y” and “X is being developed by Y.”The authors assembled prompts made of five such statements, all in the same category and format, in which the last word was missing, such as, “The iPhone is being developed by _.” The missing word is, of course, “Apple.” They provided versions of these prompts in all 120 possible orders of the five statements, always with the final word missing, prompting the model to fill in the blank.They used cross-validation to find the prompt format that, given four complete and one incomplete examples, prompted the best performance on average across all formats and categories.For each model, they compared performance prompted by the best format according to cross-validation, the format associated with the highest accuracy on the test set, and the mean accuracy on the test set across all formats and categories. Results: For all models tested, the accuracy prompted by the format selected according to cross-validation was only marginally above the mean and significantly below the accuracy of the best format. For instance, for the largest model (GPT-3 with 175 billion parameters), the format chosen by cross-validation scored about 55 percent, mean accuracy was about 54 percent, and the accuracy of the best format was about 60 percent.Why it matters: Previous claims of few-shot learning in GPT-style models left out an important variable: the size of the dataset used to pick a good format. Choosing among 12 prompt formats boosted accuracy by around 5 percent; choosing among a larger set of formats could make a bigger difference. If researchers don’t include all the information that went into the results they report, follow-up studies are unlikely to duplicate their work.We’re thinking: We like prompt engineering that gets things done on time. We’re less enamored with prompt engineering that muddies the water around few-shot learning.

Image to article:
- Caption to image: Animation showing example questions and answers obtained by a pretrained language model
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/09/TRUEFEWv5--1-.gif
-----------------"
What is the main advantage of the TReCS system compared to earlier methods in generating images from verbal descriptions?,"TReCS addresses the issue of producing blank spots by translating descriptive words into object labels and distinguishing between foreground and background, resulting in higher quality and better-aligned images.","The main advantage of the TReCS system compared to earlier methods is its ability to synthesize complex scenes from verbal descriptions more accurately by using mouse traces to translate descriptive words into object labels, distinguishing foreground from background, and arranging masks on a canvas. This approach results in higher quality and better-aligned images, as evidenced by judges preferring TReCS' output over a state-of-the-art text-to-image generator.","-----------------
Title: Pictures From Words and Gestures
Content: A new system combines verbal descriptions and crude lines to visualize complex scenes. What’s new: Google researchers led by Jing Yu Koh proposed Tag-Retrieve-Compose-Synthesize (TReCS), a system that generates photorealistic images by describing what they want to see while mousing around on a blank screen.Key insight: Earlier work proposed a similar system to showcase a dataset, Localized Narratives, that comprises synchronized descriptions and mouse traces captured as people described an image while moving a cursor over it. That method occasionally produced blank spots. The authors addressed that shortcoming by translating descriptive words into object labels (rather than simply matching words with labels) and distinguishing foreground from background.How it works: The Local Narratives dataset provides an inherent correspondence between every word in a description and a mouse trace over an image. TReCS uses this correspondence to translate words into labels of objects that populate a scene. The authors trained the system on the portion of Localized Narratives that used images in COCO and tested it on the portion that used Open Images. Given a description, a BERT model assigned an object label to each word in the description. The authors obtained ground-truth labels by matching the mouse traces for each word to object segmentation masks (silhouettes) for the images described. Then they fine-tuned the pretrained BERT to, say, attach the label “snow” to each of the words in “skiing on the snow.”For each label assigned by BERT, the system chose a mask from a similar image (say, a photo taken in a snowy setting). The authors trained a cross-modal dual encoder to maximize the similarity between a description and the associated image, and to minimize the similarity between that description and other images. On inference, given a description, the authors used the resulting vectors to select the five most similar training images.The system used these five images differently for foreground and background classes (an attribute noted in the mask dataset). For foreground classes such as “person,” it retrieved the masks with the same label and chose the one whose shape best matched the label’s corresponding mouse trace. For background classes such as “snow,” it chose all of the masks from the image whose masks best matched the labels and combined shape of the corresponding mouse traces.The authors arranged the masks on a blank canvas in the locations indicated by the mouse traces. They positioned first background and then foreground masks, reversing the order in which they were described. This put the first-mentioned object in front.A generative adversarial network learned to generate realistic images from the assembled masks. Results: Five judges compared TReCS’ output with that of AttnGAN, a state-of-the-art, text-to-image generator that did not have access to mouse traces. The judges preferred TReCS’ image quality 77.2 percent to 22.8 percent. They also preferred the alignment of TReCS output with the description, 45.8 percent to 40.5 percent. They rated both images well aligned 8.9 percent of the time and neither image 4.8 percent of the time.Why it matters: The authors took advantage of familiar techniques and datasets to extract high-level visual concepts and fill in the details in a convincing way. Their method uncannily synthesized complex scenes from verbal descriptions (though the featured example, a skier standing on a snowfield with trees in the background, lacks the railing and mountain mentioned in the description). We’re thinking: Stock photo companies may want to invest in systems like this. Customers could compose photos via self-service rather than having to choose from limited options. To provide the best service, they would still need to hire photographers to produce raw material.

Image to article:
- Caption to image: Tag-Retrieve-Compose-Synthesize (TReCS)
- URL to image: https://dl-staging-website.ghost.io/content/images/2022/10/pictures-from-words-and-gestures-1.gif
-----------------"
What is the name of the machine learning system unveiled by Physical Intelligence that enables robots to perform household chores?,π0 (pronounced 'pi-zero'),The machine learning system unveiled by Physical Intelligence that enables robots to perform household chores is called π0 (pronounced 'pi-zero').,"-----------------
Title: π0, A Machine Learning System for Household Robotics
Content: A new generation of robots can handle some household chores with unusual skill. What’s new: Physical Intelligence, a startup based in San Francisco, unveiled π0 (pronounced “pi-zero”), a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables. The company also announced $400 million in investments from OpenAI, Jeff Bezos, and several Silicon Valley venture capital firms. How it works: π0 is a version of the pretrained PaliGemma vision-language model that has been modified for flow matching. (Flow matching is similar to diffusion, in which a model learns to remove noise from inputs to which noise has been added, and ultimately generates output by removing noise from an input of pure noise). A user supplies a text command, and the robot uses its sensor inputs to remove noise from a pure-noise action embedding to generate an appropriate action. PaliGemma comprises SigLIP, a vision transformer that turns images into embeddings; a linear layer that adapts the image embeddings to serve as input for the pretrained large language model Gemma; and Gemma, which estimates the noise to be removed from a robot action embedding to which noise has been added.The authors modified PaliGemma as follows: (i) They adapted it to accept embeddings that represent the robots’ state and previous actions, and to generate embeddings that represent the noise to be removed from noisy robot actions. (ii) They added a vanilla neural network to the input to turn the current timestep into an embedding. (iii) They modified Gemma to be a mixture-of-experts model: One expert, or subset of weights, is the pretrained weights, which process image and text embeddings. The other is a new set of weights that process robot action embeddings.They pretrained π0 to remove noise from action embeddings. (Since π0 produces embeddings of the noise to be removed, removing that noise is as simple as adding the two embeddings.)Training data included the Open X-Embodiment Dataset and a proprietary dataset of 10,000 hours of robotic states (for instance, current positions of a robot’s joints), actions (for instance, motions of the robot’s joints), and an associated language command. The proprietary dataset included data collected from seven different robots (such as a single stationary robot arm to two robot arms mounted on a mobile base) and 68 tasks (for example, folding laundry, making coffee, or bussing a table).After pretraining, the authors fine-tuned π0 to remove noise from action tokens in 15 further tasks, some of which were not represented in the pretraining set. These tasks improved the model’s ability to follow more detailed instructions and perform multi-stage tasks such as packing food into a to-go box.At inference, given the robot’s camera view of the surrounding scene, SigLip embeds the images. A linear layer projects the resulting embeddings to fit Gemma’s expected input size and data distribution. Given the images, text command, robot’s state, current timestep, and 50 noisy action tokens (starting with pure noise), Gemma iteratively removes noise. To complete longer tasks, the process repeats: The robot takes more images of the surrounding scene and retrieves the robot’s state, which π0 uses to generate further actions. Results: π0 outperformed the open robotics models OpenVLA, Octo, ACT, and Diffusion Policy, all of which were fine-tuned on the same data, on all tasks tested, as measured by a robot’s success rate in completing each task. For example, using a single robotic arm to stack a set of bowls of four sizes, π0 completed about 100 percent on average. Diffusion Policy completed about 55 percent, ACT about 45 percent, and OpenVLA and Octo below 10 percent. Across all tasks, π0 completed about 80 percent on average, while Diffusion Policy completed about 35 percent on average. Yes, but: The robot occasionally makes mistakes. In one video, it puts too many eggs into a carton and tries to force it shut. In another, it throws a container off a table instead of filling it with items. Behind the news: Commercial robotics appears to be undergoing a renaissance. Skild raised $300 million to develop a “general-purpose brain for robots.” Figure AI secured $675 million to build humanoid robots powered by multimodal models. Covariant, which specializes in industrial robotics, licensed its technology to Amazon. (Disclosure: Andrew Ng is a member of Amazon's board of directors). OpenAI renewed its robotics effort after dismantling its robotics department in 2020. Why it matters: Robots have been slow to benefit from machine learning, but the generative AI revolution is driving rapid innovations that make them much more useful. Large language models have made it possible to command robots using plain English. Meanwhile, the team at Physical Intelligence collected a dataset of sufficient size and variety to train the model to generate highly articulated and practical actions. Household robots may not be right around the corner, but π0 shows that they can perform tasks that people need done. We’re thinking: One of the team members compared π0 to GPT-1 for robotics — an inkling of things to come. Although there are significant differences between text data (which is available in large quantities) and robot data (which is hard to get and varies per robot), it looks like a new era of large robotics foundation models is dawning.

Image to article:
- Caption to image: Robotic arms collaborating to fold a red garment on a table.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2Funnamed--33-.gif&w=3840&q=75
-----------------"
What action did Amazon and Microsoft take regarding their face recognition technology in response to protests over police killings of unarmed Black people in the U.S.?,"Amazon imposed a one year moratorium on licensing its Rekognition technology to police departments, and Microsoft announced a similar hiatus.",Not found in context,"-----------------
Title: Amazon Prepares for a Crackdown
Content: Amazon is writing what it hopes will become U.S. law governing use of face recognition technology. What happened: At a press event announcing new features for Amazon’s Alexa smart-home service, Jeff Bezos told a reporter that his company’s lawyers are drafting a statutory framework to guide what he views as an inevitable federal crackdown on face recognition. Amazon sells the cloud-based face recognition service Rekognition, whose use by law enforcement agencies has raised alarm among civil liberties advocates.What it says: The company has released no details about the model legislation in progress. However, in February, Amazon VP of Global Public Policy Michael Punke published a blog that could provide clues to the company’s aims. Face recognition should be used in accordance with existing laws, Punke writes in the post proposing ethical guidelines for the technology. He points out that the U.S. Constitution’s Fourth Amendment and Civil Rights Act of 1964 explicitly outline an individual’s right to privacy and freedom from discrimination.Law enforcement groups, government agencies, and businesses using face recognition should be held to high standards of transparency, the post says.Law enforcement should be allowed to use the technology only to narrow down groups of suspects, and only when a model is at least 99 percent confident in its prediction. Models should never be used as the final arbiter of a person’s guilt or innocence. Behind the news: Face recognition’s rapid proliferation has spawned a widespread backlash in the U.S. cities. San Francisco and Oakland, California, and Somerville, Massachusetts, have banned the technology. California’s legislature is considering a statewide ban. Several bills restricting its use are wending their way through Congress, and two representatives have vowed to propose further legislation.We’re thinking: Punke’s guidelines are sound, and Amazon is well situated to understand how the technology could be abused. When industries propose their own regulations, though, legislators need to take special care to make sure any resulting laws benefit society as a whole.

Image to article:
- Caption to image: Amazon Rekognition screen capture
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-09-3020at202.08.4320PM.png
-----------------"
What percentage of suspects did independent researchers find the face recognition system in London misidentified during recent trials?,81 percent,Independent researchers found that the face recognition system in London misidentified 81 percent of suspects it flagged during recent trials.,"-----------------
Title: Nowhere to Hide
Content: Real-time face recognition has become standard operating procedure for cops in a few cities, in both authoritarian and democratic countries.What’s new: After years of trials, police departments in Moscow and London are using face recognition to scan the streets for suspected criminals.How it works: Systems in both cities connect to pre-existing closed-circuit television networks. Enforcers in Moscow aim to deploy the tech city-wide, according to The Verge. So far, though, they’re using only a fraction of the city’s tens of thousands of cameras. London plans a more limited rollout in popular shopping and tourist areas. Moscow paid NTechLabs, a homegrown company, $3.2 million to license its technology. The company maintains a watch list of suspects and notifies authorities if it finds a match.Prior to serving the law enforcement market, NTechLabs offered a consumer app for matching pictures of people to their social media profile. Its FindFace app made headlines in 2016 when internet trolls used it to dox sex workers.London’s Metropolitan Police licenses face recognition tech from NEC. It runs cameras for five to six hours at a time as it tries to match watch lists of suspected violent criminals and child sex traffickers. Why it matters: Face recognition technology is becoming routine for police forces around the globe. It has been used to catch a murderer in Chongqing, helped stop street crime in New York City, and figured in 30 percent of solved cases in one small U.S. city.Yes, but: Independent researchers evaluating recent trials in London found that the system misidentified 81 percent of suspects it flagged. The police department contests those numbers, saying its own studies show only one in 1,000 false positives.We’re thinking: Law enforcement agencies worldwide need thoughtfully designed and clearly worded regulatory guidance so they can use these tools without overstepping civil liberties.

Image to article:
- Caption to image: Security camera next to the Big Ben in London
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/London20ASPECT.png
-----------------"
What does Eric Topol believe is a significant ethical concern regarding algorithms in medicine?,Human bias that finds its way into algorithms is a significant ethical concern that needs extensive review and scrutiny.,"Eric Topol believes that human bias finding its way into algorithms is a significant ethical concern in medicine, as it can exacerbate existing health inequalities.","-----------------
Title: A Visionary Doctor Prescribes AI
Content: Eric Topol is one of the world’s leading advocates for AI in medicine. He believes the technology can not only liberate physicians from the growing burden of clerical work, but also synthesize layers of patient data — behavioral, genomic, microbiomic, and so on — into truly personalized healthcare. A cardiologist and geneticist at Scripps Research Institute in Southern California, he is the author of Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again. Below he shares his insights into the fusion of AI and medicine and advice for machine learning engineers who want to get involved.The Batch: Let’s start with the topic on everyone’s mind: Where do you see AI’s greatest potential in addressing the Covid-19 pandemic?Topol: One thing that’s been overlooked is the ability to develop and validate algorithms for at-home monitoring. We don’t want everyone who has Covid-19 symptoms to go to the hospital. On the other hand, some people who catch Covid-19 have sudden demise, and it’s hard to predict. If we could tell who’s safe to monitor at home, that would be great help in managing this epidemic around the world.The Batch: You’re concerned with the depersonalization of doctor-patient relationships. How can AI help?Topol: Four words: the gift of time. Clinicians spend too much of their time being data clerks. There shouldn’t be any need for a screen and a keyboard to see a patient. Entering notes into the medical record should be done by AI.The Batch: Researchers have had experimental success interpreting medical images. Yet these innovations haven’t had much impact on clinical practice. What’s the holdup?Topol: The medical community feels threatened that the machines will encroach on their lives. Also, some companies working on things like this have proprietary algorithms and don’t publish their data, so there’s a lack of transparency. They get their FDA clearance based on retrospective studies and use the same data over and over, because there aren’t many large, annotated medical datasets. We need prospective studies based on real-world patients in multiple real-world clinical settings. And we need more randomized trials — there have been only six or seven of those.The Batch: If you could collect any data you wanted for everyone in the world, what would it be, and for what AI task?Topol: That’s easy: We need a planetary health system. We’d have multilevel data for every person, and each person would teach the rest of their species about preventing and managing illnesses using nearest neighbor analysis and other tools of AI. It’s possible now, but it requires an international commitment. I wrote about this with my colleague Kai-Fu Lee in an article called “It Takes a Planet.” The Batch: How can we build a planetary health system that protects data privacy and security?Topol: The tools are in front of us now. We can use federated and homomorphic computing. No country has to hand their data over. The algorithms can be used at the locale.The Batch: Much of the AI community is deeply concerned about making sure the technology is used ethically. What should AI practitioners keep in mind in that regard?Topol: Anything that exacerbates the very significant health inequalities that exist today is not acceptable. Human bias that finds its way into algorithms is a significant ethical concern that needs extensive review and scrutiny. And that’s not all. Algorithms in medicine need to be under constant surveillance because if an algorithm is hacked, it could hurt a lot of people.The Batch: What advice would you give machine learning engineers who want to make a positive impact in medicine?Topol: We’re still in the early phase. We need more interdisciplinary or transdisciplinary efforts between clinicians and AI practitioners. We need more large, annotated datasets, or to use self-supervised learning that preempts the need for them. We need to go to a higher validation plane, however we get there. Then we’ll be able to take advantage of this extraordinary opportunity to transform medicine and return the human essence that has been largely lost.

Image to article:
- Caption to image: Eric Topol illustration
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/7-interview-with-Eric-Topol201.png
-----------------"
What are the names of the latest models launched by Anthropic in their Claude 4 series?,Claude 4 Sonnet 4 and Claude Opus 4,The latest models launched by Anthropic in their Claude 4 series are Claude 4 Sonnet 4 and Claude Opus 4.,"-----------------
Title: A MESSAGE FROM DEEPLEARNING.AI AND SNOWFLAKE
Content: We’re featured partners of Snowflake’s Dev Day 2025, a full day for AI and data practitioners to explore cutting-edge demos, make valuable contacts, and hear from top voices in the field (including Andrew Ng). See you on June 5! Register here Anthropic continued its tradition of building AI models that raise the bar in coding tasks. What’s new: Anthropic launched Claude 4 Sonnet 4 and Claude Opus 4, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit. Input/output: Text, images, PDF files in (up to 200,000 tokens); text out (Claude Sonnet 4 up to 64,000 tokens, Claude Opus 4 up to 32,000 tokens)Features: Parallel tool use including computer use, selectable reasoning mode with visible reasoning tokens, multilingual (15 languages)Performance: Ranked Number One in LMSys WebDev Arena, state-of-the-art on SWE-bench and Terminal-benchAvailability/price: Anthropic API, Amazon Bedrock, Google Cloud Vertex AI. Claude Sonnet 4 $3/$15 per million input/output tokens, Claude Opus 4 $15/$75 per million input/output tokensUndisclosed: Parameter counts, specific training methods and datasets How it works: The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to be helpful, honest, and harmless according to human and AI feedback. The models make reasoning tokens visible within limits. For especially lengthy chains of thought, an unspecified smaller model summarizes reasoning tokens.Given local file access, Claude Opus 4 can create and manipulate files to store information. For instance, prompted to maintain a knowledge base while playing a Pokémon video game, the model produced a guide to the game that offered advice such as, “If stuck, try OPPOSITE approach” and “Change Y-coordinate when horizontal movement fails.” Results: Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests. On SWE-bench Verified, which tests the model’s ability to solve software issues from GitHub, Claude Opus 4 succeeded 72.5 percent of the time, and Claude Sonnet 4 succeeded 72.7 percent of the time. The next best model, OpenAI o3, succeeded 70.3 percent of the time.Terminal-bench evaluates how well models work with the benchmark’s built-in agentic framework to perform tasks on a computer terminal. Claude Opus 4 succeeded 39.2 percent of the time and Claude Sonnet 4 succeeded 33.5 percent of the time, whereas the closest competitor, OpenAI GPT 4.1, succeeded 30.3 percent of the time. Using Claude Code as the agentic framework, Claude Opus 4 succeeded 43.2 percent of the time and Claude Sonnet 4 succeeded 35.5 percent of the time. Why it matters: The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including a Tetris clone built in one shot and a seven-hour stint refactoring Rakutan’s open-source code base. We’re thinking: Prompting expert @elder_plinius published a text file that is purported to be Claude 4’s system prompt and includes some material that does not appear in Anthropic’s own publication of the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning.

Image to article:
- Caption to image: AI model performance comparison chart: Claude Opus 4, Sonnet 4, Sonnet 3.7, OpenAI o3, GPT-4.1, and Gemini 2.5 Pro.
- URL to image: https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png
-----------------"
What is the name of the voice recognition tool announced by Intel that aims to moderate voice chat automatically?,Bleep,The name of the voice recognition tool announced by Intel that aims to moderate voice chat automatically is 'Bleep'.,"-----------------
Title: Haters Gonna [Mute]
Content: Dear friends, Last Sunday was my birthday. That got me thinking about the days leading to this one and those that may lie ahead.As a reader of The Batch, you’re probably pretty good at math. But let me ask you a question, and please answer from your gut, without calculating.How many days is a typical human lifespan? 20,000 days100,000 days1 million days5 million days When I ask friends, many choose a number in the hundreds of thousands. (Many others can’t resist calculating the answer, to my annoyance!)When I was a grad student, I remember plugging my statistics into a mortality calculator to figure out my life expectancy. The calculator said I could expect to live a total of 27,649 days. It struck me how small this number is. I printed it in a large font and pasted it on my office wall as a daily reminder. That’s all the days we have to spend with loved ones, learn, build for the future, and help others. Whatever you’re doing today, is it worth 1/30,000 of your life?Let’s make every day count. Keep learning! P.S. Don’t worry about me. I’m healthy and plan to stick around for awhile. P.P.S. A huge thank-you to everyone who responded to my earlier online note about my birthday! ❤️ A new tool aims to let video gamers control how much vitriol they receive from fellow players. What’s new: Intel announced a voice recognition tool called Bleep that the company claims can moderate voice chat automatically, allowing users to silence offensive language. The system is in beta-test and scheduled for release later this year. How it works: Chip maker Intel worked with Spirit AI, which develops technology for content moderation, to let users of voice chat fine-tune how much of specific types of offensive language can reach their ears. Bleep combines speech detection technology with Spirit’s flagship product, which determines whether a phrase constitutes harassment in the context of surrounding chatter.The system classifies offensive speech in nine categories including misogyny, sexually explicit language, and anti-LGBTQ hate speech. Users can opt to filter out none, some, most, or all content in any category. For a tenth category called N-word, the system offers an on/off switch.It runs on Windows PCs and, since it interacts directly with Windows’ audio controls, it can work with a variety of voice-chat apps. Behind the news: ToxMod also aims to moderate video game voice chat and provides a dashboard for human moderators to track offensive speech across servers. Hive’s system is designed to moderate audio, video, text, and images. Its customers include Chatroulette, which uses Hive’s technology to help users avoid unwanted nudity. Two-Hat’s text-moderation system detects efforts to subvert moderation by, say, intentionally misspelling slurs and other potentially offensive language. Why it matters: There’s a clear need for tools that help people enjoy networked communications without being targeted by abuse. Twenty-two percent of U.S. online gamers stopped playing certain games after experiencing verbal harassment, according to a survey by the Anti-Defamation League. We’re thinking: For those whose first thought is, “Censorship!,” note that users will control this auto-moderation capability locally. At the same time, there’s a fine line between blocking harassment and shutting out perspectives we don't currently share. In an ideal world, players would take it upon themselves to keep their conversations civil. Until that day comes, AI will play a valid — if worrisome at times — role.

Image to article:
- Caption to image: Voice recognition tool ""Bleep"" working
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/05/image-6.png
-----------------"
What was the percentage reduction in deaths among high-risk patients due to the deep learning model developed by Chin-Sheng Lin and colleagues?,31 percent,The deep learning model developed by Chin-Sheng Lin and colleagues reduced deaths among high-risk patients by 31 percent.,"-----------------
Title: Deep Learning Model Identifies High-Risk Patients from EKG Readings
Content: A deep learning model significantly reduced deaths among critically ill hospital patients. What’s new: A system built by Chin-Sheng Lin and colleagues at Taiwan’s National Defense Medical Center analyzed patients’ heart signals and alerted physicians if it detected a high risk of death. It reduced deaths of high-risk patients by 31 percent in a randomized clinical trial. How it works: Researchers trained a convolutional neural network, given an electrocardiogram (a measurement of the heart’s electrical activity), to estimate a risk score. The system compares a patient’s risk score against those of other patients. Scores that rank in the 95th percentile or higher are considered high risk of death within 90 days. The authors tested the system on 16,000 patients at two hospitals for 90 days.Patients in the experimental group were measured by electrocardiograms, which were fed to the system. If the system identified a high-risk patient, it alerted their attending physician.The control group received typical care. The model monitored their electrocardiograms, but physicians saw its output only after the trial was over. Results: 8.6 percent of patients in the control group and 8.9 percent of patients in the experimental group raised a high-risk alert during the trial. In the experimental group, 16 percent of high-risk patients died; in the control group, 23 percent of high-risk patients died. Overall, in the experimental group, 3.6 percent of patients died; in the control group, 4.3 percent of patients died. The model was trained to predict mortality from all causes, but it showed unusually strong predictive capability for heart-related deaths. Examining causes of death, the authors found that 0.2 percent of patients in the experimental group died from heart-related conditions such as cardiac arrest versus 2.4 percent in the control group.Behind the news: Hospitals use AI-powered alert systems to identify patients in need of urgent medical attention. Such systems monitor emergency room patients for sepsis, predict whether those patients need intensive care, and predict the risk that discharged patients will require further care. They help hospitals to allocate resources by directing attention where it’s needed most urgently.Why it matters: It’s rare for any kind of medical intervention to reduce mortality in a subgroup by 31 percent. The authors speculate that the system not only helped direct attention to patients urgently in need of attention but also may have identified electrocardiogram features that doctors typically either don’t understand well or can’t detect. We’re thinking: This relatively low-cost AI system unambiguously saved lives over three months at different hospitals! We look forward to seeing it scale up.

Image to article:
- Caption to image: Heart-Risk Model Saves Lives: Deep learning model identifies high-risk patients from EKG readings
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F05%2Funnamed---2024-05-29T152617.317-1.png&w=3840&q=75
-----------------"
What technology is being used by farmers in Australia and the U.S. to pollinate greenhouse tomatoes?,Robots from Israeli startup Arugga Farming.,"Farmers in Australia and the U.S. are using robots from Israeli startup Arugga Farming to pollinate greenhouse tomatoes. These robots are equipped with cameras, vision algorithms, and air compressors to recognize flowers ready to produce fruit and blast them with air to release pollen.","-----------------
Title: To Bee or Not to Bee
Content: Insects that spread pollen to fruiting plants are in trouble. A possible alternative: Robots.What’s new: Farmers in Australia and the U.S. are using robots from Israeli startup Arugga Farming to pollinate greenhouse tomatoes, The Wall Street Journal reported.How it works: The system is designed for growing tomatoes, which self-pollinate when their pollen is stirred up by the beating of insect wings. Robots equipped with cameras, vision algorithms, and air compressors wheel themselves between rows of plants. When they recognize a flower that’s ready to produce fruit, they blast it with air to release its pollen. The company trained the computer vision system using tens of thousands of photos of tomato flowers shot in multiple greenhouses under a variety of lighting conditions.U.S. greenhouse grower AppHarvest tested the system. It found that the plants pollinated by robots produced a harvest comparable to those pollinated by bumblebees and much larger than those pollinated by hand.Costa Group Holdings, an Australian farming company that grows crops in vertical greenhouse arrays, recently tested two of the robots in a 25-acre facility. It plans to add more, aiming for a total of around 30. Behind the news: A number of other companies are using AI-enabled robots to pollinate plants. Edete Precision Technologies has had success with almonds, and Bumblebee AI hopes to pollinate avocados, kiwis, and cocoa. Developed at West Virginia University, a robot called BrambleBee aims to pollinate blackberries, raspberries, and brambleberries.Why it matters: Robotic pollinators may prove to be an important technology outside of greenhouses. Climate change and habitat loss are ravaging Earth’s insect populations including bees. Meanwhile, such machines could be helpful to farmers: Bees are expensive to rent, they can spread plant diseases, and importing them is restricted in places such as Australia.We’re thinking: These robots are sure to generate a buzz.

Image to article:
- Caption to image: Video showing how robots from Israeli startup Arugga Farming work
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/08/ARUGGA.gif
-----------------"
What were the names of the hybrid recipes developed by Google engineers using AI?,Cakie and breakie,The hybrid recipes developed by Google engineers using AI were called a 'cakie' and a 'breakie'.,"-----------------
Title: Cake + Cookie = Cakie
Content: AI may help revolutionize the human diet – or dessert, at least. What’s new: Google applied AI engineer Dale Markowitz and developer advocate Sara Robinson trained a model to predict whether a recipe is a bread, cake, or cookie. They brightened the recent holiday season by using it to develop novel hybrid recipes. How it works: The engineers conceived their project to demonstrate Google’s AutoML, a software suite for easy-bake machine learning. They compiled and labeled roughly 600 recipes for breads, cakes, or cookies and limited ingredient lists to 16 essentials, like eggs, flour, and milk.They trained a model to classify recipes as bread, cake, or cookies with high accuracy.For each recipe, AutoML’s explainability feature assigned the ingredients a percentage that described their importance to the classification. Butter, sugar, and yeast were most important overall.The authors adjusted ingredient ratios until they developed a recipe that the model classified as equal parts cookie and cake, and another that was equal parts bread and cookie. They baked and tasted these creations: a cakie (“nasty”) and a breakie (“pretty good”). Behind the news: Machine learning’s culinary education is coming along, though some of its creations are tastier than others. Sony AI’s Flagship Gastronomy Project recently launched an app to help professional chefs develop new recipes, food pairings, and menus.In 2019, Facebook trained a vision model to recognize different types of food and generate recipes to produce them.In 2016, IBM debuted Chef Watson, an app trained partly on Bon Appétit’s recipe archive, which generates recipes based on ingredients specified by users.Blogger Janelle Shane prompted GPT-2 to generate recipes for dish names that were themselves generated by AI, producing gustatory horrors like Chocolate Chicken Chicken Cake. Why it matters: Experimenting with new recipes isn’t just fun for home cooks. Commercial test kitchens are on the lookout for novel flavors, textures, and dishes. AI could help chefs invent smorgasbords of culinary delights. We’re thinking: These AI-powered recipes may seem half-baked, but suddenly we have a craving for Chocolate Chicken Chicken Cake.

Image to article:
- Caption to image: Model predicting ingredients in a recipe and woman cooking
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/02/unnamed-3.gif
-----------------"
What is the main advantage of the IRIS method in batch reinforcement learning?,"IRIS allows models to learn different portions of a task from various demonstrations, enabling them to absorb the best skills from inconsistent examples.","The main advantage of the IRIS method in batch reinforcement learning is that it enables models to learn different portions of a task from different examples, allowing the model to absorb the best skills from each demonstration. This approach breaks down tasks into sequences of intermediate subgoals, enabling the agent to learn from the best parts of each demonstration and combine them to accomplish the task effectively.","-----------------
Title: Different Skills From Different Demos
Content: Reinforcement learning trains models by trial and error. In batch reinforcement learning (BRL), models learn by observing many demonstrations by a variety of actors. For instance, a robot might learn how to fix ingrown toenails by watching hundreds of surgeons perform the procedure. But what if one doctor is handier with a scalpel while another excels at suturing? A new method lets models absorb the best skills from each.What’s new: Ajay Mandlekar and collaborators at Nvidia, Stanford, and the University of Toronto devised a BRL technique that enables models to learn different portions of a task from different examples. This way, the model can gain useful information from inconsistent examples. Implicit Reinforcement without Interaction at Scale (IRIS) achieved state-of-the-art BRL performance in three tasks performed in a virtual environment.Key insight: Learning from demonstrations is a double-edged sword. An agent gets to see how to complete a task, but the scope of its action is limited to the most complete demonstration of a given task. IRIS breaks down tasks into sequences of intermediate subgoals. Then it performs the actions required to accomplish each subgoal. In this way, the agent learns from the best parts of each demonstration and combines them to accomplish the task.How it works: IRIS includes a subgoal selection model that predicts intermediate points on the way to accomplishing an assigned task. These subgoals are defined automatically by the algorithm, and may not correspond to parts of a task as humans would describe them. A controller network tries to replicate the optimal sequence of actions leading to a given subgoal. The subgoal selection model is made up of a conditional variational autoencoder that produces a set of possible subgoals and a value function (trained via a BRL version of Q-learning) that predicts which next subgoal will lead to the highest reward.The controller is a recurrent neural network that decides on the actions required to accomplish the current subgoal. It learns to predict how demonstrations tend to unfold, and to imitate short sequences of actions from specific demonstrations.Once it’s trained, the subgoal selection model determines the next subgoal. The controller takes the requisite actions. Then the subgoal selection model evaluates the current state and computes a new subgoal, and so on. Results: In the Robosuite’s lifting and pick-and-place tasks, previous state-of-the-art BRL approaches couldn’t pick up objects reliably, nor place them elsewhere at all. IRIS learned to pick up objects with over 80 percent success and placed them with 30 percent success.Why it matters: Automatically identifying subgoals has been a holy grail in reinforcement learning, with active research in hierarchical RL and other areas. The method used in this paper applies to relatively simple tasks where things happen in a predictable sequence (such as picking and then placing), but might be a small step in an important direction.We’re thinking: Batch reinforcement learning is useful when a model must be interpretable or safe — after all, a robotic surgeon shouldn’t experiment on living patients — but it hasn’t been terribly effective. IRIS could make it a viable option.

Image to article:
- Caption to image: Information related to Implicit Reinforcement without Interaction at Scale (IRIS)
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/IRIS20ASPECT.png
-----------------"
What advancements in video and audio generation models are anticipated in the coming year?,"Models that produce video clips complete with audio soundtracks including speech, music, and sound effects are anticipated.",Not found in context,"-----------------
Title: Generative Video Models Revolutionize Content Creation with Stunning Realism
Content: Video generation exploded in an abundance of powerful models. What happened: Companies big and small introduced new or updated text-to-video generators. Some added image-to-video and/or video-to-video capabilities. While most models focus on generating cinematic clips, some specialize in videos for social media. Driving the story: Even at the extraordinary pace of AI lately, video generators in the past year matured with remarkable speed. Virtually every major model produces convincing, highly detailed scenes, both realistic and fantastical, while ramping up image resolution, speed, output length, and users’ ability to control their outputs. OpenAI Sora set a high bar early in the year. Introduced in February and shown privately to Hollywood creators, it built a formidable buzz despite being available to only selected users. Unauthorized users gained access in November, and OpenAI made the model available the following month. Built on a diffusion transformer, Sora generates consistent (if somewhat dreamlike) scenes of up to 1 minute long.Runway Gen 3 Alpha and Gen 3 Alpha Turbo improved on their predecessors, generating higher-resolution videos (up to 1,280x768-pixel resolution) and introducing an API. Runway struck a deal with the film studio Lionsgate, which will use a custom version fine-tuned on its archive for visual effects and pre-visualizations.Adobe took a different approach with its Firefly Video model. In addition to offering a web application, the company incorporated the model directly into its best-selling Adobe Premiere Pro video editing suite. The integration enables video artists to generate clips, extend or enhance existing ones, and add effects within the program.Meta introduced Movie Gen, a suite of four systems. While its video output rivals that of competitors, it stands out especially for its ability to generate soundtracks. One system produces sound effects and music that match video. Another specializes in producing videos in which characters’ faces remain consistent, and another performs video-to-video alterations. Movie Gen will be available on Instagram in 2025.Model builders in China tailored their models for producing social media. Kling AI emphasized making TikTok and Instagram Reels. PixVerse and Jimeng AI likewise introduced video generators designed for social media users. In October, TikTok’s parent ByteDance added two video generation models, PixelDance and Seaweed, that produce 10-second and 30-second clips respectively. Behind the news: Video generation is already reshaping the movie industry. In February, after seeing a preview of Sora, American filmmaker Tyler Perry halted a planned expansion of his production studio, arguing that within a few years, AI video could put traditional studios out of business. Members of the video graphics team at The Late Show with Stephen Colbert use Runway’s technology to add special effects to conventional digital video, cutting editing time from hours to minutes. Where things stand: Video generation came a long way in 2024, but there’s still plenty of room for improvement. Because most models only generate a small number of frames at a time, they can struggle to track physics and geometry and to generate consistent characters and scenery over time. The computational demands of maintaining consistency across frames means that generated clips are brief. And even short outputs take substantial time and resources to generate: Sora can take 10 to 20 minutes to render clips as short as 3 seconds. OpenAI and Runway released faster versions — Sora Turbo and Gen-3 Alpha Turbo — to address the challenge.

Image to article:
- Caption to image: Snowman using a camera during snowfall.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F12%2Funnamed--43--1.jpg&w=3840&q=75
-----------------"
What is one potential nonlethal use of AI mentioned in the context?,Mine removal and search and rescue.,One potential nonlethal use of AI mentioned in the context is the deployment of machine learning systems by the Israeli Defense Force to analyze streams of incoming intelligence. This analysis helps identify targets and warn ground troops about incoming attacks.,"-----------------
Title: Deadly Drones Act Alone
Content: Dear friends, In school, most questions have only one right answer. But elsewhere, decisions often come down to a difficult choice among imperfect options. I’d like to share with you some approaches that have helped me make such decisions. When I was deciding where to set up a satellite office outside the U.S., there were many options. My team and I started by listing important criteria such as supply of talent, availability of local partners, safety and rule of law, availability of visas, and cost. Then we evaluated different options against these criteria and built a matrix with cities along one axis and our criteria along the other. That clarified which country would make a great choice. When I feel stuck, I find it helpful to write out my thoughts: What options am I choosing among?What criteria are driving the choice?How does each option rate with respect to the criteria?if I need more information, how can I get it? Documenting decisions in this way also builds a foundation for further choices. For example, over the years, I’ve collected training data for many different kinds of problems. When I need to select among tactics for acquiring data, having been through the process many times, I know that some of the most important criteria are (i) the time needed, (ii) the number of examples, (iii) accuracy of the labels, (iv) how representative the input distribution is, and (v) cost. If I’m making a decision as part of a team, I check with teammates at each step to make sure we’re accurately capturing the top options, criteria, and so on. (The comments feature in Google Docs is a great way to facilitate open debate within a team.) This helps me avoid losing track of some criteria and acting based on an incomplete set; for example, picking the satellite office’s location based only on the availability of talent. It also helps align everyone on the final decision. As you may know, I wound up setting up a satellite office in Colombia because of the availability of talent and a supportive ecosystem of partners. The team there has become a key part of many projects. Lately I’ve worried about their wellbeing amid Covid-19 and widespread unrest. But in hindsight, setting up in Colombia was one of my best decisions, and I remain as committed as ever to supporting my friends there. Keep learning!Andrew Autonomous weapons are often viewed as an alarming potential consequence of advances in AI — but they may already have been used in combat. What’s new: Libyan forces unleashed armed drones capable of choosing their own targets against a breakaway rebel faction last year, said a recent United Nations (UN) report. The document, a letter from the organization’s Panel of Experts on Libya to the president of the Security Council, does not specify whether the drones targeted, attacked, or killed anyone. It was brought to light by New Scientist. Killer robots: In March of 2020, amid Libya’s ongoing civil war, the UN-supported Government of National Accord allegedly attacked retreating rebel forces using Kargu-2 quadcopters manufactured by Turkish company STM. The fliers are equipped with object-detection and face-recognition algorithms to find and strike targets without explicit human direction.Upon acquiring a target, the drone flies directly at it and detonates a small warhead just before impact.STM claims that its systems can distinguish soldiers from civilians.The Turkish military bought at least 500 such units for use in its border conflict with Syria. STM is negotiating sales to three other nations, according to Forbes. Behind the news: Many nations use machine learning in their armed forces, usually to bolster existing systems, typically with a human in the loop. In the most recent battle between Israel and Palestinians in Gaza, the Israeli Defense Force deployed machine learning systems that analyzed streams of incoming intelligence. The analysis helped its air force identify targets and warn ground troops about incoming attacks.The U.S. Army is testing a drone that uses computer vision to identify targets up to a kilometer away and determine whether they’re armed.The European Union has funded several AI-powered military projects including explosive device detection and small unmanned ground vehicles that follow foot soldiers through rough terrain. Why it matters: Observers have long warned that deploying lethal autonomous weapons on the battlefield could ignite an arms race of deadly machines that decide for themselves who to kill. Assuming the UN report is accurate, the skirmish in Libya appears to have set a precedent. We’re thinking: Considering the problems that have emerged in using today’s AI for critical processes like deploying police, sentencing convicts, and making loans, it’s clear that the technology simply should not be used to make life-and-death decisions. We urge all nations and the UN to develop rules to ensure that the world never sees a real AI war.

Image to article:
- Caption to image: A group of drones flying over a field
- URL to image: https://dl-staging-website.ghost.io/content/images/2021/06/Kargu-Redo-2.gif
-----------------"
How does the AI-driven augmented reality attachment developed by Google Health improve cancer diagnosis in analog microscopes?,It outlines signs of breast and prostate cancer in real time by superimposing AI predictions over cell slides at around 27 frames per second.,"The AI-driven augmented reality attachment developed by Google Health improves cancer diagnosis in analog microscopes by using a computer-vision system to spot cancer in cell slides and augmented-reality technology to superimpose the AI's predictions over the slide in real time. This system operates at around 27 frames per second, allowing pathologists to see AI-generated outlines of cancerous signs directly in the microscope eyepiece. This enhancement helps make diagnoses more reliable, especially in areas where trained pathologists are scarce, by reducing subjectivity and inconsistency in interpreting cell slides.","-----------------
Title: Seeing Cancer
Content: Microscopes outfitted with AI-driven augmented reality could improve the accuracy of cancer diagnoses. What’s happened: Google Health developed an attachment for analog microscopes that outlines signs of breast and prostate cancer in real time. How it works: A computer-vision system spots cancer in a cell slide, while augmented-reality tech superimposes the AI’s prediction over the slide at around 27 frames per second. The developers combined the Inception V3 image classifier with a fully convolutional neural network, which allowed the system to recognize tumorous patterns much faster.A camera captures a head-on view of the slide and projects it, overlaid with the AI prediction, into the microscope eyepiece. Behind the news: Pathologists use microscopes to measure tumor size relative to nearby lymph nodes and to count the number of cells nearing or undergoing mitosis. That information tells them how aggressively a patient’s cancer is spreading.Why it matters: Interpreting cell slides is subjective, and one pathologist’s understanding can differ greatly from another’s. Patients in locations where trained pathologists are scarce tend to suffer most from this inconsistency. AI-enhanced tools could help make diagnoses more reliable. We’re thinking: AI is a natural complement to digital microscopes, but analog microscopes are far more common. This technology promises to upgrade those tools at a fraction of the cost of replacing them.

Image to article:
- Caption to image: Hardware components of the Augmented Reality Microscope (ARM)
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/1_Screen20Shot202019-09-0320at203.25.3120PM.png
-----------------"
What is the main technique used by Google Brain researchers to enable an agent to build a world model without direct observations of its environment?,Observational dropout,"The main technique used by Google Brain researchers is called observational dropout, which involves blocking an agent's observations of the world at random moments, forcing it to generate its own internal representation to fill in the gaps.","-----------------
Title: Seeing the World Blindfolded
Content: In reinforcement learning, if researchers want an agent to have an internal representation of its environment, they’ll build and train a world model that it can refer to. New research shows that world models can emerge from standard training, rather than needing to be built separately.What’s new: Google Brain researchers C. Daniel Freeman, Luke Metz, and David Ha enabled an agent to build a world model by blindfolding it as it learned to accomplish tasks. They call their approach observational dropout.Key insight: Blocking an agent’s observations of the world at random moments forces it to generate its own internal representation to fill in the gaps. The agent learns this representation without being instructed to predict how the environment will change in response to its actions.How it works: At every timestep, the agent acts on either its observation (framed in red in the video above) or its prediction of what it wasn’t able to observe (imagery not framed in red). The agent contains a controller that decides on the most rewarding action. To compute the potential reward of a given action, the agent includes an additional deep net trained using the RL algorithm REINFORCE. Observational dropout blocks the agent from observing the environment according to a user-defined probability. When this happens, the agent predicts an observation.If random blindfolding blocks several observations in a row, the agent uses its most recent prediction to generate the next one.This procedure over many iterations produces a sequence of observations and predictions. The agent learns from this sequence, and its ability to predict blocked observations is tantamount to a world model. Results: Observational dropout solved the task known as Cartpole, in which the model must balance a pole upright on a rolling cart, even when its view of the world was blocked 90 percent of the time. In a more complex Car Racing task, in which a model must navigate a car around a track as fast as possible, the model performed almost equally well whether it was allowed to see its surroundings or blindfolded up to 60 percent of the time.Why it matters: Modeling reality is often part art and part science. World models generated by observational dropout aren’t perfect representations, but they’re sufficient for some tasks. This work could lead to simple-but-effective world models of complex environments that are impractical to model completely.We’re thinking: Technology being imperfect, observational dropout is a fact of life, not just a research technique. A self-driving car or auto-piloted airplane reliant on sensors that drop data points could create a catastrophe. This technique could make high-stakes RL models more robust.

Image to article:
- Caption to image: Observational dropout
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize201.gif
-----------------"
What is the main challenge that Voice Activity Detection (VAD) systems face in noisy environments?,"VAD systems struggle to accurately detect when a user is talking, especially when background noise comes from other human speech.","The main challenge that Voice Activity Detection (VAD) systems face in noisy environments is accurately detecting when the user is talking, especially when the background noise includes other human speech. In such environments, VAD systems may mistakenly interpret background conversations as the user speaking or fail to recognize when the user has stopped talking.","-----------------
Title: Wait Your Turn! Conversation by Voice Versus Text
Content: Dear friends, Continuing our discussion on the Voice Stack, I’d like to explore an area that today’s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication. When communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm. A key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user’s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI’s output, and let the user take a turn. This works reasonably well in quiet environments. However, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD — which is usually trained to detect human speech — tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you’ve stopped talking. This is why today’s speech applications often struggle in noisy environments. Intriguingly, last year, Kyutai Labs published Moshi, a model (GitHub) that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user. If you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we’d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It’s listening all the time, and it’s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an “inner monologue” that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.) Just as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of “reasoning tokens” before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I’m confident we’ll see many more good voice models released this year. It feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in. Keep building!

Image to article:
- Caption to image: Diagram of an RQ-Transformer speech system with Helium and Depth Transformers for audio processing.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F03%2Funnamed--56--3.png&w=3840&q=75
-----------------"
What is the name of the digital ID program that the French government plans to implement based on face recognition?,Alicem,The name of the digital ID program that the French government plans to implement based on face recognition is Alicem.,"-----------------
Title: Want Your Pension? Send a Selfie
Content: The French government plans to roll out a national identification service based on face recognition. Critics warn that the new system violates citizens’ privacy.What’s new: Beginning in November, President Emmanual Macron’s administration plans to implement a digital ID program based on an Android app. While French citizens aren’t required to enroll, the app will be the only digital portal to many government services.How it works: Called Alicem, the app is designed to authenticate the user’s identity for interactions such as filing taxes, applying for pension benefits, and paying utility bills. Alicem starts by capturing video of the user’s face from various angles.Then it compares the video to the user’s passport photo to determine whether they depict the same individual.The app will delete the video selfie once it has completed enrollment, according to France’s Ministry of Interior. Behind the news: France isn’t the first government to use face recognition in this way. Singapore also offers access to government services via face print.Yes, but: Emilie Seruga-Cau, who heads France’s privacy regulator, says Alicem’s authentication scheme violates the European Union’s General Data Protection Regulation by failing to offer an alternative to face recognition. A privacy group called La Quadrature du Net has sued the government over the issue. France’s Interior Ministry has shown no sign of bowing to such concerns.We’re thinking: Any democratic government aiming to use face recognition for identification must protect its citizens on two fronts. Laws must restrict use of the data to its intended purpose, and due care must be taken to secure the data against hacks.

Image to article:
- Caption to image: Alicem website home
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Alicem.png
-----------------"
What is the primary purpose of the website Papers Without Code?,"The primary purpose of the website Papers Without Code is to maintain a directory of AI systems that researchers tried but failed to reproduce, in order to save researchers time wasted trying to replicate results published with insufficient technical detail.","The primary purpose of the website Papers Without Code is to maintain a directory of AI systems that researchers tried but failed to reproduce, aiming to save researchers time wasted trying to replicate results published with insufficient technical detail.","-----------------
Title: Spotlight on Unreproducible Results
Content: A new website calls out AI research that may not lend itself to being reproduced. What’s new:Papers Without Code maintains a directory of AI systems that researchers tried but failed to reproduce. The site (the name of which is a play on the indispensable Papers With Code), aims to save researchers time wasted trying to replicate results published with insufficient technical detail.How it works: Users can submit a link to a paper, a link to their attempt to reproduce it, and an explanation of why their effort failed. After reviewing the links, the site’s administrators contact the original authors and request data, code, and pointers necessary to reproduce their work. If the authors don’t reply or provide insufficient information, the administrators add the paper to a public list.To date, the website has received more than 10 submissions, six of which have been posted. Two authors have uploaded their code. Once a paper passes muster, its author is encouraged to post it to Papers With Code, which documents 40,000 replicated computer science studies.The researcher behind Papers Without Code, who goes by the user name ContributionSecure14 on Reddit, started the website after wasting a week trying to replicate a machine learning study.They advise authors who can’t release their code, data, or infrastructure for proprietary reasons to work directly with others trying to replicate their efforts. “There’s no point in publishing the paper in the public domain if others cannot build off it,” they told TechTalks. Behind the news: Google engineer Pete Warden proclaimed a “machine learning reproducibility crisis” in 2018. Since then the issue has emerged as a widespread concern. Last year, 31 researchers criticized the lack of technical detail in a Google paper that describes a cancer system that purportedly outperformed human doctors.One of that paper’s coauthors, Joelle Pineau of McGill University and Facebook, worked with NeurIPS to ensure that papers submitted to the conference come with working code and data. She also published a Machine Learning Reproducibility Checklist.Rescience C is a peer-reviewed journal that publishes replication efforts of computer science papers. Why it matters: Reproducibility is an essential part of science, and AI is one of many fields facing a so-called replication crisis brought on by growing numbers of papers that report unreliable results.We’re thinking: While we applaud the spirit of this effort, without a transparent review process and a public list of reviewers, it could be used to demean researchers unfairly. We urge other research venues and institutions to take up the cause.

Image to article:
- Caption to image: Square brackets with lines disappearing inside
- URL to image: https://dl-staging-website.ghost.io/content/images/2022/10/Spotlight-on-Unreproducible-Results-1.gif
-----------------"
What technology is being used in France to monitor compliance with face mask mandates on public transportation?,Cameras that detect face masks using an object recognition model installed in existing CCTV cameras.,"In France, Datakalab, a French AI startup, is using computer vision technology to monitor compliance with face mask mandates on public transportation. They are installing chips in existing CCTV cameras that run an object recognition model trained to distinguish masked faces from unmasked ones. This system counts mask wearers every 15 minutes and transmits aggregate statistics to the authorities.","-----------------
Title: News
Content: Cameras that detect face masks are helping French authorities to evaluate citizens’ adherence to government mandates intended to fight Covid-19.What’s new: Starting this week, everyone riding public transportation in France is required to wear a face mask. Paris and Cannes are using computer vision to count people who comply.How it works: Datakalab, a French AI startup, is installing chips in existing CCTV cameras that run an object recognition model. The model is trained to distinguish masked faces from unmasked ones. Paris is testing the cameras at the busy Chatelet-Les Halles metro station. Cannes has installed them on buses and in public markets.The software counts mask wearers every 15 minutes and transmits aggregate statistics to the authorities. The company says the system is meant to help authorities determine where to step up efforts to promote mask-wearingDatakalab provides similar technology for use in retailing. Those systems note customers’ age, gender, how long they stay in certain areas, and whether they’re smiling. Behind the news: AI is being used widely to monitor compliance with rules designed to combat the spread of Covid-19. The Indian state of Punjab is using drones from Skylark Laboratories to enforce social distancing and curfew regulations.Hospitality companies have deployed cameras from Wobot Intelligence to ensure that employees are washing their hands for at least 20 seconds. Yes, but: France’s privacy commission warns that mask detection technology may violate European rules that limit personal data collection. Datakalab counters that its systems neither identify individuals nor store data. In any case, 94 percent of French citizens support wearing masks in public according to a recent poll. (France continues to outlaw burqas and other religious face coverings under a 2011 law.)Why it matters: As France and other countries begin to lift rules that keep people physically apart, wearing masks is critical to limiting coronavirus transmission.We’re thinking: Covid-19 surveillance is a double-edged sword: helpful in containing the pandemic but worrisome in other contexts. Governments and businesses must use it appropriately and only while the need persists.

Image to article:
- Caption to image: Face recognition system identifying a person wearing a mask
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-optimize2010.gif
-----------------"
What is the main improvement of Grounding DINO 1.5 compared to the original Grounding DINO?,"Grounding DINO 1.5 performs significantly faster and uses only the smallest image embeddings for processing, enabling it to run on devices with limited processing power.","The main improvement of Grounding DINO 1.5 compared to the original Grounding DINO is its significantly faster performance, achieving 10.7 frames per second versus 1.1 frames per second on an Nvidia Jetson Orin NX computer. This speed increase is due to the use of a more efficient image encoder and a smaller image embedding for cross-attention between image and text embeddings.","-----------------
Title: Grounding DINO 1.5, An Edge Device Model Built for Faster, Smarter Object Detection
Content: An open source model is designed to perform sophisticated object detection on edge devices like phones, cars, medical equipment, and smart doorbells. What’s new: Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, and colleagues at the International Digital Economy Academy introduced Grounding DINO 1.5, a system that enables devices with limited processing power to detect arbitrary objects in images based on a text list of objects (also known as open-vocabulary object detection). You can download the code and weights here. Key insight: The original Grounding DINO follows many of its predecessors by using image embeddings of different levels (from lower-level embeddings produced by an image encoder’s earlier layers, which are larger and represent simple patterns such as edges, to higher-level embeddings produced by later layers, which are smaller and represent complex patterns such as objects). This enables it to better detect objects at different scales. However, it takes a lot of computation. To enable the system to run on devices that have less processing power, Grounding DINO 1.5 uses only the smallest (highest-level) image embeddings for a crucial part of the process. How it works: Grounding DINO 1.5 is made up of components that produce text and image embeddings, fuse them, and classify them. It follows the system architecture and training of Grounding DINO with the following exceptions: (i) It uses a different image encoder, (ii) a different model combines text and image embeddings, and (iii) it was trained on a newer dataset of 20 million publicly available text-image examples. Given an image, a pretrained EfficientViT-L1 image encoder produced three levels of image embeddings.Given the corresponding text, BERT produced a text embedding composed of tokens.Given the highest-level image embedding and the text embedding, a cross-attention model updated each one to incorporate information from the other (fusing text and image modalities, in effect). After the update, a CNN-based model combined the updated highest-level image embedding with the lower-level image embeddings to create a single image embedding.Grounding DINO 1.5 calculated which 900 tokens in the image embedding were most similar to the tokens in the text embedding.A cross-attention model detected objects using both the image and text embeddings. For each token in the updated image embedding, it determined: (i) which text token(s), if any, matched the image token, thereby giving each image token a classification including “not an object” and (ii) a bounding box that enclosed the corresponding object (except for tokens that were labeled “not an object”).The system learned to (i) maximize the similarity between matching tokens from the text and image embeddings and minimize the similarity between tokens that didn’t match and (ii) minimize the difference between its own bounding boxes and those in the training dataset. Results: Grounding DINO 1.5 performed significantly faster than the original Grounding DINO: 10.7 frames per second versus 1.1 frames per second running on an Nvidia Jetson Orin NX computer. Tested on a dataset of images of common objects annotated with labels and bounding boxes, Grounding DINO 1.5 achieved better average precision (a measure of how many objects it identified correctly in their correct location, higher is better) than both Grounding DINO and YOLO-Worldv2-L (a CNN-based object detector). Grounding DINO 1.5 scored 33.5 percent, Grounding DINO 27.4 percent, and YOLO-Worldv2-L 33 percent. Why it matters: The authors achieved 10 times the speed with just a couple of small changes (a more efficient image encoder and a smaller image embedding when performing cross-attention between embeddings of images and texts). Small changes can yield big results. We’re thinking: Lately model builders have been building better, smaller, faster large language models for edge devices. We’re glad to see object detection get similar treatment.

Image to article:
- Caption to image: Grounding DINO animation depicting object detection with bounding boxes on images.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F11%2Funnamed--35--2.gif&w=3840&q=75
-----------------"
What method did Benjamin Sanchez-Lengeling and his team use to predict a chemical's smell from its molecular structure?,They developed a model that uses a graph neural network (GNN) to process a graph representation of a molecule.,"Benjamin Sanchez-Lengeling and his team used a graph neural network (GNN) to predict a chemical's smell from its molecular structure. They represented molecules as graphs with atoms as nodes and bonds as edges, which the GNN processed to learn a more information-rich graph representation. This was then used to classify the molecule's odor.","-----------------
Title: Nose Job
Content: Predicting a molecule’s aroma is hard because slight changes in structure lead to huge shifts in perception. Good thing deep learning is developing a sense of smell.What’s new: Benjamin Sanchez-Lengeling and a team from Google Brain, Arizona State University, and the University of Toronto developed a model that predicts a chemical’s smell from an embedding of its molecular structure.Key insight: A molecule is composed of atoms with bonds between them. Representing atoms as nodes and bonds as edges yields a graph ripe for processing by a graph neural network, or GNN.How it works: The researchers gathered about 5,030 molecules and 138 odor descriptions, such as “fruity” or “medicinal,” from the GoodScents and Leffingwell PMP 2001 fragrance databases. They treated each description as a class in a classification task. Their model included a GNN, a component that converts graphs into vectors, and a fully connected layer that performs classification. The GNN takes a graph representation of a molecule as its input and learns a more information-rich graph representation. The network learns a vector that describes each node. The network’s layers update these vectors based on the values of neighboring nodes. The model converts the enriched output graph to a vector by summing the values of each node’s neighbors.A sequence of feed-forward layers then classifies the molecule’s odor.The network’s penultimate layer encodes the molecule-scent embedding, which can be used for other tasks as well. For instance, the authors applied it to the DREAM Olfaction Prediction Challenge to predict an odor’s strength (“how fruity is this smell?”) on a scale of 1 to 100. Results: The GNN achieved a 5 percent higher F1 score than random-forest or nearest-neighbor methods trained on hand-crafted features. On the DREAM Olfaction Prediction Challenge, the authors matched the original winner’s 2015 score, even though their embedding wasn’t designed for this particular task.Why it matters: Chemists often struggle to predict properties of molecules based on their structure. This work suggests that deep learning can aid in the effort. Beyond predicting smells, the molecule-scent embedding is suited to transfer learning for other scent-related tasks and possibly generative methods that might, say, predict molecules having a particular scent.We’re thinking: One of the biggest challenges to building an artificial nose is not in the software, but in the hardware: How to build a sensor that can detect minute numbers of scent molecules in the air. This research could help design new fragrances, but further work in chemical sensing technology is also needed. Whoever cracks this problem will come up smelling like roses.

Image to article:
- Caption to image: Information related to a model that predicts a chemical's smell
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ScentsSIZED.png
-----------------"
What method did the researchers use to generate new designs for integrated circuits?,They used deep learning and an evolutionary algorithm.,"The researchers used deep learning and an evolutionary algorithm to generate new designs for integrated circuits. They trained convolutional neural networks (CNNs) to predict electromagnetic scattering and radiative properties of circuit designs. Using these predictions, they generated new binary circuit images through an evolutionary process, selecting and modifying designs based on their fitness to desired properties.","-----------------
Title: Researchers Used Deep Learning and an Evolutionary Algorithm to Design Chips in Minutes
Content: Designing integrated circuits typically requires years of human expertise. Recent work set AI to the task with surprising results. What’s new: Emir Ali Karahan, Zheng Liu, Aggraj Gupta, and colleagues at Princeton and Indian Institute of Technology Madras used deep learning and an evolutionary algorithm, which generates variations and tests their fitness, to generate designs for antennas, filters, power splitters, resonators, and other chips with applications in wireless communications and other applications. They fabricated a handful of the generated designs and found they worked — but in mysterious ways. How it works: The authors trained convolutional neural networks (CNNs), given a binary image of a circuit design (in which each pixel represents whether the corresponding portion of a semiconductor surface is raised or lowered), to predict its electromagnetic scattering properties and radiative properties. Based on this simulation, they generated new binary circuit images using evolution. The authors produced a training set of images and associated properties using Matlab EM Toolbox. The images depicted designs for chip sizes between 200x200 micrometers (which they represented as 10x10 pixels) and 500x500 micrometers (represented as 25x25 pixels).They trained a separate CNN on designs of each size.They generated 4,000 designs at random and predicted their properties using the appropriate CNN.Given the properties, the authors used a tournament method to select the designs whose properties were closest to the desired values. They randomly modified the selected designs to produce a new pool of 4,000 designs, predicted their properties, and repeated the tournament. The number of iterations isn’t specified. Results: The authors fabricated some of the designs to test their real-world properties. The chips showed similar performance than the CNNs had predicted. The authors found the designs themselves baffling; they “delivered stunning high-performances devices that ran counter to the usual rules of thumb and human intuition,” co-author Uday Khankhoje told the tech news site Tech Xplore. Moreover, the design process was faster than previous approaches. The authors’ method designed a 300x300 micrometer chip in approximately 6 minutes. Using traditional methods it would have taken 21 days. Behind the news: Rather than wireless chips, Google has used AI to accelerate design of the Tensor Processing Units that process neural networks in its data centers. AlphaChip used reinforcement learning to learn how to position chip components such as SRAM and logic gates on silicon. Why it matters: Designing circuits usually requires rules of thumb, templates, and hundreds of hours of simulations and experiments to determine the best design. AI can cut the required expertise and time and possibly find effective designs that wouldn’t occur to human designers. We’re thinking: AI-generated circuit designs could help circuit designers to break out of set ways of thinking and discover new design principles.

Image to article:
- Caption to image: Workflow for inverse design using deep learning to predict S-parameters and radiation in structures.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2025%2F01%2Funnamed--48--1.png&w=3840&q=75
-----------------"
What is the primary intention of GCHQ's plan to use AI according to their new report?,"To combat security threats, human trafficking, and disinformation while ensuring ethical use.","The primary intention of GCHQ's plan to use AI, according to their new report, is to combat security threats, human trafficking, and disinformation while ensuring the ethical use of AI. The agency aims to augment human analysts with AI systems that prioritize privacy, fairness, transparency, and accountability.","-----------------
Title: On Her Majesty’s Secret Algorithm
Content: The UK’s electronic surveillance agency published its plan to use AI. What’s new: Government Communications Headquarters (GCHQ) outlined its intention to use machine learning to combat security threats, human trafficking, and disinformation — and to do so ethically — in a new report. What it says: GCHQ said its AI will augment, rather than supplant, human analysts. Moreover, the agency will strive to use AI with privacy, fairness, transparency, and accountability by emphasizing ethics training and thoroughly reviewing all systems. Such systems will: Analyze data on large computer networks to prevent cyber attacks, identify malicious software, and trace them back to their origins.Intercept sexually explicit imagery featuring minors and messages from sexual predators.Combat drug smuggling and human trafficking by analyzing financial transactions and mapping connections between the individuals behind them.Counter misinformation using models that detect deepfakes, assist with checking facts, and track both content farms that pump out fake news and botnets that spread it. Behind the news: While intelligence agencies rarely detail their AI efforts, several examples have come to light. German law enforcement agencies use AI-generated images of minors to trap online predators.The U.S. National Reconnaissance Office is developing a system to guide surveillance satellites.The U.S. National Security Agency is training models to audit regulatory compliance by other intelligence agencies as they search for international criminals and look for warning of emerging crises. Why it matters: The GCHQ plan emphasizes the utility of AI systems in securing nations and fighting crime — and highlights the need to ensure that sound ethical principles are built into their design and use.We’re thinking: GPT-007 prefers its data shaken, not perturbed.

Image to article:
- Caption to image: Graph showing key AI characteristics
- URL to image: https://cdn2.hubspot.net/hub/5871640/hubfs/UKSPY-1.gif?upscale=true&name=UKSPY-1.gif
-----------------"
What was the main reason the federal judge ruled against Cleveland State University's use of Honorlock for proctoring exams?,"The judge ruled that the use of Honorlock violated students' rights, specifically their Constitutional protection against unreasonable searches.","The federal judge ruled against Cleveland State University's use of Honorlock because it violated students' rights. Specifically, the court found that the virtual room scan conducted by Honorlock constituted an unreasonable search, infringing on the Constitutional protection against such searches.","-----------------
Title: Court Blocks AI-Assisted Proctoring
Content: A U.S. court ruled against an implementation of AI-powered software designed to catch students who cheat on academic examinations. What’s new: A federal judge determined that Cleveland State University’s use of Honorlock, a system that scans students’ behavior and surroundings for signs of cheating, violates their rights, National Public Radio reported. How it works: Students install Honorlock as a web browser extension and permit access to the computer’s microphone and camera. During a test, the extension uses voice detection and computer vision to issue alerts if it detects tablets, phones, open textbooks, dimmed lighting, faces of people other than the student, talking, phrases like “Hey Siri” or “Okay Google,” the student’s looking down or away from the screen before answering questions or absence from the camera’s view for an extended time, and other signs.Instructors can initiate a 360-degree scan of a student’s room prior to a test. Scans take about a minute to complete. Honorlock stores the recorded video data for a year.If it detects anything amiss, the system alerts a human proctor. The case: In 2021, Cleveland State University student Aaron Ogletree sued the school for subjecting him to a virtual room scan, which he claimed violated his Constitutional protection against unreasonable searches. He complied with the scan but filed suit later. The university argued that a room scan doesn’t constitute a search because it’s limited in scope and conducted to ensure academic integrity. The judge ruled that the university had violated Ogletree’s rights. Behind the News: Scientific investigations of other AI-powered proctoring systems have reached conflicting conclusions about their effectiveness. A 2021 study of a program called Proctorio found that it failed to catch any of 30 students whom the authors instructed to cheat. It also incorrectly flagged non-cheating students as engaging in suspicious activities.A 2020 study by Radford University found that test-takers scored lower when they were monitored by proctoring software than when they weren’t. The authors interpreted this result as evidence thatautomated proctoring discourages cheating. Why it matters: Automated proctoring has value, especially in the era of remote education. Although the ruling against Cleveland State applies only to that school, it raises questions about the legality of such automated room scans nationwide. We’re thinking: While the judge's decision ostensibly affects AI-powered proctor software, many institutions use human proctors who might occasionally request a manual room scan. The underlying question — what proctoring methods are reasonable, ethical, fair, and legal? — is independent of whether machines or humans should do the job.

Image to article:
- Caption to image: Capture of Honorlock, an AI-powered software designed to catch students who cheat on academic examinations
- URL to image: https://dl-staging-website.ghost.io/content/images/2022/09/unnamed--1-.png
-----------------"
How did Claude 3.5 Sonnet's proposals compare to those generated by human researchers in terms of novelty and overall quality?,"Human judges deemed proposals generated by Claude 3.5 Sonnet as good as or better than those produced by humans, with AI-generated proposals being significantly more novel.","Claude 3.5 Sonnet's proposals were deemed as good as or better than those produced by human researchers. Human judges scored the AI-generated and human-written proposals roughly equally in feasibility, expected effectiveness, how exciting they were, and overall quality. However, the AI-generated proposals were considered significantly more novel, achieving an average novelty score of 5.78 compared to 4.86 for human-written proposals.","-----------------
Title: Stanford Study Finds AI Matches Human Experts at Writing Research Proposals
Content: How do agents based on large language models compare to human experts when it comes to proposing machine learning research? Pretty well, according to one study. What’s new: Chenglei Si, Diyi Yang, and Tatsunori Hashimoto at Stanford produced ideas for research in machine learning using Anthropic’s Claude 3.5 Sonnet and human researchers, and also evaluated them using both manual and automated methods. Claude 3.5 Sonnet generated competitive proposals, but its evaluations of proposals were less compelling. How it works: Each proposal included a problem statement, motivation, step-by-step plan, backup plan, and examples of baseline outcomes versus expected experimental outcomes. Automated proposal generation: Given one of seven topics (bias, coding, safety, multilinguality, factuality, math, or uncertainty) and 10 related papers found by the Semantic Scholar search engine, Claude 3.5 Sonnet generated 4,000 research ideas. The authors embedded the ideas using all-MiniLM-L6-v2 and removed duplicate ideas based on the cosine similarity of their embeddings. This left roughly 200 AI-generated ideas for each topic. For each remaining idea, the model generated a proposal.Automated ranking: Claude Sonnet 3.5 ranked the proposals in a five-round tournament that awarded points for superior quality and pitted highest-scoring proposals against one another. In addition, one of the authors manually ranked the generated proposals.Human proposal generation: The authors paid 49 machine learning engineers to propose their own ideas. They obscured authorship by prompting an unidentified large language model to edit them according to a style guide. Then they manually checked the rewritten proposals to ensure that the model’s editing didn’t change their content significantly.Human ranking: A group of 79 machine learning engineers reviewed the 49 human-written proposals, the top 49 AI-generated proposals ranked by humans, and the top 49 AI-generated proposals ranked by AI (resulting in two to four reviews per proposal). They scored the proposals between 1 and 10 on five factors: novelty, feasibility, expected effectiveness, how exciting they were, and overall quality. Results: Human judges deemed proposals generated by Claude 3.5 Sonnet as good as or better than those produced by humans. However, large language models proved less effective at judging the proposals’ quality. On average, humans scored the AI-generated and human-written proposals roughly equally in feasibility, expected effectiveness, how exciting they were, and overall quality. They deemed the AI-generated proposals significantly more novel. The top AI-generated proposals as ranked by humans achieved an average 5.78 novelty. The top AI-generated proposal as ranked by AI achieved an average 5.62 novelty. Human-written proposals achieved an average 4.86 novelty.The authors found that LLMs don’t yet match human performance when it comes to judging scientific papers. They compared the rates of agreement among five LLMs that evaluated proposals in their experiment, human judgements of the proposals, and human reviews of papers submitted to the NeurIPS and ICLR conferences. The most consistent LLM, Claude 3.5 Sonnet, was 53.3 percent consistent with average human judgment. The human judges were 56.1 percent consistent. Reviewers for NeurIPS and ICLR were 66 and 71.9 percent consistent respectively. Random chance was 50 percent. Why it matters: AI models play a growing role in scientific discovery. This work shows they can set directions for research — in machine learning, at least — that rival those set by humans. However, human evaluation remains the gold standard for comparing performance on complex problems like generating text. We’re thinking: Coming up with good research ideas is hard! That a large language model can do it with some competency has exciting implications for the future of both AI and science.

Image to article:
- Caption to image: Animation showcasing 7 key NLP topics visually expanding on the screen.
- URL to image: https://www.deeplearning.ai/_next/image/?url=https%3A%2F%2Fdl-staging-website.ghost.io%2Fcontent%2Fimages%2F2024%2F12%2Funnamed--41--1.gif&w=3840&q=75
-----------------"
What actions did Facebook take in response to the rise of disinformation during the pandemic and U.S. election?,"Facebook cracked down on manipulated media, banned deepfake videos, and developed deep learning tools to detect hate speech and misinformation.","In response to the rise of disinformation during the pandemic and U.S. election, Facebook took several actions. They cracked down on manipulated media deemed misleading, banned deepfake videos outright, and developed deep learning tools to detect hate speech, bigotry-promoting memes, and misinformation about Covid-19. Facebook also shut down accounts considered fronts for state-backed propaganda operations and added disclaimers to content deemed misleading about the U.S. election.","-----------------
Title: Algorithms Against Disinformation
Content: The worldwide pandemic and a contentious U.S. election whipped up a storm of automated disinformation, and some big AI companies reaped the whirlwind. What happened: Facing rising public pressure to block inflammatory falsehoods, Facebook, Google’s YouTube division, and Twitter scrambled to update their recommendation engines. Members of the U.S. Congress grilled the companies, a popular Netflix documentary excoriated them, and public opinion polls showed that they had lost the trust of most Americans.Driving the story: The companies addressed the issue through various algorithmic and policy fixes — though they apparently stopped short of making changes that might seriously threaten the bottom line. After discovering hundreds of fake user profiles that included head shots generated by AI, Facebook cracked down on manipulated media it deemed misleading and banned deepfake videos outright. The company continues to develop deep learning tools to detect hate speech, memes that promote bigotry, and misinformation about Covid-19.YouTube developed a classifier to identify so-called borderline content: videos that comply with its rules against hate speech but promote conspiracy theories, medical misinformation, and other fringe ideas.Facebook and Twitter shut down accounts they considered fronts for state-backed propaganda operations.All three companies added disclaimers to content deemed to contain misleading information about the U.S. election. Twitter took its policy furthest, flagging falsehoods from President Donald Trump. Yes, but: The reforms may not stick. The companies have diluted some, and others have already backfired. In June, the Wall Street Journal reported that some Facebook executives had squelched tools for policing extreme content. The company later reversed algorithmic changes made during the election that boosted reputable news sources. Perceptions that Facebook’s effort was halfhearted prompted some employees to resign.YouTube’s algorithmic tweaks targeting disinformation has succeeded in cutting traffic to content creators who promote falsehoods. But they also boosted traffic to larger entities, like Fox News, that often spread the same dubious information. Where things stand: There’s no clear way to win the online cat-and-mouse game against fakers, cranks, and propagandists. But the big cats must stay ahead or lose public trust — and regulators’ forbearance. Learn more: For more details on using AI to stem the tide of disinformation and hate speech online, see our earlier stories on Facebook’s efforts here and here, and on YouTube’s here and here.

Image to article:
- Caption to image: Group of people having a snowball fight and covering with a giant Facebook like button
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/The-Batch-New-Year_SnowballFight_576x324px-2.jpg
-----------------"
What is the main advantage of the Gated Transformer-XL (GTrXL) in reinforcement learning tasks compared to traditional transformers?,"GTrXL allows the network to learn sequential relationships early on, which helps it model shorter-term relationships more effectively during training, leading to improved stability and performance in reinforcement learning tasks.","The main advantage of the Gated Transformer-XL (GTrXL) in reinforcement learning tasks is its ability to learn sequential relationships early on, which helps in modeling shorter-term relationships more effectively during training. This makes the network more stable and allows it to outperform traditional transformers and LSTMs in tasks requiring long-term memory, such as those in the DMLab 30 environment.","-----------------
Title: Melding Transformers with RL
Content: Large NLP models like BERT can answer questions about a document thanks to the transformer network, a sequence-processing architecture that retains information across much longer sequences than previous methods. But transformers have had little success in reinforcement learning — until now.What’s new: Research in reinforcement learning (RL) has focused primarily on immediate tasks such as moving a single object. Transformers could support tasks that require longer-term memory. However, past research struggled to train transformer-based RL models. Emilio Parisotto and a DeepMind team combined them successfully with Gated Transformer-XL, or GTrXL. This network can substitute directly for an LSTM in RL applications.Key insight: A transformer’s attention component models out-of-sequence relationships. Consider a block-stacking task where the first and sixth actions taken are the most important to predicting whether the stack will be in the right order. GTrXL modifies the transformer architecture to allow it to learn sequential relationships early on (say, between the first and second actions, where the first action places the initial block and the second identifies which block needs to be picked up next) before it has learned out-of-sequence relationships.How it works: GTrXL modifies the transformer network (TrXL) as shown in the diagram above. GTrXL replaces the typical transformer’s residual connections with gated connections. This reduces errors that otherwise could flow through the residual connections.GTrXL applies layer normalization to the transformer’s sub-components but not to the gated connections. This allows the network to preserve information, including information derived directly from the input, over many residual connections while maintaining the attention mechanism’s performance.These modifications allow the network to learn from the order of input data while the attention mechanism hasn’t learned to model longer-term relationships. The shorter-term relationships are easier to model early on in training, making the network more stable during training. Results: On DMLab 30, an RL environment that supports puzzle tasks requiring long-term memory, GTrXL outperformed the previous state of the art (MERLIN) averaged across all 30 tasks. It also outperformed an LSTM, the ubiquitous recurrent layer in RL research.Why it matters: LSTMs have been essential to sequence-processing neural networks that work on short-term data. GTrXL give such networks longer-term memory. Longer time horizons eventually may help boost performance in life-long learning and meta-learning.We’re thinking: Since the original paper describing transformer networks was published in 2017, researchers have developed extensions. This work continues to show that, when it comes to transformers, there’s more than meets the eye.

Image to article:
- Caption to image: Comparison between TrXL and GTrXL
- URL to image: https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202019-11-2720at2011.00.3120AM.png
-----------------"
