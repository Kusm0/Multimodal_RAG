<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Doctors Distrust AI, New Life For Old Songs, ImageNet...</title><meta name="description" content="The Batch - AI News &amp; Insights: Would Your Doctor Take AI’s Advice? | ImageNet now comes with privacy protection | Neural networks can tease apart the different sounds in musical recordings" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><link rel="canonical" href="https://www.deeplearning.ai/the-batch/issue-86/"/><meta property="og:type" content="article" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:title" content="Doctors Distrust AI, New Life For Old Songs, ImageNet..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:description" content="The Batch - AI News &amp; Insights: Would Your Doctor Take AI’s Advice? | ImageNet now comes with privacy protection | Neural networks can tease apart the different sounds in musical recordings" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:site_name" content="Doctors Distrust AI, New Life For Old Songs, ImageNet..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:url" content="https://www.deeplearning.ai/the-batch/issue-86/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="article:published_time" content="2021-04-07T12:00:00.000-07:00"/><meta property="article:modified_time" content="2022-10-06T08:07:43.000-07:00"/><meta property="article:tag" content="The Batch Newsletter"/><meta property="article:tag" content="issue-86"/><meta property="article:tag" content="Apr 07, 2021"/><meta property="article:author" content="https://www.facebook.com/DeepLearningAIHQ/"/><meta property="twitter:title" content="Doctors Distrust AI, New Life For Old Songs, ImageNet..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:description" content="The Batch - AI News &amp; Insights: Would Your Doctor Take AI’s Advice? | ImageNet now comes with privacy protection | Neural networks can tease apart the different sounds in musical recordings" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:url" content="https://www.deeplearning.ai/the-batch/issue-86/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:label1" content="Written by"/><meta property="twitter:data1" content="DeepLearning.AI"/><meta property="twitter:label2" content="Filed under"/><meta property="twitter:data2" content="The Batch Newsletter"/><meta property="twitter:card" content="summary_large_image" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:creator" content="@DeepLearningAI"/><meta property="twitter:site" content="https://twitter.com/DeepLearningAI/"/><meta name="twitter:image" content="https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1--2.png"/><meta property="og:image" content="https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1--2.png"/><meta property="og:image:width" content="576"/><meta property="og:image:height" content="324"/><script type="application/ld+json">{"@context":"https://schema.org/","@type":"Article","datePublished":"2021-04-07T12:00:00.000-07:00","dateModified":"2022-10-06T08:07:43.000-07:00","author":{"@type":"Article","name":"DeepLearning.AI","image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","sameAs":"[\"https://www.deeplearning.ai/\", \"https://twitter.com/DeepLearningAI_/\", \"https://www.facebook.com/DeepLearningAIHQ/\"]"},"keywords":"[object Object], [object Object], [object Object]","headline":"Doctors Distrust AI, New Life For Old Songs, ImageNet...","image":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1--2.png","width":576,"height":324},"publisher":{"@type":"Organization","name":"Doctors Distrust AI, New Life For Old Songs, ImageNet...","logo":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","width":60,"height":60}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.deeplearning.ai"},"description":"The Batch - AI News & Insights: Would Your Doctor Take AI’s Advice? | ImageNet now comes with privacy protection | Neural networks can tease apart the different sounds in musical recordings"}</script><meta name="next-head-count" content="31"/><link href="/static/favicons/favicon.ico" rel="shortcut icon"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&amp;display=swap"/><link rel="preload" href="/_next/static/css/54b174af9991199d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/54b174af9991199d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6f396d9f2f265155.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6f396d9f2f265155.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-7690c02358b700f2.js" defer=""></script><script src="/_next/static/chunks/framework-c54e8763846ac33b.js" defer=""></script><script src="/_next/static/chunks/main-9fcc7c9a63f70912.js" defer=""></script><script src="/_next/static/chunks/pages/_app-bece9363d7a6c0bb.js" defer=""></script><script src="/_next/static/chunks/3a17f596-e9670ced3bfab562.js" defer=""></script><script src="/_next/static/chunks/36d2f571-09e32b60ec93a4dc.js" defer=""></script><script src="/_next/static/chunks/5c0b189e-c0f75ca9b44e520b.js" defer=""></script><script src="/_next/static/chunks/5567-47eb911ba5f222d7.js" defer=""></script><script src="/_next/static/chunks/2251-380253169bb2e795.js" defer=""></script><script src="/_next/static/chunks/3864-a4ae56e91852593b.js" defer=""></script><script src="/_next/static/chunks/4965-3585432476b27ea9.js" defer=""></script><script src="/_next/static/chunks/3791-0bbbd4cd0bf31c22.js" defer=""></script><script src="/_next/static/chunks/pages/the-batch/%5Bslug%5D-c03a8bcef188ef99.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_buildManifest.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_ssgManifest.js" defer=""></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&display=swap"/></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5C5VGGJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div data-sentry-component="Layout" data-sentry-source-file="Layout.tsx"><aside id="top-announcement" class="text-neutral-900  " style="color:#FFFFFF;background-color:#05256C;background-image:linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)" data-sentry-component="AnnouncementBanner" data-sentry-source-file="AnnouncementBanner.tsx"><div class="container--boxed py-3 flex items-center justify-between "><div class="flex items-center"><p class="text-sm lg:text-base">✨ New course! Enroll in<!-- --> <a href="https://bit.ly/3HlCuvP" class="underline" target="_blank">DSPy: Build and Optimize Agentic Apps</a></p></div><div class="flex items-center"><button class="bg-transparent p-0 border-none text-xl opacity-70 hover:opacity-100 transition-opacity"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><line x1="15" y1="9" x2="9" y2="15"></line><line x1="9" y1="9" x2="15" y2="15"></line></svg></button></div></div></aside><header id="main-navigation" data-testid="main-navigation-testid" class="static h-[64px] md:h-[100px] top-0 w-full z-[100] flex items-center bg-white transition-all ease-in-out duration-200" data-sentry-component="Header" data-sentry-source-file="index.tsx"><div class="container--boxed flex justify-between items-center"><div class="max-w-[185px] lg:max-w-[235px]"><a href="/the-batch/"><div class="flex items-center"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27300%27%20height=%2792%27/%3e"/></span><img alt="The Batch" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="The Batch" loading="lazy" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcSet="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=384&amp;q=75 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75 2x" src="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75"/></noscript></span></div></a></div><div class="flex items-center"><div class="hidden lg:flex items-center"><nav aria-label="Primary" data-sentry-component="Nav" data-sentry-source-file="Nav.tsx"><ul class="flex items-center p-0 m-0 list-none"><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/courses/"><div class="NavItem_navItemLink__Aq6E5"><span>Explore Courses</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/the-batch/"><div class="NavItem_navItemLink__Aq6E5"><span>AI Newsletter</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/"><div>The Batch</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/letters/"><div>Andrew&#x27;s Letter</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/data-points/"><div>Data Points</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/research/"><div>ML Research</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/"><div>Blog</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/community/"><div class="NavItem_navItemLink__Aq6E5"><span>Community</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="https://community.deeplearning.ai/"><div>Forum</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/events/"><div>Events</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/ambassador/"><div>Ambassadors</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/category/ambassador-spotlight/"><div>Ambassador Spotlight</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/resources/"><div class="NavItem_navItemLink__Aq6E5"><span>Resources</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/about/"><div class="NavItem_navItemLink__Aq6E5"><span>Company</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/about/"><div>About</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/careers/"><div>Careers</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/contact/"><div>Contact</div></a></li></ul></li></ul></nav><a class="btn--primary text-base font-medium whitespace-nowrap shadow h-fit py-3 px-4 lg:ml-2 xl:ml-10" data-sentry-element="Link" data-sentry-source-file="index.tsx" href="https://bit.ly/3RB9T8a">Start Learning</a></div><div class="flex items-center" data-sentry-element="Menu" data-sentry-component="MobileMenu" data-sentry-source-file="MobileMenu.tsx" data-headlessui-state=""><button class="lg:hidden" data-sentry-element="MenuButton" data-sentry-source-file="MobileMenu.tsx" id="headlessui-menu-button-:R1qa6:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-4xl text-slate-600" data-sentry-element="FiMenu" data-sentry-source-file="MobileMenu.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button></div></div></div></header><main><div id="content"><nav aria-label="Secondary" class="h-[66px] bg-white sticky top-[60px] z-40 shadow hidden lg:block" data-sentry-component="SecondaryNav" data-sentry-source-file="index.tsx"><div class="container--boxed h-full w-full flex items-center justify-between "><div class="relative flex h-full"><button class="h-full w-14 items-center justify-center absolute top-0 left-0 z-10 group bg-white bg-opacity-75 hidden"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="15 18 9 12 15 6"></polyline></svg></span></button><ul id="nav-secondary" class="list-none p-0 m-0 h-full flex items-center  overflow-x-scroll relative SecondaryNav_navItems__dok3i"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Weekly Issues</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/letters/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Andrew&#x27;s Letters</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/data-points/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Data Points</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/research/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">ML Research</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/business/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Business</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/science/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Science</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/culture/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Culture</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/hardware/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Hardware</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/ai-careers/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">AI Careers</div></a></li></ul><button class="h-full w-9 items-center justify-center absolute top-0 right-0 group bg-white bg-opacity-75 flex"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span></button></div><div class="flex items-center h-full"><ul class="list-none p-0 m-0 h-full flex items-center mr-5"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/about/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">About</div></a></li></ul><button type="button" class="bg-white border btn--tracking border-solid border-brand text-brand hover:bg-brand hover:text-white transition-colors px-3 py-1 rounded-md mr-4">Subscribe</button><a href="/search/"><div title="Search" class="transition-colors text-slate-400 hover:text-slate-500"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-2xl" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></div></a></div></div></nav><article class="pt-5 pb-16 bg-white lg:pt-16 lg:pb-16" data-sentry-component="ArticleDefault" data-sentry-source-file="ArticleDefault.tsx"><header class="post_layoutGrid__0BDX2"><nav aria-label="Breadcrumb flex items-center" data-sentry-component="Main" data-sentry-source-file="Breadcrumb.tsx"><ul class="list-none flex items-center flex-wrap"><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">The Batch</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">Weekly Issues</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/issue-86/"><div class="text-sm md:text-base text-slate-400 font-normal "><h1 class="capitalize">issue 86</h1></div></a></li></ul></nav><aside class="flex mt-6 lg:hidden"><div class="flex flex-col items-start items-start mr-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm text-sm">Published</div></div><div class="mt-1 text-slate-600 text-base text-sm">Apr 7, 2021</div></div><div class="flex flex-col items-start items-start" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm text-sm">Reading time</div></div><div class="mt-1 text-slate-600 text-base text-sm">11<!-- --> min read</div></div></aside></header><div class="post_layoutGrid__0BDX2 mt-9"><aside style="grid-column:wide-start / main-start" class="flex-col items-end hidden pr-10 lg:flex"><div class="flex flex-col items-start items-end" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm undefined">Published</div></div><div class="mt-1 text-slate-600 text-base undefined"><a href="/the-batch/tag/apr-07-2021/"><div class="inline-flex px-2 py-1 text-sm font-normal transition-colors rounded-md bg-slate-100 hover:bg-slate-200 text-slate-500">Apr 07, 2021</div></a></div></div><div class="flex flex-col items-start items-end mt-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm undefined">Reading time</div></div><div class="mt-1 text-slate-600 text-base undefined">11<!-- --> min read</div></div><div class="flex flex-col items-start items-end mt-6 sticky top-20" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-86/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-86/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-86/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></aside><div class="prose--styled justify-self-center post_postContent__wGZtc"><p><em>Dear friends,</em></p><p><em>Each year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latest <a href="https://www.edelman.com/sites/g/files/aatuss191/files/2021-03/2021%20Edelman%20Trust%20Barometer%20Tech%20Sector%20Report_0.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">Edelman Trust Barometer</a> contains a worrisome finding: While technology was ranked the most trusted industry in the U.S. last year, this year we plunged to ninth place. Trust in the tech industry fell to new lows in the majority of 27 countries surveyed.</em><br><br><em>Tech can be a huge force for moving the world forward, but many well meaning efforts will run into headwinds if we aren’t able to gain others’ trust. It’s more urgent than ever that we collectively act in a way that is genuinely deserving of the rest of society’s trust.</em><br><br><em>Trust is much harder to build than to destroy. One company that hypes AI can do more damage than 10 others that speak about it responsibly. One company that makes misleading statements can do more damage than 10 that speak honestly.</em></p><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1-.png" class="kg-image" alt="Chart with percent trust in tech sector vs. business" loading="lazy" width="576" height="324"></figure><p><em>How can we regain trust? Several steps are needed, but to my mind, chief among them are:</em></p><ul><li><em><strong>Straight talk.</strong> I think we’re all tired of hearing tech companies say they’re fighting for small businesses when they’re just fighting for their own bottom line. I realize that no company can address every issue under the sun, but when we speak about something, we owe it to the public to tell it like it is.</em><br></li><li><em><strong>Take responsibility.</strong> Tech’s influence on what people see and hear has a huge impact on their perception of reality. Our collective influence on automation has a huge impact on jobs. I hope that each organization will acknowledge the power it has and use it to benefit society.</em><br></li><li><em><strong>Engage and empathize.</strong> When someone who is honest and well meaning has a problem with what we do, our first step should be to try to understand their point of view, not to dismiss their concerns. Society has reasonable worries about tech’s concentration of power, fairness, and impact on jobs. Whether we agree or disagree in a certain instance, let's acknowledge the concern and see if we can address it honestly.</em></li></ul><p><em>Trying to fool the public and government officials doesn’t work. We often read in the news about politicians who know little about tech, and say things that reflect their lack of understanding. But let me tell you this: Every large government has at least a handful of people who are tech-savvy enough to see through the spin to the heart of an issue. Companies shouldn’t try to fool people and instead do the harder — but more effective — work of solving problems thoughtfully.</em></p><p><em>On the plus side, 62 percent of respondents to Edelman’s survey agreed that employees have the power to force corporations to change. CEO aren’t the only people responsible for what companies do. All employees have a responsibility to help build trustworthy businesses. Wherever you work, I hope you’ll support straight talk, taking responsibility, and engaging and empathizing.</em></p><p><em>Keep learning!</em></p><p><em>Andrew</em></p><p></p><h2 id="news">News</h2><figure class="kg-card kg-image-card"><img src="https://cdn2.hubspot.net/hub/5871640/hubfs/BLUR%20TG.gif?upscale=true&amp;name=BLUR%20TG.gif" class="kg-image" alt="Blurred human faces in different pictures" loading="lazy"></figure><h2 id="de-facing-imagenet">De-Facing ImageNet</h2><p>ImageNet now comes with privacy protection.</p><p><strong>What’s new:</strong> The team that manages the machine learning community’s go-to image dataset <a href="http://image-net.org/face-obfuscation/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">blurred</a> all the human faces pictured in it and tested how models trained on the modified images on a variety of image recognition tasks. The faces originally were included without consent.</p><p><strong>How it worked:</strong> The team used Amazon’s Rekognition platform to find faces in ImageNet’s nearly 1.5 million examples.</p><ul><li>Rekognition drew a bounding box around each of over 500,000 faces. (Some images contained more than one face.) Crowdsourced workers checked the model’s work and corrected errors where necessary. Then the team applied Gaussian blur to the area within bounding boxes.</li><li>The authors trained 24 image recognition architectures on the original ImageNet and copies of the same architectures on the blurred version, and compared their performance. The models trained on the blurred images were, on average, less accurate by under 1 percent. However, the decline was severe with respect to objects typically found close to a face, such as masks (-8.71 percent) and harmonicas (-8.93 percent).</li><li>They tested the blurred data’s effect on transfer learning by pretraining models using the unmodified and modified ImageNet and fine-tuning them for <a href="https://www.cs.toronto.edu/~kriz/cifar.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">object recognition</a>, <a href="https://vision.princeton.edu/projects/2010/SUN/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">scene recognition</a>, <a href="http://host.robots.ox.ac.uk/pascal/VOC/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">object detection</a>, and <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">facial attribute classification</a> (whether a person is smiling, wearing glasses, and the like). The models trained on blurred images performed roughly as well as those trained on unmodified ImageNet.</li><li>The face-blurred ImageNet will become the new official version, according to <em><a href="https://venturebeat.com/2021/03/16/imagenet-creators-find-blurring-faces-for-privacy-has-a-minimal-impact-on-accuracy/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">VentureBeat</a></em>.</li></ul><p><strong>Behind the news:</strong> This work is part of a wider movement toward protecting privacy in machine learning data. For instance, papers submitted to CVPR in recent years proposed models to automatically blur faces and license plates in <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Uittenbogaard_Privacy_Protection_in_Street-View_Panoramas_Using_Depth_and_Multi-View_Imagery_CVPR_2019_paper.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">Google Street View</a> as well as data for training <a href="https://arxiv.org/abs/1903.11027?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">autonomous vehicles</a>, and <a href="https://arxiv.org/abs/2007.05515?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">action recognition models</a>.</p><p><strong>Why it matters:</strong> Machine learning datasets need not violate privacy. We can develop datasets that both protect privacy and train good models.</p><p><strong>We’re thinking:</strong> Any loss of accuracy is painful, but a small loss is worthwhile to protect privacy. There’s more to life than optimizing test-set accuracy! We expect that most ImageNet-trained applications won’t suffer from the change, as they don’t involve objects that typically appear near to faces. Fine-tuning on a dataset obtained with permission might help for the rest.</p><hr><figure class="kg-card kg-image-card"><img src="https://cdn2.hubspot.net/hub/5871640/hubfs/SEER%20(1).gif?upscale=true&amp;name=SEER%20(1).gif" class="kg-image" alt="Data related to SElf-supERvised (SEER), an image classifier pretrained on uncurated, unlabeled images" loading="lazy"></figure><h2 id="pretraining-on-uncurated-data"><strong>Pretraining on Uncurated Data</strong></h2><p>It’s well established that pretraining a model on a large dataset improves performance on fine-tuned tasks. In sufficient quantity and paired with a big model, even data scraped from the internet at random can contribute to the performance boost.<br><br><strong>What’s new:</strong> Facebook researchers led by Priya Goyal built <a href="https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgBLHW3rbBF872GPwmW3FwJ6G1PMVpnW4S2pft3FVHzyN8DyDVNGlnMGW4PP4L97cFqslVlbwbL7yvFDfW2Fn8h-2WNsLhW6ctbWL1j8yZqW8QpsXs7LRPYmW85G0WR8BQkNVW6rNClY7zbRhhW6CPVz91LqySlW1hThV91rb9KJW7k540y64sJNFW3rSflk6RMsLgW5CGgfj3x07y7N3gCXmlJJsKgW39y9PH1FLmkGW4DBD6B4sZ86fMxX7v-9j6Y933B11?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985" rel="noopener">SElf-supERvised</a> (SEER), an image classifier pretrained on a huge number of uncurated, unlabeled images. It achieved better fine-tuned ImageNet accuracy than models pretrained on large datasets that were curated  to represent particular labels.<strong>Key insight:</strong> Large language models pretrained on billions of uncurated documents found in the wild, such as <a href="https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgX7zW6Z0FGx310VxbW64wx5c6QqqCSW8VnTRB6r3YWTVtYrhy79rFy7W5_dnck1wd87gW6T1jS86RVxH9W8h6sKq7Q6pYGW6Wyr1S85ZbyJN4Qd9rMZ55XFW22zwDy17bwCSW38zR7q1gf-k8W7Y11R815ZY5gW7Wf0_k235s-jW59Npct2H0r2RW22G-YK58TddKW2L5pfM5ZXQCtW2t0WHW90vCr_W4n_L4694STNzW5y7GBV4DvmdXW7d75V32kT_nC33wD1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985" rel="noopener">GPT-3</a>, have achieved state-of-the-art performance after fine-tuning on a smaller dataset. Computer vision should benefit likewise from such scale.<br><br><strong>How it works:</strong> The authors used a 1.3-billion parameter <a href="https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSyc3p_b1V1-WJV7CgW7QW3lCvwr2QsmvKW2YfKsS6nSlfsW5fm5GR5qGYpGW72kNsS3ckR5qW2Fb8hS5lZqCcW3kjy4w4xJJFSN2pCYmh-rDTdW5qM6hl6YgHXCW77GRq63cRsbyW6h2FZ1407Kb-W7TYHb39bVYNVW3YFj7z8Lgyy6N8Q-yxFBY35rW4X72HC2tDDLbVrfkrB9lqPDmW4ZXvXL6ggkZJW4LSDbv4905FRW4D0tcD5DCm8qW2ZwRvb3zD3W-W3TPcvc4jWdrnW3dhfL-5kfk6GW4WM94Q9hLHYNW4Kkj172kM4nPW83rZ7Y7DDj7mW5yXtlp740bj4W1d08jH4-G4PsW6RT1Jl2rsVcJW5bdGrw1y4w2qW4BhGgJ663lmRW3WJkpm7w2yJt3nQt1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985" rel="noopener">RegNet</a>, a convolutional neural network architecture similar to <a href="https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSxX3p_9LV1-WJV7CgPzcW5-pbKQ5KLn5FW7BQJmM3p0R5vW5PVLNL7l6mvZW80-pwZ6lzn22W4FHg9y94-0N3W3Q_hdT2lMKBxW7HZSkd2Lv-QxW409-2x3QQHh4W50Z0Yj5G2R-yW8bY5-Z6W_1r3W29HmdN3gDmcPW8T65R524LqY1W4s-N1g1cXGPtN7vNM-f-fCMPW45n7g94RtH3yVtnMND7cm6y6W8cZNVM8MXQ6zW1gXwjR4z8WfVW7j8YpF3sPczGW4TJXPh42dzNKW7GFzJQ3gDl_sW78hV-w3wtKy8VfnWv66BJHBkW551l9788GcYmN70DVr_jFGGkW5tvzV291FC7xW7PW7LR51k3SLW8t1WQs3KTZDm34CZ1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985" rel="noopener">ResNet</a>, pretrained on 1 billion images randomly scraped from Instagram.</p><ul><li>The pretraining procedure followed <a href="https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgSlHW986Zp749ZwVLVD7h8455CkrLW5vRhmG3tTxGRW1kWq0Q5gFRPWW4Cp4pj1JhRj1N22j0sMK5LbFW6mXXnt3-GSybVtsH9h2WhdGzW7yPFbd5zJz1VW6Z3TmR5y1LjDW2zDpxX9gzgYBN6qbqFqvdLZ_W7XcywY6CbS8ZW8zJp4Y5JppQ3W2hwMDM4X1fJ_N29H17y5ZN73W3h_q3L6V-T9NW6p_BJ95b6xL5W6VHfLd3ZmfwYW3SYk1y21FfJx3c0k1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985" rel="noopener">SwAV</a>, which was devised by several of the same researchers. SwAV receives representations — in this case, from the RegNet — and learns to group related images into a number of clusters by emphasizing similarities among them (similar to contrastive learning).</li><li>The authors fine-tuned the model on over 1 million images from <a href="https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSx13p_8SV1-WJV7CgMsZW2qVNfP5HkRHMW5jMBCL2Gn0m0W4F_YMd7zpj_mW8TclHy35ZVXxVMypKR7mngVrW6pQPFB6f-YyYV9QJ8W9flcm9W87vl2p744z6CW5Zc-7B85pn2sW5kKN7y1wShvbW3PvCDx7y88lwW1lV8lh2z3YTJW5JYw9W8Xh9rjW8_5N5S4kszVfW91fmQk7k5KnBW3r-xh-2GJsCYW2t7s4K1qjnqHW5t2jyj2Pz76kW12skZV6076_rW8p0hLz5BDcGKW4H_MX33LcSf4W9bM3zt4pDxb23lfz1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985" rel="noopener">ImageNet</a>.</li></ul><p><strong>Results:</strong> SEER achieved 84.2 percent top-1 accuracy on the ImageNet test set, 1.1 percent better than the best previous self-supervised, pretrained model (a ResNet of 795 million parameters pretrained on ImageNet using <a href="https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7Cg_2JN8MwkDY2fFxFMkq7m9Nf4jcW2sDmlh8VMxBDW8MxbN146LykpW5cC1cF7kCj3yW1Lp-BY2xHmqMW3kS4JZ2NrKPZW67WzW15_SQK2W7sr_PC5wzQ2hW7PXZkl5XBP2tW8LWNy49cXNNgW7tmvjc58j67dN7HlF0ckVCQZW7k5xkK61PgvFW4kvFHz44K_HKW3sPHJ74m9vMjW795y2Y40tGBtW5F-HDN14pJcJW7GtDVY1pWWJ4W8VDsbx4Xl-dl3gM61?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985" rel="noopener">SimCLRv2</a>). It was 4.3 percentage points better than a 91-million parameter <a href="https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgZQCW560Pvw3G0l03W4ZlnZF1wZZzDW95RlS16FBFBSW7dDGr_2SBlbxVbpZyC2j0vJqW99H_-W7j8L8LW7B19l55Njhj_VpXY_X5lVgXJT_LPn6xwrhkN5kJDkt8k_bjW3vHMC86K9rJBW42mwkk4xH90nVy8bN_28d7J0W8Wt3Wc54Mlk-W2K4_rq5CLkLPW1RG0FH2SpQWHW2YssrK5bZrZ7W5JW_Qk3QM6vZW1KSRFh5ChM_qF6nbHP6PL7s38Zy1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985" rel="noopener">ViT</a> pretrained on <a href="https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSyc3p_b1V1-WJV7CgVTLW7dh4-j6HxXWxW8pSyQF1ntC2bW3dwYs2461yKGW4PyDMh7xDgRwW8PklRP5dVSYGW2KHSYS4CjMd7W9jHh8w2Q3kZTW3mg5h53xtW6bW9lTPP-30RsXSW88nk1J6LJ-VXW4Vc-xF2BYw7zW7qsGJb1jDJ66W8R3xwq4C7LqtW6p90xm2DmcbYM7Y-Y6y-l6zVVtlCp6VcqgYMqYGM-dQ8F-W5brXdq4WccmzW5GW8QY3KrSY8W31Gmgv5rZvbDW76dBCk2rNTRwW5f6_xZ1T91C2W8SBdkd7WBfSvW4rDbZ21LmF6dW5LTRrT229NlwW580xk56_tmw_W8Nlxdg1d321jW1GRHvb59kG9bW8dLlhr4kmRMcN9g3FcWxlBgg32Zb1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx&__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6&__hssc=45788219.1.1623103068191&__hsfp=814333985" rel="noopener">JFT-300M</a>, a curated dataset of 300 million images from Google Search. SEER also excelled at few-shot learning: Fine-tuned on 10 percent of ImageNet, it achieved 77.9 percent accuracy, 2.2 percentage points lower than a SimCLRv2 model pretrained on 100 percent of ImageNet and fine-tuned on 10 percent of ImageNet.<br><br><strong>Why it matters:</strong> Scraping the internet could be as productive in computer vision as it has been in language processing. Just keep in mind that training models on such data risks violating privacy and consent as well as absorbing the various biases — including objectionable social biases — inherent on the internet.<br><br><strong>We’re thinking:</strong> This paper suggests a tradeoff between the costs of building a curated dataset and training on a larger corpus plucked from the wild. If the cost of curation is high for your application, maybe you can cut it and spend more on training.</p><hr><figure class="kg-card kg-image-card"><img src="https://cdn2.hubspot.net/hub/5871640/hubfs/INFLUENCE%20TG.gif?upscale=true&amp;name=INFLUENCE%20TG.gif" class="kg-image" alt="Data related to a diagnostic advice received from a machine learning model vs a human expert" loading="lazy"></figure><h2 id="would-your-doctor-take-ai%E2%80%99s-advice">Would Your Doctor Take AI’s Advice?</h2><p>Some doctors don’t trust a second opinion when it comes from an AI system.</p><p><strong>What’s new:</strong> A team at MIT and Regensburg University <a href="https://www.nature.com/articles/s41746-021-00385-9?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">investigated</a> how physicians responded to diagnostic advice they received from a machine learning model versus a human expert.</p><p><strong>How it works:</strong> The authors recruited doctors to diagnose chest X-rays.</p><ul><li>The physicians fell into two groups: 138 radiologists highly experienced in reading X-rays and 127 internal or emergency medicine specialists with less experience in that task.</li><li>For each case, the doctors were given either accurate or inaccurate advice and told that it was generated by either a model or human expert.</li><li>The physicians rated the advice and offered their own diagnosis.</li></ul><p><strong>Results:</strong> The radiologists generally rated as lower-quality advice they believed was generated by AI. The others rated AI and human advice to be roughly of equal quality.</p><p>Both groups made more accurate diagnoses when given accurate advice, regardless of its source. However, 27 percent of radiologists and 41 percent of the less experienced offered an incorrect diagnosis when given inaccurate advice.</p><p><strong>Behind the news:</strong> AI-powered diagnostic tools are proliferating and becoming more widely accepted <a href="https://www.nature.com/articles/s41746-020-00324-0?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">in the U.S.</a> and elsewhere. These tools may <a href="https://www.aiin.healthcare/topics/diagnostics/machine-learning-risk-assessment-cardiovascular-disease?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">work about as well as traditional methods</a> at predicting clinical outcomes. Those that work well <a href="https://www.wired.com/story/ai-diagnose-illnesses-country-rich/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">may only do so on certain populations</a> due to biased training data.</p><p><strong>Why it matters:</strong> It’s not enough to develop AI systems in isolation. It’s important also to understand how humans use them. The best diagnostic algorithm in the world won’t help if people don’t heed its recommendations.</p><p><strong>We’re thinking:</strong> While some doctors are skeptical of AI, others may trust it too much, which also can lead to errors. Practitioners in a wide variety of fields will need to cultivate a balance between skepticism and trust in machine learning systems. We welcome help from the computer-human interface community in wrestling with these challenges.</p><hr><h2 id="a-message-from-deeplearningai">A MESSAGE FROM <a href="https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io">DEEPLEARNING.AI</a></h2><figure class="kg-card kg-image-card"><a href="https://www.coursera.org/specializations/deep-learning?ref=dl-staging-website.ghost.io"><img src="https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20(1)-1.png?upscale=true&amp;name=The%20Batch%20(1)-1.png" class="kg-image" alt="The Batch (1)-1" loading="lazy"></a></figure><p>We've updated our Deep Learning Specialization with the latest advances. The new program covers TensorFlow 2 as well as advanced architectures like U-Net, MobileNet, and EfficientNet. Stay tuned for transformers! <a href="https://www.coursera.org/specializations/deep-learning?ref=dl-staging-website.ghost.io">Enroll now</a></p><hr><figure class="kg-card kg-image-card"><img src="https://cdn2.hubspot.net/hub/5871640/hubfs/upmix.gif?upscale=true&amp;name=upmix.gif" class="kg-image" alt="Recording unmixed" loading="lazy"></figure><h2 id="new-life-for-old-songs">New Life for Old Songs</h2><p>Neural networks can tease apart the different sounds in musical recordings.</p><p><strong>What’s new:</strong> Companies and hobbyists are using deep learning to separate voices and instruments in commercial recordings, <a href="https://www.wired.com/story/upmixing-audio-recordings-artificial-intelligence/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx"><em>Wired</em></a> reported. The process can improve the sound of old recordings and opens new possibilities for sampling, mash-ups, and other fresh uses.</p><p><strong>How it works:</strong> Finished recordings often combine voices, instruments, and other sounds recorded in a multitrack format into a smaller number of audio channels; say, one for mono or two for stereo. The mingling of signals limits how much the sonic balance can be changed afterward, but neural networks have learned to disentangle individual sounds — including noise and distortion — so they can be rebalanced or removed without access to the multitrack recordings.</p><ul><li><a href="https://www.theaudioresearchgroup.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">Audio Research Group</a> was founded by an audio technician at Abbey Road Studios, who developed a deep learning system to remix the Beatles’ 1964 hit, “She Loves You,” which was produced without multitracking.</li><li><a href="https://audionamix.com/2019/05/08/audionamix-showcases-advanced-audio-ai-at-vivatech-2/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">Audionamix</a> separates mono recordings into tracks for vocals, drums, bass guitar, and other sounds. The service has been used to manipulate old recordings for use in commercials and to purge television and film soundtracks of music, inadvertently playing in the background, that would be expensive to license.</li><li>French music streaming service Deezer offers <a href="https://github.com/deezer/spleeter?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">Spleeter</a>, an open-source system that unmixes recordings (pictured above). Users have scrubbed vocals to produce custom karaoke tracks, create <a href="https://twitter.com/waxpancake/status/1191519229029142528?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx">oddball mashups</a>, and cleanse their own recordings of unwanted noises.</li></ul><p><strong>Why it matters:</strong> Many worthwhile recordings are distorted or obscured by noises like an audience’s cheers or analog tape hiss, making the quality of the musicianship difficult to hear. Others could simply use a bit of buffing after decades of improvement in playback equipment. AI-powered unmixing can upgrade such recordings as well as inspire new uses for old sounds.</p><p><strong>We’re thinking:</strong> Endless remixes of our favorite Taylor Swift tracks? We like the sound of that!</p><hr><figure class="kg-card kg-image-card"><img src="https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif?upscale=true&amp;name=ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif" class="kg-image" alt="Examples of image generators using GANsformer" loading="lazy"></figure><h2 id="attention-for-image-generation">Attention for Image Generation</h2><p>Attention quantifies how each part of one input affects the various parts of another. Researchers added a step that reverses this comparison to produce more convincing images.<br><br><strong>What’s new:</strong> Drew A. Hudson at Stanford and C. Lawrence Zitnick at Facebook chalked up a new state of the art in generative modeling by integrating attention layers into a generative adversarial network (GAN). They call their system <a href="https://arxiv.org/abs/2103.01209?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">GANsformer</a>.<br><br><strong>Key insight:</strong> Typically, a GAN learns through competition between a generator that aims to produce realistic images and a discriminator that judges whether images are generated or real. <a href="https://arxiv.org/abs/1812.04948?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">StyleGAN</a> splits the generator into (a) a mapping network and (b) a synthesis network, and uses the output of the mapping network to control high-level properties (for example, pose and facial expression) of an image generated by the synthesis network. The output of the mapping layer can be viewed as a high-level representation of the scene, and the output of each layer of the synthesis network as a low-level representation. The authors devised a two-way version of attention, which they call duplex attention, to refine each representation based on the other.<br><br><strong>How it works:</strong> GANsformer is a modified StyleGAN. The authors trained it on four types of subject matter: faces in <a href="https://github.com/NVlabs/ffhq-dataset?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">FFHQ;</a> scenes composed of cubes, cylinders, and spheres in <a href="https://cs.stanford.edu/people/jcjohns/clevr/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">CLEVR</a>; pictures of bedrooms in <a href="https://github.com/fyu/lsun?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">LSUN</a>; and urban scenes in <a href="https://www.cityscapes-dataset.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">Cityscapes.</a></p><ul><li>Given a random vector, the mapping network produced an intermediate representation via a series of fully connected layers. Given a random vector, the synthesis network produced an image via alternating layers of convolution and duplex attention.</li><li>The authors fed the mapping network's intermediate representation to the synthesis network’s first duplex attention layer.</li><li>Duplex attention updated the synthesis network’s representation by calculating how each part of the image influenced the parts of the intermediate representation. Then it updated the intermediate representation by calculating how each of its parts influenced the parts of the image. In this way, the system refined the mapping network’s high-level view according to the synthesis network’s low-level details and vice versa.</li><li>The discriminator used duplex attention to iteratively hone the image representation along with a learned vector representing general scene characteristics. Like the synthesis network, it comprised alternating layers of convolution and duplex attention.</li></ul><p><strong>Results:</strong> GANsformer outperformed the previous state of the art on CLEVR, LSUN-Bedroom, and Cityscapes (comparing Fréchet Inception Distance based on representations produced by a pretrained <a href="https://arxiv.org/abs/1409.4842v1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">Inception</a> model). For example, on Cityscapes, GANsformer achieved 5.7589 FID compared to <a href="https://arxiv.org/abs/1912.04958?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">StyleGAN2</a>’s 8.3500 FID. GANsformer also learned more efficiently than a <a href="https://arxiv.org/abs/1406.2661?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">vanilla GAN</a>, StyleGAN, StyleGAN2, <a href="https://arxiv.org/abs/1810.10340?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">k-GAN</a>, and <a href="https://arxiv.org/abs/1805.08318?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx" rel="noopener">SAGAN</a>. It required a third as many training iterations to achieve equal performance.<br><br><strong>Why it matters:</strong> Duplex attention helps to generate scenes that make sense in terms of both the big picture and the details. Moreover, it uses memory and compute efficiently: Consumption grows linearly as input size increases. (In transformer-style self-attention, which evaluates the importance of each part of an input with respect to other parts of the same input, memory and compute cost grows quadratically with input size.)<br><br><strong>We’re thinking:</strong> Transformers, which alternate attention and fully connected layers, perform better than other architectures in language processing. This work, which alternates attention and convolutional layers, may bring similar improvements to image processing.</p></div><aside style="grid-column:main-end / wide-end" class="flex-col hidden pl-10 lg:flex"><div class="relative shadow rounded-lg overflow-hidden hover:shadow-sm transition-shadow" data-sentry-component="Advertisement" data-sentry-source-file="Advertisement.tsx"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:200%"></span><img alt="Become an AI professional with one of the world&#x27;s most popular Machine Learning courses. #BreakIntoAI with Machine Learning Specialization" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" class="rounded-lg" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="Become an AI professional with one of the world&#x27;s most popular Machine Learning courses. #BreakIntoAI with Machine Learning Specialization" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" loading="lazy" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="rounded-lg" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F4.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F4.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F4.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F4.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F4.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F4.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F4.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F4.png&amp;w=3840&amp;q=75 3840w" src="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F4.png&amp;w=3840&amp;q=75"/></noscript></span><a href="https://www.deeplearning.ai/courses/machine-learning-specialization/"><div class="absolute inset-0" data-gtm-event-title="BreakIntoAI with Machine Learning Specialization"></div></a></div></aside><footer class="mt-8 lg:hidden"><div class="flex flex-col items-start items-start " data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-86/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-86/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-86/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></footer></div></article><div class="py-16 bg-slate-50"><section id="subscribe" data-sentry-component="CtaNewsletter" data-sentry-source-file="CtaNewsletter.tsx"><div class="container--boxed relative"><div class="text-center"><h2 class="text--l2 text-slate-900">Subscribe to The Batch</h2><p class="text-base lg:text-lg text-slate-500 mt-3 max-w-md mx-auto">Stay updated with weekly AI News and Insights delivered to your inbox</p></div><div class="flex flex-col items-center mt-9"><div><div id="reactHubspotForm352" style="display:none"></div><div class="flex items-center justify-center"><svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-brand-teal" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" data-sentry-element="svg" data-sentry-component="Spinner" data-sentry-source-file="Spinner.tsx"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4" data-sentry-element="circle" data-sentry-source-file="Spinner.tsx"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z" data-sentry-element="path" data-sentry-source-file="Spinner.tsx"></path></svg></div></div></div></div></section></div></div></main><footer class="py-16 bg-brand-teal-900" data-sentry-component="Footer" data-sentry-source-file="index.tsx"><div class="flex flex-col items-center justify-center text-center container--boxed"><div class="max-w-[220px] flex items-center justify-center text-white"><svg viewBox="0 0 272 34" fill="none" xmlns="http://www.w3.org/2000/svg" width="272" height="34" aria-label="DeepLearning.AI" data-sentry-element="svg" data-sentry-component="DLAILogo" data-sentry-source-file="DLAILogo.tsx"><g fill="currentColor" data-sentry-element="g" data-sentry-source-file="DLAILogo.tsx"><path d="M56.775 16.108c0 6.206-4.252 10.005-10.747 10.005H39.42V5.961h6.608c6.495 0 10.747 3.924 10.747 10.147zm-10.747 7.306c4.778 0 7.337-2.724 7.337-7.306 0-4.581-2.56-7.452-7.337-7.452H42.73v14.758h3.298z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M66.97 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.259-8.226 8.006-8.226c4.571 0 7.804 3.159 7.804 7.856.007.535-.027 1.07-.103 1.6H62.412c.233 2.638 2.123 4.232 4.57 4.232 2.038 0 3.173-.988 3.786-2.234h3.582c-.915 2.802-3.448 5.032-7.38 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.484-4.03-2.24 0-4.044 1.525-4.394 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M84.938 26.371c-4.601 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.792 9.792 0 01-.116 1.6H80.367c.233 2.638 2.128 4.232 4.57 4.232 2.042 0 3.177-.988 3.786-2.234h3.582c-.902 2.802-3.435 5.032-7.367 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M104.917 9.885c4.221 0 7.541 3.245 7.541 8.166 0 4.92-3.32 8.32-7.541 8.32a7.277 7.277 0 01-3.095-.664 7.248 7.248 0 01-2.516-1.914v9.915h-3.319v-23.57h3.32v2.347a7.004 7.004 0 015.61-2.6zm-.729 2.871c-2.473 0-4.86 1.943-4.86 5.364 0 3.42 2.387 5.39 4.86 5.39 2.473 0 4.894-2.02 4.894-5.46 0-3.437-2.391-5.303-4.894-5.303v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M119.074 5.961v17.484h6.841v2.668h-10.16V5.961h3.319z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M135.584 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.927 9.927 0 01-.116 1.625h-12.258c.233 2.639 2.128 4.233 4.571 4.233 2.041 0 3.176-.988 3.785-2.235h3.582c-.902 2.777-3.435 5.007-7.367 5.007zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M153.116 9.885a6.864 6.864 0 013.088.633 6.839 6.839 0 012.475 1.946v-2.325h3.349v15.974h-3.349v-2.38a6.906 6.906 0 01-5.611 2.638c-4.165 0-7.514-3.39-7.514-8.32s3.353-8.166 7.562-8.166zm.699 2.87c-2.473 0-4.86 1.853-4.86 5.304 0 3.451 2.387 5.45 4.86 5.45 2.473 0 4.864-1.938 4.864-5.39 0-3.45-2.361-5.372-4.864-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M169.72 26.114h-3.319V10.139h3.319v2.325c.928-1.595 2.534-2.579 4.804-2.579v3.438h-.863c-2.447 0-3.958 1.014-3.958 4.405l.017 8.386z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M188.966 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.044 1.504-4.044 4.435v8.922h-3.319V10.14h3.319v1.826a6.16 6.16 0 014.774-2.08c3.755 0 6.582 2.347 6.582 6.812v9.416h-3.293v-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M196.049 5.905a2.105 2.105 0 011.309-1.931 2.117 2.117 0 012.294.458 2.102 2.102 0 01-1.48 3.588 2.109 2.109 0 01-1.509-.612 2.08 2.08 0 01-.614-1.503zm.431 4.234h3.319v15.974h-3.319V10.14z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M215.558 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.048 1.504-4.048 4.435v8.922h-3.337V10.14h3.319v1.826a6.192 6.192 0 014.796-2.08c3.755 0 6.56 2.338 6.56 6.803v9.425h-3.289l.018-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M229.538 9.885c2.62 0 4.571 1.216 5.559 2.579v-2.325h3.349V26.37c0 4.35-2.823 7.629-7.829 7.629-4.282 0-7.454-2.119-7.864-5.656h3.289c.496 1.655 2.274 2.785 4.575 2.785 2.559 0 4.48-1.569 4.48-4.758v-2.664a6.903 6.903 0 01-5.559 2.665c-4.222 0-7.571-3.392-7.571-8.321 0-4.93 3.337-8.166 7.571-8.166zm.699 2.87c-2.478 0-4.864 1.853-4.864 5.304 0 3.452 2.386 5.45 4.864 5.45 2.477 0 4.86-1.938 4.86-5.39 0-3.45-2.357-5.372-4.86-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M242.702 26.316a2.134 2.134 0 01-1.987-1.287 2.114 2.114 0 011.53-2.908 2.138 2.138 0 012.194.896c.235.349.361.76.361 1.18a2.096 2.096 0 01-1.289 1.956 2.11 2.11 0 01-.809.163z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M260.584 21.996h-8.477l-1.455 4.117h-3.453l7.251-20.2h3.846l7.247 20.2h-3.492l-1.467-4.117zM256.38 9.932l-3.341 9.365h6.612l-3.271-9.365z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M268.681 5.961H272v20.152h-3.319V5.961z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218zm.126-1.59c-3.734 0-6.76-3.207-6.76-7.16 0-3.954 3.018-7.16 6.75-7.16 3.734 0 6.76 3.206 6.76 7.16s-3.021 7.16-6.76 7.16h.01zm-.126-6.28c.729 0 1.44-.214 2.046-.617a3.67 3.67 0 001.356-1.646 3.652 3.652 0 00-.798-3.995 3.687 3.687 0 00-4.012-.794 3.679 3.679 0 00-1.653 1.35 3.655 3.655 0 00-.62 2.037c.002.971.39 1.902 1.08 2.59a3.698 3.698 0 002.601 1.076z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path></g></svg></div><nav class="mt-6 md:mt-10"><ul class="flex flex-wrap justify-center space-x-8 gap-x-8"><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/courses/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Courses</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/the-batch/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">The Batch</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/community/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Community</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/careers/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Careers</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/about/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">About</div></a></li></ul></nav><div class="flex mt-12"><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.facebook.com/1027125564106325" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsFacebook" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.instagram.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsInstagram" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.9 3.9 0 0 0-1.417.923A3.9 3.9 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.9 3.9 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.9 3.9 0 0 0-.923-1.417A3.9 3.9 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599s.453.546.598.92c.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.5 2.5 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.5 2.5 0 0 1-.92-.598 2.5 2.5 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233s.008-2.388.046-3.231c.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92s.546-.453.92-.598c.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92m-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217m0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://twitter.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsTwitter" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.linkedin.com/company/18246783" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsLinkedin" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.youtube.com/c/Deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsYoutube" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.01 2.01 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.01 2.01 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31 31 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.01 2.01 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A100 100 0 0 1 7.858 2zM6.4 5.209v4.818l4.157-2.408z"></path></svg></a></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"donations_enabled":false,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.121","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-86","id":"60bfddae274d5b003b1062f6","uuid":"a077dbec-ae04-44f4-8c95-8fbc7023af9e","title":"The Batch: Doctors Distrust AI, New Life For Old Songs, ImageNet Without Faces, Learning From Random Data","html":"\u003cp\u003e\u003cem\u003eDear friends,\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eEach year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latest \u003ca href=\"https://www.edelman.com/sites/g/files/aatuss191/files/2021-03/2021%20Edelman%20Trust%20Barometer%20Tech%20Sector%20Report_0.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eEdelman Trust Barometer\u003c/a\u003e contains a worrisome finding: While technology was ranked the most trusted industry in the U.S. last year, this year we plunged to ninth place. Trust in the tech industry fell to new lows in the majority of 27 countries surveyed.\u003c/em\u003e\u003cbr\u003e\u003cbr\u003e\u003cem\u003eTech can be a huge force for moving the world forward, but many well meaning efforts will run into headwinds if we aren’t able to gain others’ trust. It’s more urgent than ever that we collectively act in a way that is genuinely deserving of the rest of society’s trust.\u003c/em\u003e\u003cbr\u003e\u003cbr\u003e\u003cem\u003eTrust is much harder to build than to destroy. One company that hypes AI can do more damage than 10 others that speak about it responsibly. One company that makes misleading statements can do more damage than 10 that speak honestly.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1-.png\" class=\"kg-image\" alt=\"Chart with percent trust in tech sector vs. business\" loading=\"lazy\" width=\"576\" height=\"324\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eHow can we regain trust? Several steps are needed, but to my mind, chief among them are:\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cem\u003e\u003cstrong\u003eStraight talk.\u003c/strong\u003e I think we’re all tired of hearing tech companies say they’re fighting for small businesses when they’re just fighting for their own bottom line. I realize that no company can address every issue under the sun, but when we speak about something, we owe it to the public to tell it like it is.\u003c/em\u003e\u003cbr\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003e\u003cstrong\u003eTake responsibility.\u003c/strong\u003e Tech’s influence on what people see and hear has a huge impact on their perception of reality. Our collective influence on automation has a huge impact on jobs. I hope that each organization will acknowledge the power it has and use it to benefit society.\u003c/em\u003e\u003cbr\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003e\u003cstrong\u003eEngage and empathize.\u003c/strong\u003e When someone who is honest and well meaning has a problem with what we do, our first step should be to try to understand their point of view, not to dismiss their concerns. Society has reasonable worries about tech’s concentration of power, fairness, and impact on jobs. Whether we agree or disagree in a certain instance, let's acknowledge the concern and see if we can address it honestly.\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cem\u003eTrying to fool the public and government officials doesn’t work. We often read in the news about politicians who know little about tech, and say things that reflect their lack of understanding. But let me tell you this: Every large government has at least a handful of people who are tech-savvy enough to see through the spin to the heart of an issue. Companies shouldn’t try to fool people and instead do the harder — but more effective — work of solving problems thoughtfully.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eOn the plus side, 62 percent of respondents to Edelman’s survey agreed that employees have the power to force corporations to change. CEO aren’t the only people responsible for what companies do. All employees have a responsibility to help build trustworthy businesses. Wherever you work, I hope you’ll support straight talk, taking responsibility, and engaging and empathizing.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eKeep learning!\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eAndrew\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"news\"\u003eNews\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/BLUR%20TG.gif?upscale=true\u0026amp;name=BLUR%20TG.gif\" class=\"kg-image\" alt=\"Blurred human faces in different pictures\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"de-facing-imagenet\"\u003eDe-Facing ImageNet\u003c/h2\u003e\u003cp\u003eImageNet now comes with privacy protection.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e The team that manages the machine learning community’s go-to image dataset \u003ca href=\"http://image-net.org/face-obfuscation/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eblurred\u003c/a\u003e all the human faces pictured in it and tested how models trained on the modified images on a variety of image recognition tasks. The faces originally were included without consent.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it worked:\u003c/strong\u003e The team used Amazon’s Rekognition platform to find faces in ImageNet’s nearly 1.5 million examples.\u003c/p\u003e\u003cul\u003e\u003cli\u003eRekognition drew a bounding box around each of over 500,000 faces. (Some images contained more than one face.) Crowdsourced workers checked the model’s work and corrected errors where necessary. Then the team applied Gaussian blur to the area within bounding boxes.\u003c/li\u003e\u003cli\u003eThe authors trained 24 image recognition architectures on the original ImageNet and copies of the same architectures on the blurred version, and compared their performance. The models trained on the blurred images were, on average, less accurate by under 1 percent. However, the decline was severe with respect to objects typically found close to a face, such as masks (-8.71 percent) and harmonicas (-8.93 percent).\u003c/li\u003e\u003cli\u003eThey tested the blurred data’s effect on transfer learning by pretraining models using the unmodified and modified ImageNet and fine-tuning them for \u003ca href=\"https://www.cs.toronto.edu/~kriz/cifar.html?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eobject recognition\u003c/a\u003e, \u003ca href=\"https://vision.princeton.edu/projects/2010/SUN/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003escene recognition\u003c/a\u003e, \u003ca href=\"http://host.robots.ox.ac.uk/pascal/VOC/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eobject detection\u003c/a\u003e, and \u003ca href=\"http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003efacial attribute classification\u003c/a\u003e (whether a person is smiling, wearing glasses, and the like). The models trained on blurred images performed roughly as well as those trained on unmodified ImageNet.\u003c/li\u003e\u003cli\u003eThe face-blurred ImageNet will become the new official version, according to \u003cem\u003e\u003ca href=\"https://venturebeat.com/2021/03/16/imagenet-creators-find-blurring-faces-for-privacy-has-a-minimal-impact-on-accuracy/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eVentureBeat\u003c/a\u003e\u003c/em\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e This work is part of a wider movement toward protecting privacy in machine learning data. For instance, papers submitted to CVPR in recent years proposed models to automatically blur faces and license plates in \u003ca href=\"https://openaccess.thecvf.com/content_CVPR_2019/papers/Uittenbogaard_Privacy_Protection_in_Street-View_Panoramas_Using_Depth_and_Multi-View_Imagery_CVPR_2019_paper.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eGoogle Street View\u003c/a\u003e as well as data for training \u003ca href=\"https://arxiv.org/abs/1903.11027?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eautonomous vehicles\u003c/a\u003e, and \u003ca href=\"https://arxiv.org/abs/2007.05515?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eaction recognition models\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e Machine learning datasets need not violate privacy. We can develop datasets that both protect privacy and train good models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e Any loss of accuracy is painful, but a small loss is worthwhile to protect privacy. There’s more to life than optimizing test-set accuracy! We expect that most ImageNet-trained applications won’t suffer from the change, as they don’t involve objects that typically appear near to faces. Fine-tuning on a dataset obtained with permission might help for the rest.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/SEER%20(1).gif?upscale=true\u0026amp;name=SEER%20(1).gif\" class=\"kg-image\" alt=\"Data related to SElf-supERvised (SEER), an image classifier pretrained on uncurated, unlabeled images\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"pretraining-on-uncurated-data\"\u003e\u003cstrong\u003ePretraining on Uncurated Data\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIt’s well established that pretraining a model on a large dataset improves performance on fine-tuned tasks. In sufficient quantity and paired with a big model, even data scraped from the internet at random can contribute to the performance boost.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e Facebook researchers led by Priya Goyal built \u003ca href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgBLHW3rbBF872GPwmW3FwJ6G1PMVpnW4S2pft3FVHzyN8DyDVNGlnMGW4PP4L97cFqslVlbwbL7yvFDfW2Fn8h-2WNsLhW6ctbWL1j8yZqW8QpsXs7LRPYmW85G0WR8BQkNVW6rNClY7zbRhhW6CPVz91LqySlW1hThV91rb9KJW7k540y64sJNFW3rSflk6RMsLgW5CGgfj3x07y7N3gCXmlJJsKgW39y9PH1FLmkGW4DBD6B4sZ86fMxX7v-9j6Y933B11?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\u0026__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6\u0026__hssc=45788219.1.1623103068191\u0026__hsfp=814333985\" rel=\"noopener\"\u003eSElf-supERvised\u003c/a\u003e (SEER), an image classifier pretrained on a huge number of uncurated, unlabeled images. It achieved better fine-tuned ImageNet accuracy than models pretrained on large datasets that were curated  to represent particular labels.\u003cstrong\u003eKey insight:\u003c/strong\u003e Large language models pretrained on billions of uncurated documents found in the wild, such as \u003ca href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgX7zW6Z0FGx310VxbW64wx5c6QqqCSW8VnTRB6r3YWTVtYrhy79rFy7W5_dnck1wd87gW6T1jS86RVxH9W8h6sKq7Q6pYGW6Wyr1S85ZbyJN4Qd9rMZ55XFW22zwDy17bwCSW38zR7q1gf-k8W7Y11R815ZY5gW7Wf0_k235s-jW59Npct2H0r2RW22G-YK58TddKW2L5pfM5ZXQCtW2t0WHW90vCr_W4n_L4694STNzW5y7GBV4DvmdXW7d75V32kT_nC33wD1?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\u0026__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6\u0026__hssc=45788219.1.1623103068191\u0026__hsfp=814333985\" rel=\"noopener\"\u003eGPT-3\u003c/a\u003e, have achieved state-of-the-art performance after fine-tuning on a smaller dataset. Computer vision should benefit likewise from such scale.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e The authors used a 1.3-billion parameter \u003ca href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSyc3p_b1V1-WJV7CgW7QW3lCvwr2QsmvKW2YfKsS6nSlfsW5fm5GR5qGYpGW72kNsS3ckR5qW2Fb8hS5lZqCcW3kjy4w4xJJFSN2pCYmh-rDTdW5qM6hl6YgHXCW77GRq63cRsbyW6h2FZ1407Kb-W7TYHb39bVYNVW3YFj7z8Lgyy6N8Q-yxFBY35rW4X72HC2tDDLbVrfkrB9lqPDmW4ZXvXL6ggkZJW4LSDbv4905FRW4D0tcD5DCm8qW2ZwRvb3zD3W-W3TPcvc4jWdrnW3dhfL-5kfk6GW4WM94Q9hLHYNW4Kkj172kM4nPW83rZ7Y7DDj7mW5yXtlp740bj4W1d08jH4-G4PsW6RT1Jl2rsVcJW5bdGrw1y4w2qW4BhGgJ663lmRW3WJkpm7w2yJt3nQt1?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\u0026__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6\u0026__hssc=45788219.1.1623103068191\u0026__hsfp=814333985\" rel=\"noopener\"\u003eRegNet\u003c/a\u003e, a convolutional neural network architecture similar to \u003ca href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSxX3p_9LV1-WJV7CgPzcW5-pbKQ5KLn5FW7BQJmM3p0R5vW5PVLNL7l6mvZW80-pwZ6lzn22W4FHg9y94-0N3W3Q_hdT2lMKBxW7HZSkd2Lv-QxW409-2x3QQHh4W50Z0Yj5G2R-yW8bY5-Z6W_1r3W29HmdN3gDmcPW8T65R524LqY1W4s-N1g1cXGPtN7vNM-f-fCMPW45n7g94RtH3yVtnMND7cm6y6W8cZNVM8MXQ6zW1gXwjR4z8WfVW7j8YpF3sPczGW4TJXPh42dzNKW7GFzJQ3gDl_sW78hV-w3wtKy8VfnWv66BJHBkW551l9788GcYmN70DVr_jFGGkW5tvzV291FC7xW7PW7LR51k3SLW8t1WQs3KTZDm34CZ1?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\u0026__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6\u0026__hssc=45788219.1.1623103068191\u0026__hsfp=814333985\" rel=\"noopener\"\u003eResNet\u003c/a\u003e, pretrained on 1 billion images randomly scraped from Instagram.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe pretraining procedure followed \u003ca href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgSlHW986Zp749ZwVLVD7h8455CkrLW5vRhmG3tTxGRW1kWq0Q5gFRPWW4Cp4pj1JhRj1N22j0sMK5LbFW6mXXnt3-GSybVtsH9h2WhdGzW7yPFbd5zJz1VW6Z3TmR5y1LjDW2zDpxX9gzgYBN6qbqFqvdLZ_W7XcywY6CbS8ZW8zJp4Y5JppQ3W2hwMDM4X1fJ_N29H17y5ZN73W3h_q3L6V-T9NW6p_BJ95b6xL5W6VHfLd3ZmfwYW3SYk1y21FfJx3c0k1?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\u0026__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6\u0026__hssc=45788219.1.1623103068191\u0026__hsfp=814333985\" rel=\"noopener\"\u003eSwAV\u003c/a\u003e, which was devised by several of the same researchers. SwAV receives representations — in this case, from the RegNet — and learns to group related images into a number of clusters by emphasizing similarities among them (similar to contrastive learning).\u003c/li\u003e\u003cli\u003eThe authors fine-tuned the model on over 1 million images from \u003ca href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSx13p_8SV1-WJV7CgMsZW2qVNfP5HkRHMW5jMBCL2Gn0m0W4F_YMd7zpj_mW8TclHy35ZVXxVMypKR7mngVrW6pQPFB6f-YyYV9QJ8W9flcm9W87vl2p744z6CW5Zc-7B85pn2sW5kKN7y1wShvbW3PvCDx7y88lwW1lV8lh2z3YTJW5JYw9W8Xh9rjW8_5N5S4kszVfW91fmQk7k5KnBW3r-xh-2GJsCYW2t7s4K1qjnqHW5t2jyj2Pz76kW12skZV6076_rW8p0hLz5BDcGKW4H_MX33LcSf4W9bM3zt4pDxb23lfz1?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\u0026__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6\u0026__hssc=45788219.1.1623103068191\u0026__hsfp=814333985\" rel=\"noopener\"\u003eImageNet\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e SEER achieved 84.2 percent top-1 accuracy on the ImageNet test set, 1.1 percent better than the best previous self-supervised, pretrained model (a ResNet of 795 million parameters pretrained on ImageNet using \u003ca href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7Cg_2JN8MwkDY2fFxFMkq7m9Nf4jcW2sDmlh8VMxBDW8MxbN146LykpW5cC1cF7kCj3yW1Lp-BY2xHmqMW3kS4JZ2NrKPZW67WzW15_SQK2W7sr_PC5wzQ2hW7PXZkl5XBP2tW8LWNy49cXNNgW7tmvjc58j67dN7HlF0ckVCQZW7k5xkK61PgvFW4kvFHz44K_HKW3sPHJ74m9vMjW795y2Y40tGBtW5F-HDN14pJcJW7GtDVY1pWWJ4W8VDsbx4Xl-dl3gM61?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\u0026__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6\u0026__hssc=45788219.1.1623103068191\u0026__hsfp=814333985\" rel=\"noopener\"\u003eSimCLRv2\u003c/a\u003e). It was 4.3 percentage points better than a 91-million parameter \u003ca href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSwL3p_8yV1-WJV7CgZQCW560Pvw3G0l03W4ZlnZF1wZZzDW95RlS16FBFBSW7dDGr_2SBlbxVbpZyC2j0vJqW99H_-W7j8L8LW7B19l55Njhj_VpXY_X5lVgXJT_LPn6xwrhkN5kJDkt8k_bjW3vHMC86K9rJBW42mwkk4xH90nVy8bN_28d7J0W8Wt3Wc54Mlk-W2K4_rq5CLkLPW1RG0FH2SpQWHW2YssrK5bZrZ7W5JW_Qk3QM6vZW1KSRFh5ChM_qF6nbHP6PL7s38Zy1?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\u0026__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6\u0026__hssc=45788219.1.1623103068191\u0026__hsfp=814333985\" rel=\"noopener\"\u003eViT\u003c/a\u003e pretrained on \u003ca href=\"https://info.deeplearning.ai/e2t/tc/VVyn_G8m8PZKW1sqz8b36_c47W7zr5cm4pNmBTN2jjSyc3p_b1V1-WJV7CgVTLW7dh4-j6HxXWxW8pSyQF1ntC2bW3dwYs2461yKGW4PyDMh7xDgRwW8PklRP5dVSYGW2KHSYS4CjMd7W9jHh8w2Q3kZTW3mg5h53xtW6bW9lTPP-30RsXSW88nk1J6LJ-VXW4Vc-xF2BYw7zW7qsGJb1jDJ66W8R3xwq4C7LqtW6p90xm2DmcbYM7Y-Y6y-l6zVVtlCp6VcqgYMqYGM-dQ8F-W5brXdq4WccmzW5GW8QY3KrSY8W31Gmgv5rZvbDW76dBCk2rNTRwW5f6_xZ1T91C2W8SBdkd7WBfSvW4rDbZ21LmF6dW5LTRrT229NlwW580xk56_tmw_W8Nlxdg1d321jW1GRHvb59kG9bW8dLlhr4kmRMcN9g3FcWxlBgg32Zb1?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\u0026__hstc=45788219.68fb612ae4f193db4e77760adab4a8c9.1619038314269.1623097239970.1623103068191.6\u0026__hssc=45788219.1.1623103068191\u0026__hsfp=814333985\" rel=\"noopener\"\u003eJFT-300M\u003c/a\u003e, a curated dataset of 300 million images from Google Search. SEER also excelled at few-shot learning: Fine-tuned on 10 percent of ImageNet, it achieved 77.9 percent accuracy, 2.2 percentage points lower than a SimCLRv2 model pretrained on 100 percent of ImageNet and fine-tuned on 10 percent of ImageNet.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e Scraping the internet could be as productive in computer vision as it has been in language processing. Just keep in mind that training models on such data risks violating privacy and consent as well as absorbing the various biases — including objectionable social biases — inherent on the internet.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e This paper suggests a tradeoff between the costs of building a curated dataset and training on a larger corpus plucked from the wild. If the cost of curation is high for your application, maybe you can cut it and spend more on training.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/INFLUENCE%20TG.gif?upscale=true\u0026amp;name=INFLUENCE%20TG.gif\" class=\"kg-image\" alt=\"Data related to a diagnostic advice received from a machine learning model vs a human expert\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"would-your-doctor-take-ai%E2%80%99s-advice\"\u003eWould Your Doctor Take AI’s Advice?\u003c/h2\u003e\u003cp\u003eSome doctors don’t trust a second opinion when it comes from an AI system.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e A team at MIT and Regensburg University \u003ca href=\"https://www.nature.com/articles/s41746-021-00385-9?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003einvestigated\u003c/a\u003e how physicians responded to diagnostic advice they received from a machine learning model versus a human expert.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e The authors recruited doctors to diagnose chest X-rays.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe physicians fell into two groups: 138 radiologists highly experienced in reading X-rays and 127 internal or emergency medicine specialists with less experience in that task.\u003c/li\u003e\u003cli\u003eFor each case, the doctors were given either accurate or inaccurate advice and told that it was generated by either a model or human expert.\u003c/li\u003e\u003cli\u003eThe physicians rated the advice and offered their own diagnosis.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e The radiologists generally rated as lower-quality advice they believed was generated by AI. The others rated AI and human advice to be roughly of equal quality.\u003c/p\u003e\u003cp\u003eBoth groups made more accurate diagnoses when given accurate advice, regardless of its source. However, 27 percent of radiologists and 41 percent of the less experienced offered an incorrect diagnosis when given inaccurate advice.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e AI-powered diagnostic tools are proliferating and becoming more widely accepted \u003ca href=\"https://www.nature.com/articles/s41746-020-00324-0?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003ein the U.S.\u003c/a\u003e and elsewhere. These tools may \u003ca href=\"https://www.aiin.healthcare/topics/diagnostics/machine-learning-risk-assessment-cardiovascular-disease?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003ework about as well as traditional methods\u003c/a\u003e at predicting clinical outcomes. Those that work well \u003ca href=\"https://www.wired.com/story/ai-diagnose-illnesses-country-rich/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003emay only do so on certain populations\u003c/a\u003e due to biased training data.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e It’s not enough to develop AI systems in isolation. It’s important also to understand how humans use them. The best diagnostic algorithm in the world won’t help if people don’t heed its recommendations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e While some doctors are skeptical of AI, others may trust it too much, which also can lead to errors. Practitioners in a wide variety of fields will need to cultivate a balance between skepticism and trust in machine learning systems. We welcome help from the computer-human interface community in wrestling with these challenges.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"a-message-from-deeplearningai\"\u003eA MESSAGE FROM \u003ca href=\"https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io\"\u003eDEEPLEARNING.AI\u003c/a\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003ca href=\"https://www.coursera.org/specializations/deep-learning?ref=dl-staging-website.ghost.io\"\u003e\u003cimg src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/The%20Batch%20(1)-1.png?upscale=true\u0026amp;name=The%20Batch%20(1)-1.png\" class=\"kg-image\" alt=\"The Batch (1)-1\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/figure\u003e\u003cp\u003eWe've updated our Deep Learning Specialization with the latest advances. The new program covers TensorFlow 2 as well as advanced architectures like U-Net, MobileNet, and EfficientNet. Stay tuned for transformers! \u003ca href=\"https://www.coursera.org/specializations/deep-learning?ref=dl-staging-website.ghost.io\"\u003eEnroll now\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/upmix.gif?upscale=true\u0026amp;name=upmix.gif\" class=\"kg-image\" alt=\"Recording unmixed\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"new-life-for-old-songs\"\u003eNew Life for Old Songs\u003c/h2\u003e\u003cp\u003eNeural networks can tease apart the different sounds in musical recordings.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e Companies and hobbyists are using deep learning to separate voices and instruments in commercial recordings, \u003ca href=\"https://www.wired.com/story/upmixing-audio-recordings-artificial-intelligence/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003e\u003cem\u003eWired\u003c/em\u003e\u003c/a\u003e reported. The process can improve the sound of old recordings and opens new possibilities for sampling, mash-ups, and other fresh uses.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e Finished recordings often combine voices, instruments, and other sounds recorded in a multitrack format into a smaller number of audio channels; say, one for mono or two for stereo. The mingling of signals limits how much the sonic balance can be changed afterward, but neural networks have learned to disentangle individual sounds — including noise and distortion — so they can be rebalanced or removed without access to the multitrack recordings.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://www.theaudioresearchgroup.com/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eAudio Research Group\u003c/a\u003e was founded by an audio technician at Abbey Road Studios, who developed a deep learning system to remix the Beatles’ 1964 hit, “She Loves You,” which was produced without multitracking.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://audionamix.com/2019/05/08/audionamix-showcases-advanced-audio-ai-at-vivatech-2/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eAudionamix\u003c/a\u003e separates mono recordings into tracks for vocals, drums, bass guitar, and other sounds. The service has been used to manipulate old recordings for use in commercials and to purge television and film soundtracks of music, inadvertently playing in the background, that would be expensive to license.\u003c/li\u003e\u003cli\u003eFrench music streaming service Deezer offers \u003ca href=\"https://github.com/deezer/spleeter?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eSpleeter\u003c/a\u003e, an open-source system that unmixes recordings (pictured above). Users have scrubbed vocals to produce custom karaoke tracks, create \u003ca href=\"https://twitter.com/waxpancake/status/1191519229029142528?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\"\u003eoddball mashups\u003c/a\u003e, and cleanse their own recordings of unwanted noises.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e Many worthwhile recordings are distorted or obscured by noises like an audience’s cheers or analog tape hiss, making the quality of the musicianship difficult to hear. Others could simply use a bit of buffing after decades of improvement in playback equipment. AI-powered unmixing can upgrade such recordings as well as inspire new uses for old sounds.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e Endless remixes of our favorite Taylor Swift tracks? We like the sound of that!\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://cdn2.hubspot.net/hub/5871640/hubfs/ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif?upscale=true\u0026amp;name=ezgif.com-gif-maker%20-%202021-03-30T133105.644.gif\" class=\"kg-image\" alt=\"Examples of image generators using GANsformer\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"attention-for-image-generation\"\u003eAttention for Image Generation\u003c/h2\u003e\u003cp\u003eAttention quantifies how each part of one input affects the various parts of another. Researchers added a step that reverses this comparison to produce more convincing images.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e Drew A. Hudson at Stanford and C. Lawrence Zitnick at Facebook chalked up a new state of the art in generative modeling by integrating attention layers into a generative adversarial network (GAN). They call their system \u003ca href=\"https://arxiv.org/abs/2103.01209?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eGANsformer\u003c/a\u003e.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eKey insight:\u003c/strong\u003e Typically, a GAN learns through competition between a generator that aims to produce realistic images and a discriminator that judges whether images are generated or real. \u003ca href=\"https://arxiv.org/abs/1812.04948?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eStyleGAN\u003c/a\u003e splits the generator into (a) a mapping network and (b) a synthesis network, and uses the output of the mapping network to control high-level properties (for example, pose and facial expression) of an image generated by the synthesis network. The output of the mapping layer can be viewed as a high-level representation of the scene, and the output of each layer of the synthesis network as a low-level representation. The authors devised a two-way version of attention, which they call duplex attention, to refine each representation based on the other.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e GANsformer is a modified StyleGAN. The authors trained it on four types of subject matter: faces in \u003ca href=\"https://github.com/NVlabs/ffhq-dataset?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eFFHQ;\u003c/a\u003e scenes composed of cubes, cylinders, and spheres in \u003ca href=\"https://cs.stanford.edu/people/jcjohns/clevr/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eCLEVR\u003c/a\u003e; pictures of bedrooms in \u003ca href=\"https://github.com/fyu/lsun?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eLSUN\u003c/a\u003e; and urban scenes in \u003ca href=\"https://www.cityscapes-dataset.com/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eCityscapes.\u003c/a\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eGiven a random vector, the mapping network produced an intermediate representation via a series of fully connected layers. Given a random vector, the synthesis network produced an image via alternating layers of convolution and duplex attention.\u003c/li\u003e\u003cli\u003eThe authors fed the mapping network's intermediate representation to the synthesis network’s first duplex attention layer.\u003c/li\u003e\u003cli\u003eDuplex attention updated the synthesis network’s representation by calculating how each part of the image influenced the parts of the intermediate representation. Then it updated the intermediate representation by calculating how each of its parts influenced the parts of the image. In this way, the system refined the mapping network’s high-level view according to the synthesis network’s low-level details and vice versa.\u003c/li\u003e\u003cli\u003eThe discriminator used duplex attention to iteratively hone the image representation along with a learned vector representing general scene characteristics. Like the synthesis network, it comprised alternating layers of convolution and duplex attention.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e GANsformer outperformed the previous state of the art on CLEVR, LSUN-Bedroom, and Cityscapes (comparing Fréchet Inception Distance based on representations produced by a pretrained \u003ca href=\"https://arxiv.org/abs/1409.4842v1?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eInception\u003c/a\u003e model). For example, on Cityscapes, GANsformer achieved 5.7589 FID compared to \u003ca href=\"https://arxiv.org/abs/1912.04958?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eStyleGAN2\u003c/a\u003e’s 8.3500 FID. GANsformer also learned more efficiently than a \u003ca href=\"https://arxiv.org/abs/1406.2661?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003evanilla GAN\u003c/a\u003e, StyleGAN, StyleGAN2, \u003ca href=\"https://arxiv.org/abs/1810.10340?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003ek-GAN\u003c/a\u003e, and \u003ca href=\"https://arxiv.org/abs/1805.08318?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-_kCZ2EMFEUjnma6RV0MqqP4isrt_adR3dMfJW9LznQfQBba3w-knSdbtILOCgFhxirBXqx\" rel=\"noopener\"\u003eSAGAN\u003c/a\u003e. It required a third as many training iterations to achieve equal performance.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e Duplex attention helps to generate scenes that make sense in terms of both the big picture and the details. Moreover, it uses memory and compute efficiently: Consumption grows linearly as input size increases. (In transformer-style self-attention, which evaluates the importance of each part of an input with respect to other parts of the same input, memory and compute cost grows quadratically with input size.)\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e Transformers, which alternate attention and fully connected layers, perform better than other architectures in language processing. This work, which alternates attention and convolutional layers, may bring similar improvements to image processing.\u003c/p\u003e","comment_id":"60be9659274d5b003b10601f","feature_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1--2.png","featured":false,"visibility":"public","created_at":"2021-06-07T14:57:45.000-07:00","updated_at":"2022-10-06T08:07:43.000-07:00","published_at":"2021-04-07T12:00:00.000-07:00","custom_excerpt":"Each year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latest Edelman Trust Barometer contains a worrisome finding: While technology was ranked the most trusted industry in the U.S. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"60bfddad274d5b003b1062b8","name":"issue-86","slug":"issue-86","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-86/"},{"id":"63176d4e1dce94004d638e14","name":"Apr 07, 2021","slug":"apr-07-2021","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/apr-07-2021/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-86/","excerpt":"Each year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latest Edelman Trust Barometer contains a worrisome finding: While technology was ranked the most trusted industry in the U.S. ","reading_time":11,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Doctors Distrust AI, New Life For Old Songs, ImageNet...","meta_description":"The Batch - AI News \u0026 Insights: Would Your Doctor Take AI’s Advice? | ImageNet now comes with privacy protection | Neural networks can tease apart the different sounds in musical recordings","email_subject":null,"frontmatter":null,"feature_image_alt":"Chart with percent trust in tech sector vs. business","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen-Shot-2021-04-06-at-2.07.27-PM-copy--1--2.png","dimensions":{"width":576,"height":324}},"banner":{"title":"BreakIntoAI with Machine Learning Specialization","databaseId":28989,"id":"cG9zdDoyODk4OQ==","featuredImage":{"node":{"altText":"Become an AI professional with one of the world's most popular Machine Learning courses. #BreakIntoAI with Machine Learning Specialization","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/4.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://www.deeplearning.ai/courses/machine-learning-specialization/","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Databricks C1","date":"2025-06-04T07:12:29","databaseId":36631,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/3HlCuvP","courseName":"DSPy: Build and Optimize Agentic Apps","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)","isOpenInNewTab":true}},{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}}]}}},"__N_SSG":true},"page":"/the-batch/[slug]","query":{"slug":"issue-86"},"buildId":"Qy9Eh4N0MrfE3ddReY9v4","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>