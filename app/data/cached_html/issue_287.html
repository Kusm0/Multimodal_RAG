<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, and more...</title><meta name="description" content="The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><link rel="canonical" href="https://www.deeplearning.ai/the-batch/issue-287/"/><meta property="og:type" content="article" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:title" content="o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:description" content="The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:site_name" content="o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:url" content="https://www.deeplearning.ai/the-batch/issue-287/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="article:published_time" content="2025-02-05T14:40:00.000-08:00"/><meta property="article:modified_time" content="2025-02-05T14:43:07.000-08:00"/><meta property="article:tag" content="The Batch Newsletter"/><meta property="article:tag" content="Feb 05, 2025"/><meta property="article:author" content="https://www.facebook.com/DeepLearningAIHQ/"/><meta property="twitter:title" content="o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:description" content="The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:url" content="https://www.deeplearning.ai/the-batch/issue-287/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:label1" content="Written by"/><meta property="twitter:data1" content="DeepLearning.AI"/><meta property="twitter:label2" content="Filed under"/><meta property="twitter:data2" content="The Batch Newsletter"/><meta property="twitter:card" content="summary_large_image" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:creator" content="@DeepLearningAI"/><meta property="twitter:site" content="https://twitter.com/DeepLearningAI/"/><meta name="twitter:image" content="https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6.jpg"/><meta property="og:image" content="https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6.jpg"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="676"/><script type="application/ld+json">{"@context":"https://schema.org/","@type":"Article","datePublished":"2025-02-05T14:40:00.000-08:00","dateModified":"2025-02-05T14:43:07.000-08:00","author":{"@type":"Article","name":"DeepLearning.AI","image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","sameAs":"[\"https://www.deeplearning.ai/\", \"https://twitter.com/DeepLearningAI_/\", \"https://www.facebook.com/DeepLearningAIHQ/\"]"},"keywords":"[object Object], [object Object]","headline":"o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, and more...","image":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6.jpg","width":1200,"height":676},"publisher":{"@type":"Organization","name":"o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, and more...","logo":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","width":60,"height":60}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.deeplearning.ai"},"description":"The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer."}</script><meta name="next-head-count" content="30"/><link href="/static/favicons/favicon.ico" rel="shortcut icon"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&amp;display=swap"/><link rel="preload" href="/_next/static/css/54b174af9991199d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/54b174af9991199d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6f396d9f2f265155.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6f396d9f2f265155.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-7690c02358b700f2.js" defer=""></script><script src="/_next/static/chunks/framework-c54e8763846ac33b.js" defer=""></script><script src="/_next/static/chunks/main-9fcc7c9a63f70912.js" defer=""></script><script src="/_next/static/chunks/pages/_app-bece9363d7a6c0bb.js" defer=""></script><script src="/_next/static/chunks/3a17f596-e9670ced3bfab562.js" defer=""></script><script src="/_next/static/chunks/36d2f571-09e32b60ec93a4dc.js" defer=""></script><script src="/_next/static/chunks/5c0b189e-c0f75ca9b44e520b.js" defer=""></script><script src="/_next/static/chunks/5567-47eb911ba5f222d7.js" defer=""></script><script src="/_next/static/chunks/2251-380253169bb2e795.js" defer=""></script><script src="/_next/static/chunks/3864-a4ae56e91852593b.js" defer=""></script><script src="/_next/static/chunks/4965-3585432476b27ea9.js" defer=""></script><script src="/_next/static/chunks/3791-0bbbd4cd0bf31c22.js" defer=""></script><script src="/_next/static/chunks/pages/the-batch/%5Bslug%5D-c03a8bcef188ef99.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_buildManifest.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_ssgManifest.js" defer=""></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&display=swap"/></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5C5VGGJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div data-sentry-component="Layout" data-sentry-source-file="Layout.tsx"><aside id="top-announcement" class="text-neutral-900  " style="color:#FFFFFF;background-color:#05256C;background-image:linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)" data-sentry-component="AnnouncementBanner" data-sentry-source-file="AnnouncementBanner.tsx"><div class="container--boxed py-3 flex items-center justify-between "><div class="flex items-center"><p class="text-sm lg:text-base">✨ New course! Enroll in<!-- --> <a href="https://bit.ly/3HlCuvP" class="underline" target="_blank">DSPy: Build and Optimize Agentic Apps</a></p></div><div class="flex items-center"><button class="bg-transparent p-0 border-none text-xl opacity-70 hover:opacity-100 transition-opacity"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><line x1="15" y1="9" x2="9" y2="15"></line><line x1="9" y1="9" x2="15" y2="15"></line></svg></button></div></div></aside><header id="main-navigation" data-testid="main-navigation-testid" class="static h-[64px] md:h-[100px] top-0 w-full z-[100] flex items-center bg-white transition-all ease-in-out duration-200" data-sentry-component="Header" data-sentry-source-file="index.tsx"><div class="container--boxed flex justify-between items-center"><div class="max-w-[185px] lg:max-w-[235px]"><a href="/the-batch/"><div class="flex items-center"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27300%27%20height=%2792%27/%3e"/></span><img alt="The Batch" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="The Batch" loading="lazy" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcSet="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=384&amp;q=75 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75 2x" src="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75"/></noscript></span></div></a></div><div class="flex items-center"><div class="hidden lg:flex items-center"><nav aria-label="Primary" data-sentry-component="Nav" data-sentry-source-file="Nav.tsx"><ul class="flex items-center p-0 m-0 list-none"><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/courses/"><div class="NavItem_navItemLink__Aq6E5"><span>Explore Courses</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/the-batch/"><div class="NavItem_navItemLink__Aq6E5"><span>AI Newsletter</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/"><div>The Batch</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/letters/"><div>Andrew&#x27;s Letter</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/data-points/"><div>Data Points</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/research/"><div>ML Research</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/"><div>Blog</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/community/"><div class="NavItem_navItemLink__Aq6E5"><span>Community</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="https://community.deeplearning.ai/"><div>Forum</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/events/"><div>Events</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/ambassador/"><div>Ambassadors</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/category/ambassador-spotlight/"><div>Ambassador Spotlight</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/resources/"><div class="NavItem_navItemLink__Aq6E5"><span>Resources</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/about/"><div class="NavItem_navItemLink__Aq6E5"><span>Company</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/about/"><div>About</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/careers/"><div>Careers</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/contact/"><div>Contact</div></a></li></ul></li></ul></nav><a class="btn--primary text-base font-medium whitespace-nowrap shadow h-fit py-3 px-4 lg:ml-2 xl:ml-10" data-sentry-element="Link" data-sentry-source-file="index.tsx" href="https://bit.ly/3RB9T8a">Start Learning</a></div><div class="flex items-center" data-sentry-element="Menu" data-sentry-component="MobileMenu" data-sentry-source-file="MobileMenu.tsx" data-headlessui-state=""><button class="lg:hidden" data-sentry-element="MenuButton" data-sentry-source-file="MobileMenu.tsx" id="headlessui-menu-button-:R1qa6:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-4xl text-slate-600" data-sentry-element="FiMenu" data-sentry-source-file="MobileMenu.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button></div></div></div></header><main><div id="content"><nav aria-label="Secondary" class="h-[66px] bg-white sticky top-[60px] z-40 shadow hidden lg:block" data-sentry-component="SecondaryNav" data-sentry-source-file="index.tsx"><div class="container--boxed h-full w-full flex items-center justify-between "><div class="relative flex h-full"><button class="h-full w-14 items-center justify-center absolute top-0 left-0 z-10 group bg-white bg-opacity-75 hidden"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="15 18 9 12 15 6"></polyline></svg></span></button><ul id="nav-secondary" class="list-none p-0 m-0 h-full flex items-center  overflow-x-scroll relative SecondaryNav_navItems__dok3i"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Weekly Issues</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/letters/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Andrew&#x27;s Letters</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/data-points/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Data Points</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/research/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">ML Research</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/business/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Business</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/science/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Science</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/culture/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Culture</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/hardware/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Hardware</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/ai-careers/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">AI Careers</div></a></li></ul><button class="h-full w-9 items-center justify-center absolute top-0 right-0 group bg-white bg-opacity-75 flex"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span></button></div><div class="flex items-center h-full"><ul class="list-none p-0 m-0 h-full flex items-center mr-5"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/about/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">About</div></a></li></ul><button type="button" class="bg-white border btn--tracking border-solid border-brand text-brand hover:bg-brand hover:text-white transition-colors px-3 py-1 rounded-md mr-4">Subscribe</button><a href="/search/"><div title="Search" class="transition-colors text-slate-400 hover:text-slate-500"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-2xl" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></div></a></div></div></nav><article class="pt-5 pb-16 bg-white lg:pt-16 lg:pb-16" data-sentry-component="ArticleDefault" data-sentry-source-file="ArticleDefault.tsx"><header class="post_layoutGrid__0BDX2"><nav aria-label="Breadcrumb flex items-center" data-sentry-component="Main" data-sentry-source-file="Breadcrumb.tsx"><ul class="list-none flex items-center flex-wrap"><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">The Batch</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">Weekly Issues</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/issue-287/"><div class="text-sm md:text-base text-slate-400 font-normal "><h1 class="capitalize">issue 287</h1></div></a></li></ul></nav><aside class="flex mt-6 lg:hidden"><div class="flex flex-col items-start items-start mr-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm text-sm">Published</div></div><div class="mt-1 text-slate-600 text-base text-sm">Feb 5, 2025</div></div><div class="flex flex-col items-start items-start" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm text-sm">Reading time</div></div><div class="mt-1 text-slate-600 text-base text-sm">14<!-- --> min read</div></div></aside></header><div class="post_layoutGrid__0BDX2 mt-9"><aside style="grid-column:wide-start / main-start" class="flex-col items-end hidden pr-10 lg:flex"><div class="flex flex-col items-start items-end" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm undefined">Published</div></div><div class="mt-1 text-slate-600 text-base undefined"><a href="/the-batch/tag/feb-05-2025/"><div class="inline-flex px-2 py-1 text-sm font-normal transition-colors rounded-md bg-slate-100 hover:bg-slate-200 text-slate-500">Feb 05, 2025</div></a></div></div><div class="flex flex-col items-start items-end mt-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm undefined">Reading time</div></div><div class="mt-1 text-slate-600 text-base undefined">14<!-- --> min read</div></div><div class="flex flex-col items-start items-end mt-6 sticky top-20" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-287/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-287/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-287/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></aside><div class="prose--styled justify-self-center post_postContent__wGZtc"><p>Dear friends,</p><p>A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer. But we don’t seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more “10x professionals.”</p><p>There aren’t already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they’re not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely reimagine that job).</p><p>But for many jobs that primarily involve applying knowledge or processing information, AI will be transformative. In a few roles, I’m starting to see tech-savvy individuals coordinate a suite of technology tools to do things differently and start to have, if not yet 10x impact, then easily 2x impact. I expect this gap to grow.</p><p>10x engineers don’t write code 10 times faster. Instead, they make technical architecture decisions that result in dramatically better downstream impact, they spot problems and prioritize tasks more effectively, and instead of rewriting 10,000 lines of code (or labeling 10,000 training examples) they might figure out how to write just 100 lines (or collect 100 examples) to get the job done.</p><p>I think 10x marketers, recruiters, and analysts will, similarly, do things differently. For example, perhaps traditional marketers repeatedly write social media posts. 10x marketers might use AI to help write, but the transformation will go deeper than that. If they are deeply sophisticated in how to apply AI — ideally able to write code themselves to test ideas, automate tasks, or analyze data — they might end up running a lot more experiments, get better insights about what customers want, and generate much more precise or personalized messages than a traditional marketer, and thereby end up making 10x impact.</p><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6-1.jpg" class="kg-image" alt="Comic-style illustration of a confident woman and man standing beside bold ‘10X’ text on a bright background." loading="lazy" width="1200" height="676" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/10x_1200px_6-1.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/10x_1200px_6-1.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6-1.jpg 1200w" sizes="(min-width: 720px) 720px"></figure><p>Similarly, 10x recruiters won’t just use generative AI to help write emails to candidates or summarize interviews. (This level of use of prompting-based AI will soon become table stakes for many knowledge roles.) They might coordinate a suite of AI tools to efficiently identify and carry out research on a large set of candidates, enabling them to have dramatically greater impact than the average recruiter. And 10x analysts won’t just use generative AI to edit their reports. They might write code to orchestrate a suite of AI agents to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way.</p><p>A 2023 Harvard/BCG&nbsp;<a href="https://www.hbs.edu/faculty/Pages/item.aspx?num=64700&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">study</a>&nbsp;estimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the average, using 2023 technology. The maximum advantage to be gained by using AI in a sophisticated way will be much bigger, and will only grow as technology improves.</p><p>Here in Silicon Valley, I see more and more AI-native teams reinvent workflows and do things very differently. In software engineering, we've venerated the best engineers because they can have a really massive impact. This has motivated many generations of engineers to keep learning and working hard, because doing those things increases the odds of doing high-impact work. As AI becomes more helpful in many more job roles, I believe we will open up similar paths to a lot more people becoming a “10x professional.”</p><p>Keep learning!</p><p>Andrew</p><hr><h2 id="a-message-from-deeplearningai">A MESSAGE FROM&nbsp;DEEPLEARNING.AI</h2><figure class="kg-card kg-image-card"><a href="https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?ref=dl-staging-website.ghost.io"><img src="https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners--6-.png" class="kg-image" alt="Promo banner for &quot;How Transformer LLMs Work&quot;" loading="lazy" width="1680" height="945" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 1600w, https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 1680w" sizes="(min-width: 720px) 720px"></a></figure><p>Learn in detail how transformer-based large language models work in this new course by the authors of&nbsp;<em>Hands-On Large Language Models</em>. Explore the architecture introduced in the paper “Attention Is All You Need,” and learn through intuitive explanations and code examples.&nbsp;<a href="https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?ref=dl-staging-website.ghost.io" rel="noreferrer">Join in for free</a></p><h1 id="news">News</h1><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/02/O3.gif" class="kg-image" alt="Bar chart animation showing accuracy improvements in AIME 2024 competition math models." loading="lazy" width="600" height="338" srcset="https://dl-staging-website.ghost.io/content/images/2025/02/O3.gif 600w"></figure><h1 id="reasoning-in-high-gear"><strong>Reasoning in High Gear</strong></h1><p>OpenAI introduced a successor to its o1 models that’s faster, less expensive, and especially strong in coding, math, and science.</p><p><strong>What’s new:</strong>&nbsp;o3-mini is a large language model that offers selectable low, medium, and high levels of reasoning “effort.” These levels consume progressively higher numbers of reasoning tokens (specific numbers and methods are undisclosed), and thus greater time and cost, to generate a chain of thought. It’s&nbsp;<a href="https://openai.com/index/openai-o3-mini/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">available</a>&nbsp;to subscribers to ChatGPT Plus, Team, and Pro, as well as to higher-volume users of the API (tiers 3 through 5). Registered users can try it via the free ChatGPT service by selecting “reason” in the message composer or selecting o3-mini before regenerating a response.</p><p><strong>How it works:</strong>&nbsp;o3-mini’s training set emphasized structured problem-solving in science and technology fields, and fine-tuning used reinforcement learning on chain-of-thought (CoT) data. Like the o1 family, it charges for tokens that are processed during reasoning operations and hides them from the user. (Competing reasoning models DeepSeek-R1, Gemini 2.0 Flash Thinking, and QwQ-32B-Preview make these tokens available to users.) o3-mini has a maximum input of 200,000 tokens and a maximum output of 100,000 tokens. Its knowledge cutoff is October 2023.</p><ul><li>In OpenAI’s tests, o3-mini beat o1 and o1-mini on multiple benchmarks including math (AIME 2024), science (GPQA Diamond), and coding (Codeforces and LiveBench). It outperformed o1 by 1 to 4 percentage points when set at high or medium effort, and it outperformed o1-mini when set at low effort. It did significantly less well on tests of general knowledge, even with high effort. On MMLU (multiple-choice questions in many fields) and SimpleQA (questions about basic facts), o3-mini with high effort (which achieved 86.9 percent and 13.8 percent respectively) underperformed o1 (92.3 percent and 47 percent) and GPT-4o (88.7 percent and 39 percent).</li><li>Unlike o1-mini, o3-mini supports&nbsp;<a href="https://platform.openai.com/docs/guides/function-calling?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">function calling</a>,&nbsp;<a href="https://platform.openai.com/docs/guides/structured-outputs?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">structured outputs</a>&nbsp;(JSON format),&nbsp;<a href="https://platform.openai.com/docs/guides/text-generation?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP#building-prompts" rel="noopener">developer messages</a>&nbsp;(system prompts that specify the model’s context or persona separately from user input), and&nbsp;<a href="https://platform.openai.com/docs/api-reference/streaming?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">streaming</a>&nbsp;(delivering responses token-by-token in real time).</li><li>API access&nbsp;<a href="https://openai.com/api/pricing/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">costs</a>&nbsp;$1.10/$4.40 per million input/output tokens with a discounted rate of $0.55 per million cached input tokens. OpenAI’s&nbsp;<a href="https://platform.openai.com/docs/guides/batch?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Batch API</a>, which processes high-volume requests asynchronously, costs half as much. In comparison, access to o1 costs $15/$60 per million input/output tokens and o1-mini costs $3/$12 per million input/output tokens. (OpenAI recently removed API pricing for o1-mini and, in the ChatGPT model picker, replaced it with o3-mini, which suggests that o1-mini is being phased out.)</li><li>OpenAI&nbsp;<a href="https://platform.openai.com/docs/guides/rate-limits/usage-tiers?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP#tier-3-rate-limits" rel="noopener">limits</a>&nbsp;the number API calls users can make per minute and per day depending on how frequently they use the API and how much money they’ve spent. Rate limits range from 5,000/4 million requests/tokens per per minute (Tier 3) to 30,000/150 million requests/tokens per minute (Tier 5), with higher limits for batch requests.</li><li>o3-mini’s&nbsp;<a href="https://openai.com/index/o3-mini-system-card/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">system card</a>&nbsp;highlights safety measures taken during the model’s training. OpenAI notes that o3-mini’s improved coding ability puts it at a medium risk for autonomous misuse, the first OpenAI model to be so flagged.</li></ul><p><strong>What they’re saying:</strong>&nbsp;Users&nbsp;<a href="https://x.com/noahmacca/status/1885519380655857999?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">praised</a>&nbsp;o3-mini for its speed, reasoning, and coding abilities. They noted that it responds best to “chunkier” prompts with lots of context. However, due to its smaller size, it lacks extensive real-world knowledge and struggles to recall facts.</p><p><strong>Behind the news:</strong>&nbsp;Days after releasing o3-mini, OpenAI launched&nbsp;<a href="https://openai.com/index/introducing-deep-research/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">deep research</a>, a ChatGPT research agent based on o3. OpenAI had&nbsp;<a href="https://www.deeplearning.ai/the-batch/deepseek-v3-is-the-new-best-open-model/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">announced</a>&nbsp;the o3 model family in December, positioning it as an evolution of its chain-of-thought approach. The release followed hard upon that of DeepSeek-R1, an open weights model that captivated the AI community with its high performance and low training cost, but OpenAI&nbsp;<a href="https://www.wired.com/story/openai-o3-mini-release/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">maintained</a>&nbsp;that the debut took place on its original schedule.</p><p><strong>Why it matters:</strong>&nbsp;o3-mini continues OpenAI’s leadership in language models and further refines the reasoning capabilities introduced with the o1 family. In focusing on coding, math, and science tasks, it takes advantage of the strengths of reasoning models and raises the bar for other model builders. In practical terms, it pushes AI toward applications in which it’s a reliable professional partner rather than a smart intern.</p><p><strong>We’re thinking:</strong>&nbsp;We’re glad that o3-mini is available to users of ChatGPT’s free tier as well as paid subscribers and API users. The more users become familiar with how to prompt reasoning models, the more value they’ll deliver.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/02/UITARS.png" class="kg-image" alt="Flowchart illustrating the automation of opening, editing, and saving a Word document using PyAutoGUI." loading="lazy" width="2000" height="1125" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/UITARS.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/UITARS.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2025/02/UITARS.png 1600w, https://dl-staging-website.ghost.io/content/images/size/w2400/2025/02/UITARS.png 2400w" sizes="(min-width: 720px) 720px"></figure><h1 id="training-for-computer-use"><strong>Training for Computer Use</strong></h1><p>As Anthropic, Google, OpenAI, and others roll out agents that are capable of computer use, new work shows how underlying models can be trained to do this.</p><p><strong>What’s new:</strong>&nbsp;Yujian Qin and colleagues at ByteDance and Tsinghua University introduced&nbsp;<a href="https://arxiv.org/abs/2501.12326?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">UI-TARS</a>, a fine-tuned version of the vision-language model Qwen2-VL that uses lines of reasoning to decide which mouse clicks, keyboard presses, and other actions to take in desktop and mobile apps. The model’s weights are&nbsp;<a href="https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">licensed</a>&nbsp;freely for commercial and noncommercial uses via Apache 2.0. You can download them&nbsp;<a href="https://huggingface.co/bytedance-research/UI-TARS-72B-DPO?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">here</a>.</p><ul><li>The authors added&nbsp;<a href="https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">chains of thought</a>&nbsp;(CoTs) to their training set of screenshots and actions by prompting an unspecified vision-language model to explain the current action given previous screenshots, actions, and generated CoTs. Sometimes that process led to bad explanations, so they also generated multiple CoTs and actions for a given screenshot and selected the CoT that led to the correct action.</li><li>They fine-tuned UI-TARS to generate a CoT and action from an instruction (such as “Open the document and add the text ‘hello’”) plus the screenshots, CoTs, and actions so far.</li><li>They ran UI-TARS within a virtual PC, generating a large number of screenshots, CoTs, and actions so far. They filtered out erroneous CoTs and actions using rules (such as removing those that included redundant actions), scoring outputs automatically and removing those with low scores, and reviewing them manually. They fine-tuned the model on the remaining outputs and repeatedly generated, filtered, and fine-tuned.</li><li>They also fine-tuned the model on corrected examples of&nbsp;erroneous CoTs and actions. Human annotators corrected the CoT and action to (a) avoid the error and (b) fix the error after it occurred.</li><li>Finally, they fine-tuned the model using&nbsp;<a href="https://www.deeplearning.ai/the-batch/human-feedback-without-reinforcement-learning/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Direct Preference Optimization</a>&nbsp;(DPO) to prefer generating the corrected examples over the erroneous examples from the previous step.</li><li>At inference, given a screenshot, an instruction, and potential actions (as is typical with open computer use models; the authors provide a handy list in a sample prompt), UI-TARS generated a CoT and an action to take. After taking that action (via&nbsp;<a href="https://pypi.org/project/PyAutoGUI/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">PyAutoGUI</a>, a Python module that controls computers), the model received a new screenshot and generated another chain of thought and action, and so on. At each step, the model produced a new chain of thought and action, taking into account the instruction and all CoTs, actions, and screenshots so far.</li></ul><p><strong>Behind the news:</strong>&nbsp;Adept&nbsp;<a href="https://techcrunch.com/2022/04/26/2304039/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">touted</a>&nbsp;computer use in early 2022, and&nbsp;<a href="https://arxiv.org/abs/2408.00203?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">OmniParser</a>&nbsp;<a href="https://arxiv.org/abs/2412.04454?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Aguvis</a>&nbsp;soon followed with practical implementations. In October 2024, Anthropic set off the current wave of model/app interaction with its&nbsp;<a href="https://www.deeplearning.ai/the-batch/anthropic-empowers-claude-sonnet-3-5-to-operate-desktop-apps-but-cautions-remain/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">announcement</a>&nbsp;of computer use for Claude 3.5 Sonnet. OpenAI recently&nbsp;<a href="https://www.deeplearning.ai/the-batch/openais-operator-automates-online-tasks-with-a-new-ai-agent/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">responded</a>&nbsp;with Operator, its own foray into using vision and language models to control computers.</p><p><strong>Results:</strong>&nbsp;UI-TARS matched or outperformed Claude 3.5 Sonnet with computer use, GPT-4o with various computer use frameworks, and the Aguvis framework with its native model on 11 benchmarks. On OSWorld, which asks models to perform tasks using a variety of real-world applications and operating systems, UI-TARS successfully completed 22.7 percent of the tasks in 15 steps, whereas Claude 3.5 Sonnet with computer use completed 14.9 percent, GPT-4o with Aguvis 17 percent, and Aguvis with its native model 10.3 percent.</p><p><strong>Why it matters:</strong>&nbsp;Training a model to take good actions enables it to perform well. Training it to correct its mistakes after making them enables it to recover from unexpected issues that may occur in the real world.</p><p><strong>We</strong>’<strong>re thinking:</strong>&nbsp;Since computer use can be simulated in a virtual machine, it’s possible to generate massive amounts of training data automatically. This is bound to spur rapid progress in computer use by large language models.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/02/FLASH2THINKING.png" class="kg-image" alt="Line charts showing performance improvements in math and science with 2.0 Flash Thinking models." loading="lazy" width="1200" height="640" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/FLASH2THINKING.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/FLASH2THINKING.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/02/FLASH2THINKING.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="gemini-thinks-faster">Gemini Thinks Faster</h1><p>Google updated the December-vintage reasoning model Gemini 2.0 Flash Thinking and other Flash models, gaining ground on OpenAI o1 and DeepSeek-R1.</p><p><strong>What’s new:</strong>&nbsp;<a href="https://deepmind.google/technologies/gemini/flash-thinking?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Gemini 2.0 Flash Thinking Experimental 1-21</a>&nbsp;is a vision-language model (images and text in, text out) that’s trained to generate a structured reasoning process or chain of thought. The new version improves on its predecessor’s reasoning capability and extends its context window. It's free to access via&nbsp;<a href="https://ai.google.dev/gemini-api/docs/thinking?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">API</a>&nbsp;while it remains designated “experimental” and&nbsp;<a href="https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">available</a>&nbsp;to paid users of the Gemini app, along with&nbsp;<a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Gemini 2.0 Flash</a>&nbsp;(fresh out of experimental mode) and the newly released&nbsp;<a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-pro-exp-02-05&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Gemini 2.0 Pro Experimental</a>. The company also launched a preview of&nbsp;<a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-lite-preview-02-05&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Gemini 2.0 Flash Lite</a>, a vision-language model (images and text in, text out) that outperforms Gemini 1.5 Flash at the same price.</p><p><strong>How it works:</strong>&nbsp;Gemini 2.0 Flash Thinking Experimental 1-21 is based on&nbsp;<a href="https://www.deeplearning.ai/the-batch/google-introduces-gemini-2-0-flash-a-faster-more-capable-ai-model/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Gemini 2.0 Flash Experimental</a>&nbsp;(parameter count undisclosed). It processes up to 1 million tokens of input context, compared to its predecessor’s 32,000 and o1’s 128,000.</p><ul><li>Unlike o1, which hides its chain of thought, but like DeepSeek-R1 and Qwen QwQ, Gemini 2.0 Flash Thinking Experimental 1-21 includes its reasoning in its output.</li><li>On the graduate-level science exam GPQA-Diamond, it achieved 74.2 percent compared to the earlier version’s 58.6 percent, surpassing DeepSeek-R1 (71.5 percent) but behind o1 (77.3 percent).</li><li>On the advanced math benchmark AIME 2024, it achieved 73.3 percent compared to the previous version’s 35.5 percent, but it trails behind DeepSeek-R1 (79.8 percent) and o1 (74.4 percent).</li><li>On the visual and multimedia understanding test MMMU, it achieved 75.4 percent to outperform the previous version (70.7 percent) but fell short of o1 (78.2 percent).</li><li>Developers can integrate Python code execution via the&nbsp;<a href="https://ai.google.dev/gemini-api/docs/thinking?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">API</a>, with support for data analysis and visualization through&nbsp;<a href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution?t&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">pre-installed libraries</a>.</li></ul><p><strong>Speed bumps:</strong>&nbsp;Large language models that are trained to generate a&nbsp;<a href="https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">chain of thought</a>&nbsp;(CoT) are boosting accuracy even as the additional processing increases inference costs and latency. Reliable measures of Gemini 2.0 Flash Thinking Experimental 1-21’s speed are not yet available, but its base model runs faster (168.8 tokens per second with 0.46 seconds of latency to the first token,&nbsp;<a href="https://artificialanalysis.ai/models/gemini-2-0-flash-experimental?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">according to</a>&nbsp;Artificial Analysis) than all models in its class except o1-mini (which outputs 200 tokens per second with 10.59 seconds of latency to the first token).</p><p><strong>Why it matters:</strong>&nbsp;The combination of CoT reasoning and long context — assuming the new model can take advantage of its 1 million-token context window, as measured by a benchmark such as&nbsp;<a href="https://arxiv.org/abs/2404.06654?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">RULER</a>&nbsp;— could open up valuable applications. Imagine a reasoning model that can take an entire codebase as input and analyze it without breaking it into smaller chunks.<br><strong>We’re thinking:</strong>&nbsp;Regardless of benchmark performance, this model topped the&nbsp;<a href="https://lmarena.ai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Chatbot Arena</a>&nbsp;leaderboard at the time of writing. This suggests that users preferred it over o1 and DeepSeek-R1 — at least for common, everyday prompts.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/02/MOSHI.gif" class="kg-image" alt="Diagram illustrating Moshi’s use of an LLM to process user audio input, inner monologue, and output." loading="lazy" width="600" height="336" srcset="https://dl-staging-website.ghost.io/content/images/2025/02/MOSHI.gif 600w"></figure><h1 id="okay-but-please-don%E2%80%99t-stop-talking">Okay, But Please Don’t Stop Talking</h1><p>Even cutting-edge, end-to-end, speech-to-speech systems like ChatGPT’s Advanced Voice Mode tend to get interrupted by interjections like “I see” and “uh-huh” that keep human conversations going. Researchers built an open alternative that’s designed to go with the flow of overlapping speech.</p><p><strong>What’s new:</strong>&nbsp;Alexandre Défossez, Laurent Mazaré, and colleagues at Kyutai, a nonprofit research lab in Paris, released&nbsp;<a href="https://kyutai.org/Moshi.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Moshi</a>, an end-to-end, speech-to-speech system that’s always listening and always responding. The&nbsp;<a href="https://huggingface.co/kyutai/moshiko-pytorch-bf16?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">weights</a>&nbsp;and&nbsp;<a href="https://github.com/kyutai-labs/moshi/tree/main?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">code</a>&nbsp;are free for noncommercial and commercial uses under&nbsp;<a href="https://choosealicense.com/licenses/cc-by-4.0/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">CC-BY 4.0</a>,&nbsp;<a href="https://github.com/square/moshi/blob/master/LICENSE.txt?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Apache 2.0</a>, and&nbsp;<a href="https://github.com/kyutai-labs/moshi/blob/main/LICENSE-MIT?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">MIT</a>&nbsp;licenses. You can try a web demo&nbsp;<a href="https://moshi.chat/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">here</a>.</p><p><strong>Key insight:</strong>&nbsp;Up to 20 percent of spoken conversation consists of&nbsp;<a href="https://www.isca-archive.org/interspeech_2006/cetin06_interspeech.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">overlapping speech</a>, including interjections like “okay” and “I see.”</p><ul><li>To respond appropriately despite such overlaps, a system must both listen and generate sound continuously — although much of what it will generate is silence.</li><li>To respond without delay, it must keep latency to a minimum. This goal requires an end-to-end design rather than a pipeline of stand-alone models to perform voice detection, speech-to-text, text processing, and text-to-speech in turn.</li></ul><p><strong>How it works:</strong>&nbsp;The authors combined an encoder-decoder called Mimi and an&nbsp;<a href="https://arxiv.org/abs/2203.01941?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">RQ-Transformer</a>, which is made up of the Helium transformer-based large language model (LLM) plus another transformer.</p><ul><li>Mimi’s encoder embedded spoken input using 8 audio tokens per timestep (80 milliseconds). The authors trained Mimi on 7 million hours of mostly English speech from undisclosed sources. The training involved two loss terms. (i) The first loss term encouraged Mimi, given one audio timestep, to produce audio that fooled a pretrained&nbsp;<a href="https://arxiv.org/pdf/2210.13438?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">MS-STFT discriminator</a>&nbsp;into thinking it was human speech. The second loss term distilled knowledge from a pretrained&nbsp;<a href="https://huggingface.co/microsoft/wavlm-large?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">WavLM</a>, an audio embedding model. It encouraged Mimi’s encoder, when Mimi and WavLM received the same audio timestep, to produce one audio token (of its 8 audio tokens per timestep) whose embedding was similar to the corresponding embedding produced by WavLM.</li><li>Given the audio tokens, the Helium LLM produced text tokens that were used internally to help the additional transformer predict the next audio token (the idea being that the LLM’s skill with words would inform which audio token to generate next). The authors trained Helium to predict the next text token in 2.1 trillion tokens of English text (12.5 percent from&nbsp;<a href="https://dumps.wikimedia.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Wikipedia</a>&nbsp;and&nbsp;<a href="https://archive.org/details/stackexchange?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Stack Exchange</a>, and the remaining 87.5 percent from&nbsp;<a href="https://commoncrawl.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Common Crawl</a>).</li><li>RQ-Transformer received many sets of 17 tokens per time step: 8 audio tokens encoded by Mimi from the audio input, 8 audio tokens from Moshi’s previously generated audio output, and 1 text token produced by Helium. RQ-Transformer learned to predict the next set of 17 tokens in 7 million hours of audio and transcribed text.</li><li>To train the system specifically on conversational interaction, the authors further trained it to predict the next token in 2,000 hours of&nbsp;<a href="https://catalog.ldc.upenn.edu/LDC2004S13?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">recorded phone conversations</a>&nbsp;between randomly paired participants.</li><li>At inference, given a user's speech, Mimi turned it into audio tokens. Given the audio tokens and RQ-Transformer’s previously generated audio and text tokens, RQ-Transformer generated new audio and text tokens. From the generated audio tokens, Mimi produced synthetic speech.</li></ul><p><strong>Results:</strong>&nbsp;In tests, Moshi proved fast and relatively accurate.</p><ul><li>Moshi (7 billion parameters) took around 200 milliseconds to respond to user input. In comparison, GPT-4o, which also produces speech output directly from speech input, took 232 milliseconds minimum (320 milliseconds average). Prior to GPT-4o, ChatGPT Voice Mode (a pipeline of speech-to-text, text-to-text, and text-to-speech models) took an average of 5.4 seconds.</li><li>Moshi achieved 26.6 percent accuracy on Web Questions, higher than the speech-to-text-to-speech models tested by the authors:&nbsp;<a href="https://arxiv.org/abs/2305.15255?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">Spectron</a>&nbsp;(1 billion parameters) achieved 6.1 percent accuracy and&nbsp;<a href="https://arxiv.org/abs/2305.11000?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP" rel="noopener">SpeechGPT</a>&nbsp;(7 billion parameters) achieved 6.5 percent accuracy. The authors didn’t provide comparable results for GPT-4o or ChatGPT Voice.</li></ul><p><strong>Why it matters:</strong>&nbsp;While a turn-based approach may suffice for text input, voice-to-voice interactions benefit from a system that processes both input and output quickly and continuously. Previous systems process input and output separately, making users wait. Moshi delivers seamless interactivity.</p><p><strong>We’re thinking:</strong>&nbsp;Generating silence is golden!</p></div><aside style="grid-column:main-end / wide-end" class="flex-col hidden pl-10 lg:flex"><div class="relative shadow rounded-lg overflow-hidden hover:shadow-sm transition-shadow" data-sentry-component="Advertisement" data-sentry-source-file="Advertisement.tsx"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:200%"></span><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" class="rounded-lg" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" loading="lazy" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="rounded-lg" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=3840&amp;q=75 3840w" src="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=3840&amp;q=75"/></noscript></span><a href="https://bit.ly/3YTVMir"><div class="absolute inset-0" data-gtm-event-title="Generative AI for Software Development"></div></a></div></aside><footer class="mt-8 lg:hidden"><div class="flex flex-col items-start items-start " data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-287/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-287/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-287/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></footer></div></article><div class="py-16 bg-slate-50"><section id="subscribe" data-sentry-component="CtaNewsletter" data-sentry-source-file="CtaNewsletter.tsx"><div class="container--boxed relative"><div class="text-center"><h2 class="text--l2 text-slate-900">Subscribe to The Batch</h2><p class="text-base lg:text-lg text-slate-500 mt-3 max-w-md mx-auto">Stay updated with weekly AI News and Insights delivered to your inbox</p></div><div class="flex flex-col items-center mt-9"><div><div id="reactHubspotForm337" style="display:none"></div><div class="flex items-center justify-center"><svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-brand-teal" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" data-sentry-element="svg" data-sentry-component="Spinner" data-sentry-source-file="Spinner.tsx"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4" data-sentry-element="circle" data-sentry-source-file="Spinner.tsx"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z" data-sentry-element="path" data-sentry-source-file="Spinner.tsx"></path></svg></div></div></div></div></section></div></div></main><footer class="py-16 bg-brand-teal-900" data-sentry-component="Footer" data-sentry-source-file="index.tsx"><div class="flex flex-col items-center justify-center text-center container--boxed"><div class="max-w-[220px] flex items-center justify-center text-white"><svg viewBox="0 0 272 34" fill="none" xmlns="http://www.w3.org/2000/svg" width="272" height="34" aria-label="DeepLearning.AI" data-sentry-element="svg" data-sentry-component="DLAILogo" data-sentry-source-file="DLAILogo.tsx"><g fill="currentColor" data-sentry-element="g" data-sentry-source-file="DLAILogo.tsx"><path d="M56.775 16.108c0 6.206-4.252 10.005-10.747 10.005H39.42V5.961h6.608c6.495 0 10.747 3.924 10.747 10.147zm-10.747 7.306c4.778 0 7.337-2.724 7.337-7.306 0-4.581-2.56-7.452-7.337-7.452H42.73v14.758h3.298z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M66.97 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.259-8.226 8.006-8.226c4.571 0 7.804 3.159 7.804 7.856.007.535-.027 1.07-.103 1.6H62.412c.233 2.638 2.123 4.232 4.57 4.232 2.038 0 3.173-.988 3.786-2.234h3.582c-.915 2.802-3.448 5.032-7.38 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.484-4.03-2.24 0-4.044 1.525-4.394 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M84.938 26.371c-4.601 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.792 9.792 0 01-.116 1.6H80.367c.233 2.638 2.128 4.232 4.57 4.232 2.042 0 3.177-.988 3.786-2.234h3.582c-.902 2.802-3.435 5.032-7.367 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M104.917 9.885c4.221 0 7.541 3.245 7.541 8.166 0 4.92-3.32 8.32-7.541 8.32a7.277 7.277 0 01-3.095-.664 7.248 7.248 0 01-2.516-1.914v9.915h-3.319v-23.57h3.32v2.347a7.004 7.004 0 015.61-2.6zm-.729 2.871c-2.473 0-4.86 1.943-4.86 5.364 0 3.42 2.387 5.39 4.86 5.39 2.473 0 4.894-2.02 4.894-5.46 0-3.437-2.391-5.303-4.894-5.303v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M119.074 5.961v17.484h6.841v2.668h-10.16V5.961h3.319z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M135.584 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.927 9.927 0 01-.116 1.625h-12.258c.233 2.639 2.128 4.233 4.571 4.233 2.041 0 3.176-.988 3.785-2.235h3.582c-.902 2.777-3.435 5.007-7.367 5.007zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M153.116 9.885a6.864 6.864 0 013.088.633 6.839 6.839 0 012.475 1.946v-2.325h3.349v15.974h-3.349v-2.38a6.906 6.906 0 01-5.611 2.638c-4.165 0-7.514-3.39-7.514-8.32s3.353-8.166 7.562-8.166zm.699 2.87c-2.473 0-4.86 1.853-4.86 5.304 0 3.451 2.387 5.45 4.86 5.45 2.473 0 4.864-1.938 4.864-5.39 0-3.45-2.361-5.372-4.864-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M169.72 26.114h-3.319V10.139h3.319v2.325c.928-1.595 2.534-2.579 4.804-2.579v3.438h-.863c-2.447 0-3.958 1.014-3.958 4.405l.017 8.386z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M188.966 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.044 1.504-4.044 4.435v8.922h-3.319V10.14h3.319v1.826a6.16 6.16 0 014.774-2.08c3.755 0 6.582 2.347 6.582 6.812v9.416h-3.293v-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M196.049 5.905a2.105 2.105 0 011.309-1.931 2.117 2.117 0 012.294.458 2.102 2.102 0 01-1.48 3.588 2.109 2.109 0 01-1.509-.612 2.08 2.08 0 01-.614-1.503zm.431 4.234h3.319v15.974h-3.319V10.14z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M215.558 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.048 1.504-4.048 4.435v8.922h-3.337V10.14h3.319v1.826a6.192 6.192 0 014.796-2.08c3.755 0 6.56 2.338 6.56 6.803v9.425h-3.289l.018-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M229.538 9.885c2.62 0 4.571 1.216 5.559 2.579v-2.325h3.349V26.37c0 4.35-2.823 7.629-7.829 7.629-4.282 0-7.454-2.119-7.864-5.656h3.289c.496 1.655 2.274 2.785 4.575 2.785 2.559 0 4.48-1.569 4.48-4.758v-2.664a6.903 6.903 0 01-5.559 2.665c-4.222 0-7.571-3.392-7.571-8.321 0-4.93 3.337-8.166 7.571-8.166zm.699 2.87c-2.478 0-4.864 1.853-4.864 5.304 0 3.452 2.386 5.45 4.864 5.45 2.477 0 4.86-1.938 4.86-5.39 0-3.45-2.357-5.372-4.86-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M242.702 26.316a2.134 2.134 0 01-1.987-1.287 2.114 2.114 0 011.53-2.908 2.138 2.138 0 012.194.896c.235.349.361.76.361 1.18a2.096 2.096 0 01-1.289 1.956 2.11 2.11 0 01-.809.163z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M260.584 21.996h-8.477l-1.455 4.117h-3.453l7.251-20.2h3.846l7.247 20.2h-3.492l-1.467-4.117zM256.38 9.932l-3.341 9.365h6.612l-3.271-9.365z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M268.681 5.961H272v20.152h-3.319V5.961z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218zm.126-1.59c-3.734 0-6.76-3.207-6.76-7.16 0-3.954 3.018-7.16 6.75-7.16 3.734 0 6.76 3.206 6.76 7.16s-3.021 7.16-6.76 7.16h.01zm-.126-6.28c.729 0 1.44-.214 2.046-.617a3.67 3.67 0 001.356-1.646 3.652 3.652 0 00-.798-3.995 3.687 3.687 0 00-4.012-.794 3.679 3.679 0 00-1.653 1.35 3.655 3.655 0 00-.62 2.037c.002.971.39 1.902 1.08 2.59a3.698 3.698 0 002.601 1.076z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path></g></svg></div><nav class="mt-6 md:mt-10"><ul class="flex flex-wrap justify-center space-x-8 gap-x-8"><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/courses/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Courses</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/the-batch/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">The Batch</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/community/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Community</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/careers/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Careers</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/about/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">About</div></a></li></ul></nav><div class="flex mt-12"><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.facebook.com/1027125564106325" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsFacebook" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.instagram.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsInstagram" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.9 3.9 0 0 0-1.417.923A3.9 3.9 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.9 3.9 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.9 3.9 0 0 0-.923-1.417A3.9 3.9 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599s.453.546.598.92c.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.5 2.5 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.5 2.5 0 0 1-.92-.598 2.5 2.5 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233s.008-2.388.046-3.231c.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92s.546-.453.92-.598c.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92m-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217m0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://twitter.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsTwitter" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.linkedin.com/company/18246783" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsLinkedin" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.youtube.com/c/Deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsYoutube" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.01 2.01 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.01 2.01 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31 31 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.01 2.01 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A100 100 0 0 1 7.858 2zM6.4 5.209v4.818l4.157-2.408z"></path></svg></a></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"donations_enabled":false,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.121","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-287","id":"67a3e4c802b44f0001f4d881","uuid":"635cb7af-505a-4e72-a6b4-e11419459892","title":"o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, More-Responsive Voice Interactions","html":"\u003cp\u003eDear friends,\u003c/p\u003e\u003cp\u003eA “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer. But we don’t seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more “10x professionals.”\u003c/p\u003e\u003cp\u003eThere aren’t already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they’re not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely reimagine that job).\u003c/p\u003e\u003cp\u003eBut for many jobs that primarily involve applying knowledge or processing information, AI will be transformative. In a few roles, I’m starting to see tech-savvy individuals coordinate a suite of technology tools to do things differently and start to have, if not yet 10x impact, then easily 2x impact. I expect this gap to grow.\u003c/p\u003e\u003cp\u003e10x engineers don’t write code 10 times faster. Instead, they make technical architecture decisions that result in dramatically better downstream impact, they spot problems and prioritize tasks more effectively, and instead of rewriting 10,000 lines of code (or labeling 10,000 training examples) they might figure out how to write just 100 lines (or collect 100 examples) to get the job done.\u003c/p\u003e\u003cp\u003eI think 10x marketers, recruiters, and analysts will, similarly, do things differently. For example, perhaps traditional marketers repeatedly write social media posts. 10x marketers might use AI to help write, but the transformation will go deeper than that. If they are deeply sophisticated in how to apply AI — ideally able to write code themselves to test ideas, automate tasks, or analyze data — they might end up running a lot more experiments, get better insights about what customers want, and generate much more precise or personalized messages than a traditional marketer, and thereby end up making 10x impact.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6-1.jpg\" class=\"kg-image\" alt=\"Comic-style illustration of a confident woman and man standing beside bold ‘10X’ text on a bright background.\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/10x_1200px_6-1.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/10x_1200px_6-1.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6-1.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eSimilarly, 10x recruiters won’t just use generative AI to help write emails to candidates or summarize interviews. (This level of use of prompting-based AI will soon become table stakes for many knowledge roles.) They might coordinate a suite of AI tools to efficiently identify and carry out research on a large set of candidates, enabling them to have dramatically greater impact than the average recruiter. And 10x analysts won’t just use generative AI to edit their reports. They might write code to orchestrate a suite of AI agents to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way.\u003c/p\u003e\u003cp\u003eA 2023 Harvard/BCG\u0026nbsp;\u003ca href=\"https://www.hbs.edu/faculty/Pages/item.aspx?num=64700\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003estudy\u003c/a\u003e\u0026nbsp;estimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the average, using 2023 technology. The maximum advantage to be gained by using AI in a sophisticated way will be much bigger, and will only grow as technology improves.\u003c/p\u003e\u003cp\u003eHere in Silicon Valley, I see more and more AI-native teams reinvent workflows and do things very differently. In software engineering, we've venerated the best engineers because they can have a really massive impact. This has motivated many generations of engineers to keep learning and working hard, because doing those things increases the odds of doing high-impact work. As AI becomes more helpful in many more job roles, I believe we will open up similar paths to a lot more people becoming a “10x professional.”\u003c/p\u003e\u003cp\u003eKeep learning!\u003c/p\u003e\u003cp\u003eAndrew\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"a-message-from-deeplearningai\"\u003eA MESSAGE FROM\u0026nbsp;DEEPLEARNING.AI\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003ca href=\"https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?ref=dl-staging-website.ghost.io\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners--6-.png\" class=\"kg-image\" alt=\"Promo banner for \u0026quot;How Transformer LLMs Work\u0026quot;\" loading=\"lazy\" width=\"1680\" height=\"945\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 1600w, https://dl-staging-website.ghost.io/content/images/2025/02/The-Batch-ads-and-exclusive-banners--6-.png 1680w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/a\u003e\u003c/figure\u003e\u003cp\u003eLearn in detail how transformer-based large language models work in this new course by the authors of\u0026nbsp;\u003cem\u003eHands-On Large Language Models\u003c/em\u003e. Explore the architecture introduced in the paper “Attention Is All You Need,” and learn through intuitive explanations and code examples.\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?ref=dl-staging-website.ghost.io\" rel=\"noreferrer\"\u003eJoin in for free\u003c/a\u003e\u003c/p\u003e\u003ch1 id=\"news\"\u003eNews\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/02/O3.gif\" class=\"kg-image\" alt=\"Bar chart animation showing accuracy improvements in AIME 2024 competition math models.\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2025/02/O3.gif 600w\"\u003e\u003c/figure\u003e\u003ch1 id=\"reasoning-in-high-gear\"\u003e\u003cstrong\u003eReasoning in High Gear\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOpenAI introduced a successor to its o1 models that’s faster, less expensive, and especially strong in coding, math, and science.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;o3-mini is a large language model that offers selectable low, medium, and high levels of reasoning “effort.” These levels consume progressively higher numbers of reasoning tokens (specific numbers and methods are undisclosed), and thus greater time and cost, to generate a chain of thought. It’s\u0026nbsp;\u003ca href=\"https://openai.com/index/openai-o3-mini/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eavailable\u003c/a\u003e\u0026nbsp;to subscribers to ChatGPT Plus, Team, and Pro, as well as to higher-volume users of the API (tiers 3 through 5). Registered users can try it via the free ChatGPT service by selecting “reason” in the message composer or selecting o3-mini before regenerating a response.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;o3-mini’s training set emphasized structured problem-solving in science and technology fields, and fine-tuning used reinforcement learning on chain-of-thought (CoT) data. Like the o1 family, it charges for tokens that are processed during reasoning operations and hides them from the user. (Competing reasoning models DeepSeek-R1, Gemini 2.0 Flash Thinking, and QwQ-32B-Preview make these tokens available to users.) o3-mini has a maximum input of 200,000 tokens and a maximum output of 100,000 tokens. Its knowledge cutoff is October 2023.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIn OpenAI’s tests, o3-mini beat o1 and o1-mini on multiple benchmarks including math (AIME 2024), science (GPQA Diamond), and coding (Codeforces and LiveBench). It outperformed o1 by 1 to 4 percentage points when set at high or medium effort, and it outperformed o1-mini when set at low effort. It did significantly less well on tests of general knowledge, even with high effort. On MMLU (multiple-choice questions in many fields) and SimpleQA (questions about basic facts), o3-mini with high effort (which achieved 86.9 percent and 13.8 percent respectively) underperformed o1 (92.3 percent and 47 percent) and GPT-4o (88.7 percent and 39 percent).\u003c/li\u003e\u003cli\u003eUnlike o1-mini, o3-mini supports\u0026nbsp;\u003ca href=\"https://platform.openai.com/docs/guides/function-calling?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003efunction calling\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://platform.openai.com/docs/guides/structured-outputs?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003estructured outputs\u003c/a\u003e\u0026nbsp;(JSON format),\u0026nbsp;\u003ca href=\"https://platform.openai.com/docs/guides/text-generation?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP#building-prompts\" rel=\"noopener\"\u003edeveloper messages\u003c/a\u003e\u0026nbsp;(system prompts that specify the model’s context or persona separately from user input), and\u0026nbsp;\u003ca href=\"https://platform.openai.com/docs/api-reference/streaming?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003estreaming\u003c/a\u003e\u0026nbsp;(delivering responses token-by-token in real time).\u003c/li\u003e\u003cli\u003eAPI access\u0026nbsp;\u003ca href=\"https://openai.com/api/pricing/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003ecosts\u003c/a\u003e\u0026nbsp;$1.10/$4.40 per million input/output tokens with a discounted rate of $0.55 per million cached input tokens. OpenAI’s\u0026nbsp;\u003ca href=\"https://platform.openai.com/docs/guides/batch?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eBatch API\u003c/a\u003e, which processes high-volume requests asynchronously, costs half as much. In comparison, access to o1 costs $15/$60 per million input/output tokens and o1-mini costs $3/$12 per million input/output tokens. (OpenAI recently removed API pricing for o1-mini and, in the ChatGPT model picker, replaced it with o3-mini, which suggests that o1-mini is being phased out.)\u003c/li\u003e\u003cli\u003eOpenAI\u0026nbsp;\u003ca href=\"https://platform.openai.com/docs/guides/rate-limits/usage-tiers?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP#tier-3-rate-limits\" rel=\"noopener\"\u003elimits\u003c/a\u003e\u0026nbsp;the number API calls users can make per minute and per day depending on how frequently they use the API and how much money they’ve spent. Rate limits range from 5,000/4 million requests/tokens per per minute (Tier 3) to 30,000/150 million requests/tokens per minute (Tier 5), with higher limits for batch requests.\u003c/li\u003e\u003cli\u003eo3-mini’s\u0026nbsp;\u003ca href=\"https://openai.com/index/o3-mini-system-card/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003esystem card\u003c/a\u003e\u0026nbsp;highlights safety measures taken during the model’s training. OpenAI notes that o3-mini’s improved coding ability puts it at a medium risk for autonomous misuse, the first OpenAI model to be so flagged.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhat they’re saying:\u003c/strong\u003e\u0026nbsp;Users\u0026nbsp;\u003ca href=\"https://x.com/noahmacca/status/1885519380655857999?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003epraised\u003c/a\u003e\u0026nbsp;o3-mini for its speed, reasoning, and coding abilities. They noted that it responds best to “chunkier” prompts with lots of context. However, due to its smaller size, it lacks extensive real-world knowledge and struggles to recall facts.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;Days after releasing o3-mini, OpenAI launched\u0026nbsp;\u003ca href=\"https://openai.com/index/introducing-deep-research/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003edeep research\u003c/a\u003e, a ChatGPT research agent based on o3. OpenAI had\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/deepseek-v3-is-the-new-best-open-model/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eannounced\u003c/a\u003e\u0026nbsp;the o3 model family in December, positioning it as an evolution of its chain-of-thought approach. The release followed hard upon that of DeepSeek-R1, an open weights model that captivated the AI community with its high performance and low training cost, but OpenAI\u0026nbsp;\u003ca href=\"https://www.wired.com/story/openai-o3-mini-release/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003emaintained\u003c/a\u003e\u0026nbsp;that the debut took place on its original schedule.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;o3-mini continues OpenAI’s leadership in language models and further refines the reasoning capabilities introduced with the o1 family. In focusing on coding, math, and science tasks, it takes advantage of the strengths of reasoning models and raises the bar for other model builders. In practical terms, it pushes AI toward applications in which it’s a reliable professional partner rather than a smart intern.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;We’re glad that o3-mini is available to users of ChatGPT’s free tier as well as paid subscribers and API users. The more users become familiar with how to prompt reasoning models, the more value they’ll deliver.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/02/UITARS.png\" class=\"kg-image\" alt=\"Flowchart illustrating the automation of opening, editing, and saving a Word document using PyAutoGUI.\" loading=\"lazy\" width=\"2000\" height=\"1125\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/UITARS.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/UITARS.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2025/02/UITARS.png 1600w, https://dl-staging-website.ghost.io/content/images/size/w2400/2025/02/UITARS.png 2400w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"training-for-computer-use\"\u003e\u003cstrong\u003eTraining for Computer Use\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eAs Anthropic, Google, OpenAI, and others roll out agents that are capable of computer use, new work shows how underlying models can be trained to do this.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Yujian Qin and colleagues at ByteDance and Tsinghua University introduced\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2501.12326?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eUI-TARS\u003c/a\u003e, a fine-tuned version of the vision-language model Qwen2-VL that uses lines of reasoning to decide which mouse clicks, keyboard presses, and other actions to take in desktop and mobile apps. The model’s weights are\u0026nbsp;\u003ca href=\"https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003elicensed\u003c/a\u003e\u0026nbsp;freely for commercial and noncommercial uses via Apache 2.0. You can download them\u0026nbsp;\u003ca href=\"https://huggingface.co/bytedance-research/UI-TARS-72B-DPO?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe authors added\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003echains of thought\u003c/a\u003e\u0026nbsp;(CoTs) to their training set of screenshots and actions by prompting an unspecified vision-language model to explain the current action given previous screenshots, actions, and generated CoTs. Sometimes that process led to bad explanations, so they also generated multiple CoTs and actions for a given screenshot and selected the CoT that led to the correct action.\u003c/li\u003e\u003cli\u003eThey fine-tuned UI-TARS to generate a CoT and action from an instruction (such as “Open the document and add the text ‘hello’”) plus the screenshots, CoTs, and actions so far.\u003c/li\u003e\u003cli\u003eThey ran UI-TARS within a virtual PC, generating a large number of screenshots, CoTs, and actions so far. They filtered out erroneous CoTs and actions using rules (such as removing those that included redundant actions), scoring outputs automatically and removing those with low scores, and reviewing them manually. They fine-tuned the model on the remaining outputs and repeatedly generated, filtered, and fine-tuned.\u003c/li\u003e\u003cli\u003eThey also fine-tuned the model on corrected examples of\u0026nbsp;erroneous CoTs and actions. Human annotators corrected the CoT and action to (a) avoid the error and (b) fix the error after it occurred.\u003c/li\u003e\u003cli\u003eFinally, they fine-tuned the model using\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/human-feedback-without-reinforcement-learning/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eDirect Preference Optimization\u003c/a\u003e\u0026nbsp;(DPO) to prefer generating the corrected examples over the erroneous examples from the previous step.\u003c/li\u003e\u003cli\u003eAt inference, given a screenshot, an instruction, and potential actions (as is typical with open computer use models; the authors provide a handy list in a sample prompt), UI-TARS generated a CoT and an action to take. After taking that action (via\u0026nbsp;\u003ca href=\"https://pypi.org/project/PyAutoGUI/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003ePyAutoGUI\u003c/a\u003e, a Python module that controls computers), the model received a new screenshot and generated another chain of thought and action, and so on. At each step, the model produced a new chain of thought and action, taking into account the instruction and all CoTs, actions, and screenshots so far.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;Adept\u0026nbsp;\u003ca href=\"https://techcrunch.com/2022/04/26/2304039/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003etouted\u003c/a\u003e\u0026nbsp;computer use in early 2022, and\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2408.00203?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eOmniParser\u003c/a\u003e\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2412.04454?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eAguvis\u003c/a\u003e\u0026nbsp;soon followed with practical implementations. In October 2024, Anthropic set off the current wave of model/app interaction with its\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/anthropic-empowers-claude-sonnet-3-5-to-operate-desktop-apps-but-cautions-remain/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eannouncement\u003c/a\u003e\u0026nbsp;of computer use for Claude 3.5 Sonnet. OpenAI recently\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/openais-operator-automates-online-tasks-with-a-new-ai-agent/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eresponded\u003c/a\u003e\u0026nbsp;with Operator, its own foray into using vision and language models to control computers.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;UI-TARS matched or outperformed Claude 3.5 Sonnet with computer use, GPT-4o with various computer use frameworks, and the Aguvis framework with its native model on 11 benchmarks. On OSWorld, which asks models to perform tasks using a variety of real-world applications and operating systems, UI-TARS successfully completed 22.7 percent of the tasks in 15 steps, whereas Claude 3.5 Sonnet with computer use completed 14.9 percent, GPT-4o with Aguvis 17 percent, and Aguvis with its native model 10.3 percent.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;Training a model to take good actions enables it to perform well. Training it to correct its mistakes after making them enables it to recover from unexpected issues that may occur in the real world.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe\u003c/strong\u003e’\u003cstrong\u003ere thinking:\u003c/strong\u003e\u0026nbsp;Since computer use can be simulated in a virtual machine, it’s possible to generate massive amounts of training data automatically. This is bound to spur rapid progress in computer use by large language models.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/02/FLASH2THINKING.png\" class=\"kg-image\" alt=\"Line charts showing performance improvements in math and science with 2.0 Flash Thinking models.\" loading=\"lazy\" width=\"1200\" height=\"640\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/02/FLASH2THINKING.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/02/FLASH2THINKING.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/02/FLASH2THINKING.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"gemini-thinks-faster\"\u003eGemini Thinks Faster\u003c/h1\u003e\u003cp\u003eGoogle updated the December-vintage reasoning model Gemini 2.0 Flash Thinking and other Flash models, gaining ground on OpenAI o1 and DeepSeek-R1.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;\u003ca href=\"https://deepmind.google/technologies/gemini/flash-thinking?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eGemini 2.0 Flash Thinking Experimental 1-21\u003c/a\u003e\u0026nbsp;is a vision-language model (images and text in, text out) that’s trained to generate a structured reasoning process or chain of thought. The new version improves on its predecessor’s reasoning capability and extends its context window. It's free to access via\u0026nbsp;\u003ca href=\"https://ai.google.dev/gemini-api/docs/thinking?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eAPI\u003c/a\u003e\u0026nbsp;while it remains designated “experimental” and\u0026nbsp;\u003ca href=\"https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eavailable\u003c/a\u003e\u0026nbsp;to paid users of the Gemini app, along with\u0026nbsp;\u003ca href=\"https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eGemini 2.0 Flash\u003c/a\u003e\u0026nbsp;(fresh out of experimental mode) and the newly released\u0026nbsp;\u003ca href=\"https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-pro-exp-02-05\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eGemini 2.0 Pro Experimental\u003c/a\u003e. The company also launched a preview of\u0026nbsp;\u003ca href=\"https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-lite-preview-02-05\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eGemini 2.0 Flash Lite\u003c/a\u003e, a vision-language model (images and text in, text out) that outperforms Gemini 1.5 Flash at the same price.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;Gemini 2.0 Flash Thinking Experimental 1-21 is based on\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/google-introduces-gemini-2-0-flash-a-faster-more-capable-ai-model/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eGemini 2.0 Flash Experimental\u003c/a\u003e\u0026nbsp;(parameter count undisclosed). It processes up to 1 million tokens of input context, compared to its predecessor’s 32,000 and o1’s 128,000.\u003c/p\u003e\u003cul\u003e\u003cli\u003eUnlike o1, which hides its chain of thought, but like DeepSeek-R1 and Qwen QwQ, Gemini 2.0 Flash Thinking Experimental 1-21 includes its reasoning in its output.\u003c/li\u003e\u003cli\u003eOn the graduate-level science exam GPQA-Diamond, it achieved 74.2 percent compared to the earlier version’s 58.6 percent, surpassing DeepSeek-R1 (71.5 percent) but behind o1 (77.3 percent).\u003c/li\u003e\u003cli\u003eOn the advanced math benchmark AIME 2024, it achieved 73.3 percent compared to the previous version’s 35.5 percent, but it trails behind DeepSeek-R1 (79.8 percent) and o1 (74.4 percent).\u003c/li\u003e\u003cli\u003eOn the visual and multimedia understanding test MMMU, it achieved 75.4 percent to outperform the previous version (70.7 percent) but fell short of o1 (78.2 percent).\u003c/li\u003e\u003cli\u003eDevelopers can integrate Python code execution via the\u0026nbsp;\u003ca href=\"https://ai.google.dev/gemini-api/docs/thinking?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eAPI\u003c/a\u003e, with support for data analysis and visualization through\u0026nbsp;\u003ca href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution?t\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003epre-installed libraries\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eSpeed bumps:\u003c/strong\u003e\u0026nbsp;Large language models that are trained to generate a\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003echain of thought\u003c/a\u003e\u0026nbsp;(CoT) are boosting accuracy even as the additional processing increases inference costs and latency. Reliable measures of Gemini 2.0 Flash Thinking Experimental 1-21’s speed are not yet available, but its base model runs faster (168.8 tokens per second with 0.46 seconds of latency to the first token,\u0026nbsp;\u003ca href=\"https://artificialanalysis.ai/models/gemini-2-0-flash-experimental?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eaccording to\u003c/a\u003e\u0026nbsp;Artificial Analysis) than all models in its class except o1-mini (which outputs 200 tokens per second with 10.59 seconds of latency to the first token).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;The combination of CoT reasoning and long context — assuming the new model can take advantage of its 1 million-token context window, as measured by a benchmark such as\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2404.06654?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eRULER\u003c/a\u003e\u0026nbsp;— could open up valuable applications. Imagine a reasoning model that can take an entire codebase as input and analyze it without breaking it into smaller chunks.\u003cbr\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Regardless of benchmark performance, this model topped the\u0026nbsp;\u003ca href=\"https://lmarena.ai/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eChatbot Arena\u003c/a\u003e\u0026nbsp;leaderboard at the time of writing. This suggests that users preferred it over o1 and DeepSeek-R1 — at least for common, everyday prompts.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/02/MOSHI.gif\" class=\"kg-image\" alt=\"Diagram illustrating Moshi’s use of an LLM to process user audio input, inner monologue, and output.\" loading=\"lazy\" width=\"600\" height=\"336\" srcset=\"https://dl-staging-website.ghost.io/content/images/2025/02/MOSHI.gif 600w\"\u003e\u003c/figure\u003e\u003ch1 id=\"okay-but-please-don%E2%80%99t-stop-talking\"\u003eOkay, But Please Don’t Stop Talking\u003c/h1\u003e\u003cp\u003eEven cutting-edge, end-to-end, speech-to-speech systems like ChatGPT’s Advanced Voice Mode tend to get interrupted by interjections like “I see” and “uh-huh” that keep human conversations going. Researchers built an open alternative that’s designed to go with the flow of overlapping speech.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Alexandre Défossez, Laurent Mazaré, and colleagues at Kyutai, a nonprofit research lab in Paris, released\u0026nbsp;\u003ca href=\"https://kyutai.org/Moshi.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eMoshi\u003c/a\u003e, an end-to-end, speech-to-speech system that’s always listening and always responding. The\u0026nbsp;\u003ca href=\"https://huggingface.co/kyutai/moshiko-pytorch-bf16?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eweights\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://github.com/kyutai-labs/moshi/tree/main?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003ecode\u003c/a\u003e\u0026nbsp;are free for noncommercial and commercial uses under\u0026nbsp;\u003ca href=\"https://choosealicense.com/licenses/cc-by-4.0/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eCC-BY 4.0\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://github.com/square/moshi/blob/master/LICENSE.txt?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eApache 2.0\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://github.com/kyutai-labs/moshi/blob/main/LICENSE-MIT?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eMIT\u003c/a\u003e\u0026nbsp;licenses. You can try a web demo\u0026nbsp;\u003ca href=\"https://moshi.chat/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKey insight:\u003c/strong\u003e\u0026nbsp;Up to 20 percent of spoken conversation consists of\u0026nbsp;\u003ca href=\"https://www.isca-archive.org/interspeech_2006/cetin06_interspeech.html?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eoverlapping speech\u003c/a\u003e, including interjections like “okay” and “I see.”\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo respond appropriately despite such overlaps, a system must both listen and generate sound continuously — although much of what it will generate is silence.\u003c/li\u003e\u003cli\u003eTo respond without delay, it must keep latency to a minimum. This goal requires an end-to-end design rather than a pipeline of stand-alone models to perform voice detection, speech-to-text, text processing, and text-to-speech in turn.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;The authors combined an encoder-decoder called Mimi and an\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2203.01941?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eRQ-Transformer\u003c/a\u003e, which is made up of the Helium transformer-based large language model (LLM) plus another transformer.\u003c/p\u003e\u003cul\u003e\u003cli\u003eMimi’s encoder embedded spoken input using 8 audio tokens per timestep (80 milliseconds). The authors trained Mimi on 7 million hours of mostly English speech from undisclosed sources. The training involved two loss terms. (i) The first loss term encouraged Mimi, given one audio timestep, to produce audio that fooled a pretrained\u0026nbsp;\u003ca href=\"https://arxiv.org/pdf/2210.13438?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eMS-STFT discriminator\u003c/a\u003e\u0026nbsp;into thinking it was human speech. The second loss term distilled knowledge from a pretrained\u0026nbsp;\u003ca href=\"https://huggingface.co/microsoft/wavlm-large?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eWavLM\u003c/a\u003e, an audio embedding model. It encouraged Mimi’s encoder, when Mimi and WavLM received the same audio timestep, to produce one audio token (of its 8 audio tokens per timestep) whose embedding was similar to the corresponding embedding produced by WavLM.\u003c/li\u003e\u003cli\u003eGiven the audio tokens, the Helium LLM produced text tokens that were used internally to help the additional transformer predict the next audio token (the idea being that the LLM’s skill with words would inform which audio token to generate next). The authors trained Helium to predict the next text token in 2.1 trillion tokens of English text (12.5 percent from\u0026nbsp;\u003ca href=\"https://dumps.wikimedia.org/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eWikipedia\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://archive.org/details/stackexchange?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eStack Exchange\u003c/a\u003e, and the remaining 87.5 percent from\u0026nbsp;\u003ca href=\"https://commoncrawl.org/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eCommon Crawl\u003c/a\u003e).\u003c/li\u003e\u003cli\u003eRQ-Transformer received many sets of 17 tokens per time step: 8 audio tokens encoded by Mimi from the audio input, 8 audio tokens from Moshi’s previously generated audio output, and 1 text token produced by Helium. RQ-Transformer learned to predict the next set of 17 tokens in 7 million hours of audio and transcribed text.\u003c/li\u003e\u003cli\u003eTo train the system specifically on conversational interaction, the authors further trained it to predict the next token in 2,000 hours of\u0026nbsp;\u003ca href=\"https://catalog.ldc.upenn.edu/LDC2004S13?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003erecorded phone conversations\u003c/a\u003e\u0026nbsp;between randomly paired participants.\u003c/li\u003e\u003cli\u003eAt inference, given a user's speech, Mimi turned it into audio tokens. Given the audio tokens and RQ-Transformer’s previously generated audio and text tokens, RQ-Transformer generated new audio and text tokens. From the generated audio tokens, Mimi produced synthetic speech.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;In tests, Moshi proved fast and relatively accurate.\u003c/p\u003e\u003cul\u003e\u003cli\u003eMoshi (7 billion parameters) took around 200 milliseconds to respond to user input. In comparison, GPT-4o, which also produces speech output directly from speech input, took 232 milliseconds minimum (320 milliseconds average). Prior to GPT-4o, ChatGPT Voice Mode (a pipeline of speech-to-text, text-to-text, and text-to-speech models) took an average of 5.4 seconds.\u003c/li\u003e\u003cli\u003eMoshi achieved 26.6 percent accuracy on Web Questions, higher than the speech-to-text-to-speech models tested by the authors:\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2305.15255?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eSpectron\u003c/a\u003e\u0026nbsp;(1 billion parameters) achieved 6.1 percent accuracy and\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2305.11000?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP\" rel=\"noopener\"\u003eSpeechGPT\u003c/a\u003e\u0026nbsp;(7 billion parameters) achieved 6.5 percent accuracy. The authors didn’t provide comparable results for GPT-4o or ChatGPT Voice.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;While a turn-based approach may suffice for text input, voice-to-voice interactions benefit from a system that processes both input and output quickly and continuously. Previous systems process input and output separately, making users wait. Moshi delivers seamless interactivity.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Generating silence is golden!\u003c/p\u003e","comment_id":"67a3e4c802b44f0001f4d881","feature_image":"https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6.jpg","featured":false,"visibility":"public","created_at":"2025-02-05T14:23:04.000-08:00","updated_at":"2025-02-05T14:43:07.000-08:00","published_at":"2025-02-05T14:40:00.000-08:00","custom_excerpt":"The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"67a3e94602b44f0001f4d8bd","name":"Feb 05, 2025","slug":"feb-05-2025","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/feb-05-2025/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-287/","excerpt":"The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer.","reading_time":14,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, and more...","meta_description":"The Batch AI News and Insights: A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer.","email_subject":null,"frontmatter":null,"feature_image_alt":"Comic-style illustration of a confident woman and man standing beside bold ‘10X’ text on a bright background.","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2025/02/10x_1200px_6.jpg","dimensions":{"width":1200,"height":676}},"banner":{"title":"Generative AI for Software Development","databaseId":35156,"id":"cG9zdDozNTE1Ng==","featuredImage":{"node":{"altText":"","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/08/Vertical-side-banner-ads-5.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/3YTVMir","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Databricks C1","date":"2025-06-04T07:12:29","databaseId":36631,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/3HlCuvP","courseName":"DSPy: Build and Optimize Agentic Apps","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)","isOpenInNewTab":true}},{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}}]}}},"__N_SSG":true},"page":"/the-batch/[slug]","query":{"slug":"issue-287"},"buildId":"Qy9Eh4N0MrfE3ddReY9v4","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>