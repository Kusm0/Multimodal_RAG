<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes</title><meta name="description" content="The Batch - AI News &amp; Insights: Generative adversarial networks, or GANs, are said to give computers the gift of imagination | A Man, A Plan, A GAN" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><link rel="canonical" href="https://www.deeplearning.ai/the-batch/issue-59/"/><meta property="og:type" content="article" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:title" content="GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:description" content="The Batch - AI News &amp; Insights: Generative adversarial networks, or GANs, are said to give computers the gift of imagination | A Man, A Plan, A GAN" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:site_name" content="GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:url" content="https://www.deeplearning.ai/the-batch/issue-59/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="article:published_time" content="2020-09-30T12:00:00.000-07:00"/><meta property="article:modified_time" content="2022-10-06T16:01:54.000-07:00"/><meta property="article:tag" content="The Batch Newsletter"/><meta property="article:tag" content="issue-59"/><meta property="article:tag" content="Sep 30, 2020"/><meta property="article:author" content="https://www.facebook.com/DeepLearningAIHQ/"/><meta property="twitter:title" content="GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:description" content="The Batch - AI News &amp; Insights: Generative adversarial networks, or GANs, are said to give computers the gift of imagination | A Man, A Plan, A GAN" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:url" content="https://www.deeplearning.ai/the-batch/issue-59/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:label1" content="Written by"/><meta property="twitter:data1" content="DeepLearning.AI"/><meta property="twitter:label2" content="Filed under"/><meta property="twitter:data2" content="The Batch Newsletter"/><meta property="twitter:card" content="summary_large_image" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:creator" content="@DeepLearningAI"/><meta property="twitter:site" content="https://twitter.com/DeepLearningAI/"/><meta name="twitter:image" content="https://dl-staging-website.ghost.io/content/images/2021/06/Screen20Shot202020-09-3020at2012.png"/><meta property="og:image" content="https://dl-staging-website.ghost.io/content/images/2021/06/Screen20Shot202020-09-3020at2012.png"/><meta property="og:image:width" content="2000"/><meta property="og:image:height" content="1124"/><script type="application/ld+json">{"@context":"https://schema.org/","@type":"Article","datePublished":"2020-09-30T12:00:00.000-07:00","dateModified":"2022-10-06T16:01:54.000-07:00","author":{"@type":"Article","name":"DeepLearning.AI","image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","sameAs":"[\"https://www.deeplearning.ai/\", \"https://twitter.com/DeepLearningAI_/\", \"https://www.facebook.com/DeepLearningAIHQ/\"]"},"keywords":"[object Object], [object Object], [object Object]","headline":"GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes","image":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen20Shot202020-09-3020at2012.png","width":2000,"height":1124},"publisher":{"@type":"Organization","name":"GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes","logo":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","width":60,"height":60}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.deeplearning.ai"},"description":"The Batch - AI News & Insights: Generative adversarial networks, or GANs, are said to give computers the gift of imagination | A Man, A Plan, A GAN"}</script><meta name="next-head-count" content="31"/><link href="/static/favicons/favicon.ico" rel="shortcut icon"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&amp;display=swap"/><link rel="preload" href="/_next/static/css/54b174af9991199d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/54b174af9991199d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6f396d9f2f265155.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6f396d9f2f265155.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-7690c02358b700f2.js" defer=""></script><script src="/_next/static/chunks/framework-c54e8763846ac33b.js" defer=""></script><script src="/_next/static/chunks/main-9fcc7c9a63f70912.js" defer=""></script><script src="/_next/static/chunks/pages/_app-bece9363d7a6c0bb.js" defer=""></script><script src="/_next/static/chunks/3a17f596-e9670ced3bfab562.js" defer=""></script><script src="/_next/static/chunks/36d2f571-09e32b60ec93a4dc.js" defer=""></script><script src="/_next/static/chunks/5c0b189e-c0f75ca9b44e520b.js" defer=""></script><script src="/_next/static/chunks/5567-47eb911ba5f222d7.js" defer=""></script><script src="/_next/static/chunks/2251-380253169bb2e795.js" defer=""></script><script src="/_next/static/chunks/3864-a4ae56e91852593b.js" defer=""></script><script src="/_next/static/chunks/4965-3585432476b27ea9.js" defer=""></script><script src="/_next/static/chunks/3791-0bbbd4cd0bf31c22.js" defer=""></script><script src="/_next/static/chunks/pages/the-batch/%5Bslug%5D-c03a8bcef188ef99.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_buildManifest.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_ssgManifest.js" defer=""></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&display=swap"/></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5C5VGGJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div data-sentry-component="Layout" data-sentry-source-file="Layout.tsx"><aside id="top-announcement" class="text-neutral-900  " style="color:#FFFFFF;background-color:#05256C;background-image:linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)" data-sentry-component="AnnouncementBanner" data-sentry-source-file="AnnouncementBanner.tsx"><div class="container--boxed py-3 flex items-center justify-between "><div class="flex items-center"><p class="text-sm lg:text-base">✨ New course! Enroll in<!-- --> <a href="https://bit.ly/3HlCuvP" class="underline" target="_blank">DSPy: Build and Optimize Agentic Apps</a></p></div><div class="flex items-center"><button class="bg-transparent p-0 border-none text-xl opacity-70 hover:opacity-100 transition-opacity"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><line x1="15" y1="9" x2="9" y2="15"></line><line x1="9" y1="9" x2="15" y2="15"></line></svg></button></div></div></aside><header id="main-navigation" data-testid="main-navigation-testid" class="static h-[64px] md:h-[100px] top-0 w-full z-[100] flex items-center bg-white transition-all ease-in-out duration-200" data-sentry-component="Header" data-sentry-source-file="index.tsx"><div class="container--boxed flex justify-between items-center"><div class="max-w-[185px] lg:max-w-[235px]"><a href="/the-batch/"><div class="flex items-center"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27300%27%20height=%2792%27/%3e"/></span><img alt="The Batch" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="The Batch" loading="lazy" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcSet="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=384&amp;q=75 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75 2x" src="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75"/></noscript></span></div></a></div><div class="flex items-center"><div class="hidden lg:flex items-center"><nav aria-label="Primary" data-sentry-component="Nav" data-sentry-source-file="Nav.tsx"><ul class="flex items-center p-0 m-0 list-none"><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/courses/"><div class="NavItem_navItemLink__Aq6E5"><span>Explore Courses</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/the-batch/"><div class="NavItem_navItemLink__Aq6E5"><span>AI Newsletter</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/"><div>The Batch</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/letters/"><div>Andrew&#x27;s Letter</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/data-points/"><div>Data Points</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/research/"><div>ML Research</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/"><div>Blog</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/community/"><div class="NavItem_navItemLink__Aq6E5"><span>Community</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="https://community.deeplearning.ai/"><div>Forum</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/events/"><div>Events</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/ambassador/"><div>Ambassadors</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/category/ambassador-spotlight/"><div>Ambassador Spotlight</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/resources/"><div class="NavItem_navItemLink__Aq6E5"><span>Resources</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/about/"><div class="NavItem_navItemLink__Aq6E5"><span>Company</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/about/"><div>About</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/careers/"><div>Careers</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/contact/"><div>Contact</div></a></li></ul></li></ul></nav><a class="btn--primary text-base font-medium whitespace-nowrap shadow h-fit py-3 px-4 lg:ml-2 xl:ml-10" data-sentry-element="Link" data-sentry-source-file="index.tsx" href="https://bit.ly/3RB9T8a">Start Learning</a></div><div class="flex items-center" data-sentry-element="Menu" data-sentry-component="MobileMenu" data-sentry-source-file="MobileMenu.tsx" data-headlessui-state=""><button class="lg:hidden" data-sentry-element="MenuButton" data-sentry-source-file="MobileMenu.tsx" id="headlessui-menu-button-:R1qa6:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-4xl text-slate-600" data-sentry-element="FiMenu" data-sentry-source-file="MobileMenu.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button></div></div></div></header><main><div id="content"><nav aria-label="Secondary" class="h-[66px] bg-white sticky top-[60px] z-40 shadow hidden lg:block" data-sentry-component="SecondaryNav" data-sentry-source-file="index.tsx"><div class="container--boxed h-full w-full flex items-center justify-between "><div class="relative flex h-full"><button class="h-full w-14 items-center justify-center absolute top-0 left-0 z-10 group bg-white bg-opacity-75 hidden"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="15 18 9 12 15 6"></polyline></svg></span></button><ul id="nav-secondary" class="list-none p-0 m-0 h-full flex items-center  overflow-x-scroll relative SecondaryNav_navItems__dok3i"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Weekly Issues</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/letters/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Andrew&#x27;s Letters</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/data-points/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Data Points</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/research/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">ML Research</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/business/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Business</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/science/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Science</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/culture/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Culture</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/hardware/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Hardware</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/ai-careers/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">AI Careers</div></a></li></ul><button class="h-full w-9 items-center justify-center absolute top-0 right-0 group bg-white bg-opacity-75 flex"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span></button></div><div class="flex items-center h-full"><ul class="list-none p-0 m-0 h-full flex items-center mr-5"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/about/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">About</div></a></li></ul><button type="button" class="bg-white border btn--tracking border-solid border-brand text-brand hover:bg-brand hover:text-white transition-colors px-3 py-1 rounded-md mr-4">Subscribe</button><a href="/search/"><div title="Search" class="transition-colors text-slate-400 hover:text-slate-500"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-2xl" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></div></a></div></div></nav><article class="pt-5 pb-16 bg-white lg:pt-16 lg:pb-16" data-sentry-component="ArticleDefault" data-sentry-source-file="ArticleDefault.tsx"><header class="post_layoutGrid__0BDX2"><nav aria-label="Breadcrumb flex items-center" data-sentry-component="Main" data-sentry-source-file="Breadcrumb.tsx"><ul class="list-none flex items-center flex-wrap"><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">The Batch</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">Weekly Issues</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/issue-59/"><div class="text-sm md:text-base text-slate-400 font-normal "><h1 class="capitalize">issue 59</h1></div></a></li></ul></nav><aside class="flex mt-6 lg:hidden"><div class="flex flex-col items-start items-start mr-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm text-sm">Published</div></div><div class="mt-1 text-slate-600 text-base text-sm">Sep 30, 2020</div></div><div class="flex flex-col items-start items-start" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm text-sm">Reading time</div></div><div class="mt-1 text-slate-600 text-base text-sm">16<!-- --> min read</div></div></aside></header><div class="post_layoutGrid__0BDX2 mt-9"><aside style="grid-column:wide-start / main-start" class="flex-col items-end hidden pr-10 lg:flex"><div class="flex flex-col items-start items-end" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm undefined">Published</div></div><div class="mt-1 text-slate-600 text-base undefined"><a href="/the-batch/tag/sep-30-2020/"><div class="inline-flex px-2 py-1 text-sm font-normal transition-colors rounded-md bg-slate-100 hover:bg-slate-200 text-slate-500">Sep 30, 2020</div></a></div></div><div class="flex flex-col items-start items-end mt-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm undefined">Reading time</div></div><div class="mt-1 text-slate-600 text-base undefined">16<!-- --> min read</div></div><div class="flex flex-col items-start items-end mt-6 sticky top-20" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-59/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-59/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-59/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></aside><div class="prose--styled justify-self-center post_postContent__wGZtc"><p><em>Dear friends,</em></p><p><em>This special issue of The Batch celebrates the launch of our new <a href="https://www.coursera.org/specializations/generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io">Generative Adversarial Networks Specialization</a>!</em></p><p><em>GANs are among the most exciting technologies to emerge from deep learning. These networks learn in a very different way than typical supervised methods for learning x-to-y mappings. By pitting a discriminator network and a generator network against one another (details below), they produce photorealistic images, medical training data, children’s book illustrations, and other types of output.</em></p><p><em>Earlier today, we held an online panel discussion on “GANs for Good” with Anima Anandkumar, Alexei Efros, Ian Goodfellow, and our course instructor Sharon Zhou. I was struck by the number of new applications GANs are enabling, and the number that are likely to come.</em></p><p><em>Ian explained that GAN-generated training examples for a particular application at Apple are one-fifth as valuable as real examples but cost much less than one-fifth as much to produce. Anima described exciting progress on disentanglement and how the ability to isolate objects in images is making it easier to control image generation (“add a pair of glasses to this face”). Alexei talked about the impact GANs are having on art through tools like <a href="https://artbreeder.com/?ref=dl-staging-website.ghost.io" rel="noopener">Artbreeder</a>.</em></p><figure class="kg-card kg-image-card"><img src="https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-3020at2012.03.3120PM.png" class="kg-image" alt="Screen Shot 2020-09-30 at 12.03.31 PM" loading="lazy"></figure><p><em>All the speakers talked about alternatives to reading research papers to keep up with the exploding literature. If you missed the live discussion, you can watch a video of the entire event <a href="https://bit.ly/3il4SvI?ref=dl-staging-website.ghost.io" rel="noopener">here</a>.</em></p><p><em>We’re still in the early days of practical GAN applications, but I believe they will:</em></p><ul><li><em><em>Transform photo editing and make it easier to add or subtract elements such as background objects, trees, buildings, and clouds</em></em></li><li><em><em>Generate special effects for media and entertainment that previously were prohibitively expensive</em></em></li><li><em><em>Contribute to creative products from industrial design to fine art</em></em></li><li><em><em>Augment datasets in small data problems in fields from autonomous driving to manufacturing</em></em></li></ul><p><em>As an emerging technology, GANs have numerous untapped applications. This is a moment to dream up new ideas, because no one else may be working on them yet.</em></p><p><em>I hope this technology will spark your hunger to learn more and invent new applications that will make life better for people all over the world.</em></p><p><em>Keep learning!</em></p><p><em>Andrew</em></p><p></p><h2 id="special-issue-generation-gan">Special Issue: Generation GAN</h2><figure class="kg-card kg-image-card"><img src="https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker201-2.gif" class="kg-image" alt="Online panel discussion on “GANs for Good” with Andrew Ng, Anima Anandkumar, Alexei Efros, Ian Goodfellow, and Sharon Zhou" loading="lazy"></figure><h2 id="reality-reimagined">Reality Reimagined</h2><p>Generative adversarial networks, or GANs, are said to give computers the gift of imagination. Competition between a discriminator network, which learns to classify the system’s output as generated or real, and a generator network, which learns to produce output that fools the discriminator, produces fantasy images of uncanny realism. First <a href="https://arxiv.org/abs/1406.2661?ref=dl-staging-website.ghost.io" rel="noopener">proposed</a> in 2014, the architecture has been adopted by researchers to extend training datasets with synthetic examples and by businesses to create customized imagery for ads, entertainment, and personal services. But it has a dark side: GANs make it easy and convincing for jilted lovers to graft an ex’s face onto another person’s body, or politicians to misrepresent themselves as able to speak an ethnic minority’s language to win their votes. And it tends to tilt the training data distribution, favoring common examples while ignoring outliers. Researchers are improving the technology at breakneck pace, and developing ways to thwart, or at least detect, egregious uses. We have yet to see the best — and the worst — that GANs have to offer.</p><hr><figure class="kg-card kg-image-card"><img src="https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-2920at201.03.5020PM.png" class="kg-image" alt="Ian Goodfellow" loading="lazy"></figure><h2 id="a-man-a-plan-a-gan">A Man, A Plan, A GAN</h2><p>Brilliant ideas strike at unlikely moments. <a href="https://www.deeplearning.ai/blog/hodl-ian-goodfellow/?ref=dl-staging-website.ghost.io" rel="noopener">Ian Goodfellow</a> conceived generative adversarial networks while spitballing programming techniques with friends at a bar. Goodfellow, who views himself as “someone who works on the core technology, not the applications,” started at Stanford as a premed before switching to computer science and studying machine learning with Andrew Ng. “I realized that would be a faster path to impact more things than working on specific medical applications one at a time,” he recalls. From there, he earned a PhD in machine learning at Université de Montréal, interned at the seminal robotics lab Willow Garage, and held positions at OpenAI and Google Research. Last year, he joined Apple as director of machine learning in the Special Projects Group, which develops technologies that aren’t part of products on the market. His work at Apple is top-secret.</p><p><strong>The Batch:</strong> How did you come up with the idea for two networks that battle each other?<br><br><strong>Goodfellow:</strong> I’d been thinking about how to use something like a discriminator network as a way to score a speech synthesis contest, but I didn’t do it because it would have been too easy to cheat by overfitting to a particular discriminator. Some of my friends wanted to train a generator network using a technique that would have taken gigabytes of data per image, even for the tiny images we studied with generative models in 2014. We were discussing the problem one night at a bar, and they asked me how to write a program that efficiently manages gigabytes of data per image on a GPU that, back then, had about 1.5GB RAM. I said, that’s not a programming problem. It’s an algorithm design problem. Then I realized that a discriminator network could help a generator produce images if it were part of the learning process. I went home that night and started coding the first GAN.<br><br><strong>The Batch:</strong> How long did it take?<br><br><strong>Goodfellow:</strong> By copying and pasting bits and pieces of earlier papers, I got the first GAN to produce <a href="http://yann.lecun.com/exdb/mnist/?ref=dl-staging-website.ghost.io" rel="noopener">MNIST</a> images in only an hour of work or so. MNIST is such a small dataset that, even back then, you could train on it very quickly. I think it trained for tens of minutes before it could produce recognizable handwritten digits.<br><br><strong>The Batch:</strong> What did it feel like to see the first face?<br><br><strong>Goodfellow:</strong> That wasn’t as much of a revolutionary moment as people might expect. My colleague <a href="https://scholar.google.com/citations?user=h-BY2wgAAAAJ&hl=en&ref=dl-staging-website.ghost.io" rel="noopener">Bing Xu</a> modeled face images from the Toronto Face Database, which were only 90 pixels square and grayscale. Because the faces were always centered and looking straight at the camera, even very simple algorithms like PCA could make pretty good faces. The main thing we were surprised by were the images it made of <a href="https://www.cs.toronto.edu/~kriz/cifar.html?ref=dl-staging-website.ghost.io" rel="noopener">CIFAR10</a>, where there’s a lot of variability. They looked like crap. But we had been looking at crap from generative models for years, and we could tell that this was an exciting, new kind of crap.<br><br><strong>The Batch:</strong> Has anything surprised you about the way this work has played out?<br><br><strong>Goodfellow:</strong> In the first GAN paper, we included a list of things that might happen in future work. A lot of them did. The one big category of things I didn’t anticipate was domain-to-domain translation. GANs like <a href="https://arxiv.org/abs/1703.10593?ref=dl-staging-website.ghost.io" rel="noopener">CycleGAN</a> from Berkeley. You can take a picture of a horse and have it transformed into a zebra without training on matched pairs of horse and zebra images. That’s very powerful because it can be easy to passively collect data in each domain, but it’s time-consuming and expensive to get data that matches up across domains.<br><br><strong>The Batch:</strong> What are you most hopeful about in GAN research?<br><br><strong>Goodfellow:</strong> I’d like to see more use of GANs in the physical world, specifically for medicine. I’d like to see the community move toward more traditional science applications, where you have to get your hands dirty in the lab. That can lead to more things that have more of a tangible, positive impact on peoples’ lives. For instance, in dentistry, GANs have been used to make <a href="https://arxiv.org/abs/1804.00064?ref=dl-staging-website.ghost.io" rel="noopener">personalized crowns</a> for individual patients. Insilico is using GANs to <a href="https://insilico.com/publications?ref=dl-staging-website.ghost.io" rel="noopener">design medicinal drugs</a>.<br><br><strong>The Batch:</strong> How much do you worry about bias in GAN output? The ability to produce realistic human faces makes it a pressing issue.<br><br><strong>Goodfellow:</strong> GANs can be used to counteract biases in training data by generating training data for other machine learning algorithms. If there’s a language where you don’t have as much representation in your data, you can oversample that. At Apple, we were able to generate data for a gestural text-entry feature called <a href="https://9to5mac.com/2019/06/05/quickpath-ios-13-keyboard/?ref=dl-staging-website.ghost.io" rel="noopener">QuickPath</a>. A startup called Vue.ai uses GANs to generate images of <a href="https://vue.ai/products/automated-on-model-fashion-imagery/?ref=dl-staging-website.ghost.io" rel="noopener">what clothing would look like on different models</a>. Traditionally, there may not have been much diversity in who was hired to be a model to try on this clothing. Now you can get a model who looks like you, wearing a specific item of clothing you’re interested in. These are baby steps, but I hope there are other ways GANs can be used to address issues of underrepresentation in datasets.</p><hr><h2 id="news"><strong>News</strong></h2><figure class="kg-card kg-image-card"><img src="https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker-2.gif" class="kg-image" alt="Makeup applied on female faces with the help of augmented reality" loading="lazy"></figure><h2 id="a-good-look-for-ai">A Good Look for AI</h2><p>Trying on new makeup is a hassle — apply, inspect, wash off, repeat. Not to mention the tribulation of visiting brick-and-mortar stores during the pandemic. Augmented reality is helping people try on all the makeup they want without leaving home.<br><br><strong>What’s new:</strong> Modiface, a subsidiary of beauty giant L’Oréal, <a href="https://patents.google.com/patent/US20200160153A1/en?ref=dl-staging-website.ghost.io" rel="noopener">uses GANs</a> to let customers see how different colors of lipstick, eye shadow, and hair will look on them.<br><br><strong>How it works:</strong> Modiface’s approach is a hybrid of <a href="https://arxiv.org/abs/1703.10593?ref=dl-staging-website.ghost.io" rel="noopener">CycleGAN</a>, <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf?ref=dl-staging-website.ghost.io" rel="noopener">StarGAN</a> and <a href="https://arxiv.org/abs/1812.04948?ref=dl-staging-website.ghost.io" rel="noopener">StyleGAN</a>, company operations chief Jeff Houghton told <em>The Batch.</em> It uses a CNN to track landmarks on a user’s hair and face. At L’Oreal’s <a href="https://www.lorealparisusa.com/virtual-try-on/makeup.aspx?ref=dl-staging-website.ghost.io" rel="noopener">website</a>, users can select different lipsticks, eyeliners, blushes and hair dyes and fine-tune their shade, texture, and gloss. Then they can virtually apply those products to an uploaded selfie or a real-time video of their own faces.</p><ul><li>To train the algorithm, Modiface collected and annotated thousands of images of people wearing and not wearing makeup and hair coloring under various lighting and background conditions.</li><li>A CycleGAN takes as input an image of a user’s face without makeup. When the user selects a specific makeup product, the system generates an image that shows what the selected product looks like when applied.</li><li>Adding new makeup types to the model is as simple as adding a color swatch to the database. Modiface must retrain the model to include new products with properties, such as a metallic sheen, that aren’t represented in the training data.</li><li>Collaborations with <a href="https://www.lorealusa.com/media/press-releases/2019/june/modiface?ref=dl-staging-website.ghost.io" rel="noopener">Amazon</a> and <a href="https://mediaroom.loreal.com/en/loreals-modiface-launches-long-term-augmented-reality-collaboration-with-facebook/?ref=dl-staging-website.ghost.io" rel="noopener">Facebook</a> enable users to access Modiface directly from those platforms.</li></ul><p><strong>Behind the news:</strong> Augmented reality applications are reshaping the beauty industry. In 2018, the same year L’Oréal <a href="https://www.theverge.com/2018/3/16/17131260/loreal-modiface-acquire-makeup-ar-try-on?ref=dl-staging-website.ghost.io" rel="noopener">purchased</a> Modiface, American chain Ulta <a href="https://www.glossy.co/new-face-of-beauty/whats-next-for-beauty-and-omnichannel-retail?ref=dl-staging-website.ghost.io" rel="noopener">acquired</a> Glamst, a startup specializing in augmented reality. <a href="https://www.meitu.com/en/?ref=dl-staging-website.ghost.io" rel="noopener">Meitu</a>, a multi-billion dollar Chinese company, uses AI-driven face manipulation to make its users appear more attractive in social media posts, job applications, and other digital venues.<br><br><strong>Why it matters:</strong> E-commerce is increasingly <a href="https://www.wsj.com/articles/loreal-expands-virtual-try-on-service-11576776586?ref=dl-staging-website.ghost.io" rel="noopener">important</a> to the beauty industry’s bottom line, and the pandemic is <a href="https://www.glossy.co/beauty/covid-19-accelerates-beautys-demand-for-ai-and-ar-technology/?ref=dl-staging-website.ghost.io" rel="noopener">driving</a> even more business to the web. Tools like this make it easier for customers to try out products, which may boost sales.<br><br><strong>We’re thinking:</strong> It seems like we’re spending half our lives in video conferences. Do we still need to apply makeup at all, or can we let a GAN do it instead?</p><p><em>Learn how to build a CycleGAN and control image features in <a href="https://www.coursera.org/specializations/generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io">Courses 1 and 2</a> of the GANs Specialization from DeepLearning.AI, available now on Coursera. Build the powerful StyleGAN in <a href="https://www.coursera.org/learn/apply-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io">Course 3: Apply GANs</a>, coming soon!</em></p><hr><figure class="kg-card kg-image-card"><img src="https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2021.gif" class="kg-image" alt="Data and examples related to IMLE-GAN" loading="lazy"></figure><h2 id="making-gans-more-inclusive">Making GANs More Inclusive</h2><p>A typical GAN’s output doesn’t necessarily reflect the data distribution of its training set. Instead, GANs are prone to modeling the majority of the training distribution, sometimes ignoring rare attributes — say, faces that represent <a href="https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias?ref=dl-staging-website.ghost.io" rel="noopener">minority populations</a>. A twist on the GAN architecture forces its output to better reflect the diversity of the training data.<br><br><strong>What’s new:</strong> <a href="https://arxiv.org/abs/2004.03355?ref=dl-staging-website.ghost.io" rel="noopener">IMLE-GAN</a> learns to generate all the attributes of its training dataset, including rare ones. Ning Yu spearheaded the research with colleagues at University of Maryland, Max Planck Institute, University of California Berkeley, CISPA Helmholtz Center for Information Security, Princeton’s Institute for Advanced Study, and Google.<br><br><strong>Key insight:</strong> A GAN’s discriminator distinguishes whether or not the generator’s output is generated, while the generator learns to produce output that fools the discriminator. Ideally, a generator’s output would mirror the training data distribution, but in practice — since its only aim is to fool the discriminator, and the discriminator typically evaluates only one image at a time — it can learn to favor common types of examples. The authors had their model compare several generated works with examples from the training set, as well as interpolations between generated works, to encourage greater diversity in the output.<br><br><strong>How it works:</strong> IMLE-GAN enhances a GAN with <a href="https://arxiv.org/abs/1809.09087?ref=dl-staging-website.ghost.io" rel="noopener">Implicit Maximum Likelihood Estimation</a> (IMLE). Instead of naively adding the IMLE loss to the usual adversarial loss, the authors modified the default IMLE loss and added a novel interpolation loss to compensate for fundamental incompatibilities between the adversarial and IMLE losses.</p><ul><li>IMLE generates a set of images and penalizes the network based on how different those images are from real images by making nearest-neighbor comparisons. Instead of comparing pixels, like in standard IMLE, the authors compare the images over the feature space. The switch from pixels to features makes the adversarial and IMLE losses more comparable.</li><li>To compute the interpolation loss, the authors create an additional image that is interpolated between two generated images. Then, they compare the interpolated image’s features to those of the two non-generated images that were matched to the generated images during IMLE.</li><li>To increase inclusion of underrepresented attributes, the algorithm samples data from a set of minority examples for the IMLE and interpolation losses, but from all examples for the adversarial loss.</li></ul><p><strong>Results:</strong> The authors evaluated IMLE-GAN against <a href="https://arxiv.org/abs/1812.04948?ref=dl-staging-website.ghost.io" rel="noopener">StyleGAN</a> and a handful of other models using <a href="https://github.com/fjxmlzn/PacGAN/tree/master/stacked_MNIST_experiments?ref=dl-staging-website.ghost.io" rel="noopener">Stacked MNIST</a>, a variation of the MNIST dataset that includes handwritten digits in 1,000 distinct styles. IMLE-GAN reproduced 997 of the styles, while StyleGAN reproduced 940. Trained on <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html?ref=dl-staging-website.ghost.io" rel="noopener">CelebA</a>, a large-scale dataset of celebrity faces, IMLE-GAN generated attributes present in less than 6 percent of training examples with increased precision compared to StyleGAN. For instance, it generated wearers of eyeglasses with 0.904 precision, compared to StyleGAN’s meager 0.719.<br><br><strong>Why it matters:</strong> Much of the time, we want our models to learn the data distribution present in the training set. But when fairness or broad representation are at stake, we may need to put a finger on the scale. This work offers an approach to making GANs more useful in situations where diversity or fairness is critical.<br><br><strong>We’re thinking:</strong> This work helps counter model and dataset bias. But it’s up to us to make sure that training datasets are fair and representative.</p><p><em>Learn about what to consider when evaluating a GAN for your application and the potential impact of biases in <a href="https://www.coursera.org/learn/build-better-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io">Course 2: Build Better GANs</a>, available now on Coursera.</em></p><hr><figure class="kg-card kg-image-card"><img src="https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2023.gif" class="kg-image" alt="Examples of CT scans with different contrasts" loading="lazy"></figure><h2 id="more-data-for-medical-ai">More Data for Medical AI</h2><p>Convolutional neural networks are good at recognizing disease symptoms in medical scans of patients who were injected with iodine-based dye, known as radiocontrast, that makes their organs more visible. But some patients can’t take the dye. Now synthetic scans from a GAN are helping CNNs learn to analyze undyed images.<br><br><strong>What’s new:</strong> Researchers from the U.S. National Institutes of Health and University of Wisconsin <a href="https://www.nature.com/articles/s41598-019-52737-x?ref=dl-staging-website.ghost.io" rel="noopener">developed</a> a GAN that generates labeled, undyed computerized tomography (CT) images of lesions on kidneys, spleens, and livers. They added these images to real-world training data to improve performance of a segmentation model that marks lesions in diagnostic scans.<br><br><strong>How it works:</strong> The work is based on <a href="https://arxiv.org/abs/1703.10593?ref=dl-staging-website.ghost.io" rel="noopener">CycleGAN</a> and the <a href="https://nihcc.app.box.com/v/DeepLesion?ref=dl-staging-website.ghost.io" rel="noopener">DeepLesion</a> dataset of CTs. CycleGAN has been used to <a href="https://junyanz.github.io/CycleGAN/?ref=dl-staging-website.ghost.io" rel="noopener">turn pictures of horses into pictures of zebras</a> without needing to match particular zebra and horse pics. This work takes advantage of that capability to map between dyed and undyed CTs.</p><ul><li>The authors used a CNN to sort DeepLesion into images of dyed and undyed patients. They trained the GAN on a portion of the dataset, including both dyed and undyed CTs, and generated fake undyed images.</li><li>Using a mix of CycleGAN output and natural images, they trained a <a href="https://arxiv.org/abs/1505.04597?ref=dl-staging-website.ghost.io" rel="noopener">U-Net</a> segmentation model to isolate lesions, organs, and other areas of interest.</li><li>To compare their approach with alternatives, they trained separate U-Nets on variations of DeepLesion: dyed images in which the dye had been artificially lightened, images that had been augmented via techniques like rotation and cropping, and the dataset without alterations.</li></ul><p><strong>Results:</strong> Tested on undyed, real-world CT scans, the U-Net trained on the combination of CycleGAN output and natural images outperformed the others. It was best at identifying lesions on kidneys, achieving a 57 percent improvement over the next-best model. With lesions on spleens, the spread was 4 percent; on livers, 3 percent. In estimating lesion volume, it achieved  an average error of 0.178, compared to the next-highest score of 0.254. Tested on the remainder of the dyed DeepLesion images, all four U-Nets isolated lesions roughly equally well.<br><br><strong>Behind the news:</strong> The researchers behind this model have used it to improve screening for dangerous levels of <a href="https://www.ajronline.org/doi/10.2214/AJR.20.24415?ref=dl-staging-website.ghost.io" rel="noopener">liver</a> <a href="https://pubs.rsna.org/doi/pdf/10.1148/radiol.2019190512?ref=dl-staging-website.ghost.io" rel="noopener">fat</a> and to identify patients with high risk of <a href="https://www.ajronline.org/doi/pdf/10.2214/AJR.20.23049?ref=dl-staging-website.ghost.io" rel="noopener">metabolic syndrome</a>, a precursor to heart disease, diabetes, and stroke.<br><strong>Why it matters:</strong> Medical data can be hard to come by and labeled medical data even more so. GANs are making it easier and less expensive to create large, annotated datasets for training AI diagnostic tools.  <br><br><strong>We’re thinking:</strong> Medical AI is just beginning to be <a href="https://lukeoakdenrayner.wordpress.com/2020/09/06/the-medical-ai-floodgates-open-at-a-cost-of-1000-per-patient/?ref=dl-staging-website.ghost.io" rel="noopener">recognized by key healthcare players</a> in the U.S. Clever uses of CycleGAN and other architectures could accelerate the process.</p><p><em>To learn more about using GANs to augment training datasets, including the pros and cons, stay tuned for GANs Specialization <a href="https://www.coursera.org/learn/apply-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io">Course 3: Apply GANs</a>, coming soon to Coursera.</em></p><hr><h2 id="a-message-from-deeplearningai">A MESSAGE FROM <a href="https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io" rel="noopener">DEEPLEARNING.AI</a></h2><figure class="kg-card kg-image-card"><img src="https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2028.gif" class="kg-image" alt="ezgif.com-resize (28)" loading="lazy"></figure><p>We’re thrilled to announce the launch of our new Generative Adversarial Networks Specialization on Coursera! <a href="https://www.coursera.org/specializations/generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io">Enroll now</a></p><hr><figure class="kg-card kg-image-card"><img src="https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2022.gif" class="kg-image" alt="Data and examples related to a new technique to detect portions of an image " loading="lazy"></figure><h2 id="the-telltale-artifact">The Telltale Artifact</h2><p>Deepfakes have gone mainstream, allowing celebrities to star in <a href="https://www.youtube.com/watch?v=yPCnVeiQsUw&ref=dl-staging-website.ghost.io" rel="noopener">commercials</a> without setting foot in a film studio. A new method helps determine whether such endorsements — and other images produced by generative adversarial networks — are authentic.<br><br><strong>What’s new:</strong> Lucy Chai led MIT CSAIL researchers in an analysis of where image generators fool and where they fail. They developed a <a href="https://arxiv.org/abs/2008.10588?ref=dl-staging-website.ghost.io" rel="noopener">technique</a> to detect portions of an image that betray fakery.<br><br><strong>Key insight:</strong> Large-scale features of generated images are highly varied, but generated textures contain consistent artifacts. Convolutional neural networks (CNNs) are especially <a href="https://arxiv.org/abs/1811.12231?ref=dl-staging-website.ghost.io" rel="noopener">sensitive to textures</a>, which makes them well suited to recognizing such artifact-laden areas. A CNN tailored for analyzing small pieces of images can learn to recognize parts dominated by generated textures.<br><br><strong>How it works:</strong> The authors built classifiers that survey images one patch at a time. They ran the classifiers on output from <a href="https://arxiv.org/abs/1812.04948?ref=dl-staging-website.ghost.io" rel="noopener">StyleGAN</a>, <a href="https://arxiv.org/abs/1807.03039?ref=dl-staging-website.ghost.io" rel="noopener">Glow,</a> and a generator model based on <a href="https://arxiv.org/abs/1805.12462?ref=dl-staging-website.ghost.io" rel="noopener">Gaussian mixture models</a> (GMMs). They averaged the patchwise classifications to analyze each GAN’s vulnerability to detection.</p><ul><li>The authors created a dataset of images generated by a <a href="https://arxiv.org/abs/1710.10196?ref=dl-staging-website.ghost.io" rel="noopener">Progressive GAN</a> trained on the <a href="https://arxiv.org/abs/1710.10196?ref=dl-staging-website.ghost.io" rel="noopener">CelebA-HQ</a> dataset of celebrity portraits.</li><li>They modified <a href="https://arxiv.org/abs/1512.03385?ref=dl-staging-website.ghost.io" rel="noopener">Resnet</a> and <a href="https://arxiv.org/abs/1610.02357?ref=dl-staging-website.ghost.io" rel="noopener">Xception</a> architectures to classify patches of user-determined size and trained them on the generated images. They removed the deeper layers, which analyze larger image areas, to concentrate the models on fine details.</li><li>They used the classifications to produce heatmaps of image areas recognized as generated (blue) or not (red). Predominantly blue images were deemed to have been generated.</li><li>By averaging the heatmaps over many images produced by the same GAN, the authors were able to identify the areas where that model is especially prone to leaving artifacts. For instance, StyleGAN and Glow generated high concentrations of artifacts in facial details, while GMM tended to generate them in backgrounds.</li></ul><p><strong>Results:</strong> The authors’ best classifier achieved 100 percent average precision on StyleGAN output and 91.38 percent on GMM. These scores outperformed non-truncated <a href="https://arxiv.org/abs/1809.00888?ref=dl-staging-website.ghost.io" rel="noopener">MesoInception4</a>, Resnet-18, Xception, and CNN models, which achieved average precision between 99.75 and 73.33 percent. On Glow, the authors’ best classifier achieved 95 percent average precision, whereas the best full model scored 97.32 percent.<br><br><strong>Why it matters:</strong> The better GANs become, the more useful they can be for both good and <a href="https://cset.georgetown.edu/wp-content/uploads/CSET-Deepfakes-Report.pdf?ref=dl-staging-website.ghost.io" rel="noopener">ill</a>. In shedding light on areas where particular GANs produce more artifacts, this work illuminates pathways for researchers to improve them. But it also provides a map for malefactors to make their activities harder to detect. In fact, when the researchers trained a GAN to fool their classifiers, accuracy fell to less than 65 percent.<br><br><strong>We’re thinking:</strong> Building a discriminator that recognizes a particular generator’s output is easier than building a good generator. In fact, GAN researchers routinely degrade discriminators to give the generator a fighting chance to fool it. But social media platforms, among others, would like to catch <em>all</em> generated images, regardless of the generator that produced them. Looking for common artifacts offers a promising approach — until a variety of generators learn how to avoid producing them.</p><p><em>Learn how image translation is used to create deepfakes in the upcoming <a href="https://www.coursera.org/learn/apply-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io">Course 3: Apply GANs</a>, available soon on Coursera.</em></p><hr><figure class="kg-card kg-image-card"><img src="https://home-wordpress.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2027.gif" class="kg-image" alt="Examples of Generative Adversarial Networks used for image to illustration translation" loading="lazy"></figure><h2 id="style-and-substance">Style and Substance</h2><p>GANs are adept at mapping the artistic style of one picture onto the subject of another, known as style transfer. However, applied to the fanciful illustrations in children’s books, some GANs prove better at preserving style, others better at preserving subject matter. A new model is designed to excel at both.<br><br><strong>What’s new:</strong> Developed by researchers at Hacettepe University and Middle East Technical University, both in Turkey, <a href="https://arxiv.org/abs/2002.05638?ref=dl-staging-website.ghost.io" rel="noopener">Ganilla</a> aims to wed photographic content and artistic style for illustrations in children’s books. It converts photos into virtual artwork in the styles of 10 published children’s book illustrators, including favorites like Patricia Polacco and Kevin Henkes, while staying true to scenes in photos.</p><p><strong>How it works:</strong> Ganilla is almost identical to <a href="https://arxiv.org/abs/1703.10593?ref=dl-staging-website.ghost.io" rel="noopener">CycleGAN</a> except for a specially crafted generator.</p><ul><li>The researchers divided their generator into a downsampling stage and an upsampling stage.</li><li>The downsampling stage is a modified <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf?ref=dl-staging-website.ghost.io" rel="noopener">Resnet-18</a> with additional skip connections to pass low-level features, such as textures and edges, from one layer to the next.</li><li>The upsampling stage consists of layers of transposed convolutions that increase the size of the feature map and skip connections from the downsampling stage. The skip connections in this stage help preserve subject matter without overwriting style information.</li><li>The authors trained the model on unpaired images from two datasets. The first contained nearly 5,500 images of landscape scenery, the second hundreds of works by each of 10 illustrators.</li></ul><p><strong>Results:</strong> There’s no way to measure objectively how well a model generates landscapes in specific artistic styles, so the authors used quantitative and qualitative approaches to compare Ganilla’s output with that of a CycleGAN, <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Yi_DualGAN_Unsupervised_Dual_ICCV_2017_paper.pdf?ref=dl-staging-website.ghost.io" rel="noopener">DualGAN</a>, and <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf?ref=dl-staging-website.ghost.io" rel="noopener">CartoonGAN</a> trained on the same data.</p><ul><li>They trained a pair of CNNs to assess the GANs’ proficiency at transferring style (trained on small portions of images from each artist) and content (trained on full-size photos). The style classifier scored CycleGAN highest, while the content classifier gave DualGAN the edge. Ganilla ranked highest when style and content scores were averaged.</li><li>The researchers asked 48 people to (a) rate whether each GAN-made illustration looked like the illustrator’s work, (b) describe what they thought the picture showed, and (c) rank generated images in terms of overall appeal. They scored Ganilla’s output highest for mimicking the human illustrators and depicting the source content. However, they rated DualGAN’s output slightly more appealing.</li></ul><p><strong>Yes, but:</strong> Based on examples in the paper, the training illustrations tended to be heavy on stylized human and animal characters, while the photos contain very few characters. We’re curious to see what Ganilla would do with more photos of people and animals.</p><p><strong>Why it matters:</strong> GANs are powerful creative tools, and — like printmaking and photography before them — they’re spawning their own adversarial dynamic in the arts. Artists working in traditional media have <a href="http://cyberlaw.stanford.edu/blog/2018/05/artificial-intelligence-art-who-owns-copyright-0?ref=dl-staging-website.ghost.io" rel="noopener">raised concerns</a> about GANs being trained to make derivatives of their work. Now, digital artists are <a href="https://www.artnome.com/news/2019/3/27/why-is-ai-art-copyright-so-complicated?ref=dl-staging-website.ghost.io" rel="noopener">accusing</a> traditional artists of creative theft for making paint-on-canvas reproductions of their AI-abetted digital compositions.<br><br><strong>We’re thinking:</strong> When it comes to art, we favor GANs as a <a href="http://www.artbreeder.com/?ref=dl-staging-website.ghost.io" rel="noopener">creative partner</a>.</p><p><em>Learn about human and algorithmic approaches to evaluating generative adversarial networks in GAN Specialization <a href="https://www.coursera.org/learn/build-better-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io">Course 2: Build Better GANs</a> on Coursera. To build your own CycleGAN for style transfer, stay tuned for <a href="https://www.coursera.org/learn/apply-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io">Course 3: Apply GANS</a>, coming soon to Coursera!</em></p></div><aside style="grid-column:main-end / wide-end" class="flex-col hidden pl-10 lg:flex"><div class="relative shadow rounded-lg overflow-hidden hover:shadow-sm transition-shadow" data-sentry-component="Advertisement" data-sentry-source-file="Advertisement.tsx"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:200%"></span><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" class="rounded-lg" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" loading="lazy" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="rounded-lg" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=3840&amp;q=75 3840w" src="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=3840&amp;q=75"/></noscript></span><a href="https://bit.ly/3XWMC3m"><div class="absolute inset-0" data-gtm-event-title="Data Analytics Professional Certificate"></div></a></div></aside><footer class="mt-8 lg:hidden"><div class="flex flex-col items-start items-start " data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-59/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-59/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-59/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></footer></div></article><div class="py-16 bg-slate-50"><section id="subscribe" data-sentry-component="CtaNewsletter" data-sentry-source-file="CtaNewsletter.tsx"><div class="container--boxed relative"><div class="text-center"><h2 class="text--l2 text-slate-900">Subscribe to The Batch</h2><p class="text-base lg:text-lg text-slate-500 mt-3 max-w-md mx-auto">Stay updated with weekly AI News and Insights delivered to your inbox</p></div><div class="flex flex-col items-center mt-9"><div><div id="reactHubspotForm285" style="display:none"></div><div class="flex items-center justify-center"><svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-brand-teal" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" data-sentry-element="svg" data-sentry-component="Spinner" data-sentry-source-file="Spinner.tsx"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4" data-sentry-element="circle" data-sentry-source-file="Spinner.tsx"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z" data-sentry-element="path" data-sentry-source-file="Spinner.tsx"></path></svg></div></div></div></div></section></div></div></main><footer class="py-16 bg-brand-teal-900" data-sentry-component="Footer" data-sentry-source-file="index.tsx"><div class="flex flex-col items-center justify-center text-center container--boxed"><div class="max-w-[220px] flex items-center justify-center text-white"><svg viewBox="0 0 272 34" fill="none" xmlns="http://www.w3.org/2000/svg" width="272" height="34" aria-label="DeepLearning.AI" data-sentry-element="svg" data-sentry-component="DLAILogo" data-sentry-source-file="DLAILogo.tsx"><g fill="currentColor" data-sentry-element="g" data-sentry-source-file="DLAILogo.tsx"><path d="M56.775 16.108c0 6.206-4.252 10.005-10.747 10.005H39.42V5.961h6.608c6.495 0 10.747 3.924 10.747 10.147zm-10.747 7.306c4.778 0 7.337-2.724 7.337-7.306 0-4.581-2.56-7.452-7.337-7.452H42.73v14.758h3.298z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M66.97 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.259-8.226 8.006-8.226c4.571 0 7.804 3.159 7.804 7.856.007.535-.027 1.07-.103 1.6H62.412c.233 2.638 2.123 4.232 4.57 4.232 2.038 0 3.173-.988 3.786-2.234h3.582c-.915 2.802-3.448 5.032-7.38 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.484-4.03-2.24 0-4.044 1.525-4.394 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M84.938 26.371c-4.601 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.792 9.792 0 01-.116 1.6H80.367c.233 2.638 2.128 4.232 4.57 4.232 2.042 0 3.177-.988 3.786-2.234h3.582c-.902 2.802-3.435 5.032-7.367 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M104.917 9.885c4.221 0 7.541 3.245 7.541 8.166 0 4.92-3.32 8.32-7.541 8.32a7.277 7.277 0 01-3.095-.664 7.248 7.248 0 01-2.516-1.914v9.915h-3.319v-23.57h3.32v2.347a7.004 7.004 0 015.61-2.6zm-.729 2.871c-2.473 0-4.86 1.943-4.86 5.364 0 3.42 2.387 5.39 4.86 5.39 2.473 0 4.894-2.02 4.894-5.46 0-3.437-2.391-5.303-4.894-5.303v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M119.074 5.961v17.484h6.841v2.668h-10.16V5.961h3.319z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M135.584 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.927 9.927 0 01-.116 1.625h-12.258c.233 2.639 2.128 4.233 4.571 4.233 2.041 0 3.176-.988 3.785-2.235h3.582c-.902 2.777-3.435 5.007-7.367 5.007zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M153.116 9.885a6.864 6.864 0 013.088.633 6.839 6.839 0 012.475 1.946v-2.325h3.349v15.974h-3.349v-2.38a6.906 6.906 0 01-5.611 2.638c-4.165 0-7.514-3.39-7.514-8.32s3.353-8.166 7.562-8.166zm.699 2.87c-2.473 0-4.86 1.853-4.86 5.304 0 3.451 2.387 5.45 4.86 5.45 2.473 0 4.864-1.938 4.864-5.39 0-3.45-2.361-5.372-4.864-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M169.72 26.114h-3.319V10.139h3.319v2.325c.928-1.595 2.534-2.579 4.804-2.579v3.438h-.863c-2.447 0-3.958 1.014-3.958 4.405l.017 8.386z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M188.966 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.044 1.504-4.044 4.435v8.922h-3.319V10.14h3.319v1.826a6.16 6.16 0 014.774-2.08c3.755 0 6.582 2.347 6.582 6.812v9.416h-3.293v-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M196.049 5.905a2.105 2.105 0 011.309-1.931 2.117 2.117 0 012.294.458 2.102 2.102 0 01-1.48 3.588 2.109 2.109 0 01-1.509-.612 2.08 2.08 0 01-.614-1.503zm.431 4.234h3.319v15.974h-3.319V10.14z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M215.558 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.048 1.504-4.048 4.435v8.922h-3.337V10.14h3.319v1.826a6.192 6.192 0 014.796-2.08c3.755 0 6.56 2.338 6.56 6.803v9.425h-3.289l.018-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M229.538 9.885c2.62 0 4.571 1.216 5.559 2.579v-2.325h3.349V26.37c0 4.35-2.823 7.629-7.829 7.629-4.282 0-7.454-2.119-7.864-5.656h3.289c.496 1.655 2.274 2.785 4.575 2.785 2.559 0 4.48-1.569 4.48-4.758v-2.664a6.903 6.903 0 01-5.559 2.665c-4.222 0-7.571-3.392-7.571-8.321 0-4.93 3.337-8.166 7.571-8.166zm.699 2.87c-2.478 0-4.864 1.853-4.864 5.304 0 3.452 2.386 5.45 4.864 5.45 2.477 0 4.86-1.938 4.86-5.39 0-3.45-2.357-5.372-4.86-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M242.702 26.316a2.134 2.134 0 01-1.987-1.287 2.114 2.114 0 011.53-2.908 2.138 2.138 0 012.194.896c.235.349.361.76.361 1.18a2.096 2.096 0 01-1.289 1.956 2.11 2.11 0 01-.809.163z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M260.584 21.996h-8.477l-1.455 4.117h-3.453l7.251-20.2h3.846l7.247 20.2h-3.492l-1.467-4.117zM256.38 9.932l-3.341 9.365h6.612l-3.271-9.365z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M268.681 5.961H272v20.152h-3.319V5.961z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218zm.126-1.59c-3.734 0-6.76-3.207-6.76-7.16 0-3.954 3.018-7.16 6.75-7.16 3.734 0 6.76 3.206 6.76 7.16s-3.021 7.16-6.76 7.16h.01zm-.126-6.28c.729 0 1.44-.214 2.046-.617a3.67 3.67 0 001.356-1.646 3.652 3.652 0 00-.798-3.995 3.687 3.687 0 00-4.012-.794 3.679 3.679 0 00-1.653 1.35 3.655 3.655 0 00-.62 2.037c.002.971.39 1.902 1.08 2.59a3.698 3.698 0 002.601 1.076z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path></g></svg></div><nav class="mt-6 md:mt-10"><ul class="flex flex-wrap justify-center space-x-8 gap-x-8"><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/courses/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Courses</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/the-batch/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">The Batch</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/community/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Community</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/careers/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Careers</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/about/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">About</div></a></li></ul></nav><div class="flex mt-12"><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.facebook.com/1027125564106325" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsFacebook" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.instagram.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsInstagram" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.9 3.9 0 0 0-1.417.923A3.9 3.9 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.9 3.9 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.9 3.9 0 0 0-.923-1.417A3.9 3.9 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599s.453.546.598.92c.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.5 2.5 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.5 2.5 0 0 1-.92-.598 2.5 2.5 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233s.008-2.388.046-3.231c.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92s.546-.453.92-.598c.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92m-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217m0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://twitter.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsTwitter" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.linkedin.com/company/18246783" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsLinkedin" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.youtube.com/c/Deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsYoutube" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.01 2.01 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.01 2.01 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31 31 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.01 2.01 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A100 100 0 0 1 7.858 2zM6.4 5.209v4.818l4.157-2.408z"></path></svg></a></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"donations_enabled":false,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.121","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-59","id":"60c03212274d5b003b10652c","uuid":"b41414fe-173d-45d3-bd16-df5c98cd1306","title":"The Batch: GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes, Including Minorities, Synthesizing Training Data","html":"\u003cp\u003e\u003cem\u003eDear friends,\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eThis special issue of The Batch celebrates the launch of our new \u003ca href=\"https://www.coursera.org/specializations/generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io\"\u003eGenerative Adversarial Networks Specialization\u003c/a\u003e!\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eGANs are among the most exciting technologies to emerge from deep learning. These networks learn in a very different way than typical supervised methods for learning x-to-y mappings. By pitting a discriminator network and a generator network against one another (details below), they produce photorealistic images, medical training data, children’s book illustrations, and other types of output.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eEarlier today, we held an online panel discussion on “GANs for Good” with Anima Anandkumar, Alexei Efros, Ian Goodfellow, and our course instructor Sharon Zhou. I was struck by the number of new applications GANs are enabling, and the number that are likely to come.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eIan explained that GAN-generated training examples for a particular application at Apple are one-fifth as valuable as real examples but cost much less than one-fifth as much to produce. Anima described exciting progress on disentanglement and how the ability to isolate objects in images is making it easier to control image generation (“add a pair of glasses to this face”). Alexei talked about the impact GANs are having on art through tools like \u003ca href=\"https://artbreeder.com/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eArtbreeder\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://www.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-3020at2012.03.3120PM.png\" class=\"kg-image\" alt=\"Screen Shot 2020-09-30 at 12.03.31 PM\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cem\u003eAll the speakers talked about alternatives to reading research papers to keep up with the exploding literature. If you missed the live discussion, you can watch a video of the entire event \u003ca href=\"https://bit.ly/3il4SvI?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ehere\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eWe’re still in the early days of practical GAN applications, but I believe they will:\u003c/em\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cem\u003e\u003cem\u003eTransform photo editing and make it easier to add or subtract elements such as background objects, trees, buildings, and clouds\u003c/em\u003e\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003e\u003cem\u003eGenerate special effects for media and entertainment that previously were prohibitively expensive\u003c/em\u003e\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003e\u003cem\u003eContribute to creative products from industrial design to fine art\u003c/em\u003e\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003e\u003cem\u003eAugment datasets in small data problems in fields from autonomous driving to manufacturing\u003c/em\u003e\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cem\u003eAs an emerging technology, GANs have numerous untapped applications. This is a moment to dream up new ideas, because no one else may be working on them yet.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eI hope this technology will spark your hunger to learn more and invent new applications that will make life better for people all over the world.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eKeep learning!\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003cem\u003eAndrew\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"special-issue-generation-gan\"\u003eSpecial Issue: Generation GAN\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://www.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker201-2.gif\" class=\"kg-image\" alt=\"Online panel discussion on “GANs for Good” with Andrew Ng, Anima Anandkumar, Alexei Efros, Ian Goodfellow, and Sharon Zhou\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"reality-reimagined\"\u003eReality Reimagined\u003c/h2\u003e\u003cp\u003eGenerative adversarial networks, or GANs, are said to give computers the gift of imagination. Competition between a discriminator network, which learns to classify the system’s output as generated or real, and a generator network, which learns to produce output that fools the discriminator, produces fantasy images of uncanny realism. First \u003ca href=\"https://arxiv.org/abs/1406.2661?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eproposed\u003c/a\u003e in 2014, the architecture has been adopted by researchers to extend training datasets with synthetic examples and by businesses to create customized imagery for ads, entertainment, and personal services. But it has a dark side: GANs make it easy and convincing for jilted lovers to graft an ex’s face onto another person’s body, or politicians to misrepresent themselves as able to speak an ethnic minority’s language to win their votes. And it tends to tilt the training data distribution, favoring common examples while ignoring outliers. Researchers are improving the technology at breakneck pace, and developing ways to thwart, or at least detect, egregious uses. We have yet to see the best — and the worst — that GANs have to offer.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://www.deeplearning.ai/wp-content/uploads/2021/01/Screen20Shot202020-09-2920at201.03.5020PM.png\" class=\"kg-image\" alt=\"Ian Goodfellow\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"a-man-a-plan-a-gan\"\u003eA Man, A Plan, A GAN\u003c/h2\u003e\u003cp\u003eBrilliant ideas strike at unlikely moments. \u003ca href=\"https://www.deeplearning.ai/blog/hodl-ian-goodfellow/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eIan Goodfellow\u003c/a\u003e conceived generative adversarial networks while spitballing programming techniques with friends at a bar. Goodfellow, who views himself as “someone who works on the core technology, not the applications,” started at Stanford as a premed before switching to computer science and studying machine learning with Andrew Ng. “I realized that would be a faster path to impact more things than working on specific medical applications one at a time,” he recalls. From there, he earned a PhD in machine learning at Université de Montréal, interned at the seminal robotics lab Willow Garage, and held positions at OpenAI and Google Research. Last year, he joined Apple as director of machine learning in the Special Projects Group, which develops technologies that aren’t part of products on the market. His work at Apple is top-secret.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eThe Batch:\u003c/strong\u003e How did you come up with the idea for two networks that battle each other?\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eGoodfellow:\u003c/strong\u003e I’d been thinking about how to use something like a discriminator network as a way to score a speech synthesis contest, but I didn’t do it because it would have been too easy to cheat by overfitting to a particular discriminator. Some of my friends wanted to train a generator network using a technique that would have taken gigabytes of data per image, even for the tiny images we studied with generative models in 2014. We were discussing the problem one night at a bar, and they asked me how to write a program that efficiently manages gigabytes of data per image on a GPU that, back then, had about 1.5GB RAM. I said, that’s not a programming problem. It’s an algorithm design problem. Then I realized that a discriminator network could help a generator produce images if it were part of the learning process. I went home that night and started coding the first GAN.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eThe Batch:\u003c/strong\u003e How long did it take?\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eGoodfellow:\u003c/strong\u003e By copying and pasting bits and pieces of earlier papers, I got the first GAN to produce \u003ca href=\"http://yann.lecun.com/exdb/mnist/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eMNIST\u003c/a\u003e images in only an hour of work or so. MNIST is such a small dataset that, even back then, you could train on it very quickly. I think it trained for tens of minutes before it could produce recognizable handwritten digits.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eThe Batch:\u003c/strong\u003e What did it feel like to see the first face?\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eGoodfellow:\u003c/strong\u003e That wasn’t as much of a revolutionary moment as people might expect. My colleague \u003ca href=\"https://scholar.google.com/citations?user=h-BY2wgAAAAJ\u0026hl=en\u0026ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eBing Xu\u003c/a\u003e modeled face images from the Toronto Face Database, which were only 90 pixels square and grayscale. Because the faces were always centered and looking straight at the camera, even very simple algorithms like PCA could make pretty good faces. The main thing we were surprised by were the images it made of \u003ca href=\"https://www.cs.toronto.edu/~kriz/cifar.html?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eCIFAR10\u003c/a\u003e, where there’s a lot of variability. They looked like crap. But we had been looking at crap from generative models for years, and we could tell that this was an exciting, new kind of crap.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eThe Batch:\u003c/strong\u003e Has anything surprised you about the way this work has played out?\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eGoodfellow:\u003c/strong\u003e In the first GAN paper, we included a list of things that might happen in future work. A lot of them did. The one big category of things I didn’t anticipate was domain-to-domain translation. GANs like \u003ca href=\"https://arxiv.org/abs/1703.10593?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eCycleGAN\u003c/a\u003e from Berkeley. You can take a picture of a horse and have it transformed into a zebra without training on matched pairs of horse and zebra images. That’s very powerful because it can be easy to passively collect data in each domain, but it’s time-consuming and expensive to get data that matches up across domains.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eThe Batch:\u003c/strong\u003e What are you most hopeful about in GAN research?\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eGoodfellow:\u003c/strong\u003e I’d like to see more use of GANs in the physical world, specifically for medicine. I’d like to see the community move toward more traditional science applications, where you have to get your hands dirty in the lab. That can lead to more things that have more of a tangible, positive impact on peoples’ lives. For instance, in dentistry, GANs have been used to make \u003ca href=\"https://arxiv.org/abs/1804.00064?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003epersonalized crowns\u003c/a\u003e for individual patients. Insilico is using GANs to \u003ca href=\"https://insilico.com/publications?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003edesign medicinal drugs\u003c/a\u003e.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eThe Batch:\u003c/strong\u003e How much do you worry about bias in GAN output? The ability to produce realistic human faces makes it a pressing issue.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eGoodfellow:\u003c/strong\u003e GANs can be used to counteract biases in training data by generating training data for other machine learning algorithms. If there’s a language where you don’t have as much representation in your data, you can oversample that. At Apple, we were able to generate data for a gestural text-entry feature called \u003ca href=\"https://9to5mac.com/2019/06/05/quickpath-ios-13-keyboard/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eQuickPath\u003c/a\u003e. A startup called Vue.ai uses GANs to generate images of \u003ca href=\"https://vue.ai/products/automated-on-model-fashion-imagery/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ewhat clothing would look like on different models\u003c/a\u003e. Traditionally, there may not have been much diversity in who was hired to be a model to try on this clothing. Now you can get a model who looks like you, wearing a specific item of clothing you’re interested in. These are baby steps, but I hope there are other ways GANs can be used to address issues of underrepresentation in datasets.\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"news\"\u003e\u003cstrong\u003eNews\u003c/strong\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://www.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-gif-maker-2.gif\" class=\"kg-image\" alt=\"Makeup applied on female faces with the help of augmented reality\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"a-good-look-for-ai\"\u003eA Good Look for AI\u003c/h2\u003e\u003cp\u003eTrying on new makeup is a hassle — apply, inspect, wash off, repeat. Not to mention the tribulation of visiting brick-and-mortar stores during the pandemic. Augmented reality is helping people try on all the makeup they want without leaving home.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e Modiface, a subsidiary of beauty giant L’Oréal, \u003ca href=\"https://patents.google.com/patent/US20200160153A1/en?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003euses GANs\u003c/a\u003e to let customers see how different colors of lipstick, eye shadow, and hair will look on them.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e Modiface’s approach is a hybrid of \u003ca href=\"https://arxiv.org/abs/1703.10593?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eCycleGAN\u003c/a\u003e, \u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.pdf?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eStarGAN\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/1812.04948?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eStyleGAN\u003c/a\u003e, company operations chief Jeff Houghton told \u003cem\u003eThe Batch.\u003c/em\u003e It uses a CNN to track landmarks on a user’s hair and face. At L’Oreal’s \u003ca href=\"https://www.lorealparisusa.com/virtual-try-on/makeup.aspx?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ewebsite\u003c/a\u003e, users can select different lipsticks, eyeliners, blushes and hair dyes and fine-tune their shade, texture, and gloss. Then they can virtually apply those products to an uploaded selfie or a real-time video of their own faces.\u003c/p\u003e\u003cul\u003e\u003cli\u003eTo train the algorithm, Modiface collected and annotated thousands of images of people wearing and not wearing makeup and hair coloring under various lighting and background conditions.\u003c/li\u003e\u003cli\u003eA CycleGAN takes as input an image of a user’s face without makeup. When the user selects a specific makeup product, the system generates an image that shows what the selected product looks like when applied.\u003c/li\u003e\u003cli\u003eAdding new makeup types to the model is as simple as adding a color swatch to the database. Modiface must retrain the model to include new products with properties, such as a metallic sheen, that aren’t represented in the training data.\u003c/li\u003e\u003cli\u003eCollaborations with \u003ca href=\"https://www.lorealusa.com/media/press-releases/2019/june/modiface?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eAmazon\u003c/a\u003e and \u003ca href=\"https://mediaroom.loreal.com/en/loreals-modiface-launches-long-term-augmented-reality-collaboration-with-facebook/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eFacebook\u003c/a\u003e enable users to access Modiface directly from those platforms.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e Augmented reality applications are reshaping the beauty industry. In 2018, the same year L’Oréal \u003ca href=\"https://www.theverge.com/2018/3/16/17131260/loreal-modiface-acquire-makeup-ar-try-on?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003epurchased\u003c/a\u003e Modiface, American chain Ulta \u003ca href=\"https://www.glossy.co/new-face-of-beauty/whats-next-for-beauty-and-omnichannel-retail?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eacquired\u003c/a\u003e Glamst, a startup specializing in augmented reality. \u003ca href=\"https://www.meitu.com/en/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eMeitu\u003c/a\u003e, a multi-billion dollar Chinese company, uses AI-driven face manipulation to make its users appear more attractive in social media posts, job applications, and other digital venues.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e E-commerce is increasingly \u003ca href=\"https://www.wsj.com/articles/loreal-expands-virtual-try-on-service-11576776586?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eimportant\u003c/a\u003e to the beauty industry’s bottom line, and the pandemic is \u003ca href=\"https://www.glossy.co/beauty/covid-19-accelerates-beautys-demand-for-ai-and-ar-technology/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003edriving\u003c/a\u003e even more business to the web. Tools like this make it easier for customers to try out products, which may boost sales.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e It seems like we’re spending half our lives in video conferences. Do we still need to apply makeup at all, or can we let a GAN do it instead?\u003c/p\u003e\u003cp\u003e\u003cem\u003eLearn how to build a CycleGAN and control image features in \u003ca href=\"https://www.coursera.org/specializations/generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io\"\u003eCourses 1 and 2\u003c/a\u003e of the GANs Specialization from DeepLearning.AI, available now on Coursera. Build the powerful StyleGAN in \u003ca href=\"https://www.coursera.org/learn/apply-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io\"\u003eCourse 3: Apply GANs\u003c/a\u003e, coming soon!\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://www.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2021.gif\" class=\"kg-image\" alt=\"Data and examples related to IMLE-GAN\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"making-gans-more-inclusive\"\u003eMaking GANs More Inclusive\u003c/h2\u003e\u003cp\u003eA typical GAN’s output doesn’t necessarily reflect the data distribution of its training set. Instead, GANs are prone to modeling the majority of the training distribution, sometimes ignoring rare attributes — say, faces that represent \u003ca href=\"https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eminority populations\u003c/a\u003e. A twist on the GAN architecture forces its output to better reflect the diversity of the training data.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e \u003ca href=\"https://arxiv.org/abs/2004.03355?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eIMLE-GAN\u003c/a\u003e learns to generate all the attributes of its training dataset, including rare ones. Ning Yu spearheaded the research with colleagues at University of Maryland, Max Planck Institute, University of California Berkeley, CISPA Helmholtz Center for Information Security, Princeton’s Institute for Advanced Study, and Google.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eKey insight:\u003c/strong\u003e A GAN’s discriminator distinguishes whether or not the generator’s output is generated, while the generator learns to produce output that fools the discriminator. Ideally, a generator’s output would mirror the training data distribution, but in practice — since its only aim is to fool the discriminator, and the discriminator typically evaluates only one image at a time — it can learn to favor common types of examples. The authors had their model compare several generated works with examples from the training set, as well as interpolations between generated works, to encourage greater diversity in the output.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e IMLE-GAN enhances a GAN with \u003ca href=\"https://arxiv.org/abs/1809.09087?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eImplicit Maximum Likelihood Estimation\u003c/a\u003e (IMLE). Instead of naively adding the IMLE loss to the usual adversarial loss, the authors modified the default IMLE loss and added a novel interpolation loss to compensate for fundamental incompatibilities between the adversarial and IMLE losses.\u003c/p\u003e\u003cul\u003e\u003cli\u003eIMLE generates a set of images and penalizes the network based on how different those images are from real images by making nearest-neighbor comparisons. Instead of comparing pixels, like in standard IMLE, the authors compare the images over the feature space. The switch from pixels to features makes the adversarial and IMLE losses more comparable.\u003c/li\u003e\u003cli\u003eTo compute the interpolation loss, the authors create an additional image that is interpolated between two generated images. Then, they compare the interpolated image’s features to those of the two non-generated images that were matched to the generated images during IMLE.\u003c/li\u003e\u003cli\u003eTo increase inclusion of underrepresented attributes, the algorithm samples data from a set of minority examples for the IMLE and interpolation losses, but from all examples for the adversarial loss.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e The authors evaluated IMLE-GAN against \u003ca href=\"https://arxiv.org/abs/1812.04948?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eStyleGAN\u003c/a\u003e and a handful of other models using \u003ca href=\"https://github.com/fjxmlzn/PacGAN/tree/master/stacked_MNIST_experiments?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eStacked MNIST\u003c/a\u003e, a variation of the MNIST dataset that includes handwritten digits in 1,000 distinct styles. IMLE-GAN reproduced 997 of the styles, while StyleGAN reproduced 940. Trained on \u003ca href=\"http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eCelebA\u003c/a\u003e, a large-scale dataset of celebrity faces, IMLE-GAN generated attributes present in less than 6 percent of training examples with increased precision compared to StyleGAN. For instance, it generated wearers of eyeglasses with 0.904 precision, compared to StyleGAN’s meager 0.719.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e Much of the time, we want our models to learn the data distribution present in the training set. But when fairness or broad representation are at stake, we may need to put a finger on the scale. This work offers an approach to making GANs more useful in situations where diversity or fairness is critical.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e This work helps counter model and dataset bias. But it’s up to us to make sure that training datasets are fair and representative.\u003c/p\u003e\u003cp\u003e\u003cem\u003eLearn about what to consider when evaluating a GAN for your application and the potential impact of biases in \u003ca href=\"https://www.coursera.org/learn/build-better-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io\"\u003eCourse 2: Build Better GANs\u003c/a\u003e, available now on Coursera.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://www.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2023.gif\" class=\"kg-image\" alt=\"Examples of CT scans with different contrasts\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"more-data-for-medical-ai\"\u003eMore Data for Medical AI\u003c/h2\u003e\u003cp\u003eConvolutional neural networks are good at recognizing disease symptoms in medical scans of patients who were injected with iodine-based dye, known as radiocontrast, that makes their organs more visible. But some patients can’t take the dye. Now synthetic scans from a GAN are helping CNNs learn to analyze undyed images.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e Researchers from the U.S. National Institutes of Health and University of Wisconsin \u003ca href=\"https://www.nature.com/articles/s41598-019-52737-x?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003edeveloped\u003c/a\u003e a GAN that generates labeled, undyed computerized tomography (CT) images of lesions on kidneys, spleens, and livers. They added these images to real-world training data to improve performance of a segmentation model that marks lesions in diagnostic scans.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e The work is based on \u003ca href=\"https://arxiv.org/abs/1703.10593?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eCycleGAN\u003c/a\u003e and the \u003ca href=\"https://nihcc.app.box.com/v/DeepLesion?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eDeepLesion\u003c/a\u003e dataset of CTs. CycleGAN has been used to \u003ca href=\"https://junyanz.github.io/CycleGAN/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eturn pictures of horses into pictures of zebras\u003c/a\u003e without needing to match particular zebra and horse pics. This work takes advantage of that capability to map between dyed and undyed CTs.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe authors used a CNN to sort DeepLesion into images of dyed and undyed patients. They trained the GAN on a portion of the dataset, including both dyed and undyed CTs, and generated fake undyed images.\u003c/li\u003e\u003cli\u003eUsing a mix of CycleGAN output and natural images, they trained a \u003ca href=\"https://arxiv.org/abs/1505.04597?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eU-Net\u003c/a\u003e segmentation model to isolate lesions, organs, and other areas of interest.\u003c/li\u003e\u003cli\u003eTo compare their approach with alternatives, they trained separate U-Nets on variations of DeepLesion: dyed images in which the dye had been artificially lightened, images that had been augmented via techniques like rotation and cropping, and the dataset without alterations.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e Tested on undyed, real-world CT scans, the U-Net trained on the combination of CycleGAN output and natural images outperformed the others. It was best at identifying lesions on kidneys, achieving a 57 percent improvement over the next-best model. With lesions on spleens, the spread was 4 percent; on livers, 3 percent. In estimating lesion volume, it achieved  an average error of 0.178, compared to the next-highest score of 0.254. Tested on the remainder of the dyed DeepLesion images, all four U-Nets isolated lesions roughly equally well.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e The researchers behind this model have used it to improve screening for dangerous levels of \u003ca href=\"https://www.ajronline.org/doi/10.2214/AJR.20.24415?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eliver\u003c/a\u003e \u003ca href=\"https://pubs.rsna.org/doi/pdf/10.1148/radiol.2019190512?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003efat\u003c/a\u003e and to identify patients with high risk of \u003ca href=\"https://www.ajronline.org/doi/pdf/10.2214/AJR.20.23049?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003emetabolic syndrome\u003c/a\u003e, a precursor to heart disease, diabetes, and stroke.\u003cbr\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e Medical data can be hard to come by and labeled medical data even more so. GANs are making it easier and less expensive to create large, annotated datasets for training AI diagnostic tools.  \u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e Medical AI is just beginning to be \u003ca href=\"https://lukeoakdenrayner.wordpress.com/2020/09/06/the-medical-ai-floodgates-open-at-a-cost-of-1000-per-patient/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003erecognized by key healthcare players\u003c/a\u003e in the U.S. Clever uses of CycleGAN and other architectures could accelerate the process.\u003c/p\u003e\u003cp\u003e\u003cem\u003eTo learn more about using GANs to augment training datasets, including the pros and cons, stay tuned for GANs Specialization \u003ca href=\"https://www.coursera.org/learn/apply-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io\"\u003eCourse 3: Apply GANs\u003c/a\u003e, coming soon to Coursera.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"a-message-from-deeplearningai\"\u003eA MESSAGE FROM \u003ca href=\"https://www.deeplearning.ai/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eDEEPLEARNING.AI\u003c/a\u003e\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://www.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2028.gif\" class=\"kg-image\" alt=\"ezgif.com-resize (28)\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003cp\u003eWe’re thrilled to announce the launch of our new Generative Adversarial Networks Specialization on Coursera! \u003ca href=\"https://www.coursera.org/specializations/generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io\"\u003eEnroll now\u003c/a\u003e\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://www.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2022.gif\" class=\"kg-image\" alt=\"Data and examples related to a new technique to detect portions of an image \" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"the-telltale-artifact\"\u003eThe Telltale Artifact\u003c/h2\u003e\u003cp\u003eDeepfakes have gone mainstream, allowing celebrities to star in \u003ca href=\"https://www.youtube.com/watch?v=yPCnVeiQsUw\u0026ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ecommercials\u003c/a\u003e without setting foot in a film studio. A new method helps determine whether such endorsements — and other images produced by generative adversarial networks — are authentic.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e Lucy Chai led MIT CSAIL researchers in an analysis of where image generators fool and where they fail. They developed a \u003ca href=\"https://arxiv.org/abs/2008.10588?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003etechnique\u003c/a\u003e to detect portions of an image that betray fakery.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eKey insight:\u003c/strong\u003e Large-scale features of generated images are highly varied, but generated textures contain consistent artifacts. Convolutional neural networks (CNNs) are especially \u003ca href=\"https://arxiv.org/abs/1811.12231?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003esensitive to textures\u003c/a\u003e, which makes them well suited to recognizing such artifact-laden areas. A CNN tailored for analyzing small pieces of images can learn to recognize parts dominated by generated textures.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e The authors built classifiers that survey images one patch at a time. They ran the classifiers on output from \u003ca href=\"https://arxiv.org/abs/1812.04948?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eStyleGAN\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/1807.03039?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eGlow,\u003c/a\u003e and a generator model based on \u003ca href=\"https://arxiv.org/abs/1805.12462?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eGaussian mixture models\u003c/a\u003e (GMMs). They averaged the patchwise classifications to analyze each GAN’s vulnerability to detection.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe authors created a dataset of images generated by a \u003ca href=\"https://arxiv.org/abs/1710.10196?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eProgressive GAN\u003c/a\u003e trained on the \u003ca href=\"https://arxiv.org/abs/1710.10196?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eCelebA-HQ\u003c/a\u003e dataset of celebrity portraits.\u003c/li\u003e\u003cli\u003eThey modified \u003ca href=\"https://arxiv.org/abs/1512.03385?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eResnet\u003c/a\u003e and \u003ca href=\"https://arxiv.org/abs/1610.02357?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eXception\u003c/a\u003e architectures to classify patches of user-determined size and trained them on the generated images. They removed the deeper layers, which analyze larger image areas, to concentrate the models on fine details.\u003c/li\u003e\u003cli\u003eThey used the classifications to produce heatmaps of image areas recognized as generated (blue) or not (red). Predominantly blue images were deemed to have been generated.\u003c/li\u003e\u003cli\u003eBy averaging the heatmaps over many images produced by the same GAN, the authors were able to identify the areas where that model is especially prone to leaving artifacts. For instance, StyleGAN and Glow generated high concentrations of artifacts in facial details, while GMM tended to generate them in backgrounds.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e The authors’ best classifier achieved 100 percent average precision on StyleGAN output and 91.38 percent on GMM. These scores outperformed non-truncated \u003ca href=\"https://arxiv.org/abs/1809.00888?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eMesoInception4\u003c/a\u003e, Resnet-18, Xception, and CNN models, which achieved average precision between 99.75 and 73.33 percent. On Glow, the authors’ best classifier achieved 95 percent average precision, whereas the best full model scored 97.32 percent.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e The better GANs become, the more useful they can be for both good and \u003ca href=\"https://cset.georgetown.edu/wp-content/uploads/CSET-Deepfakes-Report.pdf?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eill\u003c/a\u003e. In shedding light on areas where particular GANs produce more artifacts, this work illuminates pathways for researchers to improve them. But it also provides a map for malefactors to make their activities harder to detect. In fact, when the researchers trained a GAN to fool their classifiers, accuracy fell to less than 65 percent.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e Building a discriminator that recognizes a particular generator’s output is easier than building a good generator. In fact, GAN researchers routinely degrade discriminators to give the generator a fighting chance to fool it. But social media platforms, among others, would like to catch \u003cem\u003eall\u003c/em\u003e generated images, regardless of the generator that produced them. Looking for common artifacts offers a promising approach — until a variety of generators learn how to avoid producing them.\u003c/p\u003e\u003cp\u003e\u003cem\u003eLearn how image translation is used to create deepfakes in the upcoming \u003ca href=\"https://www.coursera.org/learn/apply-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io\"\u003eCourse 3: Apply GANs\u003c/a\u003e, available soon on Coursera.\u003c/em\u003e\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://www.deeplearning.ai/wp-content/uploads/2021/01/ezgif.com-resize2027.gif\" class=\"kg-image\" alt=\"Examples of Generative Adversarial Networks used for image to illustration translation\" loading=\"lazy\"\u003e\u003c/figure\u003e\u003ch2 id=\"style-and-substance\"\u003eStyle and Substance\u003c/h2\u003e\u003cp\u003eGANs are adept at mapping the artistic style of one picture onto the subject of another, known as style transfer. However, applied to the fanciful illustrations in children’s books, some GANs prove better at preserving style, others better at preserving subject matter. A new model is designed to excel at both.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e Developed by researchers at Hacettepe University and Middle East Technical University, both in Turkey, \u003ca href=\"https://arxiv.org/abs/2002.05638?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eGanilla\u003c/a\u003e aims to wed photographic content and artistic style for illustrations in children’s books. It converts photos into virtual artwork in the styles of 10 published children’s book illustrators, including favorites like Patricia Polacco and Kevin Henkes, while staying true to scenes in photos.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e Ganilla is almost identical to \u003ca href=\"https://arxiv.org/abs/1703.10593?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eCycleGAN\u003c/a\u003e except for a specially crafted generator.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe researchers divided their generator into a downsampling stage and an upsampling stage.\u003c/li\u003e\u003cli\u003eThe downsampling stage is a modified \u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eResnet-18\u003c/a\u003e with additional skip connections to pass low-level features, such as textures and edges, from one layer to the next.\u003c/li\u003e\u003cli\u003eThe upsampling stage consists of layers of transposed convolutions that increase the size of the feature map and skip connections from the downsampling stage. The skip connections in this stage help preserve subject matter without overwriting style information.\u003c/li\u003e\u003cli\u003eThe authors trained the model on unpaired images from two datasets. The first contained nearly 5,500 images of landscape scenery, the second hundreds of works by each of 10 illustrators.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e There’s no way to measure objectively how well a model generates landscapes in specific artistic styles, so the authors used quantitative and qualitative approaches to compare Ganilla’s output with that of a CycleGAN, \u003ca href=\"https://openaccess.thecvf.com/content_ICCV_2017/papers/Yi_DualGAN_Unsupervised_Dual_ICCV_2017_paper.pdf?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eDualGAN\u003c/a\u003e, and \u003ca href=\"https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eCartoonGAN\u003c/a\u003e trained on the same data.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThey trained a pair of CNNs to assess the GANs’ proficiency at transferring style (trained on small portions of images from each artist) and content (trained on full-size photos). The style classifier scored CycleGAN highest, while the content classifier gave DualGAN the edge. Ganilla ranked highest when style and content scores were averaged.\u003c/li\u003e\u003cli\u003eThe researchers asked 48 people to (a) rate whether each GAN-made illustration looked like the illustrator’s work, (b) describe what they thought the picture showed, and (c) rank generated images in terms of overall appeal. They scored Ganilla’s output highest for mimicking the human illustrators and depicting the source content. However, they rated DualGAN’s output slightly more appealing.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eYes, but:\u003c/strong\u003e Based on examples in the paper, the training illustrations tended to be heavy on stylized human and animal characters, while the photos contain very few characters. We’re curious to see what Ganilla would do with more photos of people and animals.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e GANs are powerful creative tools, and — like printmaking and photography before them — they’re spawning their own adversarial dynamic in the arts. Artists working in traditional media have \u003ca href=\"http://cyberlaw.stanford.edu/blog/2018/05/artificial-intelligence-art-who-owns-copyright-0?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eraised concerns\u003c/a\u003e about GANs being trained to make derivatives of their work. Now, digital artists are \u003ca href=\"https://www.artnome.com/news/2019/3/27/why-is-ai-art-copyright-so-complicated?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eaccusing\u003c/a\u003e traditional artists of creative theft for making paint-on-canvas reproductions of their AI-abetted digital compositions.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e When it comes to art, we favor GANs as a \u003ca href=\"http://www.artbreeder.com/?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ecreative partner\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cem\u003eLearn about human and algorithmic approaches to evaluating generative adversarial networks in GAN Specialization \u003ca href=\"https://www.coursera.org/learn/build-better-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io\"\u003eCourse 2: Build Better GANs\u003c/a\u003e on Coursera. To build your own CycleGAN for style transfer, stay tuned for \u003ca href=\"https://www.coursera.org/learn/apply-generative-adversarial-networks-gans?ref=dl-staging-website.ghost.io\"\u003eCourse 3: Apply GANS\u003c/a\u003e, coming soon to Coursera!\u003c/em\u003e\u003c/p\u003e","comment_id":"60c03212274d5b003b10652c","feature_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen20Shot202020-09-3020at2012.png","featured":false,"visibility":"public","created_at":"2021-06-08T20:14:26.000-07:00","updated_at":"2022-10-06T16:01:54.000-07:00","published_at":"2020-09-30T12:00:00.000-07:00","custom_excerpt":"This special issue of The Batch celebrates the launch of our new Generative Adversarial Networks Specialization! GANs are among the most exciting technologies to emerge from deep learning. These networks learn in a very different way than typical supervised methods...","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"60c03259274d5b003b106534","name":"issue-59","slug":"issue-59","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-59/"},{"id":"60cc328670a082003e13dd6e","name":"Sep 30, 2020","slug":"sep-30-2020","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/sep-30-2020/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-59/","excerpt":"This special issue of The Batch celebrates the launch of our new Generative Adversarial Networks Specialization! GANs are among the most exciting technologies to emerge from deep learning. These networks learn in a very different way than typical supervised methods...","reading_time":16,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"GAN Special Issue! Ian Goodfellow For Real, Detecting Fakes","meta_description":"The Batch - AI News \u0026 Insights: Generative adversarial networks, or GANs, are said to give computers the gift of imagination | A Man, A Plan, A GAN","email_subject":null,"frontmatter":null,"feature_image_alt":"Online panel discussion on “GANs for Good” with Andrew Ng, Anima Anandkumar, Alexei Efros, Ian Goodfellow, and Sharon Zhou","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/06/Screen20Shot202020-09-3020at2012.png","dimensions":{"width":2000,"height":1124}},"banner":{"title":"Data Analytics Professional Certificate","databaseId":36316,"id":"cG9zdDozNjMxNg==","featuredImage":{"node":{"altText":"","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2025/03/Vertical-side-banner-ads-7.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/3XWMC3m","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Databricks C1","date":"2025-06-04T07:12:29","databaseId":36631,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/3HlCuvP","courseName":"DSPy: Build and Optimize Agentic Apps","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)","isOpenInNewTab":true}},{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}}]}}},"__N_SSG":true},"page":"/the-batch/[slug]","query":{"slug":"issue-59"},"buildId":"Qy9Eh4N0MrfE3ddReY9v4","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>