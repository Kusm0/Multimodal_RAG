<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>DeepSeek-R1 Uncensored, QwQ-32B Puts Reasoning in Smaller Model, and more...</title><meta name="description" content="The Batch AI News and Insights: Some people today are discouraging others from learning programming on the grounds AI will automate it." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><link rel="canonical" href="https://www.deeplearning.ai/the-batch/issue-292/"/><meta property="og:type" content="article" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:title" content="DeepSeek-R1 Uncensored, QwQ-32B Puts Reasoning in Smaller Model, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:description" content="The Batch AI News and Insights: Some people today are discouraging others from learning programming on the grounds AI will automate it." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:site_name" content="DeepSeek-R1 Uncensored, QwQ-32B Puts Reasoning in Smaller Model, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:url" content="https://www.deeplearning.ai/the-batch/issue-292/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="article:published_time" content="2025-03-12T13:42:00.000-07:00"/><meta property="article:modified_time" content="2025-03-12T13:50:47.000-07:00"/><meta property="article:tag" content="The Batch Newsletter"/><meta property="article:tag" content="Mar 12, 2025"/><meta property="article:tag" content="issue-292"/><meta property="article:author" content="https://www.facebook.com/DeepLearningAIHQ/"/><meta property="twitter:title" content="DeepSeek-R1 Uncensored, QwQ-32B Puts Reasoning in Smaller Model, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:description" content="The Batch AI News and Insights: Some people today are discouraging others from learning programming on the grounds AI will automate it." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:url" content="https://www.deeplearning.ai/the-batch/issue-292/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:label1" content="Written by"/><meta property="twitter:data1" content="DeepLearning.AI"/><meta property="twitter:label2" content="Filed under"/><meta property="twitter:data2" content="The Batch Newsletter"/><meta property="twitter:card" content="summary_large_image" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:creator" content="@DeepLearningAI"/><meta property="twitter:site" content="https://twitter.com/DeepLearningAI/"/><meta name="twitter:image" content="https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54--1.jpg"/><meta property="og:image" content="https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54--1.jpg"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="676"/><script type="application/ld+json">{"@context":"https://schema.org/","@type":"Article","datePublished":"2025-03-12T13:42:00.000-07:00","dateModified":"2025-03-12T13:50:47.000-07:00","author":{"@type":"Article","name":"DeepLearning.AI","image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","sameAs":"[\"https://www.deeplearning.ai/\", \"https://twitter.com/DeepLearningAI_/\", \"https://www.facebook.com/DeepLearningAIHQ/\"]"},"keywords":"[object Object], [object Object], [object Object]","headline":"DeepSeek-R1 Uncensored, QwQ-32B Puts Reasoning in Smaller Model, and more...","image":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54--1.jpg","width":1200,"height":676},"publisher":{"@type":"Organization","name":"DeepSeek-R1 Uncensored, QwQ-32B Puts Reasoning in Smaller Model, and more...","logo":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","width":60,"height":60}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.deeplearning.ai"},"description":"The Batch AI News and Insights: Some people today are discouraging others from learning programming on the grounds AI will automate it."}</script><meta name="next-head-count" content="31"/><link href="/static/favicons/favicon.ico" rel="shortcut icon"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&amp;display=swap"/><link rel="preload" href="/_next/static/css/54b174af9991199d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/54b174af9991199d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6f396d9f2f265155.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6f396d9f2f265155.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-7690c02358b700f2.js" defer=""></script><script src="/_next/static/chunks/framework-c54e8763846ac33b.js" defer=""></script><script src="/_next/static/chunks/main-9fcc7c9a63f70912.js" defer=""></script><script src="/_next/static/chunks/pages/_app-bece9363d7a6c0bb.js" defer=""></script><script src="/_next/static/chunks/3a17f596-e9670ced3bfab562.js" defer=""></script><script src="/_next/static/chunks/36d2f571-09e32b60ec93a4dc.js" defer=""></script><script src="/_next/static/chunks/5c0b189e-c0f75ca9b44e520b.js" defer=""></script><script src="/_next/static/chunks/5567-47eb911ba5f222d7.js" defer=""></script><script src="/_next/static/chunks/2251-380253169bb2e795.js" defer=""></script><script src="/_next/static/chunks/3864-a4ae56e91852593b.js" defer=""></script><script src="/_next/static/chunks/4965-3585432476b27ea9.js" defer=""></script><script src="/_next/static/chunks/3791-0bbbd4cd0bf31c22.js" defer=""></script><script src="/_next/static/chunks/pages/the-batch/%5Bslug%5D-c03a8bcef188ef99.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_buildManifest.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_ssgManifest.js" defer=""></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&display=swap"/></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5C5VGGJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div data-sentry-component="Layout" data-sentry-source-file="Layout.tsx"><aside id="top-announcement" class="text-neutral-900  " style="color:#FFFFFF;background-color:#05256C;background-image:linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)" data-sentry-component="AnnouncementBanner" data-sentry-source-file="AnnouncementBanner.tsx"><div class="container--boxed py-3 flex items-center justify-between "><div class="flex items-center"><p class="text-sm lg:text-base">✨ New course! Enroll in<!-- --> <a href="https://bit.ly/3HlCuvP" class="underline" target="_blank">DSPy: Build and Optimize Agentic Apps</a></p></div><div class="flex items-center"><button class="bg-transparent p-0 border-none text-xl opacity-70 hover:opacity-100 transition-opacity"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><line x1="15" y1="9" x2="9" y2="15"></line><line x1="9" y1="9" x2="15" y2="15"></line></svg></button></div></div></aside><header id="main-navigation" data-testid="main-navigation-testid" class="static h-[64px] md:h-[100px] top-0 w-full z-[100] flex items-center bg-white transition-all ease-in-out duration-200" data-sentry-component="Header" data-sentry-source-file="index.tsx"><div class="container--boxed flex justify-between items-center"><div class="max-w-[185px] lg:max-w-[235px]"><a href="/the-batch/"><div class="flex items-center"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27300%27%20height=%2792%27/%3e"/></span><img alt="The Batch" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="The Batch" loading="lazy" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcSet="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=384&amp;q=75 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75 2x" src="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75"/></noscript></span></div></a></div><div class="flex items-center"><div class="hidden lg:flex items-center"><nav aria-label="Primary" data-sentry-component="Nav" data-sentry-source-file="Nav.tsx"><ul class="flex items-center p-0 m-0 list-none"><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/courses/"><div class="NavItem_navItemLink__Aq6E5"><span>Explore Courses</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/the-batch/"><div class="NavItem_navItemLink__Aq6E5"><span>AI Newsletter</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/"><div>The Batch</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/letters/"><div>Andrew&#x27;s Letter</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/data-points/"><div>Data Points</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/research/"><div>ML Research</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/"><div>Blog</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/community/"><div class="NavItem_navItemLink__Aq6E5"><span>Community</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="https://community.deeplearning.ai/"><div>Forum</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/events/"><div>Events</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/ambassador/"><div>Ambassadors</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/category/ambassador-spotlight/"><div>Ambassador Spotlight</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/resources/"><div class="NavItem_navItemLink__Aq6E5"><span>Resources</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/about/"><div class="NavItem_navItemLink__Aq6E5"><span>Company</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/about/"><div>About</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/careers/"><div>Careers</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/contact/"><div>Contact</div></a></li></ul></li></ul></nav><a class="btn--primary text-base font-medium whitespace-nowrap shadow h-fit py-3 px-4 lg:ml-2 xl:ml-10" data-sentry-element="Link" data-sentry-source-file="index.tsx" href="https://bit.ly/3RB9T8a">Start Learning</a></div><div class="flex items-center" data-sentry-element="Menu" data-sentry-component="MobileMenu" data-sentry-source-file="MobileMenu.tsx" data-headlessui-state=""><button class="lg:hidden" data-sentry-element="MenuButton" data-sentry-source-file="MobileMenu.tsx" id="headlessui-menu-button-:R1qa6:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-4xl text-slate-600" data-sentry-element="FiMenu" data-sentry-source-file="MobileMenu.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button></div></div></div></header><main><div id="content"><nav aria-label="Secondary" class="h-[66px] bg-white sticky top-[60px] z-40 shadow hidden lg:block" data-sentry-component="SecondaryNav" data-sentry-source-file="index.tsx"><div class="container--boxed h-full w-full flex items-center justify-between "><div class="relative flex h-full"><button class="h-full w-14 items-center justify-center absolute top-0 left-0 z-10 group bg-white bg-opacity-75 hidden"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="15 18 9 12 15 6"></polyline></svg></span></button><ul id="nav-secondary" class="list-none p-0 m-0 h-full flex items-center  overflow-x-scroll relative SecondaryNav_navItems__dok3i"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Weekly Issues</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/letters/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Andrew&#x27;s Letters</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/data-points/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Data Points</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/research/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">ML Research</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/business/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Business</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/science/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Science</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/culture/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Culture</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/hardware/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Hardware</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/ai-careers/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">AI Careers</div></a></li></ul><button class="h-full w-9 items-center justify-center absolute top-0 right-0 group bg-white bg-opacity-75 flex"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span></button></div><div class="flex items-center h-full"><ul class="list-none p-0 m-0 h-full flex items-center mr-5"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/about/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">About</div></a></li></ul><button type="button" class="bg-white border btn--tracking border-solid border-brand text-brand hover:bg-brand hover:text-white transition-colors px-3 py-1 rounded-md mr-4">Subscribe</button><a href="/search/"><div title="Search" class="transition-colors text-slate-400 hover:text-slate-500"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-2xl" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></div></a></div></div></nav><article class="pt-5 pb-16 bg-white lg:pt-16 lg:pb-16" data-sentry-component="ArticleDefault" data-sentry-source-file="ArticleDefault.tsx"><header class="post_layoutGrid__0BDX2"><nav aria-label="Breadcrumb flex items-center" data-sentry-component="Main" data-sentry-source-file="Breadcrumb.tsx"><ul class="list-none flex items-center flex-wrap"><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">The Batch</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">Weekly Issues</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/issue-292/"><div class="text-sm md:text-base text-slate-400 font-normal "><h1 class="capitalize">issue 292</h1></div></a></li></ul></nav><aside class="flex mt-6 lg:hidden"><div class="flex flex-col items-start items-start mr-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm text-sm">Published</div></div><div class="mt-1 text-slate-600 text-base text-sm">Mar 12, 2025</div></div><div class="flex flex-col items-start items-start" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm text-sm">Reading time</div></div><div class="mt-1 text-slate-600 text-base text-sm">13<!-- --> min read</div></div></aside></header><div class="post_layoutGrid__0BDX2 mt-9"><aside style="grid-column:wide-start / main-start" class="flex-col items-end hidden pr-10 lg:flex"><div class="flex flex-col items-start items-end" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm undefined">Published</div></div><div class="mt-1 text-slate-600 text-base undefined"><a href="/the-batch/tag/mar-12-2025/"><div class="inline-flex px-2 py-1 text-sm font-normal transition-colors rounded-md bg-slate-100 hover:bg-slate-200 text-slate-500">Mar 12, 2025</div></a></div></div><div class="flex flex-col items-start items-end mt-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm undefined">Reading time</div></div><div class="mt-1 text-slate-600 text-base undefined">13<!-- --> min read</div></div><div class="flex flex-col items-start items-end mt-6 sticky top-20" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-292/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-292/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-292/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></aside><div class="prose--styled justify-self-center post_postContent__wGZtc">
<!--kg-card-begin: html-->
<div id="elevenlabs-audionative-widget" data-height="90" data-width="100%" data-frameborder="no" data-scrolling="no" data-publicuserid="e20b5cfed36900db239c005920538f20ce435963e95a0a4106d34bdd6bf0e46d" data-playerurl="https://elevenlabs.io/player/index.html" >Loading the <a href="https://elevenlabs.io/text-to-speech?ref=dl-staging-website.ghost.io" target="_blank" rel="noopener">Elevenlabs Text to Speech</a> AudioNative Player...</div><script src="https://elevenlabs.io/player/audioNativeHelper.js" type="text/javascript"></script>
<!--kg-card-end: html-->
<p>Dear friends,</p><p>Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful!</p><p>In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment.</p><p>As coding becomes easier, more people should code, not fewer!</p><p>Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step. (By the way, to learn more about AI assisted coding, check out our video-only short course, “<a href="https://www.deeplearning.ai/short-courses/build-apps-with-windsurfs-ai-coding-agents/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Build Apps with Windsurf’s AI Coding Agents</a>.”)</p><p>I wrote previously that I see tech-savvy people coordinating AI tools to move toward being&nbsp;<a href="https://www.deeplearning.ai/the-batch/how-ai-can-make-you-a-10x-professional/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">10x professionals</a>&nbsp;— individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.</p><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.jpg" class="kg-image" alt="Illustration of a programmer at a computer displaying PyTorch code, while a smiling colleague gives a thumbs-up in approval." loading="lazy" width="1200" height="676" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--54-.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--54-.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.jpg 1200w" sizes="(min-width: 720px) 720px"></figure><p>One question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that.</p><p>When I was working on the course&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MW8Dpc7fxHxW4JZw5k1GrcYmW7jDgJ35t3v6lN35KTCK3qgyTW7Y8-PT6lZ3m2W5Mr7PQ6cbrj8W6wJ6fv80B47KW68DzfQ1lty8mW2CVHsR2ZyXtgN98Z_W0dL_71W5xtdB886dxN8W8-3Rz_63qJ-6W8tpk2p5h8dNNW8tXzLB7r7Fp4N251vz06TQ0SN2ZNYDQfhYTrW8vFw-N4xBwlLW60J8yK4Zd1xcW8ZBCDh5bVdBdW79FMNs5nV3SlW6V_pKp3pV_y9W1YcCVV81sh81W6kCb8X1qpD3nW1b0Bsh6tslhMW1Zd1vd6hmHQCW8dFKYM1hwk-mW5GKhhw7KV-VHW22Lmyq5y2lryW8x1gVs6JSr88W8CkYNJ2czZ4xW3QzLwg4d_7Skf69-Y3K04?ref=dl-staging-website.ghost.io" rel="noopener"><em>Generative AI for Everyone</em></a>&nbsp;and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result.</p><p>Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools continue to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.</p><p>Keep building!</p><p>Andrew</p><hr><h2 id="a-message-from-deeplearningai">A MESSAGE FROM&nbsp;DEEPLEARNING.AI</h2><figure class="kg-card kg-image-card"><a href="https://www.deeplearning.ai/courses/data-analytics?ref=dl-staging-website.ghost.io"><img src="https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png" class="kg-image" alt="Promo banner for: &quot;Data Analytics&quot;" loading="lazy" width="1890" height="1063" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png 1600w, https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png 1890w" sizes="(min-width: 720px) 720px"></a></figure><p>Pre-enroll now for the new Data Analytics Professional Certificate! Gain job-ready skills in data analysis, whether you’re starting a career as a data analyst or enhancing your ability to prepare, analyze, and visualize data in your current role. This program covers both classical statistical techniques and emerging AI-assisted workflows to help you work smarter with data.&nbsp;<a href="https://www.deeplearning.ai/courses/data-analytics?ref=dl-staging-website.ghost.io" rel="noreferrer">Learn more and sign up</a></p><h1 id="news">News</h1><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--60-.png" class="kg-image" alt="QwQ-32B vs DeepSeek-R1 AI model performance benchmark across AIME24, LiveCodeBench, LiveBench, IFEval, and BFCL datasets." loading="lazy" width="1200" height="675" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--60-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--60-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--60-.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="compact-reasoning">Compact Reasoning</h1><p>Most models that have learned to reason via reinforcement learning were huge models. A much smaller model now competes with them.</p><p><strong>What’s new:</strong>&nbsp;Alibaba introduced&nbsp;<a href="https://qwenlm.github.io/blog/qwq-32b/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">QwQ-32B</a>, a large language model that rivals the reasoning prowess of DeepSeek-R1 despite its relatively modest size.</p><ul><li><strong>Input/output:</strong>&nbsp;Text in (up to 131,072 tokens), text out</li><li><strong>Architecture:</strong>&nbsp;Transformer, 32.5 billion total parameter</li><li><strong>Performance:&nbsp;</strong>Outperforms OpenAI o1-mini and DeepSeek-R1 on some bencharks</li><li><strong>Features:</strong>&nbsp;Chain-of-thought reasoning, function calling, multilingual in 29 languages</li><li><strong>Undisclosed:</strong>&nbsp;Output size, training data</li><li><strong>Availability/price:</strong>&nbsp;Free via&nbsp;<a href="https://chat.qwen.ai/?models=Qwen2.5-Plus&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Qwen Chat</a>. Weights are free to&nbsp;<a href="https://huggingface.co/Qwen/QwQ-32B?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">download</a>&nbsp;for noncommercial and commercial uses under an Apache 2.0 license.</li></ul><p><strong>How it works:</strong>&nbsp;QwQ-32B is a version of&nbsp;<a href="https://arxiv.org/abs/2412.15115?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Qwen2.5-32B</a>&nbsp;that was fine-tuned to generate chains of thought using reinforcement learning (RL). Fine-tuning proceeded in two stages.</p><ul><li>The first stage of RL fine-tuning focused on math and coding tasks. The model earned rewards for correct final outcomes (no partial credit for intermediate steps). An accuracy verifier checked its math solutions, while a code-execution server verified generated code for predefined test cases.</li><li>The second stage encouraged the model to follow instructions, use tools, and align its values with human preferences while maintaining math and coding performance, again rewarding final outcomes. In this stage, the model earned rewards from an unspecified reward model and some rule-based verifiers.</li></ul><p><strong>Performance:</strong>&nbsp;On several benchmarks for math, coding, and general problem solving, QwQ-32B outperforms OpenAI o1-mini (parameter count undisclosed) and achieves performance roughly comparable to DeepSeek-R1 (671 billion parameters, 37 billion active at any moment).</p><ul><li>On AIME24 (high-school competition math problems), QwQ-32B achieved 79.5 percent accuracy, well ahead of o1-mini (63.6 percent) but slightly behind DeepSeek-R1 (79.8 percent).</li><li>On LiveCodeBench (code generation, repair, and testing), QwQ-32B achieved 63.4 percent, outperforming o1-mini (53.8 percent) but trailing DeepSeek-R1 (65.9 percent).</li><li>On LiveBench (problem-solving in math, coding, reasoning, and data analysis), QwQ-32B reached 73.1 percent, ahead of o1-mini (59.1 percent) and DeepSeek-R1 (71.6 percent).</li><li>On IFEval (following instructions), QwQ-32B achieved 83.9 percent, outperforming DeepSeek-R1 (83.8 percent) but behind o1-mini (84.8 percent).</li><li>On BFCL (function calling), QwQ-32B achieved 66.4 percent, better than DeepSeek-R1 (60.3 percent), and o1-mini (62.8 percent).</li></ul><p><strong>Behind the news:</strong>&nbsp;DeepSeek’s initial model, DeepSeek-R1-Zero, similarly applied RL to a pretrained model. That effort produced strong reasoning but poor readability (for example, math solutions with correct steps but jumbled explanations). To address this shortcoming, the team&nbsp;<a href="https://arxiv.org/pdf/2501.12948?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">fine-tuned</a>&nbsp;DeepSeek-R1 on long chain-of-thought examples before applying RL. In contrast, QwQ-32B skipped preliminary fine-tuning and applied RL in two stages, first optimizing for correct responses and then for readability.</p><p><strong>Why it matters:</strong>&nbsp;RL can dramatically boost LLMs’ reasoning abilities, but the order in which different behaviors are rewarded matters. Using RL in stages enabled the team to build a 32 billion parameter model — small enough to run locally on a consumer GPU — that rivals a much bigger mixture-of-experts model, bringing powerful reasoning models within reach for more developers. The Qwen team plans to scale its RL approach to larger models, which could improve the next-gen reasoning abilities further while adding greater knowledge.</p><p><strong>We’re thinking:</strong>&nbsp;How far we’ve come since “<a href="https://arxiv.org/pdf/2205.11916?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Let’s think step by step</a>”!</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--61-.png" class="kg-image" alt="Phi-4 Mini multimodal architecture integrating vision, audio, and text with token merging and LoRA-adapted weights for AI processing." loading="lazy" width="1200" height="675" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--61-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--61-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--61-.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="microsoft-tackles-voice-in-text-out">Microsoft Tackles Voice-In, Text-Out</h1><p>Microsoft debuted its first official large language model that responds to spoken input.</p><p><strong>What’s new:</strong>&nbsp;Microsoft released&nbsp;<a href="https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Phi-4-multimodal</a>, an open weights model that processes text, images, and speech simultaneously.</p><ul><li><strong>Input/output:</strong>&nbsp;Text, speech, images in (up to 128,000 tokens); text out (<a href="https://artificialanalysis.ai/models/phi-4-multimodal/providers?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">0.34 seconds to first token, 26 tokens per second</a>)</li><li><strong>Performance:</strong>&nbsp;State of the art in speech transcription. Comparable to similar models in other tasks</li><li><strong>Knowledge cutoff:</strong>&nbsp;June 2024</li><li><strong>Architecture:</strong>&nbsp;transformer, 5.6 billion parameters</li><li><strong>Features:</strong>&nbsp;Text-image-speech processing, multilingual, tool use.</li><li><strong>Undisclosed:&nbsp;</strong>Training datasets, output size</li><li>The company also released&nbsp;<a href="https://huggingface.co/microsoft/Phi-4-mini-instruct?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Phi-4-mini</a>, an open weights 3.8 billion-parameter version of its biggest large language model (LLM),&nbsp;<a href="https://www.deeplearning.ai/the-batch/microsofts-phi-4-blends-synthetic-and-organic-data-to-surpass-larger-models-in-math-and-reasoning-benchmarks/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Phi-4</a>. Phi-4-mini outperforms larger models including Llama 3.1 8B and Ministral-2410 8B on some benchmarks.</li><li><strong>Availability/price:&nbsp;</strong>Weights are free to&nbsp;<a href="https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">download</a>&nbsp;for noncommercial and commercial use under a&nbsp;<a href="https://choosealicense.com/licenses/mit/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">MIT license</a>.</li></ul><p><strong>How it works:</strong>&nbsp;Phi-4-multimodal has six components: Phi-4-mini, vision and speech encoders as well as corresponding projectors (which modify the vision or speech embeddings so the base model can understand them), and two LoRA adapters. The LoRA adapters modify the base weights depending on the input: One adapter modifies them for speech-text problems, and one for vision-text and vision-speech problems.</p><ul><li>The speech encoder is a&nbsp;<a href="https://arxiv.org/abs/2005.08100?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Conformer</a>&nbsp;(which combines convolutional layers with a transformer) and the speech projector is a vanilla neural network. They trained Phi-4-multimodal to convert 2 million hours of speech to text, modifying only the speech encoder and projector. They further trained the system to convert speech to text, translate speech to other languages, summarize speech, and answer questions about speech, modifying only the speech encoder and the speech-text LoRA adapter.</li><li>The vision encoder is based on a pretrained&nbsp;<a href="https://arxiv.org/abs/2303.15343?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">SigLIP-400M</a>&nbsp;vision transformer, and the vision projector is a vanilla neural network. They trained the model to process text and images in four stages: (i) They trained Phi-4-multimodal to caption images, modifying only the vision projector. (ii) They trained the system on 500 billion tokens to caption images, transcribe text in images, and perform other tasks, modifying only the vision encoder and projector. (iii) They trained the system to answer questions about images, charts, tables, and diagrams and to transcribe text in images, modifying the vision encoder, project, and vision-text LoRA adapter. (iv) Finally, they trained the system to compare images and summarize videos, modifying only the vision projector and vision-text LoRA adapter.</li><li>To adapt Phi-4-multimodal for images and speech, they trained the system to generate the text responses to a subset of the text-vision data that had been converted to speech-image using a proprietary text-to-speech engine, modifying only the text-vision LoRA adapter, vision encoder, and vision projector.</li><li>Example inference: Given a question as speech and an image, the audio encoder and projector convert the speech to tokens, and the image encoder and projector convert the image into tokens. Given the tokens, Phi-4-multimodal, which uses the weights of Phi-4-mini modified by the vision-text/vision-speech LoRA adapter, generates a text response.</li></ul><p><strong>Results:</strong>&nbsp;The authors compared Phi-4-multimodal to other multimodal models on text-vision, vision-speech, text-speech tasks.</p><ul><li>Across 11 text-vision benchmarks, Phi-4-multimodal came in fourth out of 11 models. It outperformed Qwen2.5-VL-3B, Claude 3.5 Sonnet, and GPT 4o-mini. It trailed Qwen2.5-VL-7B, GPT-4o, and Gemini-2 Flash.</li><li>Across four&nbsp;<a href="https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o/tree/main/audio_benchmark?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">vision-speech benchmarks</a>, Phi-4-multimodal outperformed by at least 6 percentage points Gemini-2.0-Flash, Gemini-2.0-Flash-Lite-preview, and InternOmni.</li><li>Phi-4-multimodal outperformed all competitors in Microsoft’s report (including Qwen2-audio, Gemini 2.0 Flash, and GPT-4o) at transcribing speech from text&nbsp;<a href="https://arxiv.org/abs/1912.06670?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">in</a>&nbsp;<a href="https://arxiv.org/abs/2205.12446?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">three</a>&nbsp;<a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">datasets</a>. It also achieved competitive performance in speech translation, outperforming its competitors on two of four datasets.</li></ul><p><strong>Behind the news:</strong>&nbsp;This work adds to the growing body of models with voice-in/text-out capability, including the open weights&nbsp;<a href="https://diva-audio.github.io/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">DiVA</a>&nbsp;model developed by a team led by Diyi Yang at Stanford University.</p><p><strong>Why it matters:</strong>&nbsp;The architectural options continue to expand for building neural networks that process text, images, audio, and various combinations. While some teams maintain separate models for separate data modalities, like&nbsp;<a href="https://qwenlm.github.io/blog/qwen2.5-max/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Qwen2.5</a>&nbsp;(for text) and&nbsp;<a href="https://qwenlm.github.io/blog/qwen2.5-vl/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Qwen2.5-VL</a>) (for vision-language tasks), others are experimenting with mixture-of-expert models like&nbsp;<a href="https://arxiv.org/abs/2412.19437?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">DeepSeek-V3</a>. Phi-4-multimodal shows that Mixture-of-LoRAs is an effective approach for processing multimodal data — and gives developers a couple of new open models to play with.</p><p><strong>We’re thinking:</strong>&nbsp;Output guardrails have been built to ensure appropriateness of text output, but this is difficult to apply to a voice-in/voice-out architecture. (Some teams have worked on guardrails that screen audio output directly, but the technology is still early.) For voice-based applications, a voice-in/text-out model can generate a candidate output without a separate, explicit speech-to-text step, and it accommodates text-based guardrails before it decides whether or not to read the output to the user.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--55-.jpg" class="kg-image" alt="Gavel striking a neural network, symbolizing legal decisions impacting AI and machine learning technologies." loading="lazy" width="1200" height="676" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--55-.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--55-.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--55-.jpg 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="judge-upholds-copyright-in-ai-training-case">Judge Upholds Copyright in AI Training Case</h1><p>A United States court delivered a major ruling that begins to answer the question whether, and under what conditions, training an AI system on copyrighted material is considered fair use that doesn’t require permission.</p><p><strong>What’s new:</strong>&nbsp;A U.S. Circuit judge&nbsp;<a href="https://www.reuters.com/legal/thomson-reuters-wins-ai-copyright-fair-use-ruling-against-one-time-competitor-2025-02-11/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">ruled</a>&nbsp;on a claim by the legal publisher Thomson Reuters that Ross Intelligence, an AI-powered legal research service, could not claim that training its AI system on materials owned by Thomson Reuters was a so-called “fair use.” Training the system did not qualify as fair use, he decided, because its output competed with Thomson Reuters’ publications.</p><p><strong>How it works:</strong>&nbsp;Thomson Reuters had&nbsp;<a href="https://fingfx.thomsonreuters.com/gfx/legaldocs/xmvjbznbkvr/THOMSON%20REUTERS%20ROSS%20LAWSUIT%20fair%20use.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">sued</a>&nbsp;Ross Intelligence after the defendant trained an AI model using 2,243 works produced by Thomson Reuters without the latter’s permission. This ruling reversed an earlier decision in 2023, when the same judge had allowed Ross Intelligence’s fair-use defense to proceed to trial. In the new ruling, he found that Ross Intelligence’s use failed to meet the definition of fair use in key respects. (A jury trial is scheduled to determine whether Thomson Reuters' copyright was in effect at the time of the infringement and other aspects of the case.)</p><ul><li>Ross Intelligence’s AI-powered service competed directly with Thomson Reuters, potentially undermining its market by offering a derivative product without licensing its works. Use in a competing commercial product undermines a key factor in fair use.</li><li>The judge found that Ross Intelligence’s use was commercial and not transformative, meaning it did not significantly alter or add new meaning to Thomson Reuters’ works — another key factor in fair use. Instead, it simply repackaged the works.</li><li>The ruling acknowledged that Thomson Reuters’ works were not highly creative but noted that they possessed sufficient originality for copyright protection due to the editorial creativity and judgment involved in producing it.</li><li>Although Ross Intelligence used only small portions of Thomson Reuters’ works, this did not weigh strongly in favor of fair use because those portions represented the most important summaries produced by Ross Intelligence.</li></ul><p><strong>Behind the news:</strong>&nbsp;The ruling comes amid a&nbsp;<a href="https://www.deeplearning.ai/the-batch/artists-and-writers-sue-big-tech-companies-over-copyright-infringement/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">wave</a>&nbsp;of lawsuits over AI training and copyright in several countries. Many of these cases are in progress, but courts have weighed in on some.</p><ul><li><em>The New York Times</em>&nbsp;is&nbsp;<a href="https://www.deeplearning.ai/the-batch/the-new-york-times-versus-openai-and-microsoft/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">suing</a>&nbsp;OpenAI and Microsoft, arguing that their models generate output that competes with its journalism.</li><li>Condé Nast, McClatchy, and other major publishers recently&nbsp;<a href="https://www.wsj.com/business/media/conde-nast-mcclatchy-and-other-publishers-accuse-ai-firm-cohere-of-copyright-violations-5f5ceaff?st=xRCgwB&reflink=desktopwebshare_permalink&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">filed</a>&nbsp;a lawsuit against Cohere, accusing it of using copyrighted news articles to train its AI models.</li><li>Sony, UMG, and Warner Music&nbsp;<a href="https://www.deeplearning.ai/the-batch/sony-music-accuses-ai-developers-of-copyright-violations/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">filed</a>&nbsp;lawsuits against AI music companies including Suno and Udio for allegedly using copyrighted recordings without permission.</li><li>A judge&nbsp;<a href="https://www.deeplearning.ai/the-batch/judge-dismisses-key-arguments-in-ai-copyright-lawsuit-against-github-microsoft-and-openai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">dismissed</a>&nbsp;key arguments brought by software developers who claimed that GitHub Copilot was trained on software they created in violation of open source licenses. The judge ruled in favor of Microsoft and OpenAI.</li><li>In Germany, the publisher of the LAION dataset&nbsp;<a href="https://www.deeplearning.ai/the-batch/laion-wins-legal-case-in-germany/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">won</a>&nbsp;a case in which a court ruled that training AI models on publicly available images did not violate copyrights.</li></ul><p><strong>Why it matters:</strong>&nbsp;The question of whether training (or copying data to train) AI systems is a fair use of copyrighted works hangs over the AI industry, from academic research to commercial projects. In the wake of this ruling, courts may be more likely to reject a fair-use defense when AI companies train models on copyrighted material to create output that overlaps with or replaces traditional media, as&nbsp;<em>The New York Times</em>&nbsp;alleges in its lawsuit against OpenAI. However, the ruling leaves room for fair use with respect to models whose output doesn’t compete directly with copyrighted works.</p><p><strong>We’re thinking:</strong>&nbsp;Current copyright laws weren’t designed with AI in mind, and rulings like this one fill in the gaps case by case.&nbsp;<a href="https://www.deeplearning.ai/the-batch/time-to-update-copyright-for-generative-ai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">Clarifying copyright</a>&nbsp;for the era of generative AI could help our field move forward faster.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--62-.png" class="kg-image" alt="AI model performance benchmark comparing R1 1776 and DeepSeek-R1 across MMLU, DROP, MATH-500, and AIME 2024 tests." loading="lazy" width="1200" height="676" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--62-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--62-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--62-.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="deepseek-r1-uncensored">DeepSeek-R1 Uncensored</h1><p>Large language models built by developers in China may, in some applications, be less useful outside that country because they avoid topics its government deems politically sensitive. A developer fine-tuned DeepSeek-R1 to widen its scope without degrading its overall performance.</p><p><strong>What’s new:</strong>&nbsp;Perplexity released&nbsp;<a href="https://www.perplexity.ai/hub/blog/open-sourcing-r1-1776?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">R1 1776</a>, a version of&nbsp;<a href="https://www.deeplearning.ai/the-batch/deepseek-r1-an-affordable-rival-to-openais-o1/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">DeepSeek-R1</a>&nbsp;that responds more freely than the original. The model weights are available to&nbsp;<a href="https://huggingface.co/perplexity-ai/r1-1776?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">download</a>&nbsp;under a commercially permissive MIT&nbsp;<a href="https://choosealicense.com/licenses/mit/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">license</a>.</p><p><strong>How it works:&nbsp;</strong>The team modified DeepSeek-R1’s knowledge of certain topics by fine-tuning it on curated question-answer pairs.</p><ul><li>Human experts identified around 300 topics that are censored in China.</li><li>The authors developed a multilingual classifier that spots text related to these topics.</li><li>They identified 40,000 prompts that the classifier classified as sensitive with high confidence. They discarded those that contained personally identifiable information.</li><li>For each prompt, they produced factual, chain-of-thought responses that mirrored DeepSeek-R1's typical reasoning processes.</li><li>They fine-tuned DeepSeek-R1 on the resulting prompt-response pairs.</li></ul><p><strong>Results:</strong>&nbsp;The fine-tuned model responded to politically charged prompts factually without degrading its ability to generate high-quality output.</p><ul><li>The authors fed their model 1,000 diverse prompts that covered frequently censored topics. An unspecified combination of human and AI judges rated the models' responses according to the degree to which they are (i) evasive and (ii) censored outright.</li><li>100 percent of the fine-tuned model’s responses were rated uncensored, whereas the original version censored around 85 percent of sensitive queries. By comparison, DeepSeek-V3 censored roughly 73 percent, Claude-3.5-Sonnet around 5 percent, o3-mini about 1 percent, and GPT-4o 0 percent.</li><li>Evaluated on four language and math benchmarks (MMLU, DROP, MATH-500, and AIME 2024) and unspecified internal benchmarks, the fine-tuned and original models performed nearly identically. Their scores differed by a few tenths of a percent except on AIME 2024 (competitive high-school math problems), where the fine-tuned model achieved 79.8 percent compared to the original’s 80.96 percent.</li></ul><p><strong>Behind the news:</strong>&nbsp;Among&nbsp;<a href="https://carnegieendowment.org/research/2023/07/chinas-ai-regulations-and-how-they-get-made?lang=en&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT">the first countries to regulate AI</a>, China&nbsp;<a href="https://www.chinalawtranslate.com/en/comparison-chart-of-current-vs-draft-rules-for-generative-ai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">requires</a>&nbsp;AI developers to build models that uphold “Core Socialist Values” and produce true and reliable output. When these objectives&nbsp;<a href="https://www.chinatalk.media/p/censorships-impact-on-chinas-chatbots?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT" rel="noopener">conflict</a>, the political goal tends to dominate. While large language models built by developers in China typically avoid contentious topics, the newer DeepSeek models enforce this more strictly than older models like Qwen and Yi, using methods akin to Western measures for aligning output, like Reinforcement Learning from Human Feedback and&nbsp;<a href="https://www.chinatalk.media/p/censorships-impact-on-chinas-chatbots?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT">keyword filters</a>.</p><p><strong>Why it matters:&nbsp;</strong>AI models tend to reflect their developers’ values and legal constraints. Perplexity’s targeted fine-tuning approach addresses this barrier to international adoption of open-source models.</p><p><strong>We’re thinking:</strong>&nbsp;As models with open weights are adopted by the global community, they become a source of soft power for their developers, since they tend to reflect their developers’ values. This work reflects a positive effort to customize a model to reflect the user’s values instead — though how many developers will seek out a fine-tuned version rather than the original remains to be seen.</p></div><aside style="grid-column:main-end / wide-end" class="flex-col hidden pl-10 lg:flex"><div class="relative shadow rounded-lg overflow-hidden hover:shadow-sm transition-shadow" data-sentry-component="Advertisement" data-sentry-source-file="Advertisement.tsx"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:200%"></span><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" class="rounded-lg" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" loading="lazy" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="rounded-lg" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F05%2Fcgpt-2.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F05%2Fcgpt-2.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F05%2Fcgpt-2.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F05%2Fcgpt-2.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F05%2Fcgpt-2.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F05%2Fcgpt-2.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F05%2Fcgpt-2.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F05%2Fcgpt-2.png&amp;w=3840&amp;q=75 3840w" src="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F05%2Fcgpt-2.png&amp;w=3840&amp;q=75"/></noscript></span><a href="https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/"><div class="absolute inset-0" data-gtm-event-title="ChatGPT Prompt Engineering for Developers"></div></a></div></aside><footer class="mt-8 lg:hidden"><div class="flex flex-col items-start items-start " data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-292/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-292/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-292/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></footer></div></article><div class="py-16 bg-slate-50"><section id="subscribe" data-sentry-component="CtaNewsletter" data-sentry-source-file="CtaNewsletter.tsx"><div class="container--boxed relative"><div class="text-center"><h2 class="text--l2 text-slate-900">Subscribe to The Batch</h2><p class="text-base lg:text-lg text-slate-500 mt-3 max-w-md mx-auto">Stay updated with weekly AI News and Insights delivered to your inbox</p></div><div class="flex flex-col items-center mt-9"><div><div id="reactHubspotForm21" style="display:none"></div><div class="flex items-center justify-center"><svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-brand-teal" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" data-sentry-element="svg" data-sentry-component="Spinner" data-sentry-source-file="Spinner.tsx"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4" data-sentry-element="circle" data-sentry-source-file="Spinner.tsx"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z" data-sentry-element="path" data-sentry-source-file="Spinner.tsx"></path></svg></div></div></div></div></section></div></div></main><footer class="py-16 bg-brand-teal-900" data-sentry-component="Footer" data-sentry-source-file="index.tsx"><div class="flex flex-col items-center justify-center text-center container--boxed"><div class="max-w-[220px] flex items-center justify-center text-white"><svg viewBox="0 0 272 34" fill="none" xmlns="http://www.w3.org/2000/svg" width="272" height="34" aria-label="DeepLearning.AI" data-sentry-element="svg" data-sentry-component="DLAILogo" data-sentry-source-file="DLAILogo.tsx"><g fill="currentColor" data-sentry-element="g" data-sentry-source-file="DLAILogo.tsx"><path d="M56.775 16.108c0 6.206-4.252 10.005-10.747 10.005H39.42V5.961h6.608c6.495 0 10.747 3.924 10.747 10.147zm-10.747 7.306c4.778 0 7.337-2.724 7.337-7.306 0-4.581-2.56-7.452-7.337-7.452H42.73v14.758h3.298z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M66.97 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.259-8.226 8.006-8.226c4.571 0 7.804 3.159 7.804 7.856.007.535-.027 1.07-.103 1.6H62.412c.233 2.638 2.123 4.232 4.57 4.232 2.038 0 3.173-.988 3.786-2.234h3.582c-.915 2.802-3.448 5.032-7.38 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.484-4.03-2.24 0-4.044 1.525-4.394 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M84.938 26.371c-4.601 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.792 9.792 0 01-.116 1.6H80.367c.233 2.638 2.128 4.232 4.57 4.232 2.042 0 3.177-.988 3.786-2.234h3.582c-.902 2.802-3.435 5.032-7.367 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M104.917 9.885c4.221 0 7.541 3.245 7.541 8.166 0 4.92-3.32 8.32-7.541 8.32a7.277 7.277 0 01-3.095-.664 7.248 7.248 0 01-2.516-1.914v9.915h-3.319v-23.57h3.32v2.347a7.004 7.004 0 015.61-2.6zm-.729 2.871c-2.473 0-4.86 1.943-4.86 5.364 0 3.42 2.387 5.39 4.86 5.39 2.473 0 4.894-2.02 4.894-5.46 0-3.437-2.391-5.303-4.894-5.303v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M119.074 5.961v17.484h6.841v2.668h-10.16V5.961h3.319z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M135.584 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.927 9.927 0 01-.116 1.625h-12.258c.233 2.639 2.128 4.233 4.571 4.233 2.041 0 3.176-.988 3.785-2.235h3.582c-.902 2.777-3.435 5.007-7.367 5.007zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M153.116 9.885a6.864 6.864 0 013.088.633 6.839 6.839 0 012.475 1.946v-2.325h3.349v15.974h-3.349v-2.38a6.906 6.906 0 01-5.611 2.638c-4.165 0-7.514-3.39-7.514-8.32s3.353-8.166 7.562-8.166zm.699 2.87c-2.473 0-4.86 1.853-4.86 5.304 0 3.451 2.387 5.45 4.86 5.45 2.473 0 4.864-1.938 4.864-5.39 0-3.45-2.361-5.372-4.864-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M169.72 26.114h-3.319V10.139h3.319v2.325c.928-1.595 2.534-2.579 4.804-2.579v3.438h-.863c-2.447 0-3.958 1.014-3.958 4.405l.017 8.386z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M188.966 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.044 1.504-4.044 4.435v8.922h-3.319V10.14h3.319v1.826a6.16 6.16 0 014.774-2.08c3.755 0 6.582 2.347 6.582 6.812v9.416h-3.293v-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M196.049 5.905a2.105 2.105 0 011.309-1.931 2.117 2.117 0 012.294.458 2.102 2.102 0 01-1.48 3.588 2.109 2.109 0 01-1.509-.612 2.08 2.08 0 01-.614-1.503zm.431 4.234h3.319v15.974h-3.319V10.14z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M215.558 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.048 1.504-4.048 4.435v8.922h-3.337V10.14h3.319v1.826a6.192 6.192 0 014.796-2.08c3.755 0 6.56 2.338 6.56 6.803v9.425h-3.289l.018-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M229.538 9.885c2.62 0 4.571 1.216 5.559 2.579v-2.325h3.349V26.37c0 4.35-2.823 7.629-7.829 7.629-4.282 0-7.454-2.119-7.864-5.656h3.289c.496 1.655 2.274 2.785 4.575 2.785 2.559 0 4.48-1.569 4.48-4.758v-2.664a6.903 6.903 0 01-5.559 2.665c-4.222 0-7.571-3.392-7.571-8.321 0-4.93 3.337-8.166 7.571-8.166zm.699 2.87c-2.478 0-4.864 1.853-4.864 5.304 0 3.452 2.386 5.45 4.864 5.45 2.477 0 4.86-1.938 4.86-5.39 0-3.45-2.357-5.372-4.86-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M242.702 26.316a2.134 2.134 0 01-1.987-1.287 2.114 2.114 0 011.53-2.908 2.138 2.138 0 012.194.896c.235.349.361.76.361 1.18a2.096 2.096 0 01-1.289 1.956 2.11 2.11 0 01-.809.163z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M260.584 21.996h-8.477l-1.455 4.117h-3.453l7.251-20.2h3.846l7.247 20.2h-3.492l-1.467-4.117zM256.38 9.932l-3.341 9.365h6.612l-3.271-9.365z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M268.681 5.961H272v20.152h-3.319V5.961z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218zm.126-1.59c-3.734 0-6.76-3.207-6.76-7.16 0-3.954 3.018-7.16 6.75-7.16 3.734 0 6.76 3.206 6.76 7.16s-3.021 7.16-6.76 7.16h.01zm-.126-6.28c.729 0 1.44-.214 2.046-.617a3.67 3.67 0 001.356-1.646 3.652 3.652 0 00-.798-3.995 3.687 3.687 0 00-4.012-.794 3.679 3.679 0 00-1.653 1.35 3.655 3.655 0 00-.62 2.037c.002.971.39 1.902 1.08 2.59a3.698 3.698 0 002.601 1.076z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path></g></svg></div><nav class="mt-6 md:mt-10"><ul class="flex flex-wrap justify-center space-x-8 gap-x-8"><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/courses/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Courses</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/the-batch/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">The Batch</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/community/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Community</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/careers/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Careers</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/about/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">About</div></a></li></ul></nav><div class="flex mt-12"><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.facebook.com/1027125564106325" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsFacebook" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.instagram.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsInstagram" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.9 3.9 0 0 0-1.417.923A3.9 3.9 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.9 3.9 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.9 3.9 0 0 0-.923-1.417A3.9 3.9 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599s.453.546.598.92c.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.5 2.5 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.5 2.5 0 0 1-.92-.598 2.5 2.5 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233s.008-2.388.046-3.231c.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92s.546-.453.92-.598c.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92m-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217m0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://twitter.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsTwitter" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.linkedin.com/company/18246783" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsLinkedin" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.youtube.com/c/Deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsYoutube" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.01 2.01 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.01 2.01 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31 31 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.01 2.01 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A100 100 0 0 1 7.858 2zM6.4 5.209v4.818l4.157-2.408z"></path></svg></a></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"donations_enabled":false,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.121","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-292","id":"67d1ef219397160001d3f1de","uuid":"73cf9ab6-85ec-4eba-9bb1-f9b6e16c81ac","title":"DeepSeek-R1 Uncensored, QwQ-32B Puts Reasoning in Smaller Model, Phi-4-multimodal Takes Spoken Input, Training AI May Not Be Fair Use","html":"\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv id=\"elevenlabs-audionative-widget\" data-height=\"90\" data-width=\"100%\" data-frameborder=\"no\" data-scrolling=\"no\" data-publicuserid=\"e20b5cfed36900db239c005920538f20ce435963e95a0a4106d34bdd6bf0e46d\" data-playerurl=\"https://elevenlabs.io/player/index.html\" \u003eLoading the \u003ca href=\"https://elevenlabs.io/text-to-speech?ref=dl-staging-website.ghost.io\" target=\"_blank\" rel=\"noopener\"\u003eElevenlabs Text to Speech\u003c/a\u003e AudioNative Player...\u003c/div\u003e\u003cscript src=\"https://elevenlabs.io/player/audioNativeHelper.js\" type=\"text/javascript\"\u003e\u003c/script\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eDear friends,\u003c/p\u003e\u003cp\u003eSome people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful!\u003c/p\u003e\u003cp\u003eIn the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment.\u003c/p\u003e\u003cp\u003eAs coding becomes easier, more people should code, not fewer!\u003c/p\u003e\u003cp\u003eOver the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step. (By the way, to learn more about AI assisted coding, check out our video-only short course, “\u003ca href=\"https://www.deeplearning.ai/short-courses/build-apps-with-windsurfs-ai-coding-agents/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eBuild Apps with Windsurf’s AI Coding Agents\u003c/a\u003e.”)\u003c/p\u003e\u003cp\u003eI wrote previously that I see tech-savvy people coordinating AI tools to move toward being\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/how-ai-can-make-you-a-10x-professional/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003e10x professionals\u003c/a\u003e\u0026nbsp;— individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.jpg\" class=\"kg-image\" alt=\"Illustration of a programmer at a computer displaying PyTorch code, while a smiling colleague gives a thumbs-up in approval.\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--54-.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--54-.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54-.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eOne question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that.\u003c/p\u003e\u003cp\u003eWhen I was working on the course\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/MW8Dpc7fxHxW4JZw5k1GrcYmW7jDgJ35t3v6lN35KTCK3qgyTW7Y8-PT6lZ3m2W5Mr7PQ6cbrj8W6wJ6fv80B47KW68DzfQ1lty8mW2CVHsR2ZyXtgN98Z_W0dL_71W5xtdB886dxN8W8-3Rz_63qJ-6W8tpk2p5h8dNNW8tXzLB7r7Fp4N251vz06TQ0SN2ZNYDQfhYTrW8vFw-N4xBwlLW60J8yK4Zd1xcW8ZBCDh5bVdBdW79FMNs5nV3SlW6V_pKp3pV_y9W1YcCVV81sh81W6kCb8X1qpD3nW1b0Bsh6tslhMW1Zd1vd6hmHQCW8dFKYM1hwk-mW5GKhhw7KV-VHW22Lmyq5y2lryW8x1gVs6JSr88W8CkYNJ2czZ4xW3QzLwg4d_7Skf69-Y3K04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003e\u003cem\u003eGenerative AI for Everyone\u003c/em\u003e\u003c/a\u003e\u0026nbsp;and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result.\u003c/p\u003e\u003cp\u003eSimilarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools continue to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.\u003c/p\u003e\u003cp\u003eKeep building!\u003c/p\u003e\u003cp\u003eAndrew\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"a-message-from-deeplearningai\"\u003eA MESSAGE FROM\u0026nbsp;DEEPLEARNING.AI\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003ca href=\"https://www.deeplearning.ai/courses/data-analytics?ref=dl-staging-website.ghost.io\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png\" class=\"kg-image\" alt=\"Promo banner for: \u0026quot;Data Analytics\u0026quot;\" loading=\"lazy\" width=\"1890\" height=\"1063\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png 1600w, https://dl-staging-website.ghost.io/content/images/2025/03/The-Batch-ads-and-exclusive-banners---2025-03-11T092327.321.png 1890w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/a\u003e\u003c/figure\u003e\u003cp\u003ePre-enroll now for the new Data Analytics Professional Certificate! Gain job-ready skills in data analysis, whether you’re starting a career as a data analyst or enhancing your ability to prepare, analyze, and visualize data in your current role. This program covers both classical statistical techniques and emerging AI-assisted workflows to help you work smarter with data.\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/courses/data-analytics?ref=dl-staging-website.ghost.io\" rel=\"noreferrer\"\u003eLearn more and sign up\u003c/a\u003e\u003c/p\u003e\u003ch1 id=\"news\"\u003eNews\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--60-.png\" class=\"kg-image\" alt=\"QwQ-32B vs DeepSeek-R1 AI model performance benchmark across AIME24, LiveCodeBench, LiveBench, IFEval, and BFCL datasets.\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--60-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--60-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--60-.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"compact-reasoning\"\u003eCompact Reasoning\u003c/h1\u003e\u003cp\u003eMost models that have learned to reason via reinforcement learning were huge models. A much smaller model now competes with them.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Alibaba introduced\u0026nbsp;\u003ca href=\"https://qwenlm.github.io/blog/qwq-32b/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eQwQ-32B\u003c/a\u003e, a large language model that rivals the reasoning prowess of DeepSeek-R1 despite its relatively modest size.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInput/output:\u003c/strong\u003e\u0026nbsp;Text in (up to 131,072 tokens), text out\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArchitecture:\u003c/strong\u003e\u0026nbsp;Transformer, 32.5 billion total parameter\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePerformance:\u0026nbsp;\u003c/strong\u003eOutperforms OpenAI o1-mini and DeepSeek-R1 on some bencharks\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFeatures:\u003c/strong\u003e\u0026nbsp;Chain-of-thought reasoning, function calling, multilingual in 29 languages\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eUndisclosed:\u003c/strong\u003e\u0026nbsp;Output size, training data\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAvailability/price:\u003c/strong\u003e\u0026nbsp;Free via\u0026nbsp;\u003ca href=\"https://chat.qwen.ai/?models=Qwen2.5-Plus\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eQwen Chat\u003c/a\u003e. Weights are free to\u0026nbsp;\u003ca href=\"https://huggingface.co/Qwen/QwQ-32B?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003edownload\u003c/a\u003e\u0026nbsp;for noncommercial and commercial uses under an Apache 2.0 license.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;QwQ-32B is a version of\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2412.15115?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eQwen2.5-32B\u003c/a\u003e\u0026nbsp;that was fine-tuned to generate chains of thought using reinforcement learning (RL). Fine-tuning proceeded in two stages.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe first stage of RL fine-tuning focused on math and coding tasks. The model earned rewards for correct final outcomes (no partial credit for intermediate steps). An accuracy verifier checked its math solutions, while a code-execution server verified generated code for predefined test cases.\u003c/li\u003e\u003cli\u003eThe second stage encouraged the model to follow instructions, use tools, and align its values with human preferences while maintaining math and coding performance, again rewarding final outcomes. In this stage, the model earned rewards from an unspecified reward model and some rule-based verifiers.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003ePerformance:\u003c/strong\u003e\u0026nbsp;On several benchmarks for math, coding, and general problem solving, QwQ-32B outperforms OpenAI o1-mini (parameter count undisclosed) and achieves performance roughly comparable to DeepSeek-R1 (671 billion parameters, 37 billion active at any moment).\u003c/p\u003e\u003cul\u003e\u003cli\u003eOn AIME24 (high-school competition math problems), QwQ-32B achieved 79.5 percent accuracy, well ahead of o1-mini (63.6 percent) but slightly behind DeepSeek-R1 (79.8 percent).\u003c/li\u003e\u003cli\u003eOn LiveCodeBench (code generation, repair, and testing), QwQ-32B achieved 63.4 percent, outperforming o1-mini (53.8 percent) but trailing DeepSeek-R1 (65.9 percent).\u003c/li\u003e\u003cli\u003eOn LiveBench (problem-solving in math, coding, reasoning, and data analysis), QwQ-32B reached 73.1 percent, ahead of o1-mini (59.1 percent) and DeepSeek-R1 (71.6 percent).\u003c/li\u003e\u003cli\u003eOn IFEval (following instructions), QwQ-32B achieved 83.9 percent, outperforming DeepSeek-R1 (83.8 percent) but behind o1-mini (84.8 percent).\u003c/li\u003e\u003cli\u003eOn BFCL (function calling), QwQ-32B achieved 66.4 percent, better than DeepSeek-R1 (60.3 percent), and o1-mini (62.8 percent).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;DeepSeek’s initial model, DeepSeek-R1-Zero, similarly applied RL to a pretrained model. That effort produced strong reasoning but poor readability (for example, math solutions with correct steps but jumbled explanations). To address this shortcoming, the team\u0026nbsp;\u003ca href=\"https://arxiv.org/pdf/2501.12948?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003efine-tuned\u003c/a\u003e\u0026nbsp;DeepSeek-R1 on long chain-of-thought examples before applying RL. In contrast, QwQ-32B skipped preliminary fine-tuning and applied RL in two stages, first optimizing for correct responses and then for readability.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;RL can dramatically boost LLMs’ reasoning abilities, but the order in which different behaviors are rewarded matters. Using RL in stages enabled the team to build a 32 billion parameter model — small enough to run locally on a consumer GPU — that rivals a much bigger mixture-of-experts model, bringing powerful reasoning models within reach for more developers. The Qwen team plans to scale its RL approach to larger models, which could improve the next-gen reasoning abilities further while adding greater knowledge.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;How far we’ve come since “\u003ca href=\"https://arxiv.org/pdf/2205.11916?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eLet’s think step by step\u003c/a\u003e”!\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--61-.png\" class=\"kg-image\" alt=\"Phi-4 Mini multimodal architecture integrating vision, audio, and text with token merging and LoRA-adapted weights for AI processing.\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--61-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--61-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--61-.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"microsoft-tackles-voice-in-text-out\"\u003eMicrosoft Tackles Voice-In, Text-Out\u003c/h1\u003e\u003cp\u003eMicrosoft debuted its first official large language model that responds to spoken input.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Microsoft released\u0026nbsp;\u003ca href=\"https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003ePhi-4-multimodal\u003c/a\u003e, an open weights model that processes text, images, and speech simultaneously.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInput/output:\u003c/strong\u003e\u0026nbsp;Text, speech, images in (up to 128,000 tokens); text out (\u003ca href=\"https://artificialanalysis.ai/models/phi-4-multimodal/providers?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003e0.34 seconds to first token, 26 tokens per second\u003c/a\u003e)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePerformance:\u003c/strong\u003e\u0026nbsp;State of the art in speech transcription. Comparable to similar models in other tasks\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eKnowledge cutoff:\u003c/strong\u003e\u0026nbsp;June 2024\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eArchitecture:\u003c/strong\u003e\u0026nbsp;transformer, 5.6 billion parameters\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFeatures:\u003c/strong\u003e\u0026nbsp;Text-image-speech processing, multilingual, tool use.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eUndisclosed:\u0026nbsp;\u003c/strong\u003eTraining datasets, output size\u003c/li\u003e\u003cli\u003eThe company also released\u0026nbsp;\u003ca href=\"https://huggingface.co/microsoft/Phi-4-mini-instruct?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003ePhi-4-mini\u003c/a\u003e, an open weights 3.8 billion-parameter version of its biggest large language model (LLM),\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/microsofts-phi-4-blends-synthetic-and-organic-data-to-surpass-larger-models-in-math-and-reasoning-benchmarks/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003ePhi-4\u003c/a\u003e. Phi-4-mini outperforms larger models including Llama 3.1 8B and Ministral-2410 8B on some benchmarks.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAvailability/price:\u0026nbsp;\u003c/strong\u003eWeights are free to\u0026nbsp;\u003ca href=\"https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003edownload\u003c/a\u003e\u0026nbsp;for noncommercial and commercial use under a\u0026nbsp;\u003ca href=\"https://choosealicense.com/licenses/mit/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eMIT license\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;Phi-4-multimodal has six components: Phi-4-mini, vision and speech encoders as well as corresponding projectors (which modify the vision or speech embeddings so the base model can understand them), and two LoRA adapters. The LoRA adapters modify the base weights depending on the input: One adapter modifies them for speech-text problems, and one for vision-text and vision-speech problems.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe speech encoder is a\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2005.08100?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eConformer\u003c/a\u003e\u0026nbsp;(which combines convolutional layers with a transformer) and the speech projector is a vanilla neural network. They trained Phi-4-multimodal to convert 2 million hours of speech to text, modifying only the speech encoder and projector. They further trained the system to convert speech to text, translate speech to other languages, summarize speech, and answer questions about speech, modifying only the speech encoder and the speech-text LoRA adapter.\u003c/li\u003e\u003cli\u003eThe vision encoder is based on a pretrained\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2303.15343?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eSigLIP-400M\u003c/a\u003e\u0026nbsp;vision transformer, and the vision projector is a vanilla neural network. They trained the model to process text and images in four stages: (i) They trained Phi-4-multimodal to caption images, modifying only the vision projector. (ii) They trained the system on 500 billion tokens to caption images, transcribe text in images, and perform other tasks, modifying only the vision encoder and projector. (iii) They trained the system to answer questions about images, charts, tables, and diagrams and to transcribe text in images, modifying the vision encoder, project, and vision-text LoRA adapter. (iv) Finally, they trained the system to compare images and summarize videos, modifying only the vision projector and vision-text LoRA adapter.\u003c/li\u003e\u003cli\u003eTo adapt Phi-4-multimodal for images and speech, they trained the system to generate the text responses to a subset of the text-vision data that had been converted to speech-image using a proprietary text-to-speech engine, modifying only the text-vision LoRA adapter, vision encoder, and vision projector.\u003c/li\u003e\u003cli\u003eExample inference: Given a question as speech and an image, the audio encoder and projector convert the speech to tokens, and the image encoder and projector convert the image into tokens. Given the tokens, Phi-4-multimodal, which uses the weights of Phi-4-mini modified by the vision-text/vision-speech LoRA adapter, generates a text response.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;The authors compared Phi-4-multimodal to other multimodal models on text-vision, vision-speech, text-speech tasks.\u003c/p\u003e\u003cul\u003e\u003cli\u003eAcross 11 text-vision benchmarks, Phi-4-multimodal came in fourth out of 11 models. It outperformed Qwen2.5-VL-3B, Claude 3.5 Sonnet, and GPT 4o-mini. It trailed Qwen2.5-VL-7B, GPT-4o, and Gemini-2 Flash.\u003c/li\u003e\u003cli\u003eAcross four\u0026nbsp;\u003ca href=\"https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o/tree/main/audio_benchmark?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003evision-speech benchmarks\u003c/a\u003e, Phi-4-multimodal outperformed by at least 6 percentage points Gemini-2.0-Flash, Gemini-2.0-Flash-Lite-preview, and InternOmni.\u003c/li\u003e\u003cli\u003ePhi-4-multimodal outperformed all competitors in Microsoft’s report (including Qwen2-audio, Gemini 2.0 Flash, and GPT-4o) at transcribing speech from text\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/1912.06670?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003ein\u003c/a\u003e\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2205.12446?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003ethree\u003c/a\u003e\u0026nbsp;\u003ca href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003edatasets\u003c/a\u003e. It also achieved competitive performance in speech translation, outperforming its competitors on two of four datasets.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;This work adds to the growing body of models with voice-in/text-out capability, including the open weights\u0026nbsp;\u003ca href=\"https://diva-audio.github.io/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eDiVA\u003c/a\u003e\u0026nbsp;model developed by a team led by Diyi Yang at Stanford University.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;The architectural options continue to expand for building neural networks that process text, images, audio, and various combinations. While some teams maintain separate models for separate data modalities, like\u0026nbsp;\u003ca href=\"https://qwenlm.github.io/blog/qwen2.5-max/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eQwen2.5\u003c/a\u003e\u0026nbsp;(for text) and\u0026nbsp;\u003ca href=\"https://qwenlm.github.io/blog/qwen2.5-vl/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eQwen2.5-VL\u003c/a\u003e) (for vision-language tasks), others are experimenting with mixture-of-expert models like\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2412.19437?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eDeepSeek-V3\u003c/a\u003e. Phi-4-multimodal shows that Mixture-of-LoRAs is an effective approach for processing multimodal data — and gives developers a couple of new open models to play with.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Output guardrails have been built to ensure appropriateness of text output, but this is difficult to apply to a voice-in/voice-out architecture. (Some teams have worked on guardrails that screen audio output directly, but the technology is still early.) For voice-based applications, a voice-in/text-out model can generate a candidate output without a separate, explicit speech-to-text step, and it accommodates text-based guardrails before it decides whether or not to read the output to the user.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--55-.jpg\" class=\"kg-image\" alt=\"Gavel striking a neural network, symbolizing legal decisions impacting AI and machine learning technologies.\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--55-.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--55-.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--55-.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"judge-upholds-copyright-in-ai-training-case\"\u003eJudge Upholds Copyright in AI Training Case\u003c/h1\u003e\u003cp\u003eA United States court delivered a major ruling that begins to answer the question whether, and under what conditions, training an AI system on copyrighted material is considered fair use that doesn’t require permission.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;A U.S. Circuit judge\u0026nbsp;\u003ca href=\"https://www.reuters.com/legal/thomson-reuters-wins-ai-copyright-fair-use-ruling-against-one-time-competitor-2025-02-11/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eruled\u003c/a\u003e\u0026nbsp;on a claim by the legal publisher Thomson Reuters that Ross Intelligence, an AI-powered legal research service, could not claim that training its AI system on materials owned by Thomson Reuters was a so-called “fair use.” Training the system did not qualify as fair use, he decided, because its output competed with Thomson Reuters’ publications.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;Thomson Reuters had\u0026nbsp;\u003ca href=\"https://fingfx.thomsonreuters.com/gfx/legaldocs/xmvjbznbkvr/THOMSON%20REUTERS%20ROSS%20LAWSUIT%20fair%20use.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003esued\u003c/a\u003e\u0026nbsp;Ross Intelligence after the defendant trained an AI model using 2,243 works produced by Thomson Reuters without the latter’s permission. This ruling reversed an earlier decision in 2023, when the same judge had allowed Ross Intelligence’s fair-use defense to proceed to trial. In the new ruling, he found that Ross Intelligence’s use failed to meet the definition of fair use in key respects. (A jury trial is scheduled to determine whether Thomson Reuters' copyright was in effect at the time of the infringement and other aspects of the case.)\u003c/p\u003e\u003cul\u003e\u003cli\u003eRoss Intelligence’s AI-powered service competed directly with Thomson Reuters, potentially undermining its market by offering a derivative product without licensing its works. Use in a competing commercial product undermines a key factor in fair use.\u003c/li\u003e\u003cli\u003eThe judge found that Ross Intelligence’s use was commercial and not transformative, meaning it did not significantly alter or add new meaning to Thomson Reuters’ works — another key factor in fair use. Instead, it simply repackaged the works.\u003c/li\u003e\u003cli\u003eThe ruling acknowledged that Thomson Reuters’ works were not highly creative but noted that they possessed sufficient originality for copyright protection due to the editorial creativity and judgment involved in producing it.\u003c/li\u003e\u003cli\u003eAlthough Ross Intelligence used only small portions of Thomson Reuters’ works, this did not weigh strongly in favor of fair use because those portions represented the most important summaries produced by Ross Intelligence.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;The ruling comes amid a\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/artists-and-writers-sue-big-tech-companies-over-copyright-infringement/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003ewave\u003c/a\u003e\u0026nbsp;of lawsuits over AI training and copyright in several countries. Many of these cases are in progress, but courts have weighed in on some.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cem\u003eThe New York Times\u003c/em\u003e\u0026nbsp;is\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/the-new-york-times-versus-openai-and-microsoft/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003esuing\u003c/a\u003e\u0026nbsp;OpenAI and Microsoft, arguing that their models generate output that competes with its journalism.\u003c/li\u003e\u003cli\u003eCondé Nast, McClatchy, and other major publishers recently\u0026nbsp;\u003ca href=\"https://www.wsj.com/business/media/conde-nast-mcclatchy-and-other-publishers-accuse-ai-firm-cohere-of-copyright-violations-5f5ceaff?st=xRCgwB\u0026reflink=desktopwebshare_permalink\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003efiled\u003c/a\u003e\u0026nbsp;a lawsuit against Cohere, accusing it of using copyrighted news articles to train its AI models.\u003c/li\u003e\u003cli\u003eSony, UMG, and Warner Music\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/sony-music-accuses-ai-developers-of-copyright-violations/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003efiled\u003c/a\u003e\u0026nbsp;lawsuits against AI music companies including Suno and Udio for allegedly using copyrighted recordings without permission.\u003c/li\u003e\u003cli\u003eA judge\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/judge-dismisses-key-arguments-in-ai-copyright-lawsuit-against-github-microsoft-and-openai/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003edismissed\u003c/a\u003e\u0026nbsp;key arguments brought by software developers who claimed that GitHub Copilot was trained on software they created in violation of open source licenses. The judge ruled in favor of Microsoft and OpenAI.\u003c/li\u003e\u003cli\u003eIn Germany, the publisher of the LAION dataset\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/laion-wins-legal-case-in-germany/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003ewon\u003c/a\u003e\u0026nbsp;a case in which a court ruled that training AI models on publicly available images did not violate copyrights.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;The question of whether training (or copying data to train) AI systems is a fair use of copyrighted works hangs over the AI industry, from academic research to commercial projects. In the wake of this ruling, courts may be more likely to reject a fair-use defense when AI companies train models on copyrighted material to create output that overlaps with or replaces traditional media, as\u0026nbsp;\u003cem\u003eThe New York Times\u003c/em\u003e\u0026nbsp;alleges in its lawsuit against OpenAI. However, the ruling leaves room for fair use with respect to models whose output doesn’t compete directly with copyrighted works.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Current copyright laws weren’t designed with AI in mind, and rulings like this one fill in the gaps case by case.\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/time-to-update-copyright-for-generative-ai/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eClarifying copyright\u003c/a\u003e\u0026nbsp;for the era of generative AI could help our field move forward faster.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--62-.png\" class=\"kg-image\" alt=\"AI model performance benchmark comparing R1 1776 and DeepSeek-R1 across MMLU, DROP, MATH-500, and AIME 2024 tests.\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/03/unnamed--62-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/03/unnamed--62-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--62-.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"deepseek-r1-uncensored\"\u003eDeepSeek-R1 Uncensored\u003c/h1\u003e\u003cp\u003eLarge language models built by developers in China may, in some applications, be less useful outside that country because they avoid topics its government deems politically sensitive. A developer fine-tuned DeepSeek-R1 to widen its scope without degrading its overall performance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Perplexity released\u0026nbsp;\u003ca href=\"https://www.perplexity.ai/hub/blog/open-sourcing-r1-1776?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eR1 1776\u003c/a\u003e, a version of\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/deepseek-r1-an-affordable-rival-to-openais-o1/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003eDeepSeek-R1\u003c/a\u003e\u0026nbsp;that responds more freely than the original. The model weights are available to\u0026nbsp;\u003ca href=\"https://huggingface.co/perplexity-ai/r1-1776?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003edownload\u003c/a\u003e\u0026nbsp;under a commercially permissive MIT\u0026nbsp;\u003ca href=\"https://choosealicense.com/licenses/mit/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003elicense\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u0026nbsp;\u003c/strong\u003eThe team modified DeepSeek-R1’s knowledge of certain topics by fine-tuning it on curated question-answer pairs.\u003c/p\u003e\u003cul\u003e\u003cli\u003eHuman experts identified around 300 topics that are censored in China.\u003c/li\u003e\u003cli\u003eThe authors developed a multilingual classifier that spots text related to these topics.\u003c/li\u003e\u003cli\u003eThey identified 40,000 prompts that the classifier classified as sensitive with high confidence. They discarded those that contained personally identifiable information.\u003c/li\u003e\u003cli\u003eFor each prompt, they produced factual, chain-of-thought responses that mirrored DeepSeek-R1's typical reasoning processes.\u003c/li\u003e\u003cli\u003eThey fine-tuned DeepSeek-R1 on the resulting prompt-response pairs.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;The fine-tuned model responded to politically charged prompts factually without degrading its ability to generate high-quality output.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe authors fed their model 1,000 diverse prompts that covered frequently censored topics. An unspecified combination of human and AI judges rated the models' responses according to the degree to which they are (i) evasive and (ii) censored outright.\u003c/li\u003e\u003cli\u003e100 percent of the fine-tuned model’s responses were rated uncensored, whereas the original version censored around 85 percent of sensitive queries. By comparison, DeepSeek-V3 censored roughly 73 percent, Claude-3.5-Sonnet around 5 percent, o3-mini about 1 percent, and GPT-4o 0 percent.\u003c/li\u003e\u003cli\u003eEvaluated on four language and math benchmarks (MMLU, DROP, MATH-500, and AIME 2024) and unspecified internal benchmarks, the fine-tuned and original models performed nearly identically. Their scores differed by a few tenths of a percent except on AIME 2024 (competitive high-school math problems), where the fine-tuned model achieved 79.8 percent compared to the original’s 80.96 percent.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;Among\u0026nbsp;\u003ca href=\"https://carnegieendowment.org/research/2023/07/chinas-ai-regulations-and-how-they-get-made?lang=en\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\"\u003ethe first countries to regulate AI\u003c/a\u003e, China\u0026nbsp;\u003ca href=\"https://www.chinalawtranslate.com/en/comparison-chart-of-current-vs-draft-rules-for-generative-ai/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003erequires\u003c/a\u003e\u0026nbsp;AI developers to build models that uphold “Core Socialist Values” and produce true and reliable output. When these objectives\u0026nbsp;\u003ca href=\"https://www.chinatalk.media/p/censorships-impact-on-chinas-chatbots?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\" rel=\"noopener\"\u003econflict\u003c/a\u003e, the political goal tends to dominate. While large language models built by developers in China typically avoid contentious topics, the newer DeepSeek models enforce this more strictly than older models like Qwen and Yi, using methods akin to Western measures for aligning output, like Reinforcement Learning from Human Feedback and\u0026nbsp;\u003ca href=\"https://www.chinatalk.media/p/censorships-impact-on-chinas-chatbots?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--3y3RXjUXVVnR-2W24Pwv0YfKeZMzD3pMv8YYZ-9CvFwDTMLGRGK4pRgeKyhGyMyH-4ZnT\"\u003ekeyword filters\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u0026nbsp;\u003c/strong\u003eAI models tend to reflect their developers’ values and legal constraints. Perplexity’s targeted fine-tuning approach addresses this barrier to international adoption of open-source models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;As models with open weights are adopted by the global community, they become a source of soft power for their developers, since they tend to reflect their developers’ values. This work reflects a positive effort to customize a model to reflect the user’s values instead — though how many developers will seek out a fine-tuned version rather than the original remains to be seen.\u003c/p\u003e","comment_id":"67d1ef219397160001d3f1de","feature_image":"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54--1.jpg","featured":false,"visibility":"public","created_at":"2025-03-12T13:31:29.000-07:00","updated_at":"2025-03-12T13:50:47.000-07:00","published_at":"2025-03-12T13:42:00.000-07:00","custom_excerpt":"The Batch AI News and Insights: Some people today are discouraging others from learning programming on the grounds AI will automate it.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"67d1f3659397160001d3f23a","name":"Mar 12, 2025","slug":"mar-12-2025","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/mar-12-2025/"},{"id":"67d1f3659397160001d3f23b","name":"issue-292","slug":"issue-292","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-292/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-292/","excerpt":"The Batch AI News and Insights: Some people today are discouraging others from learning programming on the grounds AI will automate it.","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"DeepSeek-R1 Uncensored, QwQ-32B Puts Reasoning in Smaller Model, and more...","meta_description":"The Batch AI News and Insights: Some people today are discouraging others from learning programming on the grounds AI will automate it.","email_subject":null,"frontmatter":null,"feature_image_alt":"Illustration of a programmer at a computer displaying PyTorch code, while a smiling colleague gives a thumbs-up in approval.","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2025/03/unnamed--54--1.jpg","dimensions":{"width":1200,"height":676}},"banner":{"title":"ChatGPT Prompt Engineering for Developers","databaseId":29452,"id":"cG9zdDoyOTQ1Mg==","featuredImage":{"node":{"altText":"","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/05/cgpt-2.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Databricks C1","date":"2025-06-04T07:12:29","databaseId":36631,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/3HlCuvP","courseName":"DSPy: Build and Optimize Agentic Apps","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)","isOpenInNewTab":true}},{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}}]}}},"__N_SSG":true},"page":"/the-batch/[slug]","query":{"slug":"issue-292"},"buildId":"Qy9Eh4N0MrfE3ddReY9v4","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>