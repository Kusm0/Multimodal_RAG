<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, and more...</title><meta name="description" content="The Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><link rel="canonical" href="https://www.deeplearning.ai/the-batch/issue-277/"/><meta property="og:type" content="article" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:title" content="DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:description" content="The Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:site_name" content="DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:url" content="https://www.deeplearning.ai/the-batch/issue-277/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="article:published_time" content="2024-11-27T11:42:00.000-08:00"/><meta property="article:modified_time" content="2024-11-27T11:47:14.000-08:00"/><meta property="article:tag" content="The Batch Newsletter"/><meta property="article:tag" content="issue-277"/><meta property="article:tag" content="Nov 27, 2024"/><meta property="article:author" content="https://www.facebook.com/DeepLearningAIHQ/"/><meta property="twitter:title" content="DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:description" content="The Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:url" content="https://www.deeplearning.ai/the-batch/issue-277/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:label1" content="Written by"/><meta property="twitter:data1" content="DeepLearning.AI"/><meta property="twitter:label2" content="Filed under"/><meta property="twitter:data2" content="The Batch Newsletter"/><meta property="twitter:card" content="summary_large_image" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:creator" content="@DeepLearningAI"/><meta property="twitter:site" content="https://twitter.com/DeepLearningAI/"/><meta name="twitter:image" content="https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.jpg"/><meta property="og:image" content="https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.jpg"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="676"/><script type="application/ld+json">{"@context":"https://schema.org/","@type":"Article","datePublished":"2024-11-27T11:42:00.000-08:00","dateModified":"2024-11-27T11:47:14.000-08:00","author":{"@type":"Article","name":"DeepLearning.AI","image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","sameAs":"[\"https://www.deeplearning.ai/\", \"https://twitter.com/DeepLearningAI_/\", \"https://www.facebook.com/DeepLearningAIHQ/\"]"},"keywords":"[object Object], [object Object], [object Object]","headline":"DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, and more...","image":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.jpg","width":1200,"height":676},"publisher":{"@type":"Organization","name":"DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, and more...","logo":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","width":60,"height":60}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.deeplearning.ai"},"description":"The Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object..."}</script><meta name="next-head-count" content="31"/><link href="/static/favicons/favicon.ico" rel="shortcut icon"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&amp;display=swap"/><link rel="preload" href="/_next/static/css/54b174af9991199d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/54b174af9991199d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6f396d9f2f265155.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6f396d9f2f265155.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-7690c02358b700f2.js" defer=""></script><script src="/_next/static/chunks/framework-c54e8763846ac33b.js" defer=""></script><script src="/_next/static/chunks/main-9fcc7c9a63f70912.js" defer=""></script><script src="/_next/static/chunks/pages/_app-bece9363d7a6c0bb.js" defer=""></script><script src="/_next/static/chunks/3a17f596-e9670ced3bfab562.js" defer=""></script><script src="/_next/static/chunks/36d2f571-09e32b60ec93a4dc.js" defer=""></script><script src="/_next/static/chunks/5c0b189e-c0f75ca9b44e520b.js" defer=""></script><script src="/_next/static/chunks/5567-47eb911ba5f222d7.js" defer=""></script><script src="/_next/static/chunks/2251-380253169bb2e795.js" defer=""></script><script src="/_next/static/chunks/3864-a4ae56e91852593b.js" defer=""></script><script src="/_next/static/chunks/4965-3585432476b27ea9.js" defer=""></script><script src="/_next/static/chunks/3791-0bbbd4cd0bf31c22.js" defer=""></script><script src="/_next/static/chunks/pages/the-batch/%5Bslug%5D-c03a8bcef188ef99.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_buildManifest.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_ssgManifest.js" defer=""></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&display=swap"/></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5C5VGGJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div data-sentry-component="Layout" data-sentry-source-file="Layout.tsx"><aside id="top-announcement" class="text-neutral-900  " style="color:#FFFFFF;background-color:#05256C;background-image:linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)" data-sentry-component="AnnouncementBanner" data-sentry-source-file="AnnouncementBanner.tsx"><div class="container--boxed py-3 flex items-center justify-between "><div class="flex items-center"><p class="text-sm lg:text-base">✨ New course! Enroll in<!-- --> <a href="https://bit.ly/3HlCuvP" class="underline" target="_blank">DSPy: Build and Optimize Agentic Apps</a></p></div><div class="flex items-center"><button class="bg-transparent p-0 border-none text-xl opacity-70 hover:opacity-100 transition-opacity"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><line x1="15" y1="9" x2="9" y2="15"></line><line x1="9" y1="9" x2="15" y2="15"></line></svg></button></div></div></aside><header id="main-navigation" data-testid="main-navigation-testid" class="static h-[64px] md:h-[100px] top-0 w-full z-[100] flex items-center bg-white transition-all ease-in-out duration-200" data-sentry-component="Header" data-sentry-source-file="index.tsx"><div class="container--boxed flex justify-between items-center"><div class="max-w-[185px] lg:max-w-[235px]"><a href="/the-batch/"><div class="flex items-center"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27300%27%20height=%2792%27/%3e"/></span><img alt="The Batch" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="The Batch" loading="lazy" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcSet="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=384&amp;q=75 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75 2x" src="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75"/></noscript></span></div></a></div><div class="flex items-center"><div class="hidden lg:flex items-center"><nav aria-label="Primary" data-sentry-component="Nav" data-sentry-source-file="Nav.tsx"><ul class="flex items-center p-0 m-0 list-none"><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/courses/"><div class="NavItem_navItemLink__Aq6E5"><span>Explore Courses</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/the-batch/"><div class="NavItem_navItemLink__Aq6E5"><span>AI Newsletter</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/"><div>The Batch</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/letters/"><div>Andrew&#x27;s Letter</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/data-points/"><div>Data Points</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/research/"><div>ML Research</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/"><div>Blog</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/community/"><div class="NavItem_navItemLink__Aq6E5"><span>Community</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="https://community.deeplearning.ai/"><div>Forum</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/events/"><div>Events</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/ambassador/"><div>Ambassadors</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/category/ambassador-spotlight/"><div>Ambassador Spotlight</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/resources/"><div class="NavItem_navItemLink__Aq6E5"><span>Resources</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/about/"><div class="NavItem_navItemLink__Aq6E5"><span>Company</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/about/"><div>About</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/careers/"><div>Careers</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/contact/"><div>Contact</div></a></li></ul></li></ul></nav><a class="btn--primary text-base font-medium whitespace-nowrap shadow h-fit py-3 px-4 lg:ml-2 xl:ml-10" data-sentry-element="Link" data-sentry-source-file="index.tsx" href="https://bit.ly/3RB9T8a">Start Learning</a></div><div class="flex items-center" data-sentry-element="Menu" data-sentry-component="MobileMenu" data-sentry-source-file="MobileMenu.tsx" data-headlessui-state=""><button class="lg:hidden" data-sentry-element="MenuButton" data-sentry-source-file="MobileMenu.tsx" id="headlessui-menu-button-:R1qa6:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-4xl text-slate-600" data-sentry-element="FiMenu" data-sentry-source-file="MobileMenu.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button></div></div></div></header><main><div id="content"><nav aria-label="Secondary" class="h-[66px] bg-white sticky top-[60px] z-40 shadow hidden lg:block" data-sentry-component="SecondaryNav" data-sentry-source-file="index.tsx"><div class="container--boxed h-full w-full flex items-center justify-between "><div class="relative flex h-full"><button class="h-full w-14 items-center justify-center absolute top-0 left-0 z-10 group bg-white bg-opacity-75 hidden"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="15 18 9 12 15 6"></polyline></svg></span></button><ul id="nav-secondary" class="list-none p-0 m-0 h-full flex items-center  overflow-x-scroll relative SecondaryNav_navItems__dok3i"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Weekly Issues</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/letters/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Andrew&#x27;s Letters</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/data-points/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Data Points</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/research/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">ML Research</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/business/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Business</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/science/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Science</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/culture/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Culture</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/hardware/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Hardware</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/ai-careers/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">AI Careers</div></a></li></ul><button class="h-full w-9 items-center justify-center absolute top-0 right-0 group bg-white bg-opacity-75 flex"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span></button></div><div class="flex items-center h-full"><ul class="list-none p-0 m-0 h-full flex items-center mr-5"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/about/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">About</div></a></li></ul><button type="button" class="bg-white border btn--tracking border-solid border-brand text-brand hover:bg-brand hover:text-white transition-colors px-3 py-1 rounded-md mr-4">Subscribe</button><a href="/search/"><div title="Search" class="transition-colors text-slate-400 hover:text-slate-500"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-2xl" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></div></a></div></div></nav><article class="pt-5 pb-16 bg-white lg:pt-16 lg:pb-16" data-sentry-component="ArticleDefault" data-sentry-source-file="ArticleDefault.tsx"><header class="post_layoutGrid__0BDX2"><nav aria-label="Breadcrumb flex items-center" data-sentry-component="Main" data-sentry-source-file="Breadcrumb.tsx"><ul class="list-none flex items-center flex-wrap"><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">The Batch</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">Weekly Issues</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/issue-277/"><div class="text-sm md:text-base text-slate-400 font-normal "><h1 class="capitalize">issue 277</h1></div></a></li></ul></nav><aside class="flex mt-6 lg:hidden"><div class="flex flex-col items-start items-start mr-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm text-sm">Published</div></div><div class="mt-1 text-slate-600 text-base text-sm">Nov 27, 2024</div></div><div class="flex flex-col items-start items-start" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm text-sm">Reading time</div></div><div class="mt-1 text-slate-600 text-base text-sm">13<!-- --> min read</div></div></aside></header><div class="post_layoutGrid__0BDX2 mt-9"><aside style="grid-column:wide-start / main-start" class="flex-col items-end hidden pr-10 lg:flex"><div class="flex flex-col items-start items-end" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm undefined">Published</div></div><div class="mt-1 text-slate-600 text-base undefined"><a href="/the-batch/tag/nov-27-2024/"><div class="inline-flex px-2 py-1 text-sm font-normal transition-colors rounded-md bg-slate-100 hover:bg-slate-200 text-slate-500">Nov 27, 2024</div></a></div></div><div class="flex flex-col items-start items-end mt-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm undefined">Reading time</div></div><div class="mt-1 text-slate-600 text-base undefined">13<!-- --> min read</div></div><div class="flex flex-col items-start items-end mt-6 sticky top-20" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-277/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-277/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-277/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></aside><div class="prose--styled justify-self-center post_postContent__wGZtc"><p>Dear friends,</p><p>Happy Thanksgiving! In the United States, this is a week when many reflect on their blessings and give thanks. Even as I reflect on how lucky I am to have food, shelter, family, and friends, I think about those who have much less and what we can do to help them.</p><p>Last week, I spoke with a woman who had been severely physically abused by her husband. She showed me pictures of her face from a few years ago, which had a bloodied sequence of tears down the middle. She also showed me scars left by cigarette burns inflicted by her husband, who told her these burns made her ugly so no other man would ever want her. She is no longer with her husband but continues to struggle. Her phone is badly cracked and barely holds a charge. Without a high-school degree, she has struggled to find a job and is surviving by staying on the couch of a friend. As winter approaches, they keep their place chilly to save the cost of electricity.</p><p>Working in AI, I am fortunate to interact with many of the smartest and most capable technology and business leaders in the world. But both at home and when I travel, I try to meet with people of a broad range of backgrounds, because ultimately I want to do work that helps people broadly, and this requires that I understand people broadly. When you go to a grocery store and see someone put down a $5 carton of eggs because it is too expensive, and hear them think through how to explain to their kids why they’re skipping eggs that week, it gives you a deeper appreciation for why a $1.50/hour raise can be life-changing for many people.</p><p>While I can try to help out individuals here and there, technology is advancing rapidly, and this gives me a lot of optimism for the future. Technology remains the best way I know of to help people at scale through providing better education, career guidance, healthcare, personal safety, healthier food, or other things needed to support thriving.</p><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35-.jpg" class="kg-image" alt="Cornucopia overflowing with fruits and vegetables." loading="lazy" width="1200" height="676" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/unnamed--35-.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/unnamed--35-.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35-.jpg 1200w" sizes="(min-width: 720px) 720px"></figure><p>I am optimistic about the future because I see so many ways life can be so much better for so many people. I feel blessed that, when my kids or I are cold, we have warm clothing, and when we are hungry, we have a working car to drive to the grocery and buy fresh food. I feel blessed that, rather than using a badly cracked cellphone, I have a modern laptop and a fast internet connection to do my work on.</p><p>As a child, my father taught me the aphorism “<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLh83qgyTW7Y8-PT6lZ3mYW6YGmQT8LqGGcW8g5f2v4KNWtvW3VTsh097sTzmW72rfsM1Kt6WcW3qDX_s4TN-fHW7rDs5X3CN47MVjVhm84Nsrk2W4MCCR13TmK-mW6pMzFq8gSK5-W2Th3Sk8gzPM5W3m8ZgL9kntxNW23w3NT55xYnSW2r8QvV3Xg1JnW1qfNc74bb8y6W2MSVwY6H9yZsN15PW2GCX801W2BlX8p6NwmmhW1rTfF74SxJ8zW3yT29D6Scst8W1l383v238P6sW8jVhr86bMf0hW5RGt2y6b1D2WW81WM533lT908MDLXWSxXcJ-W4LxCnM4yC2S-VSjLHx2sB2k4f5tVDs-04?ref=dl-staging-website.ghost.io" rel="noopener">there but for the grace of God go I</a>” to recognize that, in even slightly different circumstances, I might have ended up with much less. Having worked on many software products, I know that, to make good decisions, I have to understand the people I hope to serve. This is why I continue to routinely seek out, speak with, and try to understand people from all walks of life, and I hope many others in AI will do so, too.</p><p>I see so many people in the AI community building things to make the world better. I am thankful for what the AI community has already done, and I look forward to continuing to build and serve others together.</p><p>Keep building!</p><p>Andrew</p><h2 id="a-message-from-deeplearningai">A MESSAGE FROM&nbsp;DEEPLEARNING.AI</h2><figure class="kg-card kg-image-card"><a href="https://www.deeplearning.ai/short-courses/ai-python-for-beginners/?ref=dl-staging-website.ghost.io"><img src="https://dl-staging-website.ghost.io/content/images/2024/11/2--9-.png" class="kg-image" alt="Promo banner for &quot;AI Python for Beginners&quot;" loading="lazy" width="2000" height="1044" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/2--9-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/2--9-.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2024/11/2--9-.png 1600w, https://dl-staging-website.ghost.io/content/images/2024/11/2--9-.png 2042w" sizes="(min-width: 720px) 720px"></a></figure><p>Get started coding in Python with&nbsp;<em>AI Python for Beginners</em>, a four-part course led by Andrew Ng. Build projects from the very first lesson with real-time support from an AI assistant. Complete the course and bring your ideas to life!&nbsp;<a href="https://www.deeplearning.ai/short-courses/ai-python-for-beginners/?ref=dl-staging-website.ghost.io" rel="noreferrer">Start today</a></p><h1 id="news">News</h1><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--23-.png" class="kg-image" alt="Bar charts comparing performance of AI models across six tasks." loading="lazy" width="1200" height="675" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/unnamed--23-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/unnamed--23-.png 1000w, https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--23-.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="reasoning-revealed">Reasoning Revealed</h1><p>An up-and-coming Hangzhou AI lab unveiled a model that implements run-time reasoning similar to OpenAI o1 and delivers competitive performance. Unlike o1, it displays its reasoning steps.</p><p><strong>What’s new:</strong>&nbsp;DeepSeek&nbsp;<a href="https://api-docs.deepseek.com/news/news1120?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">announced</a>&nbsp;DeepSeek-R1, a model family that processes prompts by breaking them down into steps. A free preview version is&nbsp;<a href="http://chat.deepseek.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">available</a>&nbsp;on the web, limited to 50 messages daily; API pricing is not yet announced. R1-lite-preview performs comparably to o1-preview on several math and problem-solving benchmarks. DeepSeek said it would release R1 as open source but didn't announce licensing terms or a release date.</p><p><strong>How it works:</strong>&nbsp;DeepSeek-R1-lite-preview uses a&nbsp;<a href="https://x.com/phill__1/status/1859263165000729024?s=61&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">smaller base model</a>&nbsp;than DeepSeek 2.5, which comprises 236 billion parameters. Like o1-preview, most of its performance gains come from an approach known as&nbsp;<a href="https://arxiv.org/abs/2408.03314?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">test-time compute</a>, which trains an LLM to think at length in response to prompts, using more compute to generate deeper answers. Unlike o1-preview, which hides its reasoning, at inference, DeepSeek-R1-lite-preview’s reasoning steps are visible. This makes the model more transparent, but it may also make it more&nbsp;<a href="https://techcrunch.com/2024/11/20/a-chinese-lab-has-released-a-model-to-rival-openais-o1/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">vulnerable</a>&nbsp;to jailbreaks and other manipulation.</p><ul><li>According to DeepSeek, R1-lite-preview, using an unspecified number of reasoning tokens, outperforms OpenAI o1-preview, OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, Alibaba Qwen 2.5 72B, and DeepSeek-V2.5 on three out of six reasoning-intensive benchmarks.</li><li>It substantially outperforms o1-preview on&nbsp;<a href="https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">AIME</a>&nbsp;(advanced high school math problems, 52.5 percent accuracy versus 44.6 percent accuracy),&nbsp;<a href="https://arxiv.org/abs/2103.03874?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">MATH</a>&nbsp;(high school competition-level math, 91.6 percent accuracy versus 85.5 percent accuracy), and&nbsp;<a href="https://codeforces.com/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">Codeforces</a>&nbsp;(competitive programming challenges, 1,450 versus 1,428). It falls behind o1 on&nbsp;<a href="https://arxiv.org/abs/2311.12022?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">GPQA Diamond</a>&nbsp;(graduate-level science problems),&nbsp;<a href="https://arxiv.org/abs/2403.07974?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">LiveCodeBench</a>&nbsp;(real-world coding tasks), and&nbsp;<a href="https://huggingface.co/blog/yuchenlin/zebra-logic?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">ZebraLogic</a>&nbsp;(logical reasoning problems).</li><li>DeepSeek reports that the model’s accuracy improves dramatically when it uses more tokens at inference to reason about a prompt (though the web user interface doesn’t allow users to control this). On AIME math problems, performance rises from 21 percent accuracy when it uses less than 1,000 tokens to 66.7 percent accuracy when it uses more than 100,000, surpassing o1-preview’s performance. The additional performance comes at the cost of slower and more expensive output.</li></ul><p><strong>Behind the news:</strong>&nbsp;DeepSeek-R1 follows OpenAI in implementing this approach at a time when scaling laws that predict higher performance from bigger models and/or more training data are being&nbsp;<a href="https://www.deeplearning.ai/the-batch/issue-276/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">questioned</a>.</p><p><strong>Why it matters:</strong>&nbsp;DeepSeek is challenging OpenAI with a competitive large language model. It’s part of an important movement, after years of scaling models by raising parameter counts and amassing larger datasets, toward achieving high performance by spending more energy on generating output.</p><p><strong>We’re thinking:</strong>&nbsp;Models that do and don’t take advantage of additional test-time compute are complementary. Those that do increase test-time compute perform well on math and science problems, but they’re slow and costly. Those that don’t use additional test-time compute do well on language tasks at higher speed and lower cost. Applications that require facility in both math and language may benefit by switching between the two.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--34-.gif" class="kg-image" alt="Robotic arms collaborating to fold a red garment on a table." loading="lazy" width="600" height="338" srcset="https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--34-.gif 600w"></figure><h1 id="household-help">Household Help</h1><p>A new generation of robots can handle some household chores with unusual skill.</p><p><strong>What’s new:</strong>&nbsp;Physical Intelligence, a startup based in San Francisco, unveiled&nbsp;<a href="https://arxiv.org/abs/2410.24164v1?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">π0</a>&nbsp;(pronounced “pi-zero”), a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables. The company also&nbsp;<a href="https://www.nytimes.com/2024/11/04/business/dealbook/physical-intelligence-robot-ai.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">announced</a>&nbsp;$400 million in investments from OpenAI, Jeff Bezos, and several Silicon Valley venture capital firms.</p><p><strong>How it works:</strong>&nbsp;π0 is a version of the pretrained&nbsp;<a href="https://arxiv.org/abs/2407.07726?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">PaliGemma</a>&nbsp;vision-language model that has been modified for&nbsp;<a href="https://arxiv.org/abs/2210.02747?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">flow matching</a>. (Flow matching is similar to diffusion, in which a model learns to remove noise from inputs to which noise has been added, and ultimately generates output by removing noise from an input of pure noise). A user supplies a text command, and the robot uses its sensor inputs to remove noise from a pure-noise action embedding to generate an appropriate action.</p><ul><li>PaliGemma comprises&nbsp;<a href="https://arxiv.org/abs/2303.15343?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">SigLIP</a>, a vision transformer that turns images into embeddings; a linear layer that adapts the image embeddings to serve as input for the pretrained large language model Gemma; and&nbsp;<a href="https://arxiv.org/abs/2403.08295?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">Gemma</a>, which estimates the noise to be removed from a robot action embedding to which noise has been added.</li><li>The authors modified PaliGemma as follows: (i) They adapted it to accept embeddings that represent the robots’ state and previous actions, and to generate embeddings that represent the noise to be removed from noisy robot actions. (ii) They added a vanilla neural network to the input to turn the current timestep into an embedding. (iii) They modified&nbsp;Gemma to be a mixture-of-experts model:&nbsp;One expert, or subset of weights, is the pretrained weights, which process image and text embeddings. The other is a new set of weights that process robot action embeddings.</li><li>They pretrained π0 to remove noise from action embeddings. (Since π0 produces embeddings of the noise to be removed, removing that noise is as simple as adding the two embeddings.)</li><li>Training data included the&nbsp;<a href="https://arxiv.org/abs/2310.08864?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">Open X-Embodiment Dataset</a>&nbsp;and a proprietary dataset of 10,000 hours of robotic states (for instance, current positions of a robot’s joints), actions (for instance, motions of the robot’s joints), and an associated language command. The proprietary dataset included data collected from seven different robots (such as a single stationary robot arm to two robot arms mounted on a mobile base) and 68 tasks (for example, folding laundry, making coffee, or bussing a table).</li><li>After pretraining, the authors fine-tuned π0 to remove noise from action tokens in 15 further tasks, some of which were not represented in the pretraining set. These tasks improved the model’s ability to follow more detailed instructions and perform multi-stage tasks such as packing food into a to-go box.</li><li>At inference, given the robot’s camera view of the surrounding scene, SigLip embeds the images. A linear layer projects the resulting embeddings to fit Gemma’s expected input size and data distribution. Given the images, text command, robot’s state, current timestep, and 50 noisy action tokens (starting with pure noise), Gemma iteratively removes noise. To complete longer tasks, the process repeats: The robot takes more images of the surrounding scene and retrieves the robot’s state, which π0 uses to generate further actions.</li></ul><p><strong>Results:&nbsp;</strong>π0 outperformed the open robotics models&nbsp;<a href="https://arxiv.org/abs/2406.09246?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">OpenVLA</a>,&nbsp;<a href="https://arxiv.org/abs/2405.12213?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">Octo</a>,&nbsp;<a href="https://arxiv.org/abs/2304.13705?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">ACT</a>, and&nbsp;<a href="https://arxiv.org/abs/2303.04137?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">Diffusion Policy</a>, all of which were fine-tuned on the same data, on all tasks tested, as measured by a robot’s success rate in completing each task. For example, using a single robotic arm to stack a set of bowls of four sizes, π0 completed about 100 percent on average. Diffusion Policy completed about 55 percent, ACT about 45 percent, and OpenVLA and Octo below 10 percent. Across all tasks, π0 completed about 80 percent on average, while Diffusion Policy completed about 35 percent on average.</p><p><strong>Yes, but:</strong>&nbsp;The robot occasionally makes&nbsp;<a href="https://www.wired.com/story/physical-intelligence-ai-robotics-startup/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">mistakes</a>. In one video, it puts too many eggs into a carton and tries to force it shut. In another, it throws a container off a table instead of filling it with items.</p><p><strong>Behind the news:</strong>&nbsp;Commercial robotics appears to be undergoing a renaissance. Skild&nbsp;<a href="https://www.skild.ai/blogs/announcing-our-300m-series-a?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">raised</a>&nbsp;$300 million to develop a “general-purpose brain for robots.” Figure AI&nbsp;<a href="https://www.prnewswire.com/news-releases/figure-raises-675m-at-2-6b-valuation-and-signs-collaboration-agreement-with-openai-302074897.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">secured</a>&nbsp;$675 million to build humanoid robots powered by multimodal models. Covariant, which specializes in industrial robotics,&nbsp;<a href="https://www.deeplearning.ai/the-batch/amazon-strengthens-logistics-and-robotics-with-new-ai-partnership/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">licensed</a>&nbsp;its technology to Amazon. (Disclosure: Andrew Ng is a member of Amazon's board of directors). OpenAI&nbsp;<a href="https://techcrunch.com/2024/11/04/metas-former-hardware-lead-for-orion-is-joining-openai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99" rel="noopener">renewed</a>&nbsp;its robotics effort after&nbsp;<a href="https://venturebeat.com/business/openai-disbands-its-robotics-research-team/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99#:~:text=In%20a%20statement%2C%20an%20OpenAI,the%20team%20on%20other%20projects" rel="noopener">dismantling</a>&nbsp;its robotics department in 2020.</p><p><strong>Why it matters:</strong>&nbsp;Robots have been slow to benefit from machine learning, but the generative AI revolution is driving rapid innovations that make them much more useful. Large language models have made it possible to command robots using plain English. Meanwhile, the team at Physical Intelligence collected a dataset of sufficient size and variety to train the model to generate highly articulated and practical actions. Household robots may not be right around the corner, but π0 shows that they can perform tasks that people need done.</p><p><strong>We’re thinking:</strong>&nbsp;One of the team members compared π0 to GPT-1 for robotics — an inkling of things to come. Although there are significant differences between text data (which is available in large quantities) and robot data (which is hard to get and varies per robot), it looks like a new era of large robotics foundation models is dawning.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--37-.jpg" class="kg-image" alt="Illustration of a person holding a box with network nodes emerging from it." loading="lazy" width="1200" height="676" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/unnamed--37-.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/unnamed--37-.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--37-.jpg 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="ai-power-couple-recommits">AI Power Couple Recommits</h1><p>Amazon and Anthropic expanded their partnership, potentially strengthening Amazon Web Services’ AI infrastructure and lengthening the high-flying startup’s runway.</p><p><strong>What’s new:</strong>&nbsp;Amazon, already a significant investor in Anthropic,&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhs3qgyTW8wLKSR6lZ3pMW6HNTV-67pwbFW2wjk-P6XfR_JW5vFSjF87Q2h9W5JXyGt7LnQ_QW86YWlF19J--rW5wYMdW5pVD0hW8gKK7Y5SXGymW69t0zD3WwpQ1N506721chJGsW1NPGF-8N3BylVvL8Nw1kCgXyW4XlyqF7gGBQnV5l9293-wBbCVsJDqn76qq4KW7t5rhf1knVpGW1S0FbG8Dv4k6W3ND7tg18L3X5W8R-k_Q9bK2RRW64f7Q13tBcZ8W5FfyyJ3d2_9ZW1jhzH72jwcg2W6Szvpr6f7fD_W1qQjHv2D-npYW4cNrph8Kwq94W3yJX673gJ1MFW998vq287vLjpW36kX3J5LYGrdW2Cqgnf2xp9vxf1KmzHM04?ref=dl-staging-website.ghost.io" rel="noopener">put</a>&nbsp;another $4 billion into the AI company. In exchange, Anthropic will train and run its AI models on Amazon’s custom-designed chips. (Disclosure: Andrew Ng serves on Amazon’s board of directors.)</p><p><strong>How it works:</strong>&nbsp;The new round brings Amazon’s investment in Anthropic to $8 billion (though it remains a minority stake without a seat on the startup’s board). The deal extended the partnership in several ways:</p><ul><li>AWS becomes Anthropic’s primary partner for training AI models. Anthropic will train its models using Amazon’s&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgT3qgyTW7lCdLW6lZ3q3W8qfY271PmLKXW44_hg97yMzkpW2MwdB83_r3fzW6X9vx58XHG5mW5SvK1y2VWrcYW7s_d2k7SMwrwW7sJjCF3nrXNrN2TbDf5mVZR7W1lnzc54-xWcJW2QmjHR8y5lYXW1yGKBK5MplbjW4Fndpw2VLp8fW3Lpc1n1qLyvMW6Rj7zF4YD8JWW4D-7R549PvcBW7p8W125fZ6nfW7ZV17635S3LWV5wfB053zhv0W2tcf5f43_vMtW2zGPv41hqvFBW7xGZlm8m52LhW7K2Vj68vMlc1W8BG8SW3Q-R5tW6D6W8t5mHkqbf5g6MDv04?ref=dl-staging-website.ghost.io" rel="noopener">Trainium</a>&nbsp;chips, which are designed for training neural networks of 100 billion parameters and up. Amazon executives previously&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhM3qgyTW95jsWP6lZ3lYN5kYHjKwPRgkN7m-X7LKXzW9W7KRd_t7gD5Q6W5Mp-kr7PJNQjW7TCV7J4lcHYJW79lWfC4XgxvHW8F2Nlx2lfRD3W5N1FzZ2xQcmBW4t6FfJ6Nh5y7W1gcYBY5vV7pCW8rrnvr5y7ZzkW71TXN01754wsW4mpWzZ1mM91CV3qf6Y2CTx30W9fxtxZ1CRwXkW3SMk4G7bxZ2wN8N0SsmpR0bWW1wmHs16lZ_K6W94G4cC11JMTYW7Z4Mrw3_Lp8sW2mVd9X68b_yDW4KJBqD6b2NygW9fnkkj6SZkLgW4n2Bpy39b9zHW3j9NvD3QwC2mMV9TY7kw1bpW70NzV76YqLv8W8mvC206Fh-_HW18r5Hl7Lh_wVW42Vby844G__2f5nqkQv04?ref=dl-staging-website.ghost.io" rel="noopener">claimed</a>&nbsp;that these chips could cut training costs by as much as 50 percent compared to Nvidia graphics processing units (GPUs).</li><li>Previously Anthropic ran its Claude models on Nvidia hardware; going forward, Anthropic will run them on Amazon’s&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLh83qgyTW7Y8-PT6lZ3kFW8jt3sY7zp5PSW1w45dt4kB-w3W425mnP6MK-ysW19nfXS3N2PLGW83qLxt6qtmfYW24F3ZP8J7m_MW84NXPk1zjF0bV7dXfD7zbJPsW11W3rl8BSl7rW746S617DQwnNN7vg6VCP6QL8W518g1Y1VwqTsW4TykFW49hSNNW50BLYN4tJHzKW6QPLlj77k9Y3W1nfXFt3_dtyVW37Z_4223mkxXN61m335MpGzXW4JNNxY5jDkrhN5n44zLd-z-BW6PpBZV1j6c-wW3hfxLt4Ph5tLW5vcG4C82Z50WN4_9JdDV8PXhW7x1SPm25-WpGVc07LT1v-Lxmf4v10NF04?ref=dl-staging-website.ghost.io" rel="noopener">Inferentia</a>&nbsp;chips, according to&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLfH5nR32W50kH_H6lZ3lLW8g4s_R2Py_B8W1Vssm41-zZ4xW2jH8k06NzYrxW3hwSKy82SKltW55Cmf31gV_YhW40WvWJ9c6cTLW3hnh5n5nssNtW38l-PN3S1dF5W1rf1dD39RJRfW4Cm_P34KS3RXW6yjddS80JbxdW19sdbt7Fh05rW1B8wbM6ckvqpW2rR0Fc75PZNPVZr08L8F8wwNW6w_3Bv4ZhkqCW18jtpF7l_Z7nW63ZPTR7_TFpGW2k1DG-5Hf14CW3R-X5W4CCyc2W8Xv3DF71yNY5W46xVYm1jqP29W5bs1xQ47lr9ZW5-BDzQ3_p9JBW3N2LbD5LN88KW7v-D628TN96_W9cJFQk7LJsnMW5Bqsc7623M_BF7XXzb-XgxWW6gJ_9j882j8VW6Wh9L13n-HshW69Q1nx51Jv9XdQCRmz04?ref=dl-staging-website.ghost.io" rel="noopener">The Information</a>. Customers of Amazon Web Services will be able to fine-tune Claude on Bedrock, Amazon Web Services’ AI model platform.</li><li>Anthropic will contribute to developing Amazon’s&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgT3qgyTW7lCdLW6lZ3kJW613Z0G1BRY7wW8nhX5q28Gb0pW6ncRVb1VbS0qW5zq_c38xyP6MW6rrnsY2LjjdGW2lLwJP333bcKN6LDMw-tjZlvW1VjXKy2yrMWfW5_JmfW31-TpLN1vvS9Qw4-BcW87v4-g6T8kynW71K1tK7TJ75WW3TDBLm6c40zpN3KdZ36HM13gW83NTLQ1Z_GLMW1410v_1Wtcw8W3HRc0G4qLcD4VQMq9G2XDbyTW20_fLD7y7MXyW1PX38j806NVNW1QTHG376m6J3VgG67c5CVXj5W58nQbP3gpKmxW33ps424cGpsnf10SRWs04?ref=dl-staging-website.ghost.io" rel="noopener">Neuron</a>&nbsp;toolkit, software that accelerates deep learning workloads on Trainium and Inferentia chips.</li></ul><p><strong>Behind the news:</strong>&nbsp;In November, Anthropic&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLfH5nR32W50kH_H6lZ3nyW7VKlGX8HJP3nVVn6g44sQpgRW6T-tk47dMhnLW4JbD3d7CYqm-W8CPq7-1lN930W9cHzql54RjznW3kXGdR7WgxnhW5-HPl0318zdYVCWB4f6n5GcFW4VH9lP8QKqm5W88PfCZ6WV7rNVKXV4X8-Z8kmW60dc1X6nfgM2W5wRjM86R5wzgVGs3Np6sdcBDW22177j8Dxx1yW6jPJdn3f_GKzW6z4mCC6TmzsJW3XKxZQ6z5bRbW802bxG1vgYYpW86bcfZ8YdFS8Mfv6kdgKXDTW52rjXP4kRHlKN8PFPQwC1J1wW5qLZnS2MnbtMW2P61hd8P4YlPW57qFPc37qHSqN7DQ2zxc58MjW3pKWj_6xfbpJW3gdl-l6Tz_WcW3slM8Z7S29T-W4v_khz22mq-Lf854bj804?ref=dl-staging-website.ghost.io" rel="noopener">agreed</a>&nbsp;to use Google’s cloud-computing infrastructure in return for a $2 billion investment. The previous month, Amazon had&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLfH5nR32W50kH_H6lZ3mNW78vvVt3cvpV6W4x69R91B0Z1fW19wGGp6DPr8nV6Fh2D95sdpgN41SKXD7DknLW1sd3z16B_RKbW3-MHkp8CnjCZW6tdvBv52WHMXT5WSC40-1kHW7j0zGG57jkmjW3Lzswx1dPgRMW8QjvW65TYpbvN81dVyjqDSZCW2P27q076yzhgW7cybyS6Y467kW1TbRTl81Q3vdW4Wr0sB8wvbVyW8n32QD3rc39fW6cc4Xs8GzdShN7DwM0_wWjC8W1dVSHW8Xv68kW5kDC4D6JfVXwW535YRJ34ZzW1W3Qr8VY68h-F5W2gtwcp9kWLd9W7cR_jX21q1xzVFFJ7b1D-f4DN6B-1DJDj2ppW2tmBwK5mr5ZjW5p0g052Ls-NJN5hHBXTglZWxW7rKW-P1GzN_cf59WLJ604?ref=dl-staging-website.ghost.io" rel="noopener">committed</a>&nbsp;to invest as much as $4 billion in Anthropic, and Anthropic had made Amazon Web Services the primary provider of its models.</p><p><strong>Yes, but:</strong>&nbsp;The UK’s Competition and Markets Authority recently&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhs3qgyTW8wLKSR6lZ3m-W5HsMVy8VTPg1W8tsD1S2P5bnBN81HcKlmzKHKW7zSGf51b73W3W5rQBtG3DxyGsW6gGyf14HVgp2N2pF3rS7zxmyW3rFCVj8gSv7FW2yRScb4dmvFnW91w_fs12B70XW48Hsjs6Wq-r_W1YYc9553srQmN6gdxzxsLbw3W5ZlCMT18FfKxW5wdxTk3ZXYJKW1C2YpN5F3d4BVrPZww8DvdNNW4mVV_b2pP9FwW4fX3pS1CxnBnW5PyLHq2wljHmW28dyGS3Py9FqW41fKv72dn6WGV1t4TJ2HmWw-F22fV3kh5nNW4tFlgx5QfqTbW5ZfV8s6-vFnMW4QPLMD31y7FcW7mCT055WHd9Zf82V5cY04?ref=dl-staging-website.ghost.io" rel="noopener">cleared</a>&nbsp;both Amazon’s and Google’s investments in Anthropic, but regulators continue to monitor such arrangements for violations of antitrust laws. Microsoft and OpenAI face a similar&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLfH5nR32W50kH_H6lZ3pCW1Z7Q5K97Rm2bW4rMRMT20WRzpW2jLrLs6_NQtLW3LjyNl7Yf0FlW5z6DkM95g0KhW8N8Pfr5h73hQW2-qFw28HtRH_VXyxpc4D3RPPW3vQCwX1-txqDVjTKZp6mjV8JW1hCtLk2SQsyzW2gjRGW6rtLKqW8NVhX-4D8NcmW8nXSTQ1XFPrCW6NGdzv5tkd2lV2hdSv7M4DF6N8ynt2TmnGLPW2MC3f03zygMVW90NYT13tT21SW2Qbr7Z3BsY7KW2b0D2l6l8g1-W5LNNnX1FT2d_W8PbNVP1wbFZSN3KVNGwgTTv7V_XplH4fTZH8W2qvsZS9jxzvsW8pG0Pk78znj8W1x7Q1t84fzKGVh_Nb51zFXFdW17VDg28PcV6qW4tVdSb5VqmXTW2QpnrH56cTGNf5-Cyd404?ref=dl-staging-website.ghost.io" rel="noopener">investigation</a>&nbsp;by the European Commission and U.S. Federal Trade Commission.</p><p><strong>Why it matters:</strong>&nbsp;The speed and skill required to build state-of-the-art AI models is driving tech giants to collaborate with startups, while the high cost is driving startups to partner with tech giants. If the partnership between Amazon and Anthropic lives up to its promise, Claude users and developers could see gains in performance and efficiency. This could validate Amazon's hardware as a competitor with Nvidia and strengthen Amazon Web Services’ position in the cloud market. On the other hand, if Claude faces any challenges in scaling while using Trainium and Inferentia, that could affect both companies' ambitions.</p><p><strong>We’re thinking:</strong>&nbsp;Does the agreement between Amazon and Anthropic give the tech giant special access to the startup’s models for distillation, research, or integration, as the&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhs3qgyTW8wLKSR6lZ3kxW5lKtjm5HcvNZN2-NkD9H4LrXW1chqR11vg1CxW89H8Qt2G1X8qW5ntL8q4F9HlVW90CMCX1z52JbW8nTWJm3GJ4NSW26Cmlv5qSympW80ymVK1XcxDRW1kXxPF2nJhs4W12GCyl9ccc2qW3wslPl13JK5_W3bwSyp4zxw_WW4l0Gmx8L8rXfW5gxxsX1cwGw7N6sbRLYgYkw6W2ftmWX3TMgHxW2PyNRx5lD2xhW5QJVLz7xnFXDW97sgpy7jh-VnW42jbVR7jvlGhVmXKNJ5ckyZlN8fNc52gp_8-W5N_St06FdlSHW2NnN7M5LGf_wW75JRLR1c9vDSW8kJ0Tp5xqsglW4Ww_Q_23Zjnkf6Z0cLb04?ref=dl-staging-website.ghost.io" rel="noopener">partnership</a>&nbsp;between Microsoft and OpenAI does? The companies’ announcements don’t say.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.gif" class="kg-image" alt="Grounding DINO animation depicting object detection with bounding boxes on images." loading="lazy" width="600" height="336" srcset="https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.gif 600w"></figure><h1 id="object-detection-for-small-devices">Object Detection for Small Devices</h1><p>An open source model is designed to perform sophisticated object detection on edge devices like phones, cars, medical equipment, and smart doorbells.</p><p><strong>What’s new:</strong>&nbsp;Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, and colleagues at the International Digital Economy Academy introduced&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3nVW4Vvdhh5-mlZ8W5wVGDR7Xd0bCN4ZXLjTFhxj0VQskWS3j02c-W66_KgG4-0zsrW8Vjd-451b2WpVdff0M1d0YkBW395-ND4YQ5DgW4K6QH12bTbRWW6z71hZ3TpcCgW5ff5c55F5w5dW4vDX4n8ZDbJ7W6Cyjrc5XZ-WNW3KfBWF6p8qkvW7CcH1g82kCPpVmQ0BD7CGl0dW7vC2W93WlbZ2W6Hdj7x1gwrvHW5JDnVH4BmP-TW2jx2Gg78z2FhW4P7HDs42cRbrW23L15p5xHf8xf7dwgMW04?ref=dl-staging-website.ghost.io" rel="noopener">Grounding DINO 1.5</a>, a system that enables devices with limited processing power to detect arbitrary objects in images based on a text list of objects (also known as open-vocabulary object detection). You can download the code and weights&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgT3qgyTW7lCdLW6lZ3p4W1wsY108YRqTVW5R6LMs7Ms-zhW4BdD9228dtm8W6c0DN-2yf2RQVJWpkh3PRrZZW5nRKqC4wgDyxW8dQcKp5R0LBLW7RKYfZ25T5YmW4yKRXV4j9T7sVSKznq78CT7rW3JqC8c2GbDzkW8BkYGZ5bqf0fV_XrCZ1Q-HYTW2-2ZKR2X-r_3W2LFTQm5mHdm1W843mV57N-xnRW8ZVWDD8n_N7YW7k-5fj27c5P2W78l-Ss9hFln8MmSCW3Dk6jDW8YZ0tb7gr5kmW6mk00W7pkB7PW83wVhh5BhZTnW24lb2G8fZRwFf1ft9C-04?ref=dl-staging-website.ghost.io" rel="noopener">here</a>.</p><p><strong>Key insight:</strong>&nbsp;The original&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3nvW7Wx6fS4KTfjbW2fT9GQ91cBmdW26BB3L5r-nwFN72QbSNR_Rs-W6B1xgn6nQDQqW5BsGTm27J51tW7nJpWl48m6thW3b0m1m6Dty5fW4fNrJc8Sm53hN1Qz_t1JN0l5W5zQdZn4nVQC2W74P-nL8SLQc9W5YwNyS3vWmHCW2kkYkP71CZm-W6krq9M6H4fQLVz_7Vw2MQ-1MW3SqSPD10fXqnW1PhtM82TbbylW78jxZG7nCh9-W2Zbh9-6GwrXHW4Gh7Vq4gSHG0VKN8xv5CDxGFf26z6T404?ref=dl-staging-website.ghost.io" rel="noopener">Grounding DINO</a>&nbsp;follows many of its&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3p8W860YQB1BR35nW7B4Xsk26jkJyW1VXQDJ3PptJxW7wWT7N1lx135N6w4S1c5zRwRW1_jmQT14BnrvVP_vy360H__-W6Cn-xG2MqNC2W4R4Rxc2Hhp_sW3-0P_w9gsQgbW3gHFq49dvL3QW8HFKx-34NCYBW1tXN8Z1s9BNvVGV6f38Fyz3DN4hZrpH-m009W8NKmdp4-B0l7W1Vf_w83LjM6wW1QMQ0J7GmpBgW7ZvmLz5_XPxyW1B6MgF6J00mGW2vKsqM7mtf9-W6Kb9Cn8pKS4Lf6m7bgd04?ref=dl-staging-website.ghost.io" rel="noopener">predecessors</a>&nbsp;by using image embeddings of different levels (from lower-level embeddings produced by an image encoder’s earlier layers, which are larger and represent simple patterns such as edges, to higher-level embeddings produced by later layers, which are smaller and represent complex patterns such as objects). This enables it to&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3nRMQ7crLzpzX8N6qQZCt80DM8W4TVXRG6JrMWNW4t6x6P7fldx8N93yYggn26hdW2-n-pJ22WrjLW5y65c64hKVJfW2G8ZjK3JG-n1W20sgMS3nLYDtVtXsbC1BGL2HW4b2k-s64FyW9W1zq26f1VBVY6W5TZLqZ6yYmG6W2xjwNz8hC5CRW3rhh6T4M3MFwW3TdrGH4zlL_WW7XvPlP62x3B1W5YhzTx2TMZlTV5MYXT3gBzY6VYJmj_1NRrkdW5QDrQw5VT59pW4XVJD13tkx0tf4_FXTM04?ref=dl-staging-website.ghost.io" rel="noopener">better detect objects at different scales</a>. However, it takes a lot of computation. To enable the system to run on devices that have less processing power, Grounding DINO 1.5 uses only the smallest (highest-level) image embeddings for a crucial part of the process.</p><p><strong>How it works:</strong>&nbsp;Grounding DINO 1.5 is made up of components that produce text and image embeddings, fuse them, and classify them. It follows the system architecture and training of Grounding DINO with the following exceptions: (i) It uses a different image encoder, (ii) a different model combines text and image embeddings, and (iii) it was trained on a newer dataset of 20 million publicly available text-image examples.</p><ul><li>Given an image, a pretrained EfficientViT-L1 image encoder produced three levels of image embeddings.</li><li>Given the corresponding text, BERT produced a text embedding composed of tokens.</li><li>Given the highest-level image embedding and the text embedding, a cross-attention model updated each one to incorporate information from the other (fusing text and image modalities, in effect). After the update, a&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3mkW86m8gP4Cw4tgW3qzvKG4TyV-cW69XgcQ1skZ5pN3C_G7HZ-QzXN6VDDrRzKL0LM1krgm2CS91W4gTwXr3pnrwCVC14C95dmh3vW40yrth4JBqkLW3Xf3Wq4Swv0HW1sxz7h28Y864W6DC05x1rDc0cV_lbrk4r2WpYW2PdN1988JnvCW7sD-ST1hPlNDW7XvslL6d0YpxN32r-GcQqFh5W3RsqM46f1b_FW8mltLX7_y8BJW70Z2XX3kF8ykW22XsZm9lSKpZW32SnfJ6yQxY-f3ch9hv04?ref=dl-staging-website.ghost.io" rel="noopener">CNN-based model</a>&nbsp;combined the updated highest-level image embedding with the lower-level image embeddings to create a single image embedding.</li><li>Grounding DINO 1.5 calculated which 900 tokens in the image embedding were most similar to the tokens in the text embedding.</li><li>A cross-attention model detected objects using both the image and text embeddings. For each token in the updated image embedding, it determined: (i) which text token(s), if any, matched the image token, thereby giving each image token a classification including “not an object” and (ii) a bounding box that enclosed the corresponding object (except for tokens that were labeled “not an object”).</li><li>The system learned to (i) maximize the similarity between matching tokens from the text and image embeddings and minimize the similarity between tokens that didn’t match and (ii) minimize the difference between its own bounding boxes and those in the training dataset.</li></ul><p><strong>Results:</strong>&nbsp;Grounding DINO 1.5 performed significantly faster than the original Grounding DINO: 10.7 frames per second versus 1.1 frames per second running on an&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhs3qgyTW8wLKSR6lZ3pzW79n97y8gcj2dW6wKzCT5jLzGMW2kxw_w32PR0KW3WGGZh1rJQS8VF-v-J7D7qQQW8dwP2T3LjLxYW2NZB_c21fD3sW602BzD5yLT4TW7-f3BL6ybG7gW7V20Yh6yJW4GN6rp2ycy8P8hW3tcjX-8wk2LzVyj9YT4HSctnW8fYJ6980GxWpW5tD-pw3m-N37N99B19CFHY9TW2PN-d66y8PYMW1LX8FZ5DTddXW2RBp869bcqqTW4KWrJw6S6YmqW5ptbBk8CQST2W72BSFK7dZxkKW3nFcGS2JnpCPW6JRRB96FPxkvW1hst2R3XPKtrW258SYc3HMyxwW8Cbz9y6WqjnTW4qNBnY1QgBM3f2QH1Zb04?ref=dl-staging-website.ghost.io" rel="noopener">Nvidia Jetson Orin NX</a>&nbsp;computer. Tested on a&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3nYW8Kp9rZ6FZbSzN3449C5PZC0DW6TVzn36WX5yNW41-Ltv2GNSd4W1PfBsw73qCs4N7v_lqXLFk2JW4DXpMD95Jf_MW43zYQP3ZtCPWW6dqbTl3Kp74JVygsdl8pmkb_W6hnXdr4LkhrFW71yCNy5wrgPxW2L0fk31RR3DxW8gP-077FGbhLW1w9n498qKbgLW47QGKm2jLWkDW7Lv6mK7t4vhgW4mnpdz4ZYJY6VfMZhr1MljvvW5TlhJq7XlN-CW98_8Dj4fzY-3W5_XX1v8Ntrrsf2YZLmv04?ref=dl-staging-website.ghost.io" rel="noopener">dataset</a>&nbsp;of images of common objects annotated with labels and bounding boxes, Grounding DINO 1.5 achieved better average precision (a measure of how many objects it identified correctly in their correct location, higher is better) than both Grounding DINO and&nbsp;<a href="https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3mfVXmDY55HBNmpVcMzXt89k0SGW3ZhxJP6CRnjtW11fZCQ1VdCVcW1jvJjv1NcBtlW2Nrh-87pWWCbW20q_L180RfHyW9lJ5qw3RNX73W3zHWNb8j8k_1W8P0-bn3dY3Q-W4zyQnt1YRrZbW5bn5sV8wQNpyW3m7T5s6_NvfgW8rv-_X6D-Q83N8yZ7Kw4BnSbW48TBhN706ldNW10WKBb7_YjS5W41bRCQ7XqYYCVY-M-z3Txbx0W7DSKYV5-rszpW4VLWk84w60-4W8V_SnY5kkYxZf1v28kv04?ref=dl-staging-website.ghost.io" rel="noopener">YOLO-Worldv2-L</a>&nbsp;(a CNN-based object detector). Grounding DINO 1.5 scored 33.5 percent, Grounding DINO 27.4 percent, and YOLO-Worldv2-L 33 percent.</p><p><strong>Why it matters:&nbsp;</strong>The authors achieved 10 times the speed with just a couple of small changes (a more efficient image encoder and a smaller image embedding when performing cross-attention between embeddings of images and texts). Small changes can yield big results.</p><p><strong>We’re thinking:</strong>&nbsp;Lately model builders have been building better, smaller, faster large language models for edge devices. We’re glad to see object detection get similar treatment.</p></div><aside style="grid-column:main-end / wide-end" class="flex-col hidden pl-10 lg:flex"><div class="relative shadow rounded-lg overflow-hidden hover:shadow-sm transition-shadow" data-sentry-component="Advertisement" data-sentry-source-file="Advertisement.tsx"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:200%"></span><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" class="rounded-lg" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" loading="lazy" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="rounded-lg" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=3840&amp;q=75 3840w" src="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2024%2F08%2FVertical-side-banner-ads-5.png&amp;w=3840&amp;q=75"/></noscript></span><a href="https://bit.ly/3YTVMir"><div class="absolute inset-0" data-gtm-event-title="Generative AI for Software Development"></div></a></div></aside><footer class="mt-8 lg:hidden"><div class="flex flex-col items-start items-start " data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-277/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-277/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-277/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></footer></div></article><div class="py-16 bg-slate-50"><section id="subscribe" data-sentry-component="CtaNewsletter" data-sentry-source-file="CtaNewsletter.tsx"><div class="container--boxed relative"><div class="text-center"><h2 class="text--l2 text-slate-900">Subscribe to The Batch</h2><p class="text-base lg:text-lg text-slate-500 mt-3 max-w-md mx-auto">Stay updated with weekly AI News and Insights delivered to your inbox</p></div><div class="flex flex-col items-center mt-9"><div><div id="reactHubspotForm72" style="display:none"></div><div class="flex items-center justify-center"><svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-brand-teal" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" data-sentry-element="svg" data-sentry-component="Spinner" data-sentry-source-file="Spinner.tsx"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4" data-sentry-element="circle" data-sentry-source-file="Spinner.tsx"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z" data-sentry-element="path" data-sentry-source-file="Spinner.tsx"></path></svg></div></div></div></div></section></div></div></main><footer class="py-16 bg-brand-teal-900" data-sentry-component="Footer" data-sentry-source-file="index.tsx"><div class="flex flex-col items-center justify-center text-center container--boxed"><div class="max-w-[220px] flex items-center justify-center text-white"><svg viewBox="0 0 272 34" fill="none" xmlns="http://www.w3.org/2000/svg" width="272" height="34" aria-label="DeepLearning.AI" data-sentry-element="svg" data-sentry-component="DLAILogo" data-sentry-source-file="DLAILogo.tsx"><g fill="currentColor" data-sentry-element="g" data-sentry-source-file="DLAILogo.tsx"><path d="M56.775 16.108c0 6.206-4.252 10.005-10.747 10.005H39.42V5.961h6.608c6.495 0 10.747 3.924 10.747 10.147zm-10.747 7.306c4.778 0 7.337-2.724 7.337-7.306 0-4.581-2.56-7.452-7.337-7.452H42.73v14.758h3.298z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M66.97 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.259-8.226 8.006-8.226c4.571 0 7.804 3.159 7.804 7.856.007.535-.027 1.07-.103 1.6H62.412c.233 2.638 2.123 4.232 4.57 4.232 2.038 0 3.173-.988 3.786-2.234h3.582c-.915 2.802-3.448 5.032-7.38 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.484-4.03-2.24 0-4.044 1.525-4.394 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M84.938 26.371c-4.601 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.792 9.792 0 01-.116 1.6H80.367c.233 2.638 2.128 4.232 4.57 4.232 2.042 0 3.177-.988 3.786-2.234h3.582c-.902 2.802-3.435 5.032-7.367 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M104.917 9.885c4.221 0 7.541 3.245 7.541 8.166 0 4.92-3.32 8.32-7.541 8.32a7.277 7.277 0 01-3.095-.664 7.248 7.248 0 01-2.516-1.914v9.915h-3.319v-23.57h3.32v2.347a7.004 7.004 0 015.61-2.6zm-.729 2.871c-2.473 0-4.86 1.943-4.86 5.364 0 3.42 2.387 5.39 4.86 5.39 2.473 0 4.894-2.02 4.894-5.46 0-3.437-2.391-5.303-4.894-5.303v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M119.074 5.961v17.484h6.841v2.668h-10.16V5.961h3.319z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M135.584 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.927 9.927 0 01-.116 1.625h-12.258c.233 2.639 2.128 4.233 4.571 4.233 2.041 0 3.176-.988 3.785-2.235h3.582c-.902 2.777-3.435 5.007-7.367 5.007zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M153.116 9.885a6.864 6.864 0 013.088.633 6.839 6.839 0 012.475 1.946v-2.325h3.349v15.974h-3.349v-2.38a6.906 6.906 0 01-5.611 2.638c-4.165 0-7.514-3.39-7.514-8.32s3.353-8.166 7.562-8.166zm.699 2.87c-2.473 0-4.86 1.853-4.86 5.304 0 3.451 2.387 5.45 4.86 5.45 2.473 0 4.864-1.938 4.864-5.39 0-3.45-2.361-5.372-4.864-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M169.72 26.114h-3.319V10.139h3.319v2.325c.928-1.595 2.534-2.579 4.804-2.579v3.438h-.863c-2.447 0-3.958 1.014-3.958 4.405l.017 8.386z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M188.966 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.044 1.504-4.044 4.435v8.922h-3.319V10.14h3.319v1.826a6.16 6.16 0 014.774-2.08c3.755 0 6.582 2.347 6.582 6.812v9.416h-3.293v-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M196.049 5.905a2.105 2.105 0 011.309-1.931 2.117 2.117 0 012.294.458 2.102 2.102 0 01-1.48 3.588 2.109 2.109 0 01-1.509-.612 2.08 2.08 0 01-.614-1.503zm.431 4.234h3.319v15.974h-3.319V10.14z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M215.558 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.048 1.504-4.048 4.435v8.922h-3.337V10.14h3.319v1.826a6.192 6.192 0 014.796-2.08c3.755 0 6.56 2.338 6.56 6.803v9.425h-3.289l.018-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M229.538 9.885c2.62 0 4.571 1.216 5.559 2.579v-2.325h3.349V26.37c0 4.35-2.823 7.629-7.829 7.629-4.282 0-7.454-2.119-7.864-5.656h3.289c.496 1.655 2.274 2.785 4.575 2.785 2.559 0 4.48-1.569 4.48-4.758v-2.664a6.903 6.903 0 01-5.559 2.665c-4.222 0-7.571-3.392-7.571-8.321 0-4.93 3.337-8.166 7.571-8.166zm.699 2.87c-2.478 0-4.864 1.853-4.864 5.304 0 3.452 2.386 5.45 4.864 5.45 2.477 0 4.86-1.938 4.86-5.39 0-3.45-2.357-5.372-4.86-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M242.702 26.316a2.134 2.134 0 01-1.987-1.287 2.114 2.114 0 011.53-2.908 2.138 2.138 0 012.194.896c.235.349.361.76.361 1.18a2.096 2.096 0 01-1.289 1.956 2.11 2.11 0 01-.809.163z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M260.584 21.996h-8.477l-1.455 4.117h-3.453l7.251-20.2h3.846l7.247 20.2h-3.492l-1.467-4.117zM256.38 9.932l-3.341 9.365h6.612l-3.271-9.365z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M268.681 5.961H272v20.152h-3.319V5.961z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218zm.126-1.59c-3.734 0-6.76-3.207-6.76-7.16 0-3.954 3.018-7.16 6.75-7.16 3.734 0 6.76 3.206 6.76 7.16s-3.021 7.16-6.76 7.16h.01zm-.126-6.28c.729 0 1.44-.214 2.046-.617a3.67 3.67 0 001.356-1.646 3.652 3.652 0 00-.798-3.995 3.687 3.687 0 00-4.012-.794 3.679 3.679 0 00-1.653 1.35 3.655 3.655 0 00-.62 2.037c.002.971.39 1.902 1.08 2.59a3.698 3.698 0 002.601 1.076z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path></g></svg></div><nav class="mt-6 md:mt-10"><ul class="flex flex-wrap justify-center space-x-8 gap-x-8"><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/courses/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Courses</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/the-batch/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">The Batch</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/community/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Community</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/careers/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Careers</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/about/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">About</div></a></li></ul></nav><div class="flex mt-12"><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.facebook.com/1027125564106325" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsFacebook" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.instagram.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsInstagram" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.9 3.9 0 0 0-1.417.923A3.9 3.9 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.9 3.9 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.9 3.9 0 0 0-.923-1.417A3.9 3.9 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599s.453.546.598.92c.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.5 2.5 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.5 2.5 0 0 1-.92-.598 2.5 2.5 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233s.008-2.388.046-3.231c.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92s.546-.453.92-.598c.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92m-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217m0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://twitter.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsTwitter" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.linkedin.com/company/18246783" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsLinkedin" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.youtube.com/c/Deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsYoutube" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.01 2.01 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.01 2.01 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31 31 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.01 2.01 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A100 100 0 0 1 7.858 2zM6.4 5.209v4.818l4.157-2.408z"></path></svg></a></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"donations_enabled":false,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.121","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-277","id":"674772a5fdb1390001aebad3","uuid":"11fae4de-8385-47f2-a58a-97f49c3e396d","title":"DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection","html":"\u003cp\u003eDear friends,\u003c/p\u003e\u003cp\u003eHappy Thanksgiving! In the United States, this is a week when many reflect on their blessings and give thanks. Even as I reflect on how lucky I am to have food, shelter, family, and friends, I think about those who have much less and what we can do to help them.\u003c/p\u003e\u003cp\u003eLast week, I spoke with a woman who had been severely physically abused by her husband. She showed me pictures of her face from a few years ago, which had a bloodied sequence of tears down the middle. She also showed me scars left by cigarette burns inflicted by her husband, who told her these burns made her ugly so no other man would ever want her. She is no longer with her husband but continues to struggle. Her phone is badly cracked and barely holds a charge. Without a high-school degree, she has struggled to find a job and is surviving by staying on the couch of a friend. As winter approaches, they keep their place chilly to save the cost of electricity.\u003c/p\u003e\u003cp\u003eWorking in AI, I am fortunate to interact with many of the smartest and most capable technology and business leaders in the world. But both at home and when I travel, I try to meet with people of a broad range of backgrounds, because ultimately I want to do work that helps people broadly, and this requires that I understand people broadly. When you go to a grocery store and see someone put down a $5 carton of eggs because it is too expensive, and hear them think through how to explain to their kids why they’re skipping eggs that week, it gives you a deeper appreciation for why a $1.50/hour raise can be life-changing for many people.\u003c/p\u003e\u003cp\u003eWhile I can try to help out individuals here and there, technology is advancing rapidly, and this gives me a lot of optimism for the future. Technology remains the best way I know of to help people at scale through providing better education, career guidance, healthcare, personal safety, healthier food, or other things needed to support thriving.\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35-.jpg\" class=\"kg-image\" alt=\"Cornucopia overflowing with fruits and vegetables.\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/unnamed--35-.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/unnamed--35-.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35-.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eI am optimistic about the future because I see so many ways life can be so much better for so many people. I feel blessed that, when my kids or I are cold, we have warm clothing, and when we are hungry, we have a working car to drive to the grocery and buy fresh food. I feel blessed that, rather than using a badly cracked cellphone, I have a modern laptop and a fast internet connection to do my work on.\u003c/p\u003e\u003cp\u003eAs a child, my father taught me the aphorism “\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLh83qgyTW7Y8-PT6lZ3mYW6YGmQT8LqGGcW8g5f2v4KNWtvW3VTsh097sTzmW72rfsM1Kt6WcW3qDX_s4TN-fHW7rDs5X3CN47MVjVhm84Nsrk2W4MCCR13TmK-mW6pMzFq8gSK5-W2Th3Sk8gzPM5W3m8ZgL9kntxNW23w3NT55xYnSW2r8QvV3Xg1JnW1qfNc74bb8y6W2MSVwY6H9yZsN15PW2GCX801W2BlX8p6NwmmhW1rTfF74SxJ8zW3yT29D6Scst8W1l383v238P6sW8jVhr86bMf0hW5RGt2y6b1D2WW81WM533lT908MDLXWSxXcJ-W4LxCnM4yC2S-VSjLHx2sB2k4f5tVDs-04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ethere but for the grace of God go I\u003c/a\u003e” to recognize that, in even slightly different circumstances, I might have ended up with much less. Having worked on many software products, I know that, to make good decisions, I have to understand the people I hope to serve. This is why I continue to routinely seek out, speak with, and try to understand people from all walks of life, and I hope many others in AI will do so, too.\u003c/p\u003e\u003cp\u003eI see so many people in the AI community building things to make the world better. I am thankful for what the AI community has already done, and I look forward to continuing to build and serve others together.\u003c/p\u003e\u003cp\u003eKeep building!\u003c/p\u003e\u003cp\u003eAndrew\u003c/p\u003e\u003ch2 id=\"a-message-from-deeplearningai\"\u003eA MESSAGE FROM\u0026nbsp;DEEPLEARNING.AI\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003ca href=\"https://www.deeplearning.ai/short-courses/ai-python-for-beginners/?ref=dl-staging-website.ghost.io\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/11/2--9-.png\" class=\"kg-image\" alt=\"Promo banner for \u0026quot;AI Python for Beginners\u0026quot;\" loading=\"lazy\" width=\"2000\" height=\"1044\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/2--9-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/2--9-.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2024/11/2--9-.png 1600w, https://dl-staging-website.ghost.io/content/images/2024/11/2--9-.png 2042w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/a\u003e\u003c/figure\u003e\u003cp\u003eGet started coding in Python with\u0026nbsp;\u003cem\u003eAI Python for Beginners\u003c/em\u003e, a four-part course led by Andrew Ng. Build projects from the very first lesson with real-time support from an AI assistant. Complete the course and bring your ideas to life!\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/short-courses/ai-python-for-beginners/?ref=dl-staging-website.ghost.io\" rel=\"noreferrer\"\u003eStart today\u003c/a\u003e\u003c/p\u003e\u003ch1 id=\"news\"\u003eNews\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--23-.png\" class=\"kg-image\" alt=\"Bar charts comparing performance of AI models across six tasks.\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/unnamed--23-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/unnamed--23-.png 1000w, https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--23-.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"reasoning-revealed\"\u003eReasoning Revealed\u003c/h1\u003e\u003cp\u003eAn up-and-coming Hangzhou AI lab unveiled a model that implements run-time reasoning similar to OpenAI o1 and delivers competitive performance. Unlike o1, it displays its reasoning steps.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;DeepSeek\u0026nbsp;\u003ca href=\"https://api-docs.deepseek.com/news/news1120?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eannounced\u003c/a\u003e\u0026nbsp;DeepSeek-R1, a model family that processes prompts by breaking them down into steps. A free preview version is\u0026nbsp;\u003ca href=\"http://chat.deepseek.com/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eavailable\u003c/a\u003e\u0026nbsp;on the web, limited to 50 messages daily; API pricing is not yet announced. R1-lite-preview performs comparably to o1-preview on several math and problem-solving benchmarks. DeepSeek said it would release R1 as open source but didn't announce licensing terms or a release date.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;DeepSeek-R1-lite-preview uses a\u0026nbsp;\u003ca href=\"https://x.com/phill__1/status/1859263165000729024?s=61\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003esmaller base model\u003c/a\u003e\u0026nbsp;than DeepSeek 2.5, which comprises 236 billion parameters. Like o1-preview, most of its performance gains come from an approach known as\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2408.03314?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003etest-time compute\u003c/a\u003e, which trains an LLM to think at length in response to prompts, using more compute to generate deeper answers. Unlike o1-preview, which hides its reasoning, at inference, DeepSeek-R1-lite-preview’s reasoning steps are visible. This makes the model more transparent, but it may also make it more\u0026nbsp;\u003ca href=\"https://techcrunch.com/2024/11/20/a-chinese-lab-has-released-a-model-to-rival-openais-o1/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003evulnerable\u003c/a\u003e\u0026nbsp;to jailbreaks and other manipulation.\u003c/p\u003e\u003cul\u003e\u003cli\u003eAccording to DeepSeek, R1-lite-preview, using an unspecified number of reasoning tokens, outperforms OpenAI o1-preview, OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, Alibaba Qwen 2.5 72B, and DeepSeek-V2.5 on three out of six reasoning-intensive benchmarks.\u003c/li\u003e\u003cli\u003eIt substantially outperforms o1-preview on\u0026nbsp;\u003ca href=\"https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eAIME\u003c/a\u003e\u0026nbsp;(advanced high school math problems, 52.5 percent accuracy versus 44.6 percent accuracy),\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2103.03874?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eMATH\u003c/a\u003e\u0026nbsp;(high school competition-level math, 91.6 percent accuracy versus 85.5 percent accuracy), and\u0026nbsp;\u003ca href=\"https://codeforces.com/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eCodeforces\u003c/a\u003e\u0026nbsp;(competitive programming challenges, 1,450 versus 1,428). It falls behind o1 on\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2311.12022?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eGPQA Diamond\u003c/a\u003e\u0026nbsp;(graduate-level science problems),\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2403.07974?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eLiveCodeBench\u003c/a\u003e\u0026nbsp;(real-world coding tasks), and\u0026nbsp;\u003ca href=\"https://huggingface.co/blog/yuchenlin/zebra-logic?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eZebraLogic\u003c/a\u003e\u0026nbsp;(logical reasoning problems).\u003c/li\u003e\u003cli\u003eDeepSeek reports that the model’s accuracy improves dramatically when it uses more tokens at inference to reason about a prompt (though the web user interface doesn’t allow users to control this). On AIME math problems, performance rises from 21 percent accuracy when it uses less than 1,000 tokens to 66.7 percent accuracy when it uses more than 100,000, surpassing o1-preview’s performance. The additional performance comes at the cost of slower and more expensive output.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;DeepSeek-R1 follows OpenAI in implementing this approach at a time when scaling laws that predict higher performance from bigger models and/or more training data are being\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/issue-276/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003equestioned\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;DeepSeek is challenging OpenAI with a competitive large language model. It’s part of an important movement, after years of scaling models by raising parameter counts and amassing larger datasets, toward achieving high performance by spending more energy on generating output.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Models that do and don’t take advantage of additional test-time compute are complementary. Those that do increase test-time compute perform well on math and science problems, but they’re slow and costly. Those that don’t use additional test-time compute do well on language tasks at higher speed and lower cost. Applications that require facility in both math and language may benefit by switching between the two.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--34-.gif\" class=\"kg-image\" alt=\"Robotic arms collaborating to fold a red garment on a table.\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--34-.gif 600w\"\u003e\u003c/figure\u003e\u003ch1 id=\"household-help\"\u003eHousehold Help\u003c/h1\u003e\u003cp\u003eA new generation of robots can handle some household chores with unusual skill.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Physical Intelligence, a startup based in San Francisco, unveiled\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2410.24164v1?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eπ0\u003c/a\u003e\u0026nbsp;(pronounced “pi-zero”), a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables. The company also\u0026nbsp;\u003ca href=\"https://www.nytimes.com/2024/11/04/business/dealbook/physical-intelligence-robot-ai.html?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eannounced\u003c/a\u003e\u0026nbsp;$400 million in investments from OpenAI, Jeff Bezos, and several Silicon Valley venture capital firms.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;π0 is a version of the pretrained\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2407.07726?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003ePaliGemma\u003c/a\u003e\u0026nbsp;vision-language model that has been modified for\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2210.02747?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eflow matching\u003c/a\u003e. (Flow matching is similar to diffusion, in which a model learns to remove noise from inputs to which noise has been added, and ultimately generates output by removing noise from an input of pure noise). A user supplies a text command, and the robot uses its sensor inputs to remove noise from a pure-noise action embedding to generate an appropriate action.\u003c/p\u003e\u003cul\u003e\u003cli\u003ePaliGemma comprises\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2303.15343?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eSigLIP\u003c/a\u003e, a vision transformer that turns images into embeddings; a linear layer that adapts the image embeddings to serve as input for the pretrained large language model Gemma; and\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2403.08295?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eGemma\u003c/a\u003e, which estimates the noise to be removed from a robot action embedding to which noise has been added.\u003c/li\u003e\u003cli\u003eThe authors modified PaliGemma as follows: (i) They adapted it to accept embeddings that represent the robots’ state and previous actions, and to generate embeddings that represent the noise to be removed from noisy robot actions. (ii) They added a vanilla neural network to the input to turn the current timestep into an embedding. (iii) They modified\u0026nbsp;Gemma to be a mixture-of-experts model:\u0026nbsp;One expert, or subset of weights, is the pretrained weights, which process image and text embeddings. The other is a new set of weights that process robot action embeddings.\u003c/li\u003e\u003cli\u003eThey pretrained π0 to remove noise from action embeddings. (Since π0 produces embeddings of the noise to be removed, removing that noise is as simple as adding the two embeddings.)\u003c/li\u003e\u003cli\u003eTraining data included the\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2310.08864?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eOpen X-Embodiment Dataset\u003c/a\u003e\u0026nbsp;and a proprietary dataset of 10,000 hours of robotic states (for instance, current positions of a robot’s joints), actions (for instance, motions of the robot’s joints), and an associated language command. The proprietary dataset included data collected from seven different robots (such as a single stationary robot arm to two robot arms mounted on a mobile base) and 68 tasks (for example, folding laundry, making coffee, or bussing a table).\u003c/li\u003e\u003cli\u003eAfter pretraining, the authors fine-tuned π0 to remove noise from action tokens in 15 further tasks, some of which were not represented in the pretraining set. These tasks improved the model’s ability to follow more detailed instructions and perform multi-stage tasks such as packing food into a to-go box.\u003c/li\u003e\u003cli\u003eAt inference, given the robot’s camera view of the surrounding scene, SigLip embeds the images. A linear layer projects the resulting embeddings to fit Gemma’s expected input size and data distribution. Given the images, text command, robot’s state, current timestep, and 50 noisy action tokens (starting with pure noise), Gemma iteratively removes noise. To complete longer tasks, the process repeats: The robot takes more images of the surrounding scene and retrieves the robot’s state, which π0 uses to generate further actions.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u0026nbsp;\u003c/strong\u003eπ0 outperformed the open robotics models\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2406.09246?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eOpenVLA\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2405.12213?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eOcto\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2304.13705?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eACT\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2303.04137?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eDiffusion Policy\u003c/a\u003e, all of which were fine-tuned on the same data, on all tasks tested, as measured by a robot’s success rate in completing each task. For example, using a single robotic arm to stack a set of bowls of four sizes, π0 completed about 100 percent on average. Diffusion Policy completed about 55 percent, ACT about 45 percent, and OpenVLA and Octo below 10 percent. Across all tasks, π0 completed about 80 percent on average, while Diffusion Policy completed about 35 percent on average.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eYes, but:\u003c/strong\u003e\u0026nbsp;The robot occasionally makes\u0026nbsp;\u003ca href=\"https://www.wired.com/story/physical-intelligence-ai-robotics-startup/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003emistakes\u003c/a\u003e. In one video, it puts too many eggs into a carton and tries to force it shut. In another, it throws a container off a table instead of filling it with items.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;Commercial robotics appears to be undergoing a renaissance. Skild\u0026nbsp;\u003ca href=\"https://www.skild.ai/blogs/announcing-our-300m-series-a?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003eraised\u003c/a\u003e\u0026nbsp;$300 million to develop a “general-purpose brain for robots.” Figure AI\u0026nbsp;\u003ca href=\"https://www.prnewswire.com/news-releases/figure-raises-675m-at-2-6b-valuation-and-signs-collaboration-agreement-with-openai-302074897.html?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003esecured\u003c/a\u003e\u0026nbsp;$675 million to build humanoid robots powered by multimodal models. Covariant, which specializes in industrial robotics,\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/amazon-strengthens-logistics-and-robotics-with-new-ai-partnership/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003elicensed\u003c/a\u003e\u0026nbsp;its technology to Amazon. (Disclosure: Andrew Ng is a member of Amazon's board of directors). OpenAI\u0026nbsp;\u003ca href=\"https://techcrunch.com/2024/11/04/metas-former-hardware-lead-for-orion-is-joining-openai/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99\" rel=\"noopener\"\u003erenewed\u003c/a\u003e\u0026nbsp;its robotics effort after\u0026nbsp;\u003ca href=\"https://venturebeat.com/business/openai-disbands-its-robotics-research-team/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--PChA-PmMEKM6nNL57xElvflnwlDxDV5Sq2kxmxwYJVU8kg0gGwVFMbTJoU5HEeqGEgV99#:~:text=In%20a%20statement%2C%20an%20OpenAI,the%20team%20on%20other%20projects\" rel=\"noopener\"\u003edismantling\u003c/a\u003e\u0026nbsp;its robotics department in 2020.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;Robots have been slow to benefit from machine learning, but the generative AI revolution is driving rapid innovations that make them much more useful. Large language models have made it possible to command robots using plain English. Meanwhile, the team at Physical Intelligence collected a dataset of sufficient size and variety to train the model to generate highly articulated and practical actions. Household robots may not be right around the corner, but π0 shows that they can perform tasks that people need done.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;One of the team members compared π0 to GPT-1 for robotics — an inkling of things to come. Although there are significant differences between text data (which is available in large quantities) and robot data (which is hard to get and varies per robot), it looks like a new era of large robotics foundation models is dawning.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--37-.jpg\" class=\"kg-image\" alt=\"Illustration of a person holding a box with network nodes emerging from it.\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/11/unnamed--37-.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/11/unnamed--37-.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--37-.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"ai-power-couple-recommits\"\u003eAI Power Couple Recommits\u003c/h1\u003e\u003cp\u003eAmazon and Anthropic expanded their partnership, potentially strengthening Amazon Web Services’ AI infrastructure and lengthening the high-flying startup’s runway.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Amazon, already a significant investor in Anthropic,\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhs3qgyTW8wLKSR6lZ3pMW6HNTV-67pwbFW2wjk-P6XfR_JW5vFSjF87Q2h9W5JXyGt7LnQ_QW86YWlF19J--rW5wYMdW5pVD0hW8gKK7Y5SXGymW69t0zD3WwpQ1N506721chJGsW1NPGF-8N3BylVvL8Nw1kCgXyW4XlyqF7gGBQnV5l9293-wBbCVsJDqn76qq4KW7t5rhf1knVpGW1S0FbG8Dv4k6W3ND7tg18L3X5W8R-k_Q9bK2RRW64f7Q13tBcZ8W5FfyyJ3d2_9ZW1jhzH72jwcg2W6Szvpr6f7fD_W1qQjHv2D-npYW4cNrph8Kwq94W3yJX673gJ1MFW998vq287vLjpW36kX3J5LYGrdW2Cqgnf2xp9vxf1KmzHM04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eput\u003c/a\u003e\u0026nbsp;another $4 billion into the AI company. In exchange, Anthropic will train and run its AI models on Amazon’s custom-designed chips. (Disclosure: Andrew Ng serves on Amazon’s board of directors.)\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;The new round brings Amazon’s investment in Anthropic to $8 billion (though it remains a minority stake without a seat on the startup’s board). The deal extended the partnership in several ways:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAWS becomes Anthropic’s primary partner for training AI models. Anthropic will train its models using Amazon’s\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgT3qgyTW7lCdLW6lZ3q3W8qfY271PmLKXW44_hg97yMzkpW2MwdB83_r3fzW6X9vx58XHG5mW5SvK1y2VWrcYW7s_d2k7SMwrwW7sJjCF3nrXNrN2TbDf5mVZR7W1lnzc54-xWcJW2QmjHR8y5lYXW1yGKBK5MplbjW4Fndpw2VLp8fW3Lpc1n1qLyvMW6Rj7zF4YD8JWW4D-7R549PvcBW7p8W125fZ6nfW7ZV17635S3LWV5wfB053zhv0W2tcf5f43_vMtW2zGPv41hqvFBW7xGZlm8m52LhW7K2Vj68vMlc1W8BG8SW3Q-R5tW6D6W8t5mHkqbf5g6MDv04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eTrainium\u003c/a\u003e\u0026nbsp;chips, which are designed for training neural networks of 100 billion parameters and up. Amazon executives previously\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhM3qgyTW95jsWP6lZ3lYN5kYHjKwPRgkN7m-X7LKXzW9W7KRd_t7gD5Q6W5Mp-kr7PJNQjW7TCV7J4lcHYJW79lWfC4XgxvHW8F2Nlx2lfRD3W5N1FzZ2xQcmBW4t6FfJ6Nh5y7W1gcYBY5vV7pCW8rrnvr5y7ZzkW71TXN01754wsW4mpWzZ1mM91CV3qf6Y2CTx30W9fxtxZ1CRwXkW3SMk4G7bxZ2wN8N0SsmpR0bWW1wmHs16lZ_K6W94G4cC11JMTYW7Z4Mrw3_Lp8sW2mVd9X68b_yDW4KJBqD6b2NygW9fnkkj6SZkLgW4n2Bpy39b9zHW3j9NvD3QwC2mMV9TY7kw1bpW70NzV76YqLv8W8mvC206Fh-_HW18r5Hl7Lh_wVW42Vby844G__2f5nqkQv04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eclaimed\u003c/a\u003e\u0026nbsp;that these chips could cut training costs by as much as 50 percent compared to Nvidia graphics processing units (GPUs).\u003c/li\u003e\u003cli\u003ePreviously Anthropic ran its Claude models on Nvidia hardware; going forward, Anthropic will run them on Amazon’s\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLh83qgyTW7Y8-PT6lZ3kFW8jt3sY7zp5PSW1w45dt4kB-w3W425mnP6MK-ysW19nfXS3N2PLGW83qLxt6qtmfYW24F3ZP8J7m_MW84NXPk1zjF0bV7dXfD7zbJPsW11W3rl8BSl7rW746S617DQwnNN7vg6VCP6QL8W518g1Y1VwqTsW4TykFW49hSNNW50BLYN4tJHzKW6QPLlj77k9Y3W1nfXFt3_dtyVW37Z_4223mkxXN61m335MpGzXW4JNNxY5jDkrhN5n44zLd-z-BW6PpBZV1j6c-wW3hfxLt4Ph5tLW5vcG4C82Z50WN4_9JdDV8PXhW7x1SPm25-WpGVc07LT1v-Lxmf4v10NF04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eInferentia\u003c/a\u003e\u0026nbsp;chips, according to\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLfH5nR32W50kH_H6lZ3lLW8g4s_R2Py_B8W1Vssm41-zZ4xW2jH8k06NzYrxW3hwSKy82SKltW55Cmf31gV_YhW40WvWJ9c6cTLW3hnh5n5nssNtW38l-PN3S1dF5W1rf1dD39RJRfW4Cm_P34KS3RXW6yjddS80JbxdW19sdbt7Fh05rW1B8wbM6ckvqpW2rR0Fc75PZNPVZr08L8F8wwNW6w_3Bv4ZhkqCW18jtpF7l_Z7nW63ZPTR7_TFpGW2k1DG-5Hf14CW3R-X5W4CCyc2W8Xv3DF71yNY5W46xVYm1jqP29W5bs1xQ47lr9ZW5-BDzQ3_p9JBW3N2LbD5LN88KW7v-D628TN96_W9cJFQk7LJsnMW5Bqsc7623M_BF7XXzb-XgxWW6gJ_9j882j8VW6Wh9L13n-HshW69Q1nx51Jv9XdQCRmz04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eThe Information\u003c/a\u003e. Customers of Amazon Web Services will be able to fine-tune Claude on Bedrock, Amazon Web Services’ AI model platform.\u003c/li\u003e\u003cli\u003eAnthropic will contribute to developing Amazon’s\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgT3qgyTW7lCdLW6lZ3kJW613Z0G1BRY7wW8nhX5q28Gb0pW6ncRVb1VbS0qW5zq_c38xyP6MW6rrnsY2LjjdGW2lLwJP333bcKN6LDMw-tjZlvW1VjXKy2yrMWfW5_JmfW31-TpLN1vvS9Qw4-BcW87v4-g6T8kynW71K1tK7TJ75WW3TDBLm6c40zpN3KdZ36HM13gW83NTLQ1Z_GLMW1410v_1Wtcw8W3HRc0G4qLcD4VQMq9G2XDbyTW20_fLD7y7MXyW1PX38j806NVNW1QTHG376m6J3VgG67c5CVXj5W58nQbP3gpKmxW33ps424cGpsnf10SRWs04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eNeuron\u003c/a\u003e\u0026nbsp;toolkit, software that accelerates deep learning workloads on Trainium and Inferentia chips.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;In November, Anthropic\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLfH5nR32W50kH_H6lZ3nyW7VKlGX8HJP3nVVn6g44sQpgRW6T-tk47dMhnLW4JbD3d7CYqm-W8CPq7-1lN930W9cHzql54RjznW3kXGdR7WgxnhW5-HPl0318zdYVCWB4f6n5GcFW4VH9lP8QKqm5W88PfCZ6WV7rNVKXV4X8-Z8kmW60dc1X6nfgM2W5wRjM86R5wzgVGs3Np6sdcBDW22177j8Dxx1yW6jPJdn3f_GKzW6z4mCC6TmzsJW3XKxZQ6z5bRbW802bxG1vgYYpW86bcfZ8YdFS8Mfv6kdgKXDTW52rjXP4kRHlKN8PFPQwC1J1wW5qLZnS2MnbtMW2P61hd8P4YlPW57qFPc37qHSqN7DQ2zxc58MjW3pKWj_6xfbpJW3gdl-l6Tz_WcW3slM8Z7S29T-W4v_khz22mq-Lf854bj804?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eagreed\u003c/a\u003e\u0026nbsp;to use Google’s cloud-computing infrastructure in return for a $2 billion investment. The previous month, Amazon had\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLfH5nR32W50kH_H6lZ3mNW78vvVt3cvpV6W4x69R91B0Z1fW19wGGp6DPr8nV6Fh2D95sdpgN41SKXD7DknLW1sd3z16B_RKbW3-MHkp8CnjCZW6tdvBv52WHMXT5WSC40-1kHW7j0zGG57jkmjW3Lzswx1dPgRMW8QjvW65TYpbvN81dVyjqDSZCW2P27q076yzhgW7cybyS6Y467kW1TbRTl81Q3vdW4Wr0sB8wvbVyW8n32QD3rc39fW6cc4Xs8GzdShN7DwM0_wWjC8W1dVSHW8Xv68kW5kDC4D6JfVXwW535YRJ34ZzW1W3Qr8VY68h-F5W2gtwcp9kWLd9W7cR_jX21q1xzVFFJ7b1D-f4DN6B-1DJDj2ppW2tmBwK5mr5ZjW5p0g052Ls-NJN5hHBXTglZWxW7rKW-P1GzN_cf59WLJ604?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ecommitted\u003c/a\u003e\u0026nbsp;to invest as much as $4 billion in Anthropic, and Anthropic had made Amazon Web Services the primary provider of its models.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eYes, but:\u003c/strong\u003e\u0026nbsp;The UK’s Competition and Markets Authority recently\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhs3qgyTW8wLKSR6lZ3m-W5HsMVy8VTPg1W8tsD1S2P5bnBN81HcKlmzKHKW7zSGf51b73W3W5rQBtG3DxyGsW6gGyf14HVgp2N2pF3rS7zxmyW3rFCVj8gSv7FW2yRScb4dmvFnW91w_fs12B70XW48Hsjs6Wq-r_W1YYc9553srQmN6gdxzxsLbw3W5ZlCMT18FfKxW5wdxTk3ZXYJKW1C2YpN5F3d4BVrPZww8DvdNNW4mVV_b2pP9FwW4fX3pS1CxnBnW5PyLHq2wljHmW28dyGS3Py9FqW41fKv72dn6WGV1t4TJ2HmWw-F22fV3kh5nNW4tFlgx5QfqTbW5ZfV8s6-vFnMW4QPLMD31y7FcW7mCT055WHd9Zf82V5cY04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ecleared\u003c/a\u003e\u0026nbsp;both Amazon’s and Google’s investments in Anthropic, but regulators continue to monitor such arrangements for violations of antitrust laws. Microsoft and OpenAI face a similar\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLfH5nR32W50kH_H6lZ3pCW1Z7Q5K97Rm2bW4rMRMT20WRzpW2jLrLs6_NQtLW3LjyNl7Yf0FlW5z6DkM95g0KhW8N8Pfr5h73hQW2-qFw28HtRH_VXyxpc4D3RPPW3vQCwX1-txqDVjTKZp6mjV8JW1hCtLk2SQsyzW2gjRGW6rtLKqW8NVhX-4D8NcmW8nXSTQ1XFPrCW6NGdzv5tkd2lV2hdSv7M4DF6N8ynt2TmnGLPW2MC3f03zygMVW90NYT13tT21SW2Qbr7Z3BsY7KW2b0D2l6l8g1-W5LNNnX1FT2d_W8PbNVP1wbFZSN3KVNGwgTTv7V_XplH4fTZH8W2qvsZS9jxzvsW8pG0Pk78znj8W1x7Q1t84fzKGVh_Nb51zFXFdW17VDg28PcV6qW4tVdSb5VqmXTW2QpnrH56cTGNf5-Cyd404?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003einvestigation\u003c/a\u003e\u0026nbsp;by the European Commission and U.S. Federal Trade Commission.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;The speed and skill required to build state-of-the-art AI models is driving tech giants to collaborate with startups, while the high cost is driving startups to partner with tech giants. If the partnership between Amazon and Anthropic lives up to its promise, Claude users and developers could see gains in performance and efficiency. This could validate Amazon's hardware as a competitor with Nvidia and strengthen Amazon Web Services’ position in the cloud market. On the other hand, if Claude faces any challenges in scaling while using Trainium and Inferentia, that could affect both companies' ambitions.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Does the agreement between Amazon and Anthropic give the tech giant special access to the startup’s models for distillation, research, or integration, as the\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhs3qgyTW8wLKSR6lZ3kxW5lKtjm5HcvNZN2-NkD9H4LrXW1chqR11vg1CxW89H8Qt2G1X8qW5ntL8q4F9HlVW90CMCX1z52JbW8nTWJm3GJ4NSW26Cmlv5qSympW80ymVK1XcxDRW1kXxPF2nJhs4W12GCyl9ccc2qW3wslPl13JK5_W3bwSyp4zxw_WW4l0Gmx8L8rXfW5gxxsX1cwGw7N6sbRLYgYkw6W2ftmWX3TMgHxW2PyNRx5lD2xhW5QJVLz7xnFXDW97sgpy7jh-VnW42jbVR7jvlGhVmXKNJ5ckyZlN8fNc52gp_8-W5N_St06FdlSHW2NnN7M5LGf_wW75JRLR1c9vDSW8kJ0Tp5xqsglW4Ww_Q_23Zjnkf6Z0cLb04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003epartnership\u003c/a\u003e\u0026nbsp;between Microsoft and OpenAI does? The companies’ announcements don’t say.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.gif\" class=\"kg-image\" alt=\"Grounding DINO animation depicting object detection with bounding boxes on images.\" loading=\"lazy\" width=\"600\" height=\"336\" srcset=\"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.gif 600w\"\u003e\u003c/figure\u003e\u003ch1 id=\"object-detection-for-small-devices\"\u003eObject Detection for Small Devices\u003c/h1\u003e\u003cp\u003eAn open source model is designed to perform sophisticated object detection on edge devices like phones, cars, medical equipment, and smart doorbells.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, and colleagues at the International Digital Economy Academy introduced\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3nVW4Vvdhh5-mlZ8W5wVGDR7Xd0bCN4ZXLjTFhxj0VQskWS3j02c-W66_KgG4-0zsrW8Vjd-451b2WpVdff0M1d0YkBW395-ND4YQ5DgW4K6QH12bTbRWW6z71hZ3TpcCgW5ff5c55F5w5dW4vDX4n8ZDbJ7W6Cyjrc5XZ-WNW3KfBWF6p8qkvW7CcH1g82kCPpVmQ0BD7CGl0dW7vC2W93WlbZ2W6Hdj7x1gwrvHW5JDnVH4BmP-TW2jx2Gg78z2FhW4P7HDs42cRbrW23L15p5xHf8xf7dwgMW04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eGrounding DINO 1.5\u003c/a\u003e, a system that enables devices with limited processing power to detect arbitrary objects in images based on a text list of objects (also known as open-vocabulary object detection). You can download the code and weights\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgT3qgyTW7lCdLW6lZ3p4W1wsY108YRqTVW5R6LMs7Ms-zhW4BdD9228dtm8W6c0DN-2yf2RQVJWpkh3PRrZZW5nRKqC4wgDyxW8dQcKp5R0LBLW7RKYfZ25T5YmW4yKRXV4j9T7sVSKznq78CT7rW3JqC8c2GbDzkW8BkYGZ5bqf0fV_XrCZ1Q-HYTW2-2ZKR2X-r_3W2LFTQm5mHdm1W843mV57N-xnRW8ZVWDD8n_N7YW7k-5fj27c5P2W78l-Ss9hFln8MmSCW3Dk6jDW8YZ0tb7gr5kmW6mk00W7pkB7PW83wVhh5BhZTnW24lb2G8fZRwFf1ft9C-04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKey insight:\u003c/strong\u003e\u0026nbsp;The original\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3nvW7Wx6fS4KTfjbW2fT9GQ91cBmdW26BB3L5r-nwFN72QbSNR_Rs-W6B1xgn6nQDQqW5BsGTm27J51tW7nJpWl48m6thW3b0m1m6Dty5fW4fNrJc8Sm53hN1Qz_t1JN0l5W5zQdZn4nVQC2W74P-nL8SLQc9W5YwNyS3vWmHCW2kkYkP71CZm-W6krq9M6H4fQLVz_7Vw2MQ-1MW3SqSPD10fXqnW1PhtM82TbbylW78jxZG7nCh9-W2Zbh9-6GwrXHW4Gh7Vq4gSHG0VKN8xv5CDxGFf26z6T404?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eGrounding DINO\u003c/a\u003e\u0026nbsp;follows many of its\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3p8W860YQB1BR35nW7B4Xsk26jkJyW1VXQDJ3PptJxW7wWT7N1lx135N6w4S1c5zRwRW1_jmQT14BnrvVP_vy360H__-W6Cn-xG2MqNC2W4R4Rxc2Hhp_sW3-0P_w9gsQgbW3gHFq49dvL3QW8HFKx-34NCYBW1tXN8Z1s9BNvVGV6f38Fyz3DN4hZrpH-m009W8NKmdp4-B0l7W1Vf_w83LjM6wW1QMQ0J7GmpBgW7ZvmLz5_XPxyW1B6MgF6J00mGW2vKsqM7mtf9-W6Kb9Cn8pKS4Lf6m7bgd04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003epredecessors\u003c/a\u003e\u0026nbsp;by using image embeddings of different levels (from lower-level embeddings produced by an image encoder’s earlier layers, which are larger and represent simple patterns such as edges, to higher-level embeddings produced by later layers, which are smaller and represent complex patterns such as objects). This enables it to\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3nRMQ7crLzpzX8N6qQZCt80DM8W4TVXRG6JrMWNW4t6x6P7fldx8N93yYggn26hdW2-n-pJ22WrjLW5y65c64hKVJfW2G8ZjK3JG-n1W20sgMS3nLYDtVtXsbC1BGL2HW4b2k-s64FyW9W1zq26f1VBVY6W5TZLqZ6yYmG6W2xjwNz8hC5CRW3rhh6T4M3MFwW3TdrGH4zlL_WW7XvPlP62x3B1W5YhzTx2TMZlTV5MYXT3gBzY6VYJmj_1NRrkdW5QDrQw5VT59pW4XVJD13tkx0tf4_FXTM04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003ebetter detect objects at different scales\u003c/a\u003e. However, it takes a lot of computation. To enable the system to run on devices that have less processing power, Grounding DINO 1.5 uses only the smallest (highest-level) image embeddings for a crucial part of the process.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;Grounding DINO 1.5 is made up of components that produce text and image embeddings, fuse them, and classify them. It follows the system architecture and training of Grounding DINO with the following exceptions: (i) It uses a different image encoder, (ii) a different model combines text and image embeddings, and (iii) it was trained on a newer dataset of 20 million publicly available text-image examples.\u003c/p\u003e\u003cul\u003e\u003cli\u003eGiven an image, a pretrained EfficientViT-L1 image encoder produced three levels of image embeddings.\u003c/li\u003e\u003cli\u003eGiven the corresponding text, BERT produced a text embedding composed of tokens.\u003c/li\u003e\u003cli\u003eGiven the highest-level image embedding and the text embedding, a cross-attention model updated each one to incorporate information from the other (fusing text and image modalities, in effect). After the update, a\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3mkW86m8gP4Cw4tgW3qzvKG4TyV-cW69XgcQ1skZ5pN3C_G7HZ-QzXN6VDDrRzKL0LM1krgm2CS91W4gTwXr3pnrwCVC14C95dmh3vW40yrth4JBqkLW3Xf3Wq4Swv0HW1sxz7h28Y864W6DC05x1rDc0cV_lbrk4r2WpYW2PdN1988JnvCW7sD-ST1hPlNDW7XvslL6d0YpxN32r-GcQqFh5W3RsqM46f1b_FW8mltLX7_y8BJW70Z2XX3kF8ykW22XsZm9lSKpZW32SnfJ6yQxY-f3ch9hv04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eCNN-based model\u003c/a\u003e\u0026nbsp;combined the updated highest-level image embedding with the lower-level image embeddings to create a single image embedding.\u003c/li\u003e\u003cli\u003eGrounding DINO 1.5 calculated which 900 tokens in the image embedding were most similar to the tokens in the text embedding.\u003c/li\u003e\u003cli\u003eA cross-attention model detected objects using both the image and text embeddings. For each token in the updated image embedding, it determined: (i) which text token(s), if any, matched the image token, thereby giving each image token a classification including “not an object” and (ii) a bounding box that enclosed the corresponding object (except for tokens that were labeled “not an object”).\u003c/li\u003e\u003cli\u003eThe system learned to (i) maximize the similarity between matching tokens from the text and image embeddings and minimize the similarity between tokens that didn’t match and (ii) minimize the difference between its own bounding boxes and those in the training dataset.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;Grounding DINO 1.5 performed significantly faster than the original Grounding DINO: 10.7 frames per second versus 1.1 frames per second running on an\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLhs3qgyTW8wLKSR6lZ3pzW79n97y8gcj2dW6wKzCT5jLzGMW2kxw_w32PR0KW3WGGZh1rJQS8VF-v-J7D7qQQW8dwP2T3LjLxYW2NZB_c21fD3sW602BzD5yLT4TW7-f3BL6ybG7gW7V20Yh6yJW4GN6rp2ycy8P8hW3tcjX-8wk2LzVyj9YT4HSctnW8fYJ6980GxWpW5tD-pw3m-N37N99B19CFHY9TW2PN-d66y8PYMW1LX8FZ5DTddXW2RBp869bcqqTW4KWrJw6S6YmqW5ptbBk8CQST2W72BSFK7dZxkKW3nFcGS2JnpCPW6JRRB96FPxkvW1hst2R3XPKtrW258SYc3HMyxwW8Cbz9y6WqjnTW4qNBnY1QgBM3f2QH1Zb04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eNvidia Jetson Orin NX\u003c/a\u003e\u0026nbsp;computer. Tested on a\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3nYW8Kp9rZ6FZbSzN3449C5PZC0DW6TVzn36WX5yNW41-Ltv2GNSd4W1PfBsw73qCs4N7v_lqXLFk2JW4DXpMD95Jf_MW43zYQP3ZtCPWW6dqbTl3Kp74JVygsdl8pmkb_W6hnXdr4LkhrFW71yCNy5wrgPxW2L0fk31RR3DxW8gP-077FGbhLW1w9n498qKbgLW47QGKm2jLWkDW7Lv6mK7t4vhgW4mnpdz4ZYJY6VfMZhr1MljvvW5TlhJq7XlN-CW98_8Dj4fzY-3W5_XX1v8Ntrrsf2YZLmv04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003edataset\u003c/a\u003e\u0026nbsp;of images of common objects annotated with labels and bounding boxes, Grounding DINO 1.5 achieved better average precision (a measure of how many objects it identified correctly in their correct location, higher is better) than both Grounding DINO and\u0026nbsp;\u003ca href=\"https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVYMh45v0fN8W5BlvJl3hvVwJW5n2k0f5nWxKVN4-lLgz3qgyTW6N1vHY6lZ3mfVXmDY55HBNmpVcMzXt89k0SGW3ZhxJP6CRnjtW11fZCQ1VdCVcW1jvJjv1NcBtlW2Nrh-87pWWCbW20q_L180RfHyW9lJ5qw3RNX73W3zHWNb8j8k_1W8P0-bn3dY3Q-W4zyQnt1YRrZbW5bn5sV8wQNpyW3m7T5s6_NvfgW8rv-_X6D-Q83N8yZ7Kw4BnSbW48TBhN706ldNW10WKBb7_YjS5W41bRCQ7XqYYCVY-M-z3Txbx0W7DSKYV5-rszpW4VLWk84w60-4W8V_SnY5kkYxZf1v28kv04?ref=dl-staging-website.ghost.io\" rel=\"noopener\"\u003eYOLO-Worldv2-L\u003c/a\u003e\u0026nbsp;(a CNN-based object detector). Grounding DINO 1.5 scored 33.5 percent, Grounding DINO 27.4 percent, and YOLO-Worldv2-L 33 percent.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u0026nbsp;\u003c/strong\u003eThe authors achieved 10 times the speed with just a couple of small changes (a more efficient image encoder and a smaller image embedding when performing cross-attention between embeddings of images and texts). Small changes can yield big results.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Lately model builders have been building better, smaller, faster large language models for edge devices. We’re glad to see object detection get similar treatment.\u003c/p\u003e","comment_id":"674772a5fdb1390001aebad3","feature_image":"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.jpg","featured":false,"visibility":"public","created_at":"2024-11-27T11:27:33.000-08:00","updated_at":"2024-11-27T11:47:14.000-08:00","published_at":"2024-11-27T11:42:00.000-08:00","custom_excerpt":"The Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"674776ddfdb1390001aebb1c","name":"issue-277","slug":"issue-277","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-277/"},{"id":"674776ddfdb1390001aebb1d","name":"Nov 27, 2024","slug":"nov-27-2024","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/nov-27-2024/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-277/","excerpt":"The Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object Detection. ","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, and more...","meta_description":"The Batch AI News and Insights: DeepSeek Takes On OpenAI, Robots Fold Laundry, Amazon and Anthropic Expand Partnership, More Efficient Object...","email_subject":null,"frontmatter":null,"feature_image_alt":"Cornucopia overflowing with fruits and vegetables.","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2024/11/unnamed--35--1.jpg","dimensions":{"width":1200,"height":676}},"banner":{"title":"Generative AI for Software Development","databaseId":35156,"id":"cG9zdDozNTE1Ng==","featuredImage":{"node":{"altText":"","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/08/Vertical-side-banner-ads-5.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/3YTVMir","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Databricks C1","date":"2025-06-04T07:12:29","databaseId":36631,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/3HlCuvP","courseName":"DSPy: Build and Optimize Agentic Apps","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)","isOpenInNewTab":true}},{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}}]}}},"__N_SSG":true},"page":"/the-batch/[slug]","query":{"slug":"issue-277"},"buildId":"Qy9Eh4N0MrfE3ddReY9v4","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>