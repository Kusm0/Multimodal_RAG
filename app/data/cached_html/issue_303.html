<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Claude 4 Advances Code Gen, How DeepSeek Built V3 For $5.6m, Google I/O Roundup, and more...</title><meta name="description" content="The Batch AI News and Insights: I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><link rel="canonical" href="https://www.deeplearning.ai/the-batch/issue-303/"/><meta property="og:type" content="article" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:title" content="Claude 4 Advances Code Gen, How DeepSeek Built V3 For $5.6m, Google I/O Roundup, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:description" content="The Batch AI News and Insights: I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:site_name" content="Claude 4 Advances Code Gen, How DeepSeek Built V3 For $5.6m, Google I/O Roundup, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:url" content="https://www.deeplearning.ai/the-batch/issue-303/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="article:published_time" content="2025-05-28T12:53:00.000-07:00"/><meta property="article:modified_time" content="2025-05-31T15:42:34.000-07:00"/><meta property="article:tag" content="The Batch Newsletter"/><meta property="article:tag" content="May 28, 2025"/><meta property="article:tag" content="issue-303"/><meta property="article:author" content="https://www.facebook.com/DeepLearningAIHQ/"/><meta property="twitter:title" content="Claude 4 Advances Code Gen, How DeepSeek Built V3 For $5.6m, Google I/O Roundup, and more..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:description" content="The Batch AI News and Insights: I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:url" content="https://www.deeplearning.ai/the-batch/issue-303/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:label1" content="Written by"/><meta property="twitter:data1" content="DeepLearning.AI"/><meta property="twitter:label2" content="Filed under"/><meta property="twitter:data2" content="The Batch Newsletter"/><meta property="twitter:card" content="summary_large_image" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:creator" content="@DeepLearningAI"/><meta property="twitter:site" content="https://twitter.com/DeepLearningAI/"/><meta name="twitter:image" content="https://dl-staging-website.ghost.io/content/images/2025/05/Untitled-design--25-.png"/><meta property="og:image" content="https://dl-staging-website.ghost.io/content/images/2025/05/Untitled-design--25-.png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="677"/><script type="application/ld+json">{"@context":"https://schema.org/","@type":"Article","datePublished":"2025-05-28T12:53:00.000-07:00","dateModified":"2025-05-31T15:42:34.000-07:00","author":{"@type":"Article","name":"DeepLearning.AI","image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","sameAs":"[\"https://www.deeplearning.ai/\", \"https://twitter.com/DeepLearningAI_/\", \"https://www.facebook.com/DeepLearningAIHQ/\"]"},"keywords":"[object Object], [object Object], [object Object]","headline":"Claude 4 Advances Code Gen, How DeepSeek Built V3 For $5.6m, Google I/O Roundup, and more...","image":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2025/05/Untitled-design--25-.png","width":1200,"height":677},"publisher":{"@type":"Organization","name":"Claude 4 Advances Code Gen, How DeepSeek Built V3 For $5.6m, Google I/O Roundup, and more...","logo":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","width":60,"height":60}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.deeplearning.ai"},"description":"The Batch AI News and Insights: I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have..."}</script><meta name="next-head-count" content="31"/><link href="/static/favicons/favicon.ico" rel="shortcut icon"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&amp;display=swap"/><link rel="preload" href="/_next/static/css/54b174af9991199d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/54b174af9991199d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6f396d9f2f265155.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6f396d9f2f265155.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-7690c02358b700f2.js" defer=""></script><script src="/_next/static/chunks/framework-c54e8763846ac33b.js" defer=""></script><script src="/_next/static/chunks/main-9fcc7c9a63f70912.js" defer=""></script><script src="/_next/static/chunks/pages/_app-bece9363d7a6c0bb.js" defer=""></script><script src="/_next/static/chunks/3a17f596-e9670ced3bfab562.js" defer=""></script><script src="/_next/static/chunks/36d2f571-09e32b60ec93a4dc.js" defer=""></script><script src="/_next/static/chunks/5c0b189e-c0f75ca9b44e520b.js" defer=""></script><script src="/_next/static/chunks/5567-47eb911ba5f222d7.js" defer=""></script><script src="/_next/static/chunks/2251-380253169bb2e795.js" defer=""></script><script src="/_next/static/chunks/3864-a4ae56e91852593b.js" defer=""></script><script src="/_next/static/chunks/4965-3585432476b27ea9.js" defer=""></script><script src="/_next/static/chunks/3791-0bbbd4cd0bf31c22.js" defer=""></script><script src="/_next/static/chunks/pages/the-batch/%5Bslug%5D-c03a8bcef188ef99.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_buildManifest.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_ssgManifest.js" defer=""></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&display=swap"/></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5C5VGGJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div data-sentry-component="Layout" data-sentry-source-file="Layout.tsx"><aside id="top-announcement" class="text-neutral-900  " style="color:#FFFFFF;background-color:#05256C;background-image:linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)" data-sentry-component="AnnouncementBanner" data-sentry-source-file="AnnouncementBanner.tsx"><div class="container--boxed py-3 flex items-center justify-between "><div class="flex items-center"><p class="text-sm lg:text-base">✨ New course! Enroll in<!-- --> <a href="https://bit.ly/3HlCuvP" class="underline" target="_blank">DSPy: Build and Optimize Agentic Apps</a></p></div><div class="flex items-center"><button class="bg-transparent p-0 border-none text-xl opacity-70 hover:opacity-100 transition-opacity"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><line x1="15" y1="9" x2="9" y2="15"></line><line x1="9" y1="9" x2="15" y2="15"></line></svg></button></div></div></aside><header id="main-navigation" data-testid="main-navigation-testid" class="static h-[64px] md:h-[100px] top-0 w-full z-[100] flex items-center bg-white transition-all ease-in-out duration-200" data-sentry-component="Header" data-sentry-source-file="index.tsx"><div class="container--boxed flex justify-between items-center"><div class="max-w-[185px] lg:max-w-[235px]"><a href="/the-batch/"><div class="flex items-center"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27300%27%20height=%2792%27/%3e"/></span><img alt="The Batch" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="The Batch" loading="lazy" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcSet="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=384&amp;q=75 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75 2x" src="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75"/></noscript></span></div></a></div><div class="flex items-center"><div class="hidden lg:flex items-center"><nav aria-label="Primary" data-sentry-component="Nav" data-sentry-source-file="Nav.tsx"><ul class="flex items-center p-0 m-0 list-none"><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/courses/"><div class="NavItem_navItemLink__Aq6E5"><span>Explore Courses</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/the-batch/"><div class="NavItem_navItemLink__Aq6E5"><span>AI Newsletter</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/"><div>The Batch</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/letters/"><div>Andrew&#x27;s Letter</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/data-points/"><div>Data Points</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/research/"><div>ML Research</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/"><div>Blog</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/community/"><div class="NavItem_navItemLink__Aq6E5"><span>Community</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="https://community.deeplearning.ai/"><div>Forum</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/events/"><div>Events</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/ambassador/"><div>Ambassadors</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/category/ambassador-spotlight/"><div>Ambassador Spotlight</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/resources/"><div class="NavItem_navItemLink__Aq6E5"><span>Resources</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/about/"><div class="NavItem_navItemLink__Aq6E5"><span>Company</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/about/"><div>About</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/careers/"><div>Careers</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/contact/"><div>Contact</div></a></li></ul></li></ul></nav><a class="btn--primary text-base font-medium whitespace-nowrap shadow h-fit py-3 px-4 lg:ml-2 xl:ml-10" data-sentry-element="Link" data-sentry-source-file="index.tsx" href="https://bit.ly/3RB9T8a">Start Learning</a></div><div class="flex items-center" data-sentry-element="Menu" data-sentry-component="MobileMenu" data-sentry-source-file="MobileMenu.tsx" data-headlessui-state=""><button class="lg:hidden" data-sentry-element="MenuButton" data-sentry-source-file="MobileMenu.tsx" id="headlessui-menu-button-:R1qa6:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-4xl text-slate-600" data-sentry-element="FiMenu" data-sentry-source-file="MobileMenu.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button></div></div></div></header><main><div id="content"><nav aria-label="Secondary" class="h-[66px] bg-white sticky top-[60px] z-40 shadow hidden lg:block" data-sentry-component="SecondaryNav" data-sentry-source-file="index.tsx"><div class="container--boxed h-full w-full flex items-center justify-between "><div class="relative flex h-full"><button class="h-full w-14 items-center justify-center absolute top-0 left-0 z-10 group bg-white bg-opacity-75 hidden"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="15 18 9 12 15 6"></polyline></svg></span></button><ul id="nav-secondary" class="list-none p-0 m-0 h-full flex items-center  overflow-x-scroll relative SecondaryNav_navItems__dok3i"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Weekly Issues</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/letters/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Andrew&#x27;s Letters</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/data-points/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Data Points</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/research/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">ML Research</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/business/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Business</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/science/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Science</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/culture/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Culture</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/hardware/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Hardware</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/ai-careers/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">AI Careers</div></a></li></ul><button class="h-full w-9 items-center justify-center absolute top-0 right-0 group bg-white bg-opacity-75 flex"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span></button></div><div class="flex items-center h-full"><ul class="list-none p-0 m-0 h-full flex items-center mr-5"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/about/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">About</div></a></li></ul><button type="button" class="bg-white border btn--tracking border-solid border-brand text-brand hover:bg-brand hover:text-white transition-colors px-3 py-1 rounded-md mr-4">Subscribe</button><a href="/search/"><div title="Search" class="transition-colors text-slate-400 hover:text-slate-500"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-2xl" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></div></a></div></div></nav><article class="pt-5 pb-16 bg-white lg:pt-16 lg:pb-16" data-sentry-component="ArticleDefault" data-sentry-source-file="ArticleDefault.tsx"><header class="post_layoutGrid__0BDX2"><nav aria-label="Breadcrumb flex items-center" data-sentry-component="Main" data-sentry-source-file="Breadcrumb.tsx"><ul class="list-none flex items-center flex-wrap"><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">The Batch</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">Weekly Issues</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/issue-303/"><div class="text-sm md:text-base text-slate-400 font-normal "><h1 class="capitalize">issue 303</h1></div></a></li></ul></nav><aside class="flex mt-6 lg:hidden"><div class="flex flex-col items-start items-start mr-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm text-sm">Published</div></div><div class="mt-1 text-slate-600 text-base text-sm">May 28, 2025</div></div><div class="flex flex-col items-start items-start" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm text-sm">Reading time</div></div><div class="mt-1 text-slate-600 text-base text-sm">15<!-- --> min read</div></div></aside></header><div class="post_layoutGrid__0BDX2 mt-9"><aside style="grid-column:wide-start / main-start" class="flex-col items-end hidden pr-10 lg:flex"><div class="flex flex-col items-start items-end" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm undefined">Published</div></div><div class="mt-1 text-slate-600 text-base undefined"><a href="/the-batch/tag/may-28-2025/"><div class="inline-flex px-2 py-1 text-sm font-normal transition-colors rounded-md bg-slate-100 hover:bg-slate-200 text-slate-500">May 28, 2025</div></a></div></div><div class="flex flex-col items-start items-end mt-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm undefined">Reading time</div></div><div class="mt-1 text-slate-600 text-base undefined">15<!-- --> min read</div></div><div class="flex flex-col items-start items-end mt-6 sticky top-20" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-303/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-303/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-303/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></aside><div class="prose--styled justify-self-center post_postContent__wGZtc">
<!--kg-card-begin: html-->
<div id="elevenlabs-audionative-widget" data-height="90" data-width="100%" data-frameborder="no" data-scrolling="no" data-publicuserid="e20b5cfed36900db239c005920538f20ce435963e95a0a4106d34bdd6bf0e46d" data-playerurl="https://elevenlabs.io/player/index.html" >Loading the <a href="https://elevenlabs.io/text-to-speech?ref=dl-staging-website.ghost.io" target="_blank" rel="noopener">Elevenlabs Text to Speech</a> AudioNative Player...</div>
<!--kg-card-end: html-->
<p>Dear friends,</p><p>I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed&nbsp;<a href="https://www.science.org/content/article/trump-s-proposed-budget-would-mean-disastrous-cuts-science?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">here</a>, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done.</p><p>If not for&nbsp;<a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=0835878&&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">funding for my early work in deep learning</a>&nbsp;from the National Science Foundation (NSF)&nbsp; and Defense Advanced Research Projects Agency (DARPA), which disburse much of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas.</p><p>In fact, such funding benefits the U.S. more than any other nation.&nbsp;&nbsp;Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and&nbsp;(ii) the process of doing research creates new talent for that nation.</p><p>Why does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies.</p><p>In a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work.</p><p>Thus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technology&nbsp;<a href="https://www.congress.gov/117/meeting/house/114100/documents/HHRG-117-SY21-20211005-SD002.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f#:~:text=The%20directive%20does%20not%20claim,Defense%20Ashton%20Carter%20in%202010" rel="noopener">points out</a>,&nbsp;“... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.”</p><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--96--2.png" class="kg-image" alt="Bar chart showing proposed 2026 U.S. science funding cuts vs. 2025 for agencies like USFS, NSF, NASA, and DoE." loading="lazy" width="1200" height="677" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/unnamed--96--2.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/unnamed--96--2.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--96--2.png 1200w" sizes="(min-width: 720px) 720px"></figure><p>Further, generative AI is evolving so rapidly that staying on the cutting edge is what’s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable. Studies like&nbsp;<a href="https://www.etsg.org/ETSG2007/papers/strobl.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">this one</a>&nbsp;(albeit done while the internet was not as prevalent as it is today) also show how knowledge diffuses locally much faster than globally.</p><p>China was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China’s tech ecosystem is very open internally, and this has helped it to catch up over the past two years:</p><ul><li>There is ample funding for open academic research in China.</li><li>China’s businesses such as DeepSeek and Alibaba have released cutting-edge, open-weights models. This openness at the corporate level accelerates diffusion of knowledge.</li><li>China’s labor laws make non-compete agreements (which stop an employee from jumping ship to a competitor) relatively hard to enforce, and the work culture supports significant idea sharing among employees of different companies; this has made circulation of ideas relatively efficient.</li></ul><p>While there’s also much about China that I would not seek to emulate, the openness of its tech ecosystem has helped it accelerate.</p><p>In 1945, Vannevar Bush’s landmark report “<a href="https://nsf-gov-resources.nsf.gov/2023-04/EndlessFrontier75th_w.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Science, The Endless Frontier</a>”&nbsp;laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit&nbsp;the U.S.</p><p>The good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research.</p><p>Andrew</p><hr><h2 id="a-message-from-deeplearningai-and-snowflake">A MESSAGE FROM&nbsp;DEEPLEARNING.AI AND SNOWFLAKE</h2><figure class="kg-card kg-image-card"><a href="https://www.snowflake.com/en/summit/dev-day/?utm_source=deeplearningai&utm_medium=partner&utm_campaign=snowflake-partner-devday25"><img src="https://dl-staging-website.ghost.io/content/images/2025/05/GqglM2_WAAAcXxW.jpg" class="kg-image" alt="Promo banner for: Snowflake’s Dev Day 2025" loading="lazy" width="1200" height="630" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/GqglM2_WAAAcXxW.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/GqglM2_WAAAcXxW.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/GqglM2_WAAAcXxW.jpg 1200w" sizes="(min-width: 720px) 720px"></a></figure><p>We’re featured partners of Snowflake’s Dev Day 2025, a full day for AI and data practitioners to explore cutting-edge demos, make valuable contacts, and hear from top voices in the field (including Andrew Ng). See you on June 5!&nbsp;<a href="https://www.snowflake.com/en/summit/dev-day/?utm_source=deeplearningai&utm_medium=partner&utm_campaign=snowflake-partner-devday25" rel="noreferrer">Register here</a></p><h1 id="news">News</h1><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png" class="kg-image" alt="AI model performance comparison chart: Claude Opus 4, Sonnet 4, Sonnet 3.7, OpenAI o3, GPT-4.1, and Gemini 2.5 Pro." loading="lazy" width="1200" height="675" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/unnamed--97-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/unnamed--97-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="claude-4-advances-code-generation">Claude 4 Advances Code Generation</h1><p>Anthropic continued its tradition of building AI models that raise the bar in coding tasks.</p><p><strong>What’s new:</strong>&nbsp;Anthropic launched&nbsp;<a href="https://www.anthropic.com/news/claude-4?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Claude 4 Sonnet 4 and Claude Opus 4</a>, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit.</p><ul><li><strong>Input/output:&nbsp;</strong>Text, images, PDF files&nbsp;in (up to 200,000 tokens); text out (Claude Sonnet 4 up to 64,000 tokens, Claude Opus 4 up to 32,000 tokens)</li><li><strong>Features:&nbsp;</strong>Parallel tool use including computer use, selectable reasoning mode with visible reasoning tokens, multilingual (15 languages)</li><li><strong>Performance:&nbsp;</strong>Ranked Number One in LMSys WebDev Arena, state-of-the-art on SWE-bench and Terminal-bench</li><li><strong>Availability/price:</strong>&nbsp;Anthropic API, Amazon Bedrock, Google Cloud Vertex AI.&nbsp;<em>Claude Sonnet 4</em>&nbsp;$3/$15 per million input/output tokens,&nbsp;<em>Claude Opus 4</em>&nbsp;$15/$75 per million input/output tokens</li><li><strong>Undisclosed:&nbsp;</strong>Parameter counts, specific training methods and datasets</li></ul><p><strong>How it works:</strong>&nbsp;The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to be&nbsp;<a href="https://arxiv.org/abs/2112.00861?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">helpful, honest, and harmless</a>&nbsp;according to human and&nbsp;<a href="https://arxiv.org/abs/2212.08073/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">AI feedback</a>.&nbsp;</p><ul><li>The models make reasoning tokens visible within limits. For especially lengthy chains of thought, an unspecified smaller model summarizes reasoning tokens.</li><li>Given local file access, Claude Opus 4 can create and manipulate files to store information. For instance, prompted to maintain a knowledge base while playing a Pokémon video game, the model produced a guide to the game that offered advice such as, “If stuck, try OPPOSITE approach” and “Change Y-coordinate when horizontal movement fails.”</li></ul><p><strong>Results:</strong>&nbsp;Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests.&nbsp;</p><ul><li>On&nbsp;<a href="https://openai.com/index/introducing-swe-bench-verified/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">SWE-bench Verified</a>, which tests the model’s ability to solve software issues from GitHub, Claude Opus 4 succeeded 72.5 percent of the time, and Claude Sonnet 4 succeeded 72.7 percent of the time. The next best model, OpenAI o3, succeeded 70.3 percent of the time.</li><li><a href="https://www.tbench.ai/about?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Terminal-bench</a>&nbsp;evaluates how well models work with the benchmark’s built-in agentic framework to perform&nbsp;tasks on a computer terminal. Claude Opus 4 succeeded 39.2 percent of the time and Claude Sonnet 4 succeeded 33.5 percent of the time, whereas the closest competitor, OpenAI GPT 4.1, succeeded 30.3 percent of the time. Using Claude Code as the agentic framework, Claude Opus 4 succeeded 43.2 percent of the time and Claude Sonnet 4 succeeded 35.5 percent of the time.</li></ul><p><strong>Why it matters:</strong>&nbsp;The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including a&nbsp;<a href="https://x.com/petergyang/status/1925593200993214687?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Tetris clone</a>&nbsp;built in one shot&nbsp;and a seven-hour stint&nbsp;<a href="https://arstechnica.com/ai/2025/05/anthropic-calls-new-claude-4-worlds-best-ai-coding-model/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">refactoring Rakutan’s open-source code base</a>.</p><p><strong>We’re thinking:</strong>&nbsp;Prompting expert @elder_plinius published a text file that is purported to be&nbsp;<a href="https://github.com/elder-plinius/CL4R1T4S/blob/main/ANTHROPIC/Claude_4.txt?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Claude 4’s system prompt</a>&nbsp;and includes some material&nbsp;that does not appear in Anthropic’s own&nbsp;<a href="https://docs.anthropic.com/en/release-notes/system-prompts?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">publication</a>&nbsp;of the prompts. It is instructive to see how it conditions&nbsp;the model for&nbsp;tool use, agentic behavior, and reasoning.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--60-.gif" class="kg-image" alt="Side-by-side of a fern leaf and its digital code representation, illustrating nature's pattern-to-code transformation." loading="lazy" width="600" height="338" srcset="https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--60-.gif 600w"></figure><h1 id="google-io-overdrive">Google&nbsp;I/O Overdrive</h1><p>Google revamped its roster of models, closed and open, and added more AI-powered features to its existing products.</p><p><strong>What’s new:</strong>&nbsp;Google staged&nbsp;a parade of&nbsp;<a href="https://blog.google/technology/developers/google-ai-developer-updates-io-2025/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">announcements</a>&nbsp;at this year’s&nbsp;I/O developer conference. New offerings include improvements to&nbsp;<a href="https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Gemini 2.5 Pro and Gemini 2.5 Flash</a>&nbsp;and a preview of&nbsp;<a href="https://developers.googleblog.com/en/introducing-gemma-3n/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Gemma 3n</a>&nbsp;(all three generally available in June),&nbsp;the updated&nbsp;<a href="https://gemini.google/overview/video-generation/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Veo 3</a>&nbsp;video generator (available via Flow, Google’s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search.</p><p><strong>How it works:</strong>&nbsp;The I/O offerings spanned from public-facing products to developer tools.</p><ul><li>Google updated Gemini 2.5 Pro and the speedier Gemini 2.5 Flash with audio output, so both models now take in text, audio, images, and video and produce text and audio. In addition, they offer summaries of tokens produced while&nbsp;reasoning. Gemini-2.5-Pro-Preview-05-06, which topped the LMSys&nbsp;<a href="https://lmarena.ai/leaderboard/text?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Text Arena</a>&nbsp;and&nbsp;<a href="https://web.lmarena.ai/leaderboard?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">WebDev Arena</a>&nbsp;(tied with Claude 4 Opus and Sonnet), lets users set a reasoning budget up to 128,000 tokens, enabling it to outperform OpenAI o3 and o4-mini (set to high effort) on math, coding, and multimodal benchmarks in Google’s tests. Gemini-2.5-Flash-Preview-05-20&nbsp;uses 22 percent fewer tokens than its predecessor while ranking near the top of the LMSys Text Arena and WebDev Arena.</li><li>The Veo 3&nbsp;text-to-video generator produces 3840x2160-pixel video with audio (dialogue, sound effects, and music) with creative controls including the ability to add and remove objects and maintain consistent characters. It bested Kuaishu Kling 2.0, Runway Gen 3, and OpenAI Sora in Google’s comparisons.&nbsp;</li><li>New members of Google’s&nbsp;<a href="https://www.deeplearning.ai/the-batch/google-releases-gemma-3-vision-language-models-with-open-weights/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Gemma 3</a>&nbsp;family of open-weights models, Gemma 3n 5B and 8B, are multilingual (over 140 languages), multimodal (text, vision, audio in; text out), and optimized for mobile platforms. Gemma-3n-E4B-it (8 billion parameters) ranks just ahead of Anthropic Claude 3.7 Sonnet in the LMSys Text Arena. Gemma 3n 5B and 8B are 1.5 times faster than their predecessors and require 2 gigabytes and 3 gigabytes of memory, respectively, thanks to&nbsp;<a href="https://ai.google.dev/gemma/docs/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">techniques</a>&nbsp;that include per-layer embeddings, key-value caching, conditional parameter loading&nbsp;(constraining active parameters to specific modalities at inference), and a Matryoshka Transformer design that dynamically activates nested sub-models. They’re available in preview via Google’s AI Studio, AI Edge, GenAI SDK, or MediaPipe.</li><li>Google introduced several specialized AI tools and models.&nbsp;<a href="https://blog.google/technology/google-labs/jules/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">Jules</a>&nbsp;is an autonomous, asynchronous, multi-agent coding assistant that clones repos into a secure virtual machine to perform tasks like writing tests, building features, and fixing bugs (available in public beta).&nbsp;<a href="https://developers.googleblog.com/en/google-io-2025-developer-keynote-recap/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">SignGemma</a>&nbsp;translates American sign language to text (previously ASL to English).&nbsp;<a href="https://developers.google.com/health-ai-developer-foundations/medgemma?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">MedGemma</a>&nbsp;analyzes medical text and images (part of the open-weights collection Health AI Developer Foundations).&nbsp;</li><li>Building on Google Search’s AI Overviews, Google is further building AI into search. Google Search’s&nbsp;<a href="https://blog.google/products/search/google-search-ai-mode-update/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">AI Mode</a>&nbsp;uses Gemini 2.5 to deliver a “deep search” mode that decomposes users’ questions into hundreds of sub-queries for analysis and visualization. Google plans to integrate AI Mode features into its core search product. In addition, Google Search’s AI Mode will gain&nbsp;<a href="https://blog.google/products/search/google-search-ai-mode-update/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f#live-capabilities" rel="noopener">Search Live</a>&nbsp;(real-time, audio-enabled visual interaction via camera) and&nbsp;<a href="https://blog.google/products/search/google-search-ai-mode-update/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f#agentic-capabilities" rel="noopener">agentic features</a>&nbsp;(for&nbsp;tasks&nbsp;such as&nbsp;purchasing tickets). Computer-use capabilities are coming to the Gemini API and Vertex AI.&nbsp;</li></ul><p><strong>Why it matters:</strong>&nbsp;Google is catching up with the Microsoft/OpenAI colossus&nbsp;on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3’s text-to-video-plus-audio output shows&nbsp;<a href="https://x.com/jerrod_lew/status/1927092379892265139?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">marked</a>&nbsp;<a href="https://x.com/yo/status/1925722327171387670?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">improvement</a>&nbsp;over the previous version.</p><p><strong>Behind the news:</strong>&nbsp;The number of tokens Google processed&nbsp;monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google’s progress contrasts with Apple’s ongoing&nbsp;<a href="https://www.bloomberg.com/news/features/2025-05-18/how-apple-intelligence-and-siri-ai-went-so-wrong?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">struggles</a>. Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate&nbsp;them into legacy products, Apple’s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI’s&nbsp;<a href="https://www.nytimes.com/2025/05/21/technology/openai-jony-ive-deal.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">acquisition</a>&nbsp;of LoveFrom, the startup founded by its former lead product designer Jony Ive.</p><p><strong>We’re thinking:</strong>&nbsp;Google I/O 2025 was&nbsp;a strong showing of generative AI capabilities! There’s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building&nbsp;numerous innovative products.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--98-.png" class="kg-image" alt="DeepSeek computation diagram showing transformer blocks, multi-head attention, and routing, using FP8 and BF16 precision." loading="lazy" width="1200" height="675" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/unnamed--98-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/unnamed--98-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--98-.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="how-deepseek-did-it">How DeepSeek Did It</h1><p>DeepSeek made headlines late last year,&nbsp;when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer&nbsp;shared new details about&nbsp;its method.</p><p><strong>What’s new:</strong>&nbsp;Chenggang Zhao and colleagues at DeepSeek described&nbsp;<a href="https://www.arxiv.org/abs/2505.09343?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">software and hardware choices</a>&nbsp;that reduced memory&nbsp;and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and&nbsp;DeepSeek-V3.</p><p><strong>Mixture of experts (MoE) basics:</strong>&nbsp;The MoE architecture uses different subsets of a model’s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input.</p><p><strong>How it works:</strong>&nbsp;The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model’s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token.</p><ul><li>The authors built a mixed-precision training algorithm to reduce the memory requirements of training MoE models. They used FP8 (8-bit) numbers to perform computations including linear transformations and 16- or 32-bit precision to perform others such as computing embeddings. (They say DeepSeek-V3 was the first open LLM to have been trained using FP8.)</li><li>The authors noticed that communication between GPUs inside a node was four times faster than communication between nodes. To ensure fast communication when routing tokens to experts, they limited the algorithm to process them within up to 4 nodes.</li><li>To utilize GPUs more fully, they divided each GPU’s input data so the chip processes computation and communication at the same time. Specifically, the chip computes attention or MoE layers on one part of the data and simultaneously sends the other part of the data to other GPUs or aggregates it from other GPUs as necessary.</li><li>To further save inference memory, the models use multi-head latent attention, which saves memory during execution relative to other variants of attention. The authors compared their implementation to the variant GQA used in Qwen 2.5 72B and Llama 3.1 405B. Their method (70 kilobytes per token) used far less memory than Qwen-2.5 (328 kilobytes per token) or Llama 3.1 (516 kilobytes per token).</li></ul><p><strong>Behind the news:</strong>&nbsp;<a href="https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">DeepSeek-V3</a>&nbsp;made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers were&nbsp;<a href="https://stratechery.com/2025/deepseek-faq/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">skeptical</a>&nbsp;of the reported cost, pointing out that the $5.6 million dollar figure doesn’t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of training&nbsp;<a href="https://www.deeplearning.ai/the-batch/deepseek-r1-an-affordable-rival-to-openais-o1/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">DeepSeek-R1</a>&nbsp;remains unknown.</p><p><strong>Why it matters:</strong>&nbsp;Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn’t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art.</p><p><strong>We’re thinking:</strong>&nbsp;Shortly after DeepSeek-R1 was released, some engineers claimed — without presenting evidence — that DeepSeek had copied their work. DeepSeek’s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--99-.png" class="kg-image" alt="Bar chart of AUROC scores for model recognition of non-public and public data across GPT versions, highlighting performance differences." loading="lazy" width="1200" height="675" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/unnamed--99-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/unnamed--99-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--99-.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="did-gpt-4o-train-on-o%E2%80%99reilly-books">Did GPT-4o Train on O’Reilly Books?</h1><p>A study co-authored by tech-manual publisher Tim O’Reilly shows that OpenAI trained GPT-4o on parts of his company’s books that were not made freely available.</p><p><strong>What happened:</strong>&nbsp;O’Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss&nbsp;<a href="https://ssrc-static.s3.us-east-1.amazonaws.com/OpenAI-Training-Violations-OReillyBooks_Sruly-OReilly-Strauss_SSRC_04012025.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">found</a>&nbsp;that GPT-4o was able to identify verbatim excerpts from dozens of O’Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model’s training data.</p><p><strong>How it works:</strong>&nbsp;The researchers adapted the&nbsp;<a href="https://arxiv.org/pdf/2402.09910?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f">DE-COP</a>&nbsp;method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.</p><ul><li>The team selected 34 O’Reilly Media books and divided them into roughly 14,000 paragraphs.</li><li>They labeled the paragraphs private (paywalled) or public (when O’Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models’ knowledge cutoff dates.</li><li>The team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.</li></ul><p><strong>Results:</strong>&nbsp;The authors asked each model to identify the verbatim paragraph and calculated each model’s percentage of correct responses. Then they averaged each model’s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren’t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.</p><ul><li>GPT-4o tended to recognize O’Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).</li><li>GPT-4o-mini’s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model’s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.</li><li>The earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.</li></ul><p><strong>Yes, but:</strong>&nbsp;Newer large language models are better at distinguishing human-written from generated text, even if it wasn’t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while&nbsp;scores for both text that was included and text that was excluded from training sets&nbsp;remain under 96 percent AUROC. “For now,” they write, “the gap remains sufficiently large to reliably separate the two categories.”</p><p><strong>Behind the news:</strong>&nbsp;Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works’ owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that’s freely available on the web as fair game, and material that’s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, including&nbsp;<a href="https://libgenesis.net/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">LibGen</a>, which includes all 34 of the O’Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted&nbsp;without paywalls or even logins, making it possible even&nbsp;for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recently&nbsp;<a href="https://www.theverge.com/news/630079/openai-google-copyright-fair-use-exception?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">lobbied</a>&nbsp;the United States government to relax copyright laws for AI developers.</p><p><strong>Why it matters:</strong>&nbsp;The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O’Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an “extractive dead end” that ultimately diminishes the supply of the high-quality training data.</p><p><strong>We’re thinking:</strong>&nbsp;We have learned a great deal from O’Reilly Media’s books, and we’re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it’s time for the U.S. Congress —&nbsp; and legislators internationally — to&nbsp;<a href="https://www.deeplearning.ai/the-batch/time-to-update-copyright-for-generative-ai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f" rel="noopener">update</a>&nbsp;copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.</p></div><aside style="grid-column:main-end / wide-end" class="flex-col hidden pl-10 lg:flex"><div class="relative shadow rounded-lg overflow-hidden hover:shadow-sm transition-shadow" data-sentry-component="Advertisement" data-sentry-source-file="Advertisement.tsx"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:200%"></span><img alt="AI is the new electricity. Are you ready to flip the switch? Download your free copy of the ebook." data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" class="rounded-lg" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="AI is the new electricity. Are you ready to flip the switch? Download your free copy of the ebook." data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" loading="lazy" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="rounded-lg" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F2.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F2.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F2.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F2.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F2.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F2.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F2.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F2.png&amp;w=3840&amp;q=75 3840w" src="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2023%2F03%2F2.png&amp;w=3840&amp;q=75"/></noscript></span><a href="https://www.deeplearning.ai/resources/#ebooks"><div class="absolute inset-0" data-gtm-event-title="AI is the new electricity"></div></a></div></aside><footer class="mt-8 lg:hidden"><div class="flex flex-col items-start items-start " data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-303/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-303/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-303/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></footer></div></article><div class="py-16 bg-slate-50"><section id="subscribe" data-sentry-component="CtaNewsletter" data-sentry-source-file="CtaNewsletter.tsx"><div class="container--boxed relative"><div class="text-center"><h2 class="text--l2 text-slate-900">Subscribe to The Batch</h2><p class="text-base lg:text-lg text-slate-500 mt-3 max-w-md mx-auto">Stay updated with weekly AI News and Insights delivered to your inbox</p></div><div class="flex flex-col items-center mt-9"><div><div id="reactHubspotForm47" style="display:none"></div><div class="flex items-center justify-center"><svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-brand-teal" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" data-sentry-element="svg" data-sentry-component="Spinner" data-sentry-source-file="Spinner.tsx"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4" data-sentry-element="circle" data-sentry-source-file="Spinner.tsx"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z" data-sentry-element="path" data-sentry-source-file="Spinner.tsx"></path></svg></div></div></div></div></section></div></div></main><footer class="py-16 bg-brand-teal-900" data-sentry-component="Footer" data-sentry-source-file="index.tsx"><div class="flex flex-col items-center justify-center text-center container--boxed"><div class="max-w-[220px] flex items-center justify-center text-white"><svg viewBox="0 0 272 34" fill="none" xmlns="http://www.w3.org/2000/svg" width="272" height="34" aria-label="DeepLearning.AI" data-sentry-element="svg" data-sentry-component="DLAILogo" data-sentry-source-file="DLAILogo.tsx"><g fill="currentColor" data-sentry-element="g" data-sentry-source-file="DLAILogo.tsx"><path d="M56.775 16.108c0 6.206-4.252 10.005-10.747 10.005H39.42V5.961h6.608c6.495 0 10.747 3.924 10.747 10.147zm-10.747 7.306c4.778 0 7.337-2.724 7.337-7.306 0-4.581-2.56-7.452-7.337-7.452H42.73v14.758h3.298z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M66.97 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.259-8.226 8.006-8.226c4.571 0 7.804 3.159 7.804 7.856.007.535-.027 1.07-.103 1.6H62.412c.233 2.638 2.123 4.232 4.57 4.232 2.038 0 3.173-.988 3.786-2.234h3.582c-.915 2.802-3.448 5.032-7.38 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.484-4.03-2.24 0-4.044 1.525-4.394 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M84.938 26.371c-4.601 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.792 9.792 0 01-.116 1.6H80.367c.233 2.638 2.128 4.232 4.57 4.232 2.042 0 3.177-.988 3.786-2.234h3.582c-.902 2.802-3.435 5.032-7.367 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M104.917 9.885c4.221 0 7.541 3.245 7.541 8.166 0 4.92-3.32 8.32-7.541 8.32a7.277 7.277 0 01-3.095-.664 7.248 7.248 0 01-2.516-1.914v9.915h-3.319v-23.57h3.32v2.347a7.004 7.004 0 015.61-2.6zm-.729 2.871c-2.473 0-4.86 1.943-4.86 5.364 0 3.42 2.387 5.39 4.86 5.39 2.473 0 4.894-2.02 4.894-5.46 0-3.437-2.391-5.303-4.894-5.303v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M119.074 5.961v17.484h6.841v2.668h-10.16V5.961h3.319z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M135.584 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.927 9.927 0 01-.116 1.625h-12.258c.233 2.639 2.128 4.233 4.571 4.233 2.041 0 3.176-.988 3.785-2.235h3.582c-.902 2.777-3.435 5.007-7.367 5.007zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M153.116 9.885a6.864 6.864 0 013.088.633 6.839 6.839 0 012.475 1.946v-2.325h3.349v15.974h-3.349v-2.38a6.906 6.906 0 01-5.611 2.638c-4.165 0-7.514-3.39-7.514-8.32s3.353-8.166 7.562-8.166zm.699 2.87c-2.473 0-4.86 1.853-4.86 5.304 0 3.451 2.387 5.45 4.86 5.45 2.473 0 4.864-1.938 4.864-5.39 0-3.45-2.361-5.372-4.864-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M169.72 26.114h-3.319V10.139h3.319v2.325c.928-1.595 2.534-2.579 4.804-2.579v3.438h-.863c-2.447 0-3.958 1.014-3.958 4.405l.017 8.386z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M188.966 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.044 1.504-4.044 4.435v8.922h-3.319V10.14h3.319v1.826a6.16 6.16 0 014.774-2.08c3.755 0 6.582 2.347 6.582 6.812v9.416h-3.293v-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M196.049 5.905a2.105 2.105 0 011.309-1.931 2.117 2.117 0 012.294.458 2.102 2.102 0 01-1.48 3.588 2.109 2.109 0 01-1.509-.612 2.08 2.08 0 01-.614-1.503zm.431 4.234h3.319v15.974h-3.319V10.14z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M215.558 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.048 1.504-4.048 4.435v8.922h-3.337V10.14h3.319v1.826a6.192 6.192 0 014.796-2.08c3.755 0 6.56 2.338 6.56 6.803v9.425h-3.289l.018-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M229.538 9.885c2.62 0 4.571 1.216 5.559 2.579v-2.325h3.349V26.37c0 4.35-2.823 7.629-7.829 7.629-4.282 0-7.454-2.119-7.864-5.656h3.289c.496 1.655 2.274 2.785 4.575 2.785 2.559 0 4.48-1.569 4.48-4.758v-2.664a6.903 6.903 0 01-5.559 2.665c-4.222 0-7.571-3.392-7.571-8.321 0-4.93 3.337-8.166 7.571-8.166zm.699 2.87c-2.478 0-4.864 1.853-4.864 5.304 0 3.452 2.386 5.45 4.864 5.45 2.477 0 4.86-1.938 4.86-5.39 0-3.45-2.357-5.372-4.86-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M242.702 26.316a2.134 2.134 0 01-1.987-1.287 2.114 2.114 0 011.53-2.908 2.138 2.138 0 012.194.896c.235.349.361.76.361 1.18a2.096 2.096 0 01-1.289 1.956 2.11 2.11 0 01-.809.163z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M260.584 21.996h-8.477l-1.455 4.117h-3.453l7.251-20.2h3.846l7.247 20.2h-3.492l-1.467-4.117zM256.38 9.932l-3.341 9.365h6.612l-3.271-9.365z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M268.681 5.961H272v20.152h-3.319V5.961z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218zm.126-1.59c-3.734 0-6.76-3.207-6.76-7.16 0-3.954 3.018-7.16 6.75-7.16 3.734 0 6.76 3.206 6.76 7.16s-3.021 7.16-6.76 7.16h.01zm-.126-6.28c.729 0 1.44-.214 2.046-.617a3.67 3.67 0 001.356-1.646 3.652 3.652 0 00-.798-3.995 3.687 3.687 0 00-4.012-.794 3.679 3.679 0 00-1.653 1.35 3.655 3.655 0 00-.62 2.037c.002.971.39 1.902 1.08 2.59a3.698 3.698 0 002.601 1.076z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path></g></svg></div><nav class="mt-6 md:mt-10"><ul class="flex flex-wrap justify-center space-x-8 gap-x-8"><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/courses/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Courses</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/the-batch/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">The Batch</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/community/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Community</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/careers/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Careers</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/about/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">About</div></a></li></ul></nav><div class="flex mt-12"><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.facebook.com/1027125564106325" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsFacebook" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.instagram.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsInstagram" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.9 3.9 0 0 0-1.417.923A3.9 3.9 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.9 3.9 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.9 3.9 0 0 0-.923-1.417A3.9 3.9 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599s.453.546.598.92c.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.5 2.5 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.5 2.5 0 0 1-.92-.598 2.5 2.5 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233s.008-2.388.046-3.231c.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92s.546-.453.92-.598c.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92m-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217m0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://twitter.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsTwitter" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.linkedin.com/company/18246783" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsLinkedin" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.youtube.com/c/Deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsYoutube" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.01 2.01 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.01 2.01 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31 31 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.01 2.01 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A100 100 0 0 1 7.858 2zM6.4 5.209v4.818l4.157-2.408z"></path></svg></a></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"donations_enabled":false,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.121","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-303","id":"683767a8432a830001a47c59","uuid":"24ff80e3-1497-4ab2-a8f7-8282a479e14c","title":"Claude 4 Advances Code Gen, How DeepSeek Built V3 For $5.6m, Google I/O Roundup, O’Reilly Versus OpenAI","html":"\n\u003c!--kg-card-begin: html--\u003e\n\u003cdiv id=\"elevenlabs-audionative-widget\" data-height=\"90\" data-width=\"100%\" data-frameborder=\"no\" data-scrolling=\"no\" data-publicuserid=\"e20b5cfed36900db239c005920538f20ce435963e95a0a4106d34bdd6bf0e46d\" data-playerurl=\"https://elevenlabs.io/player/index.html\" \u003eLoading the \u003ca href=\"https://elevenlabs.io/text-to-speech?ref=dl-staging-website.ghost.io\" target=\"_blank\" rel=\"noopener\"\u003eElevenlabs Text to Speech\u003c/a\u003e AudioNative Player...\u003c/div\u003e\n\u003c!--kg-card-end: html--\u003e\n\u003cp\u003eDear friends,\u003c/p\u003e\u003cp\u003eI am alarmed by the proposed cuts to U.S. funding for basic research, analyzed\u0026nbsp;\u003ca href=\"https://www.science.org/content/article/trump-s-proposed-budget-would-mean-disastrous-cuts-science?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003ehere\u003c/a\u003e, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done.\u003c/p\u003e\u003cp\u003eIf not for\u0026nbsp;\u003ca href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=0835878\u0026\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003efunding for my early work in deep learning\u003c/a\u003e\u0026nbsp;from the National Science Foundation (NSF)\u0026nbsp; and Defense Advanced Research Projects Agency (DARPA), which disburse much of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas.\u003c/p\u003e\u003cp\u003eIn fact, such funding benefits the U.S. more than any other nation.\u0026nbsp;\u0026nbsp;Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and\u0026nbsp;(ii) the process of doing research creates new talent for that nation.\u003c/p\u003e\u003cp\u003eWhy does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies.\u003c/p\u003e\u003cp\u003eIn a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work.\u003c/p\u003e\u003cp\u003eThus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technology\u0026nbsp;\u003ca href=\"https://www.congress.gov/117/meeting/house/114100/documents/HHRG-117-SY21-20211005-SD002.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f#:~:text=The%20directive%20does%20not%20claim,Defense%20Ashton%20Carter%20in%202010\" rel=\"noopener\"\u003epoints out\u003c/a\u003e,\u0026nbsp;“... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.”\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--96--2.png\" class=\"kg-image\" alt=\"Bar chart showing proposed 2026 U.S. science funding cuts vs. 2025 for agencies like USFS, NSF, NASA, and DoE.\" loading=\"lazy\" width=\"1200\" height=\"677\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/unnamed--96--2.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/unnamed--96--2.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--96--2.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eFurther, generative AI is evolving so rapidly that staying on the cutting edge is what’s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable. Studies like\u0026nbsp;\u003ca href=\"https://www.etsg.org/ETSG2007/papers/strobl.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003ethis one\u003c/a\u003e\u0026nbsp;(albeit done while the internet was not as prevalent as it is today) also show how knowledge diffuses locally much faster than globally.\u003c/p\u003e\u003cp\u003eChina was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China’s tech ecosystem is very open internally, and this has helped it to catch up over the past two years:\u003c/p\u003e\u003cul\u003e\u003cli\u003eThere is ample funding for open academic research in China.\u003c/li\u003e\u003cli\u003eChina’s businesses such as DeepSeek and Alibaba have released cutting-edge, open-weights models. This openness at the corporate level accelerates diffusion of knowledge.\u003c/li\u003e\u003cli\u003eChina’s labor laws make non-compete agreements (which stop an employee from jumping ship to a competitor) relatively hard to enforce, and the work culture supports significant idea sharing among employees of different companies; this has made circulation of ideas relatively efficient.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhile there’s also much about China that I would not seek to emulate, the openness of its tech ecosystem has helped it accelerate.\u003c/p\u003e\u003cp\u003eIn 1945, Vannevar Bush’s landmark report “\u003ca href=\"https://nsf-gov-resources.nsf.gov/2023-04/EndlessFrontier75th_w.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eScience, The Endless Frontier\u003c/a\u003e”\u0026nbsp;laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit\u0026nbsp;the U.S.\u003c/p\u003e\u003cp\u003eThe good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research.\u003c/p\u003e\u003cp\u003eAndrew\u003c/p\u003e\u003chr\u003e\u003ch2 id=\"a-message-from-deeplearningai-and-snowflake\"\u003eA MESSAGE FROM\u0026nbsp;DEEPLEARNING.AI AND SNOWFLAKE\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003ca href=\"https://www.snowflake.com/en/summit/dev-day/?utm_source=deeplearningai\u0026utm_medium=partner\u0026utm_campaign=snowflake-partner-devday25\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/05/GqglM2_WAAAcXxW.jpg\" class=\"kg-image\" alt=\"Promo banner for: Snowflake’s Dev Day 2025\" loading=\"lazy\" width=\"1200\" height=\"630\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/GqglM2_WAAAcXxW.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/GqglM2_WAAAcXxW.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/GqglM2_WAAAcXxW.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/a\u003e\u003c/figure\u003e\u003cp\u003eWe’re featured partners of Snowflake’s Dev Day 2025, a full day for AI and data practitioners to explore cutting-edge demos, make valuable contacts, and hear from top voices in the field (including Andrew Ng). See you on June 5!\u0026nbsp;\u003ca href=\"https://www.snowflake.com/en/summit/dev-day/?utm_source=deeplearningai\u0026utm_medium=partner\u0026utm_campaign=snowflake-partner-devday25\" rel=\"noreferrer\"\u003eRegister here\u003c/a\u003e\u003c/p\u003e\u003ch1 id=\"news\"\u003eNews\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png\" class=\"kg-image\" alt=\"AI model performance comparison chart: Claude Opus 4, Sonnet 4, Sonnet 3.7, OpenAI o3, GPT-4.1, and Gemini 2.5 Pro.\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/unnamed--97-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/unnamed--97-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--97-.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"claude-4-advances-code-generation\"\u003eClaude 4 Advances Code Generation\u003c/h1\u003e\u003cp\u003eAnthropic continued its tradition of building AI models that raise the bar in coding tasks.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Anthropic launched\u0026nbsp;\u003ca href=\"https://www.anthropic.com/news/claude-4?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eClaude 4 Sonnet 4 and Claude Opus 4\u003c/a\u003e, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eInput/output:\u0026nbsp;\u003c/strong\u003eText, images, PDF files\u0026nbsp;in (up to 200,000 tokens); text out (Claude Sonnet 4 up to 64,000 tokens, Claude Opus 4 up to 32,000 tokens)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFeatures:\u0026nbsp;\u003c/strong\u003eParallel tool use including computer use, selectable reasoning mode with visible reasoning tokens, multilingual (15 languages)\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePerformance:\u0026nbsp;\u003c/strong\u003eRanked Number One in LMSys WebDev Arena, state-of-the-art on SWE-bench and Terminal-bench\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAvailability/price:\u003c/strong\u003e\u0026nbsp;Anthropic API, Amazon Bedrock, Google Cloud Vertex AI.\u0026nbsp;\u003cem\u003eClaude Sonnet 4\u003c/em\u003e\u0026nbsp;$3/$15 per million input/output tokens,\u0026nbsp;\u003cem\u003eClaude Opus 4\u003c/em\u003e\u0026nbsp;$15/$75 per million input/output tokens\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eUndisclosed:\u0026nbsp;\u003c/strong\u003eParameter counts, specific training methods and datasets\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to be\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2112.00861?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003ehelpful, honest, and harmless\u003c/a\u003e\u0026nbsp;according to human and\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2212.08073/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eAI feedback\u003c/a\u003e.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe models make reasoning tokens visible within limits. For especially lengthy chains of thought, an unspecified smaller model summarizes reasoning tokens.\u003c/li\u003e\u003cli\u003eGiven local file access, Claude Opus 4 can create and manipulate files to store information. For instance, prompted to maintain a knowledge base while playing a Pokémon video game, the model produced a guide to the game that offered advice such as, “If stuck, try OPPOSITE approach” and “Change Y-coordinate when horizontal movement fails.”\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic’s tests.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eOn\u0026nbsp;\u003ca href=\"https://openai.com/index/introducing-swe-bench-verified/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eSWE-bench Verified\u003c/a\u003e, which tests the model’s ability to solve software issues from GitHub, Claude Opus 4 succeeded 72.5 percent of the time, and Claude Sonnet 4 succeeded 72.7 percent of the time. The next best model, OpenAI o3, succeeded 70.3 percent of the time.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://www.tbench.ai/about?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eTerminal-bench\u003c/a\u003e\u0026nbsp;evaluates how well models work with the benchmark’s built-in agentic framework to perform\u0026nbsp;tasks on a computer terminal. Claude Opus 4 succeeded 39.2 percent of the time and Claude Sonnet 4 succeeded 33.5 percent of the time, whereas the closest competitor, OpenAI GPT 4.1, succeeded 30.3 percent of the time. Using Claude Code as the agentic framework, Claude Opus 4 succeeded 43.2 percent of the time and Claude Sonnet 4 succeeded 35.5 percent of the time.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including a\u0026nbsp;\u003ca href=\"https://x.com/petergyang/status/1925593200993214687?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eTetris clone\u003c/a\u003e\u0026nbsp;built in one shot\u0026nbsp;and a seven-hour stint\u0026nbsp;\u003ca href=\"https://arstechnica.com/ai/2025/05/anthropic-calls-new-claude-4-worlds-best-ai-coding-model/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003erefactoring Rakutan’s open-source code base\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Prompting expert @elder_plinius published a text file that is purported to be\u0026nbsp;\u003ca href=\"https://github.com/elder-plinius/CL4R1T4S/blob/main/ANTHROPIC/Claude_4.txt?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eClaude 4’s system prompt\u003c/a\u003e\u0026nbsp;and includes some material\u0026nbsp;that does not appear in Anthropic’s own\u0026nbsp;\u003ca href=\"https://docs.anthropic.com/en/release-notes/system-prompts?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003epublication\u003c/a\u003e\u0026nbsp;of the prompts. It is instructive to see how it conditions\u0026nbsp;the model for\u0026nbsp;tool use, agentic behavior, and reasoning.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--60-.gif\" class=\"kg-image\" alt=\"Side-by-side of a fern leaf and its digital code representation, illustrating nature's pattern-to-code transformation.\" loading=\"lazy\" width=\"600\" height=\"338\" srcset=\"https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--60-.gif 600w\"\u003e\u003c/figure\u003e\u003ch1 id=\"google-io-overdrive\"\u003eGoogle\u0026nbsp;I/O Overdrive\u003c/h1\u003e\u003cp\u003eGoogle revamped its roster of models, closed and open, and added more AI-powered features to its existing products.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Google staged\u0026nbsp;a parade of\u0026nbsp;\u003ca href=\"https://blog.google/technology/developers/google-ai-developer-updates-io-2025/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eannouncements\u003c/a\u003e\u0026nbsp;at this year’s\u0026nbsp;I/O developer conference. New offerings include improvements to\u0026nbsp;\u003ca href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eGemini 2.5 Pro and Gemini 2.5 Flash\u003c/a\u003e\u0026nbsp;and a preview of\u0026nbsp;\u003ca href=\"https://developers.googleblog.com/en/introducing-gemma-3n/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eGemma 3n\u003c/a\u003e\u0026nbsp;(all three generally available in June),\u0026nbsp;the updated\u0026nbsp;\u003ca href=\"https://gemini.google/overview/video-generation/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eVeo 3\u003c/a\u003e\u0026nbsp;video generator (available via Flow, Google’s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;The I/O offerings spanned from public-facing products to developer tools.\u003c/p\u003e\u003cul\u003e\u003cli\u003eGoogle updated Gemini 2.5 Pro and the speedier Gemini 2.5 Flash with audio output, so both models now take in text, audio, images, and video and produce text and audio. In addition, they offer summaries of tokens produced while\u0026nbsp;reasoning. Gemini-2.5-Pro-Preview-05-06, which topped the LMSys\u0026nbsp;\u003ca href=\"https://lmarena.ai/leaderboard/text?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eText Arena\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://web.lmarena.ai/leaderboard?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eWebDev Arena\u003c/a\u003e\u0026nbsp;(tied with Claude 4 Opus and Sonnet), lets users set a reasoning budget up to 128,000 tokens, enabling it to outperform OpenAI o3 and o4-mini (set to high effort) on math, coding, and multimodal benchmarks in Google’s tests. Gemini-2.5-Flash-Preview-05-20\u0026nbsp;uses 22 percent fewer tokens than its predecessor while ranking near the top of the LMSys Text Arena and WebDev Arena.\u003c/li\u003e\u003cli\u003eThe Veo 3\u0026nbsp;text-to-video generator produces 3840x2160-pixel video with audio (dialogue, sound effects, and music) with creative controls including the ability to add and remove objects and maintain consistent characters. It bested Kuaishu Kling 2.0, Runway Gen 3, and OpenAI Sora in Google’s comparisons.\u0026nbsp;\u003c/li\u003e\u003cli\u003eNew members of Google’s\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/google-releases-gemma-3-vision-language-models-with-open-weights/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eGemma 3\u003c/a\u003e\u0026nbsp;family of open-weights models, Gemma 3n 5B and 8B, are multilingual (over 140 languages), multimodal (text, vision, audio in; text out), and optimized for mobile platforms. Gemma-3n-E4B-it (8 billion parameters) ranks just ahead of Anthropic Claude 3.7 Sonnet in the LMSys Text Arena. Gemma 3n 5B and 8B are 1.5 times faster than their predecessors and require 2 gigabytes and 3 gigabytes of memory, respectively, thanks to\u0026nbsp;\u003ca href=\"https://ai.google.dev/gemma/docs/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003etechniques\u003c/a\u003e\u0026nbsp;that include per-layer embeddings, key-value caching, conditional parameter loading\u0026nbsp;(constraining active parameters to specific modalities at inference), and a Matryoshka Transformer design that dynamically activates nested sub-models. They’re available in preview via Google’s AI Studio, AI Edge, GenAI SDK, or MediaPipe.\u003c/li\u003e\u003cli\u003eGoogle introduced several specialized AI tools and models.\u0026nbsp;\u003ca href=\"https://blog.google/technology/google-labs/jules/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eJules\u003c/a\u003e\u0026nbsp;is an autonomous, asynchronous, multi-agent coding assistant that clones repos into a secure virtual machine to perform tasks like writing tests, building features, and fixing bugs (available in public beta).\u0026nbsp;\u003ca href=\"https://developers.googleblog.com/en/google-io-2025-developer-keynote-recap/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eSignGemma\u003c/a\u003e\u0026nbsp;translates American sign language to text (previously ASL to English).\u0026nbsp;\u003ca href=\"https://developers.google.com/health-ai-developer-foundations/medgemma?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eMedGemma\u003c/a\u003e\u0026nbsp;analyzes medical text and images (part of the open-weights collection Health AI Developer Foundations).\u0026nbsp;\u003c/li\u003e\u003cli\u003eBuilding on Google Search’s AI Overviews, Google is further building AI into search. Google Search’s\u0026nbsp;\u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eAI Mode\u003c/a\u003e\u0026nbsp;uses Gemini 2.5 to deliver a “deep search” mode that decomposes users’ questions into hundreds of sub-queries for analysis and visualization. Google plans to integrate AI Mode features into its core search product. In addition, Google Search’s AI Mode will gain\u0026nbsp;\u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f#live-capabilities\" rel=\"noopener\"\u003eSearch Live\u003c/a\u003e\u0026nbsp;(real-time, audio-enabled visual interaction via camera) and\u0026nbsp;\u003ca href=\"https://blog.google/products/search/google-search-ai-mode-update/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f#agentic-capabilities\" rel=\"noopener\"\u003eagentic features\u003c/a\u003e\u0026nbsp;(for\u0026nbsp;tasks\u0026nbsp;such as\u0026nbsp;purchasing tickets). Computer-use capabilities are coming to the Gemini API and Vertex AI.\u0026nbsp;\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;Google is catching up with the Microsoft/OpenAI colossus\u0026nbsp;on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3’s text-to-video-plus-audio output shows\u0026nbsp;\u003ca href=\"https://x.com/jerrod_lew/status/1927092379892265139?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003emarked\u003c/a\u003e\u0026nbsp;\u003ca href=\"https://x.com/yo/status/1925722327171387670?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eimprovement\u003c/a\u003e\u0026nbsp;over the previous version.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;The number of tokens Google processed\u0026nbsp;monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google’s progress contrasts with Apple’s ongoing\u0026nbsp;\u003ca href=\"https://www.bloomberg.com/news/features/2025-05-18/how-apple-intelligence-and-siri-ai-went-so-wrong?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003estruggles\u003c/a\u003e. Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate\u0026nbsp;them into legacy products, Apple’s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI’s\u0026nbsp;\u003ca href=\"https://www.nytimes.com/2025/05/21/technology/openai-jony-ive-deal.html?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eacquisition\u003c/a\u003e\u0026nbsp;of LoveFrom, the startup founded by its former lead product designer Jony Ive.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Google I/O 2025 was\u0026nbsp;a strong showing of generative AI capabilities! There’s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building\u0026nbsp;numerous innovative products.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--98-.png\" class=\"kg-image\" alt=\"DeepSeek computation diagram showing transformer blocks, multi-head attention, and routing, using FP8 and BF16 precision.\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/unnamed--98-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/unnamed--98-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--98-.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"how-deepseek-did-it\"\u003eHow DeepSeek Did It\u003c/h1\u003e\u003cp\u003eDeepSeek made headlines late last year,\u0026nbsp;when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer\u0026nbsp;shared new details about\u0026nbsp;its method.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Chenggang Zhao and colleagues at DeepSeek described\u0026nbsp;\u003ca href=\"https://www.arxiv.org/abs/2505.09343?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003esoftware and hardware choices\u003c/a\u003e\u0026nbsp;that reduced memory\u0026nbsp;and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and\u0026nbsp;DeepSeek-V3.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eMixture of experts (MoE) basics:\u003c/strong\u003e\u0026nbsp;The MoE architecture uses different subsets of a model’s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model’s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe authors built a mixed-precision training algorithm to reduce the memory requirements of training MoE models. They used FP8 (8-bit) numbers to perform computations including linear transformations and 16- or 32-bit precision to perform others such as computing embeddings. (They say DeepSeek-V3 was the first open LLM to have been trained using FP8.)\u003c/li\u003e\u003cli\u003eThe authors noticed that communication between GPUs inside a node was four times faster than communication between nodes. To ensure fast communication when routing tokens to experts, they limited the algorithm to process them within up to 4 nodes.\u003c/li\u003e\u003cli\u003eTo utilize GPUs more fully, they divided each GPU’s input data so the chip processes computation and communication at the same time. Specifically, the chip computes attention or MoE layers on one part of the data and simultaneously sends the other part of the data to other GPUs or aggregates it from other GPUs as necessary.\u003c/li\u003e\u003cli\u003eTo further save inference memory, the models use multi-head latent attention, which saves memory during execution relative to other variants of attention. The authors compared their implementation to the variant GQA used in Qwen 2.5 72B and Llama 3.1 405B. Their method (70 kilobytes per token) used far less memory than Qwen-2.5 (328 kilobytes per token) or Llama 3.1 (516 kilobytes per token).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eDeepSeek-V3\u003c/a\u003e\u0026nbsp;made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers were\u0026nbsp;\u003ca href=\"https://stratechery.com/2025/deepseek-faq/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eskeptical\u003c/a\u003e\u0026nbsp;of the reported cost, pointing out that the $5.6 million dollar figure doesn’t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of training\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/deepseek-r1-an-affordable-rival-to-openais-o1/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eDeepSeek-R1\u003c/a\u003e\u0026nbsp;remains unknown.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn’t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Shortly after DeepSeek-R1 was released, some engineers claimed — without presenting evidence — that DeepSeek had copied their work. DeepSeek’s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--99-.png\" class=\"kg-image\" alt=\"Bar chart of AUROC scores for model recognition of non-public and public data across GPT versions, highlighting performance differences.\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2025/05/unnamed--99-.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2025/05/unnamed--99-.png 1000w, https://dl-staging-website.ghost.io/content/images/2025/05/unnamed--99-.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"did-gpt-4o-train-on-o%E2%80%99reilly-books\"\u003eDid GPT-4o Train on O’Reilly Books?\u003c/h1\u003e\u003cp\u003eA study co-authored by tech-manual publisher Tim O’Reilly shows that OpenAI trained GPT-4o on parts of his company’s books that were not made freely available.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat happened:\u003c/strong\u003e\u0026nbsp;O’Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss\u0026nbsp;\u003ca href=\"https://ssrc-static.s3.us-east-1.amazonaws.com/OpenAI-Training-Violations-OReillyBooks_Sruly-OReilly-Strauss_SSRC_04012025.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003efound\u003c/a\u003e\u0026nbsp;that GPT-4o was able to identify verbatim excerpts from dozens of O’Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model’s training data.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;The researchers adapted the\u0026nbsp;\u003ca href=\"https://arxiv.org/pdf/2402.09910?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\"\u003eDE-COP\u003c/a\u003e\u0026nbsp;method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books.\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe team selected 34 O’Reilly Media books and divided them into roughly 14,000 paragraphs.\u003c/li\u003e\u003cli\u003eThey labeled the paragraphs private (paywalled) or public (when O’Reilly Media publishes a book, it distributes freely on the web chapters 1 and 4 as well as the first 1,500 characters of other chapters). They also labeled the paragraphs according to whether they were published before or after the models’ knowledge cutoff dates.\u003c/li\u003e\u003cli\u003eThe team built multiple-choice quizzes, each composed of a verbatim paragraph and three paraphrased versions generated by Claude 3.5 Sonnet. The researchers ordered the paragraphs and paraphrases in all permutations to eliminate potential position bias.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;The authors asked each model to identify the verbatim paragraph and calculated each model’s percentage of correct responses. Then they averaged each model’s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren’t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy.\u003c/p\u003e\u003cul\u003e\u003cli\u003eGPT-4o tended to recognize O’Reilly Media content whether or not it was public, but it recognized private paragraphs (82 percent AUROC) markedly more often than public paragraphs (64 percent AUROC).\u003c/li\u003e\u003cli\u003eGPT-4o-mini’s performance was nearly random for both private (56% AUROC) and public material (55% AUROC). The researchers hypothesize that either (i) the model’s smaller size may limit its ability to memorize or (2) OpenAI may reserve premium data to train larger models.\u003c/li\u003e\u003cli\u003eThe earlier GPT-3.5 Turbo recognized public paragraphs (64% AUROC) more often than private paragraphs (54% AUROC), which suggests that it was trained predominantly on freely available data.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eYes, but:\u003c/strong\u003e\u0026nbsp;Newer large language models are better at distinguishing human-written from generated text, even if it wasn’t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while\u0026nbsp;scores for both text that was included and text that was excluded from training sets\u0026nbsp;remain under 96 percent AUROC. “For now,” they write, “the gap remains sufficiently large to reliably separate the two categories.”\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBehind the news:\u003c/strong\u003e\u0026nbsp;Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works’ owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that’s freely available on the web as fair game, and material that’s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, including\u0026nbsp;\u003ca href=\"https://libgenesis.net/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eLibGen\u003c/a\u003e, which includes all 34 of the O’Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted\u0026nbsp;without paywalls or even logins, making it possible even\u0026nbsp;for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recently\u0026nbsp;\u003ca href=\"https://www.theverge.com/news/630079/openai-google-copyright-fair-use-exception?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003elobbied\u003c/a\u003e\u0026nbsp;the United States government to relax copyright laws for AI developers.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O’Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an “extractive dead end” that ultimately diminishes the supply of the high-quality training data.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;We have learned a great deal from O’Reilly Media’s books, and we’re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it’s time for the U.S. Congress —\u0026nbsp; and legislators internationally — to\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/time-to-update-copyright-for-generative-ai/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz-9f91ZK_N_NmLMp6AX94ZoLdUVc_XficAkAPC9Kou3250osRmbIYRHjy_zrS2Kxg7NGmN-f\" rel=\"noopener\"\u003eupdate\u003c/a\u003e\u0026nbsp;copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.\u003c/p\u003e","comment_id":"683767a8432a830001a47c59","feature_image":"https://dl-staging-website.ghost.io/content/images/2025/05/Untitled-design--25-.png","featured":false,"visibility":"public","created_at":"2025-05-28T12:44:40.000-07:00","updated_at":"2025-05-31T15:42:34.000-07:00","published_at":"2025-05-28T12:53:00.000-07:00","custom_excerpt":"The Batch AI News and Insights: I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have for U.S. competitiveness in AI and other areas.","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"6837677a432a830001a47c51","name":"May 28, 2025","slug":"may-28-2025","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/may-28-2025/"},{"id":"68376a03432a830001a47c9a","name":"issue-303","slug":"issue-303","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-303/"}],"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-303/","excerpt":"The Batch AI News and Insights: I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have for U.S. competitiveness in AI and other areas.","reading_time":15,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Claude 4 Advances Code Gen, How DeepSeek Built V3 For $5.6m, Google I/O Roundup, and more...","meta_description":"The Batch AI News and Insights: I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have...","email_subject":null,"frontmatter":null,"feature_image_alt":"Bar chart showing proposed 2026 U.S. science funding cuts vs. 2025 for agencies like USFS, NSF, NASA, and DoE.","feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2025/05/Untitled-design--25-.png","dimensions":{"width":1200,"height":677}},"banner":{"title":"AI is the new electricity","databaseId":29050,"id":"cG9zdDoyOTA1MA==","featuredImage":{"node":{"altText":"AI is the new electricity. Are you ready to flip the switch? Download your free copy of the ebook.","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2023/03/2.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://www.deeplearning.ai/resources/#ebooks","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Databricks C1","date":"2025-06-04T07:12:29","databaseId":36631,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/3HlCuvP","courseName":"DSPy: Build and Optimize Agentic Apps","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)","isOpenInNewTab":true}},{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}}]}}},"__N_SSG":true},"page":"/the-batch/[slug]","query":{"slug":"issue-303"},"buildId":"Qy9Eh4N0MrfE3ddReY9v4","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>