<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>AI Agents Generate Novel Research, Google Imagen 3 Raises the Bar, Alibaba’s Open Models for Specialized Tasks, Scaling Laws for Data Quality</title><meta name="description" content="The Batch AI News and Insights: I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><link rel="canonical" href="https://www.deeplearning.ai/the-batch/issue-263/"/><meta property="og:type" content="article" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:title" content="AI Agents Generate Novel Research, Google Imagen 3 Raises the Bar, Alibaba’s Open Models for Specialized Tasks, Scaling Laws for Data Quality" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:description" content="The Batch AI News and Insights: I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:site_name" content="AI Agents Generate Novel Research, Google Imagen 3 Raises the Bar, Alibaba’s Open Models for Specialized Tasks, Scaling Laws for Data Quality" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="og:url" content="https://www.deeplearning.ai/the-batch/issue-263/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="article:published_time" content="2024-08-21T12:25:32.000-07:00"/><meta property="article:modified_time" content="2024-09-05T12:54:01.000-07:00"/><meta property="article:tag" content="The Batch Newsletter"/><meta property="article:tag" content="issue-263"/><meta property="article:tag" content="Aug 21, 2024"/><meta property="article:author" content="https://www.facebook.com/DeepLearningAIHQ/"/><meta property="twitter:title" content="AI Agents Generate Novel Research, Google Imagen 3 Raises the Bar, Alibaba’s Open Models for Specialized Tasks, Scaling Laws for Data Quality" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:description" content="The Batch AI News and Insights: I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications..." data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:url" content="https://www.deeplearning.ai/the-batch/issue-263/" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:label1" content="Written by"/><meta property="twitter:data1" content="DeepLearning.AI"/><meta property="twitter:label2" content="Filed under"/><meta property="twitter:data2" content="The Batch Newsletter"/><meta property="twitter:card" content="summary_large_image" data-sentry-element="meta" data-sentry-source-file="seo.tsx"/><meta property="twitter:creator" content="@DeepLearningAI"/><meta property="twitter:site" content="https://twitter.com/DeepLearningAI/"/><meta name="twitter:image" content="https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80-.jpg"/><meta property="og:image" content="https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80-.jpg"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="676"/><script type="application/ld+json">{"@context":"https://schema.org/","@type":"Article","datePublished":"2024-08-21T12:25:32.000-07:00","dateModified":"2024-09-05T12:54:01.000-07:00","author":{"@type":"Article","name":"DeepLearning.AI","image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","sameAs":"[\"https://www.deeplearning.ai/\", \"https://twitter.com/DeepLearningAI_/\", \"https://www.facebook.com/DeepLearningAIHQ/\"]"},"keywords":"[object Object], [object Object], [object Object]","headline":"AI Agents Generate Novel Research, Google Imagen 3 Raises the Bar, Alibaba’s Open Models for Specialized Tasks, Scaling Laws for Data Quality","image":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80-.jpg","width":1200,"height":676},"publisher":{"@type":"Organization","name":"AI Agents Generate Novel Research, Google Imagen 3 Raises the Bar, Alibaba’s Open Models for Specialized Tasks, Scaling Laws for Data Quality","logo":{"@type":"ImageObject","url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","width":60,"height":60}},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.deeplearning.ai"},"description":"The Batch AI News and Insights: I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications..."}</script><meta name="next-head-count" content="31"/><link href="/static/favicons/favicon.ico" rel="shortcut icon"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;display=swap"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&amp;display=swap"/><link rel="preload" href="/_next/static/css/54b174af9991199d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/54b174af9991199d.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6f396d9f2f265155.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6f396d9f2f265155.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-7690c02358b700f2.js" defer=""></script><script src="/_next/static/chunks/framework-c54e8763846ac33b.js" defer=""></script><script src="/_next/static/chunks/main-9fcc7c9a63f70912.js" defer=""></script><script src="/_next/static/chunks/pages/_app-bece9363d7a6c0bb.js" defer=""></script><script src="/_next/static/chunks/3a17f596-e9670ced3bfab562.js" defer=""></script><script src="/_next/static/chunks/36d2f571-09e32b60ec93a4dc.js" defer=""></script><script src="/_next/static/chunks/5c0b189e-c0f75ca9b44e520b.js" defer=""></script><script src="/_next/static/chunks/5567-47eb911ba5f222d7.js" defer=""></script><script src="/_next/static/chunks/2251-380253169bb2e795.js" defer=""></script><script src="/_next/static/chunks/3864-a4ae56e91852593b.js" defer=""></script><script src="/_next/static/chunks/4965-3585432476b27ea9.js" defer=""></script><script src="/_next/static/chunks/3791-0bbbd4cd0bf31c22.js" defer=""></script><script src="/_next/static/chunks/pages/the-batch/%5Bslug%5D-c03a8bcef188ef99.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_buildManifest.js" defer=""></script><script src="/_next/static/Qy9Eh4N0MrfE3ddReY9v4/_ssgManifest.js" defer=""></script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@600&display=swap"/></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5C5VGGJ" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div data-sentry-component="Layout" data-sentry-source-file="Layout.tsx"><aside id="top-announcement" class="text-neutral-900  " style="color:#FFFFFF;background-color:#05256C;background-image:linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)" data-sentry-component="AnnouncementBanner" data-sentry-source-file="AnnouncementBanner.tsx"><div class="container--boxed py-3 flex items-center justify-between "><div class="flex items-center"><p class="text-sm lg:text-base">✨ New course! Enroll in<!-- --> <a href="https://bit.ly/3HlCuvP" class="underline" target="_blank">DSPy: Build and Optimize Agentic Apps</a></p></div><div class="flex items-center"><button class="bg-transparent p-0 border-none text-xl opacity-70 hover:opacity-100 transition-opacity"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><line x1="15" y1="9" x2="9" y2="15"></line><line x1="9" y1="9" x2="15" y2="15"></line></svg></button></div></div></aside><header id="main-navigation" data-testid="main-navigation-testid" class="static h-[64px] md:h-[100px] top-0 w-full z-[100] flex items-center bg-white transition-all ease-in-out duration-200" data-sentry-component="Header" data-sentry-source-file="index.tsx"><div class="container--boxed flex justify-between items-center"><div class="max-w-[185px] lg:max-w-[235px]"><a href="/the-batch/"><div class="flex items-center"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27300%27%20height=%2792%27/%3e"/></span><img alt="The Batch" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="The Batch" loading="lazy" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" srcSet="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=384&amp;q=75 1x, /_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75 2x" src="/_next/image/?url=%2F_next%2Fstatic%2Fmedia%2Fdlai-batch-logo.a60dbb9f.png&amp;w=640&amp;q=75"/></noscript></span></div></a></div><div class="flex items-center"><div class="hidden lg:flex items-center"><nav aria-label="Primary" data-sentry-component="Nav" data-sentry-source-file="Nav.tsx"><ul class="flex items-center p-0 m-0 list-none"><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/courses/"><div class="NavItem_navItemLink__Aq6E5"><span>Explore Courses</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/the-batch/"><div class="NavItem_navItemLink__Aq6E5"><span>AI Newsletter</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/"><div>The Batch</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/letters/"><div>Andrew&#x27;s Letter</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/data-points/"><div>Data Points</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/the-batch/tag/research/"><div>ML Research</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/"><div>Blog</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/community/"><div class="NavItem_navItemLink__Aq6E5"><span>Community</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="https://community.deeplearning.ai/"><div>Forum</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/events/"><div>Events</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/ambassador/"><div>Ambassadors</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/blog/category/ambassador-spotlight/"><div>Ambassador Spotlight</div></a></li></ul></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/resources/"><div class="NavItem_navItemLink__Aq6E5"><span>Resources</span></div></a></li><li class="NavItem_navItem__FJTIV first-of-type:ml-0 relative flex justify-center whitespace-nowrap" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a class="dlai-gtm-nav-item inline-block text-slate-500 hover:text-brand py-4 lg:px-2 xl:px-4" href="/about/"><div class="NavItem_navItemLink__Aq6E5"><span>Company</span><span class="text-xl ml-1 -mr-1"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="6 9 12 15 18 9"></polyline></svg></span></div></a><ul class="NavItem_subMenu__qK1m4"><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/about/"><div>About</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/careers/"><div>Careers</div></a></li><li class="NavItem_subMenuItem__2oKDr"><a class="dlai-gtm-nav-item NavItem_subMenuItemLink__35KOd" href="/contact/"><div>Contact</div></a></li></ul></li></ul></nav><a class="btn--primary text-base font-medium whitespace-nowrap shadow h-fit py-3 px-4 lg:ml-2 xl:ml-10" data-sentry-element="Link" data-sentry-source-file="index.tsx" href="https://bit.ly/3RB9T8a">Start Learning</a></div><div class="flex items-center" data-sentry-element="Menu" data-sentry-component="MobileMenu" data-sentry-source-file="MobileMenu.tsx" data-headlessui-state=""><button class="lg:hidden" data-sentry-element="MenuButton" data-sentry-source-file="MobileMenu.tsx" id="headlessui-menu-button-:R1qa6:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-4xl text-slate-600" data-sentry-element="FiMenu" data-sentry-source-file="MobileMenu.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button></div></div></div></header><main><div id="content"><nav aria-label="Secondary" class="h-[66px] bg-white sticky top-[60px] z-40 shadow hidden lg:block" data-sentry-component="SecondaryNav" data-sentry-source-file="index.tsx"><div class="container--boxed h-full w-full flex items-center justify-between "><div class="relative flex h-full"><button class="h-full w-14 items-center justify-center absolute top-0 left-0 z-10 group bg-white bg-opacity-75 hidden"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="15 18 9 12 15 6"></polyline></svg></span></button><ul id="nav-secondary" class="list-none p-0 m-0 h-full flex items-center  overflow-x-scroll relative SecondaryNav_navItems__dok3i"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Weekly Issues</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/letters/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Andrew&#x27;s Letters</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/data-points/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Data Points</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/research/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">ML Research</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/business/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Business</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/science/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Science</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/culture/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Culture</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/hardware/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">Hardware</div></a></li><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/tag/ai-careers/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">AI Careers</div></a></li></ul><button class="h-full w-9 items-center justify-center absolute top-0 right-0 group bg-white bg-opacity-75 flex"><span class="flex items-center justify-center w-5 h-5 text-white rounded-full bg-slate-500 group-hover:bg-slate-700"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span></button></div><div class="flex items-center h-full"><ul class="list-none p-0 m-0 h-full flex items-center mr-5"><li class="mr-6 last:mr-0 h-full" data-sentry-component="NavItem" data-sentry-source-file="NavItem.tsx"><a data-sentry-element="Link" data-sentry-source-file="NavItem.tsx" href="/the-batch/about/"><div class="h-full relative flex items-center text-base whitespace-nowrap border-solid border-t-2 border-t-transparent border-b-2 text-slate-400 transition-colors border-b-transparent hover:text-slate-500">About</div></a></li></ul><button type="button" class="bg-white border btn--tracking border-solid border-brand text-brand hover:bg-brand hover:text-white transition-colors px-3 py-1 rounded-md mr-4">Subscribe</button><a href="/search/"><div title="Search" class="transition-colors text-slate-400 hover:text-slate-500"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" class="text-2xl" aria-hidden="true" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></div></a></div></div></nav><article class="pt-5 pb-16 bg-white lg:pt-16 lg:pb-16" data-sentry-component="ArticleDefault" data-sentry-source-file="ArticleDefault.tsx"><header class="post_layoutGrid__0BDX2"><nav aria-label="Breadcrumb flex items-center" data-sentry-component="Main" data-sentry-source-file="Breadcrumb.tsx"><ul class="list-none flex items-center flex-wrap"><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">The Batch</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/"><div class="text-sm md:text-base text-brand-teal font-medium hover:text-brand-teal-1 transition ">Weekly Issues</div></a></li><span class="mx-1 md:mx-2 text-base text-slate-400" data-sentry-component="Separator" data-sentry-source-file="Breadcrumb.tsx"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" data-sentry-element="FiChevronRight" data-sentry-source-file="Breadcrumb.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><polyline points="9 18 15 12 9 6"></polyline></svg></span><li data-sentry-component="Item" data-sentry-source-file="Breadcrumb.tsx"><a data-sentry-element="Link" data-sentry-source-file="Breadcrumb.tsx" href="/the-batch/issue-263/"><div class="text-sm md:text-base text-slate-400 font-normal "><h1 class="capitalize">issue 263</h1></div></a></li></ul></nav><aside class="flex mt-6 lg:hidden"><div class="flex flex-col items-start items-start mr-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm text-sm">Published</div></div><div class="mt-1 text-slate-600 text-base text-sm">Aug 21, 2024</div></div><div class="flex flex-col items-start items-start" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm text-sm"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm text-sm">Reading time</div></div><div class="mt-1 text-slate-600 text-base text-sm">13<!-- --> min read</div></div></aside></header><div class="post_layoutGrid__0BDX2 mt-9"><aside style="grid-column:wide-start / main-start" class="flex-col items-end hidden pr-10 lg:flex"><div class="flex flex-col items-start items-end" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg></div><div class="ml-2 text-sm undefined">Published</div></div><div class="mt-1 text-slate-600 text-base undefined"><a href="/the-batch/tag/aug-21-2024/"><div class="inline-flex px-2 py-1 text-sm font-normal transition-colors rounded-md bg-slate-100 hover:bg-slate-200 text-slate-500">Aug 21, 2024</div></a></div></div><div class="flex flex-col items-start items-end mt-6" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg></div><div class="ml-2 text-sm undefined">Reading time</div></div><div class="mt-1 text-slate-600 text-base undefined">13<!-- --> min read</div></div><div class="flex flex-col items-start items-end mt-6 sticky top-20" data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-263/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-263/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-263/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></aside><div class="prose--styled justify-self-center post_postContent__wGZtc"><p>Dear friends,</p><p>I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications. Two examples are the new Federal Trade Commission (FTC)&nbsp;<a href="https://www.ftc.gov/news-events/news/press-releases/2024/08/federal-trade-commission-announces-final-rule-banning-fake-reviews-testimonials?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">ban on fake product reviews</a>&nbsp;and the&nbsp;<a href="https://www.durbin.senate.gov/imo/media/doc/defiance_act_of_2024.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">DEFIANCE Act</a>, which imposes punishments for creating and disseminating non-consensual deepfake porn. Both rules take a sensible approach to regulating AI insofar as they target harmful applications rather than general-purpose AI technology.</p><p>As I&nbsp;<a href="https://www.deeplearning.ai/the-batch/blenders-versus-bombs-or-why-californias-proposed-ai-law-is-bad-for-everyone/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">described</a>&nbsp;previously, the best way to ensure AI safety is to regulate it at the application level rather than the technology level. This is important because the technology is general-purpose and its builders (such as a developer who releases an open-weights foundation model) cannot control how someone else might use it. If, however, someone applies AI in a nefarious way, we should stop that application.&nbsp;</p><p>Even before generative AI, fake reviews were a problem on many websites, and many tech companies dedicate considerable resources to combating them. A telltale sign of old-school fake reviews is the use of similar wording in different reviews. AI’s ability to automatically paraphrase or rewrite is making fake reviews harder to detect.</p><p>Importantly, the FTC is not going after the makers of foundation models for fake reviews. The provider of an open weights AI model, after all, can’t control what someone else uses it for. Even if one were to try to train a model to put up guardrails against writing reviews, I don’t know how it could distinguish between a real user of a product asking for help writing a legitimate review and a spammer who wanted a fake review. The FTC appropriately aims to ban the application of fake reviews along with other deceptive practices such as buying positive reviews.&nbsp;</p><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80--1.jpg" class="kg-image" alt="" loading="lazy" width="1200" height="676" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed--80--1.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed--80--1.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80--1.jpg 1200w" sizes="(min-width: 720px) 720px"></figure><p>The DEFIANCE Act, which passed unanimously in the Senate (and still requires passage in the House of Representatives before the President can sign it into law) imposes civil penalties for the creating and distributing non-consensual, deepfake porn. This disgusting application is&nbsp;<a href="https://www.nytimes.com/2024/04/08/technology/deepfake-ai-nudes-westfield-high-school.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">harming</a>&nbsp;many people including underage girls. While many image generation models do have guardrails against generating porn, these guardrails often can be circumvented via jailbreak prompts or fine-tuning (for models with open weights).</p><p>Again, DEFIANCE regulates an application, not the underlying technology. It aims to punish people who engage in the application of creating and distributing non-consensual intimate images, regardless of how they are generated — whether the perpetrator uses a diffusion model, a generative adversarial network, or Microsoft Paint to create an image pixel by pixel.</p><p>I hope DEFIANCE passes in the House and gets signed into law. Both rules guard against harmful AI applications without stifling AI technology itself (unlike California’s&nbsp;<a href="https://www.deeplearning.ai/the-batch/californias-proposed-ai-safety-law-puts-developers-at-risk-california-sb-1047-is-intended-to-make-ai-safer-but-its-unclear-requirements-put-developers-innovation-and-open-source-in-jeop/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">poorly designed</a>&nbsp;SB-1047), and they offer a good model for how the U.S. and other nations can protect citizens against other potentially harmful applications.&nbsp;<br><br>Keep learning!</p><p>Andrew&nbsp;</p><p></p><h2 id="a-message-from-deeplearningai">A MESSAGE FROM&nbsp;DEEPLEARNING.AI</h2><figure class="kg-card kg-image-card"><a href="https://www.deeplearning.ai/short-courses/building-ai-applications-with-haystack?ref=dl-staging-website.ghost.io"><img src="https://dl-staging-website.ghost.io/content/images/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png" class="kg-image" alt="" loading="lazy" width="1680" height="945" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png 1600w, https://dl-staging-website.ghost.io/content/images/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png 1680w" sizes="(min-width: 720px) 720px"></a></figure><p>Build flexible, maintainable applications with our new course, “Building AI Applications with Haystack.” Guided by Tuana Çelik, you’ll build projects like a RAG app and a self-reflecting agent using the Haystack framework.&nbsp;<a href="https://www.deeplearning.ai/short-courses/building-ai-applications-with-haystack/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Join for free</a></p><h1 id="news">News</h1><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T140739.984.png" class="kg-image" alt="" loading="lazy" width="1200" height="675" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed---2024-08-21T140739.984.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed---2024-08-21T140739.984.png 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T140739.984.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="ai-agents-for-ai-research">AI Agents for AI Research</h1><p>While some observers argue that large language models can’t produce truly original output, new work prompted them to generate novel scientific research.</p><p><strong>What’s new:</strong>&nbsp;Researchers proposed&nbsp;<a href="https://www.arxiv.org/abs/2408.06292?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">AI Scientist</a>, an agentic workflow that directs large language models to generate ideas for AI research, produce code to test them, and document the enquiry. You can see examples of its output and download the code to generate your own papers&nbsp;<a href="https://github.com/SakanaAI/AI-Scientist?tab=readme-ov-file&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8#read%20me" rel="noopener">here</a>. The team included Chris Lu, Cong Lu, Robert Tjarko Lange, and colleagues at Tokyo-based startup Sakana AI, University of Oxford, University of British Columbia, Vector Institute, and the Canadian Institute for Advanced Research.&nbsp;</p><p><strong>How it works:</strong>&nbsp;The authors used Claude Sonnet 3.5, GPT-4o, DeepSeek Coder, and LLama 3.1 405B to generate papers in three categories: diffusion image modeling, transformer-based language modeling, and “grokking,” which the authors define as generalization and speed of learning in deep neural networks.&nbsp;</p><ul><li>The authors prompted a given large language model (LLM) to generate “the next creative and impactful idea for research” in one of the three categories. Then they provided an API to search papers and asked the LLM to either determine whether its idea was novel (in which case it moved to the next step) or, if it couldn’t determine an answer, generate a search query to find related works. Then the authors asked again in light of the search results. They repeated this process until the LLM made a decision.</li><li>Once they had a novel idea, they prompted the LLM to generate a list of experiments and run them using the&nbsp;<a href="https://github.com/paul-gauthier/aider?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Aider</a>&nbsp;Python library. Then they prompted it to generate notes about the results and generate figures by altering an existing Python script.</li><li>They prompted the LLM to generate a paper, one section at a time, given the notes, figures, sections generated so far, and tips on how to write a paper based on an existing&nbsp;<a href="https://docs.google.com/document/d/16R1E2ExKUCP5SlXWHr-KzbVDx9DBUclra-EbU8IB-iE/edit?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">guide</a>. Then they prompted it to search for related works and add relevant citations. Finally, they asked it to remove redundancy, reduce verbosity, and finalize the document’s format.</li></ul><p><strong>Results:</strong>&nbsp;The team used GPT-4o to evaluate the generated papers according to the&nbsp;<a href="https://neurips.cc/Conferences/2022/ReviewerGuidelines?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">guidelines</a>&nbsp;for papers presented at the Neural Information Processing Systems (NeurIPS) conference. The guidelines include an overall score between 1 (very strongly reject) and 10 (award-quality: flawless and groundbreaking) and a decision to reject or accept the paper.&nbsp;</p><ul><li>Of the four LLMs, Claude Sonnet 3.5 performed best. Its highest-scoring papers achieved 6 (weak accept). With respect to one of Claude’s works, the authors wrote, “The AI Scientist correctly identifies an interesting and well-motivated direction in diffusion modeling research . . . It proposes a comprehensive experimental plan to investigate its idea, and successfully implements it all, achieving good results." The authors provide an archive of Claude’s output&nbsp;<a href="https://drive.google.com/drive/folders/1Mmpz6M1FK4q8e-SewgZcUzdeD0Q2zC39?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">here</a>.&nbsp;</li><li>GPT-4o ranked second. Its highest-scoring paper achieved 5 (borderline accept).</li><li>The generated papers achieved an average score of 4.05 or less (4 is borderline reject) across all models and categories of experiment. The experiments generally involved small networks that were trained and tested on generated data. The authors note that the system often failed to implement its ideas, sometimes fabricated results, and sometimes failed to cite the most relevant papers, among other issues.</li></ul><p><strong>Why it matters:</strong>&nbsp;Agentic workflows are a rising theme in AI research from simpler design patterns like&nbsp;<a href="https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">reflection</a>&nbsp;to complex workflows for&nbsp;<a href="https://arxiv.org/abs/2405.11804?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">translating literature</a>. These workflows make it possible to break down complex problems into more manageable subtasks. By breaking the task of conducting AI research into various stages of generating ideas, testing them, and writing a paper, an LLM that has access to the right tools can generate novel research papers with actual experimental results.&nbsp;</p><p><strong>We’re thinking:</strong>&nbsp;Rather than merely synthesizing existing knowledge, this work points a fascinating direction for using AI to generate new knowledge! Right now, an LLM can suggest starting points for human researchers along with experiments that back up its suggestions.</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142127.807.gif" class="kg-image" alt="" loading="lazy" width="1200" height="676" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed---2024-08-21T142127.807.gif 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed---2024-08-21T142127.807.gif 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142127.807.gif 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="google-imagen-3-raises-the-bar">Google Imagen 3 Raises the Bar</h1><p>Image generation continued its rapid march forward with a new version of Google’s flagship text-to-image model.</p><p><strong>What’s new:</strong>&nbsp;Google&nbsp;<a href="https://deepmind.google/technologies/imagen-3/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">introduced</a>&nbsp;Imagen 3, a proprietary model that improves upon the previous version’s image quality and prompt adherence, with features like inpainting and outpainting to be added in the future. Imagen 3 is available via Google’s&nbsp;<a href="https://aitestkitchen.withgoogle.com/tools/image-fx?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">ImageFX</a>&nbsp;web user interface and&nbsp;<a href="https://cloud.google.com/generative-ai-studio?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Vertex AI</a>&nbsp;Platform. It follows closely upon the releases of Black Forest Labs’ Flux.1 family (open to varying degrees), Midjourney v6.1, and Stability AI Stable Diffusion XL 1 (open weights) — all in the last month.</p><p><strong>How it works:</strong>&nbsp;The accompanying&nbsp;<a href="https://arxiv.org/abs/2408.07009?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">paper</a>&nbsp;does not describe the model’s architecture and training procedures in detail. The authors trained a diffusion model on an unspecified “large” dataset of images, text, and associated annotations that was filtered to remove unsafe, violent, low-quality, generated, and duplicate images as well as personally identifying information. Google’s Gemini large language model generated some image captions used in training to make their language more diverse.&nbsp;</p><p><strong>Results:</strong>&nbsp;Imagen 3 mostly outperformed competing models in head-to-head comparisons based on prompts from datasets including&nbsp;<a href="https://arxiv.org/abs/2404.01291?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">GenAI-Bench</a>,&nbsp;<a href="https://arxiv.org/abs/2205.11487?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">DrawBench</a>, and&nbsp;<a href="https://cdn.openai.com/papers/dall-e-3.pdf?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">DALL-E 3 Eval</a>. The team compared Imagen 3 to Midjourney v6.0, OpenAI DALL-E 3, Stable Diffusion 3 Large, and Stable Diffusion XL 1.0. More than 3,000 evaluators from 71 countries rated the models’ responses in side-by-side comparisons. The raters evaluated image quality, preference regardless of the prompt, adherence to the prompt, adherence to a highly detailed prompt, and ability to generate the correct numbers of objects specified in a prompt. Their ratings (between 1 and 5) were used to compute Elo ratings.</p><ul><li>Imagen 3 swept the overall preference tests. On GenAI-Bench and DrawBench, Imagen 3 (1,099 Elo and 1,068 Elo respectively) beat the next-best Stable Diffusion 3 (1,047 Elo and 1,053 Elo respectively). On DALL-E 3 Eval, Imagen 3 (1,079 Elo) beat the next-best MidJourney v6.0 (1,068 Elo).</li><li>Likewise, Imagen 3 swept the prompt-image alignment benchmarks. On GenAI-Bench and DrawBench, Imagen 3 (1,083 Elo and 1,064 Elo respectively) outperformed the next-best Stable Diffusion 3 (1,047 Elo for both datasets). On DALL-E 3 Eval, Imagen 3 (1,078) narrowly edged out DALL-E 3 (1,077 Elo) and Stable Diffusion 3 (1,069 Elo).</li><li>Imagen 3 showed exceptional strength in following detailed prompts in the&nbsp;<a href="http://arxiv.org/abs/2404.19753?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">DOCCI</a>&nbsp;dataset (photographs with detailed descriptions that averaged 136 words). In that category, Imagen 3 (1,193 Elo) outperformed next-best Midjourney v6.0 (1,079 Elo).</li><li>Although none of the models tested did very well at generating specified numbers of objects from the&nbsp;<a href="https://arxiv.org/abs/2406.14774?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">GeckoNum</a>&nbsp;dataset, Imagen 3 (58.6 Elo) outperformed the next-best DALL-E 3 (46.0 Elo).</li><li>Imagen 3 lost to Midjourney v6.0 across the board in tests of visual appeal regardless of the prompt. It was slightly behind on GenAI-Bench (1,095 Elo versus 1,101 Elo), farther behind on DrawBench (1,063 Elo versus 1,075 Elo), and well behind on DALL-E 3 Eval (1,047 Elo versus 1,095 Elo).</li></ul><p><strong>Why it matters:</strong>&nbsp;Each wave of advances makes image generators more useful for a wider variety of purposes. Google’s emphasis on filtering the training data for safety may limit Imagen 3’s utility in some situations (indeed, some users&nbsp;<a href="https://www.reddit.com/r/Bard/comments/1eo3ge9/imagen_3_is_available_for_everyone_google_is/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">complained</a>&nbsp;that Imagen 3 is more restrictive than Imagen 2, while the Grok2 large language model’s use of an unguardrailed version of Flux.1 for image generation has garnered&nbsp;<a href="https://www.yahoo.com/tech/grok-2-0-takes-guardrails-175824863.html?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">headlines</a>). Nonetheless, precautions are an important ingredient in the evolving text-to-image recipe.</p><p><strong>We’re thinking:</strong>&nbsp;It’s difficult to compare the benchmarks reported for Imagen 3 and the recently released&nbsp;<a href="https://www.deeplearning.ai/the-batch/black-forest-labs-flux-1-outperforms-top-text-to-image-models/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Flux.1</a>, which claims similar improvements over earlier models. In any case, Google has yet to publish a benchmark for generating text, a valuable capability for commercial applications. The Flux.1 models, two of which are open to some degree, may prove to be formidable rivals in this area.&nbsp;</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142223.196.gif" class="kg-image" alt="" loading="lazy" width="1200" height="676" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed---2024-08-21T142223.196.gif 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed---2024-08-21T142223.196.gif 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142223.196.gif 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="open-models-for-math-and-audio">Open Models for Math and Audio</h1><p>Alibaba followed up its open-weights Qwen2 large language models with specialized variations.</p><p><strong>What’s new:</strong>&nbsp;<a href="https://qwenlm.github.io/blog/qwen2-math/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Qwen2-Math</a>&nbsp;and&nbsp;<a href="https://qwenlm.github.io/blog/qwen2-audio/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Qwen2-Audio</a>&nbsp;are model families devoted to, respectively, solving math problems and generating text directly from audio. Both set new states of the art in a variety of English and Chinese benchmarks, and some versions offer open weights. Notably Qwen2-Math-Instruct-72B, whose 72 billion parameters are fine-tuned according to human preferences, outperformed top models including Claude 3.5 Sonnet, Gemini 1.5-Pro, GPT-4o, and Llama-3.1-405B on some math benchmarks.</p><p><strong>Math mavens:</strong>&nbsp;Qwen2-Math models include&nbsp;<a href="https://github.com/QwenLM/Qwen2-Math?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">pretrained</a>&nbsp;and&nbsp;<a href="https://huggingface.co/Qwen/Qwen2-Math-72B-Instruct?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">instruction-tuned</a>&nbsp;variations that comprise 1.5 billion, 7 billion, and 72 billion parameters. The&nbsp;<a href="https://huggingface.co/Qwen/Qwen2-Math-72B-Instruct/blob/main/LICENSE?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">license</a>&nbsp;for the largest version is free for noncommerical development and commercial developers who have less than 100 million monthly active users.&nbsp;</p><ul><li><strong>How it works:</strong>&nbsp;Qwen2-Math base models were initialized to Qwen2 weights and further pretrained on a corpus of math articles, books, exams, and data generated by Qwen2. The instruction-tuned versions were fine-tuned on more model-generated data using supervised learning followed by a reinforcement learning algorithm called&nbsp;<a href="https://arxiv.org/pdf/2402.03300?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">group relative policy optimization</a>. The team removed examples that significantly overlapped benchmark test sets and prominent math competitions.</li><li><strong>Results:</strong>&nbsp;Using few-shot, chain-of-thought prompting, Qwen2-Math-Instruct-72B achieved state-of-the-art performance in English math benchmarks including&nbsp;<a href="https://arxiv.org/abs/2103.03874?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">MATH</a>&nbsp;and Chinese math benchmarks including&nbsp;<a href="https://arxiv.org/abs/2306.16636?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">CMATH</a>,&nbsp;<a href="https://arxiv.org/pdf/2304.06364?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">GaoKao Math Cloze, and GaoKao Math QA</a>. (The 72 billion-parameter Qwen2-Math achieved state-of-the-art scores in&nbsp;<a href="https://arxiv.org/abs/2110.14168?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">GSM8k</a>&nbsp;and&nbsp;<a href="https://arxiv.org/abs/2009.03300?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">MMLU STEM</a>.) Qwen2-Math-Instruct-72B also outperformed Claude 3 Opus, GPT-4 Turbo, Gemini 1.5 Pro and Gemini Math-Specialized 1.5 Pro in the&nbsp;<a href="https://aime24.aimedicine.info/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">AIME 2024</a>&nbsp;math competition in some settings. The smaller, instruction-tuned versions outperformed other models of the same size by some measures.</li></ul><p><strong>Audio/text to text:</strong>&nbsp;A revision of the earlier Qwen-Audio,&nbsp;<a href="https://github.com/QwenLM/Qwen2-Audio?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Qwen2-Audio</a>&nbsp;takes text and audio inputs and generates text outputs. It’s designed to (i) provide text chat in response to voice input including voice transcription and translation between eight languages and (ii) discuss audio input including voice, music, and natural sounds. Weights (8.2 billion parameters) are available for base and instruction-tuned versions. You can try it&nbsp;<a href="https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">here</a>.</p><ul><li><strong>How it works:</strong>&nbsp;Given a text prompt and audio, a&nbsp;<a href="https://github.com/openai/whisper?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Whisperlarge-v3</a>&nbsp;audio encoder embeds the audio, and a pretrained Qwen-7B language model uses the text prompt and audio embedding to generate text. The team further pretrained the system to predict the next text token based on a text-audio dataset that included 370,000 hours of recorded speech, 140,000 hours of music, and 10,000 hours of other sounds. They fine-tuned the system for chat in a supervised fashion and for factuality and prompt adherence using&nbsp;<a href="https://arxiv.org/abs/2305.18290?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">DPO</a>. You can read the technical report&nbsp;<a href="https://arxiv.org/pdf/2407.10759?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">here</a>.</li><li><strong>Results:</strong>&nbsp;Qwen2-Audio outperformed previous state-of-the-art models in benchmarks that evaluate speech recognition (<a href="https://ieeexplore.ieee.org/document/7178964?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Librispeech</a>,&nbsp;<a href="https://arxiv.org/abs/1808.10583?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">AISHELL-2</a>,&nbsp;<a href="https://arxiv.org/abs/2205.12446?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">FLEURS-ZH</a>), speech-to-text translation (<a href="https://arxiv.org/abs/2007.10310?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">CoVoST2</a>), and audio classification (<a href="https://arxiv.org/abs/2205.03433?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Vocalsound</a>) as well as&nbsp;<a href="https://arxiv.org/abs/2402.07729?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">AIR-Bench</a>&nbsp;tests for evaluating interpretation of speech, music, sound, and mixed-audio soundscapes.</li></ul><p><strong>Why it matters:</strong>&nbsp;Qwen2 delivered extraordinary performance with open weights, putting Alibaba on the map of large language models (LLMs). These specialized additions to the family push forward math performance and audio integration in AI while delivering state-of-the-art models into the hands of more developers.&nbsp;</p><p><strong>We’re thinking:</strong>&nbsp;It’s thrilling to see models with open weights that outperform proprietary models. The white-hot competition between open and closed technology is good for everyone!</p><hr><figure class="kg-card kg-image-card"><img src="https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142304.320.png" class="kg-image" alt="" loading="lazy" width="1200" height="675" srcset="https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed---2024-08-21T142304.320.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed---2024-08-21T142304.320.png 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142304.320.png 1200w" sizes="(min-width: 720px) 720px"></figure><h1 id="scaling-laws-for-data-quality">Scaling Laws for Data Quality</h1><p>When training vision-language models, developers often remove lower-quality examples from the training set. But keeping only the highest-quality examples may not be ideal, researchers found.</p><p><strong>What's new:</strong>&nbsp;Sachin Goyal, Pratyush Maini, and colleagues at Carnegie Mellon University derived&nbsp;<a href="https://arxiv.org/abs/2404.07177?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">scaling laws for filtering data</a>&nbsp;that describe how the utility of examples — in terms of how much they increase performance (or decrease loss) — falls when they are used over and over again in training.</p><p><strong>Key insight:</strong>&nbsp;When computational resources are limited relative to the amount of data available, some AI developers try to select the highest-quality examples and train on them for multiple iterations. However, the utility of examples declines a little bit every time they’re used. As computational resources rise, it’s better to introduce new examples even if they’re of slightly lower quality.&nbsp;</p><p><strong>How it works:</strong>&nbsp;The authors used 128 million text-image pairs from&nbsp;<a href="https://arxiv.org/abs/2304.14108?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">DataComp</a>&nbsp;to train various&nbsp;<a href="https://arxiv.org/abs/2103.00020?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">CLIP</a>&nbsp;models, varying the data quality and number of times a model saw each example during training.&nbsp;</p><ul><li>The authors divided the dataset into subsets, each containing 10 percent of the examples, of graduated quality. They evaluated quality according to&nbsp;<a href="https://arxiv.org/abs/2307.03132?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Text Masking and Re-Scoring</a>&nbsp;(T-MARS) scores from a pretrained&nbsp;<a href="https://openai.com/index/clip/?ref=dl-staging-website.ghost.io&utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">CLIP</a>, measuring the similarity between CLIP embeddings of an image and corresponding text.</li><li>They trained a model on each subset, repeating it up to 10 times. Each time the model was trained on a particular subset, they evaluated the model’s error rate on ImageNet classification and fit a scaling curve to the error rates.&nbsp;</li><li>They calculated scaling curves for combinations of subsets (for example, the highest-quality 30 percent of examples) by taking a weighted average of the scaling curves of the individual subsets.&nbsp;</li><li>To verify the scaling curves, the authors trained nine instances of CLIP using the highest-quality 10 percent, 30 percent, or 40 percent examples while presenting 32 million, 128 million, or 640 million examples (including repeats).</li></ul><p><strong>Results:</strong>&nbsp;The authors rated each model’s performance according to the average across 18 visual tasks, mostly involving classification accuracy (including ImageNet). The more examples a model saw, the more its performance benefited from training on lower-quality examples in addition to the highest-quality examples. Of the models that saw 32 million examples, the one trained on the highest-quality 10 percent of examples did best. Of the models that saw 128 million examples, the one trained on the highest-quality 30 percent of examples did the best. Of the models that saw 640 million examples, the one trained on the highest-quality 40 percent of examples did the best. These results confirmed theoretical predictions based on the scaling curves.</p><p><strong>Why it matters:</strong>&nbsp;The practice of pretraining vision-language models on a certain percentage of only the highest-quality examples is not ideal. A better approach is to perform experiments to determine the best percentage given the available compute budget: Train first on a small amount of data and filter for quality according to the scaling curves.</p><p><strong>We're thinking:</strong>&nbsp;This work affirms the fundamental principle of&nbsp;<a href="https://datacentricai.org/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8" rel="noopener">Data-centric AI</a>: Systematically engineering training data is essential for getting optimal performance from a given architecture. However, it shows that using only the highest-quality data works best with smaller compute budgets. With more compute, lower-quality data can improve performance more than repeating the highest-quality examples too many times.</p></div><aside style="grid-column:main-end / wide-end" class="flex-col hidden pl-10 lg:flex"><div class="relative shadow rounded-lg overflow-hidden hover:shadow-sm transition-shadow" data-sentry-component="Advertisement" data-sentry-source-file="Advertisement.tsx"><span style="box-sizing:border-box;display:block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;padding-top:200%"></span><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="responsive" class="rounded-lg" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="" data-sentry-element="Image" data-sentry-source-file="Advertisement.tsx" loading="lazy" decoding="async" data-nimg="responsive" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="rounded-lg" sizes="100vw" srcSet="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=3840&amp;q=75 3840w" src="/_next/image/?url=https%3A%2F%2Fhome-wordpress.deeplearning.ai%2Fwp-content%2Fuploads%2F2025%2F03%2FVertical-side-banner-ads-7.png&amp;w=3840&amp;q=75"/></noscript></span><a href="https://bit.ly/3XWMC3m"><div class="absolute inset-0" data-gtm-event-title="Data Analytics Professional Certificate"></div></a></div></aside><footer class="mt-8 lg:hidden"><div class="flex flex-col items-start items-start " data-sentry-component="MetaItem" data-sentry-source-file="MetaItem.tsx"><div class="flex items-center text-slate-400"><div class="text-sm undefined"><svg stroke="currentColor" fill="none" stroke-width="2" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" y1="13.51" x2="15.42" y2="17.49"></line><line x1="15.41" y1="6.51" x2="8.59" y2="10.49"></line></svg></div><div class="ml-2 text-sm undefined">Share</div></div><div class="mt-1 text-slate-600 text-base undefined"><ul class="list-none m-0 grid grid-cols-3 gap-3 items-center mt-1" data-sentry-component="ShareIcons" data-sentry-source-file="ShareIcons.tsx"><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://twitter.com/intent/tweet?url=https://www.deeplearning.ai/the-batch/issue-263/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.facebook.com/sharer/sharer.php?u=https://www.deeplearning.ai/the-batch/issue-263/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a></li><li class="text-lg text-slate-500 hover:text-slate-600 transition-colors" data-sentry-component="ShareIcon" data-sentry-source-file="ShareIcons.tsx"><a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.deeplearning.ai/the-batch/issue-263/" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a></li></ul></div></div></footer></div></article><div class="py-16 bg-slate-50"><section id="subscribe" data-sentry-component="CtaNewsletter" data-sentry-source-file="CtaNewsletter.tsx"><div class="container--boxed relative"><div class="text-center"><h2 class="text--l2 text-slate-900">Subscribe to The Batch</h2><p class="text-base lg:text-lg text-slate-500 mt-3 max-w-md mx-auto">Stay updated with weekly AI News and Insights delivered to your inbox</p></div><div class="flex flex-col items-center mt-9"><div><div id="reactHubspotForm75" style="display:none"></div><div class="flex items-center justify-center"><svg class="animate-spin -ml-1 mr-3 h-5 w-5 text-brand-teal" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" data-sentry-element="svg" data-sentry-component="Spinner" data-sentry-source-file="Spinner.tsx"><circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4" data-sentry-element="circle" data-sentry-source-file="Spinner.tsx"></circle><path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z" data-sentry-element="path" data-sentry-source-file="Spinner.tsx"></path></svg></div></div></div></div></section></div></div></main><footer class="py-16 bg-brand-teal-900" data-sentry-component="Footer" data-sentry-source-file="index.tsx"><div class="flex flex-col items-center justify-center text-center container--boxed"><div class="max-w-[220px] flex items-center justify-center text-white"><svg viewBox="0 0 272 34" fill="none" xmlns="http://www.w3.org/2000/svg" width="272" height="34" aria-label="DeepLearning.AI" data-sentry-element="svg" data-sentry-component="DLAILogo" data-sentry-source-file="DLAILogo.tsx"><g fill="currentColor" data-sentry-element="g" data-sentry-source-file="DLAILogo.tsx"><path d="M56.775 16.108c0 6.206-4.252 10.005-10.747 10.005H39.42V5.961h6.608c6.495 0 10.747 3.924 10.747 10.147zm-10.747 7.306c4.778 0 7.337-2.724 7.337-7.306 0-4.581-2.56-7.452-7.337-7.452H42.73v14.758h3.298z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M66.97 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.259-8.226 8.006-8.226c4.571 0 7.804 3.159 7.804 7.856.007.535-.027 1.07-.103 1.6H62.412c.233 2.638 2.123 4.232 4.57 4.232 2.038 0 3.173-.988 3.786-2.234h3.582c-.915 2.802-3.448 5.032-7.38 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.484-4.03-2.24 0-4.044 1.525-4.394 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M84.938 26.371c-4.601 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.792 9.792 0 01-.116 1.6H80.367c.233 2.638 2.128 4.232 4.57 4.232 2.042 0 3.177-.988 3.786-2.234h3.582c-.902 2.802-3.435 5.032-7.367 5.032zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M104.917 9.885c4.221 0 7.541 3.245 7.541 8.166 0 4.92-3.32 8.32-7.541 8.32a7.277 7.277 0 01-3.095-.664 7.248 7.248 0 01-2.516-1.914v9.915h-3.319v-23.57h3.32v2.347a7.004 7.004 0 015.61-2.6zm-.729 2.871c-2.473 0-4.86 1.943-4.86 5.364 0 3.42 2.387 5.39 4.86 5.39 2.473 0 4.894-2.02 4.894-5.46 0-3.437-2.391-5.303-4.894-5.303v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M119.074 5.961v17.484h6.841v2.668h-10.16V5.961h3.319z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M135.584 26.371c-4.602 0-8.007-3.244-8.007-8.26s3.263-8.226 8.007-8.226c4.57 0 7.803 3.159 7.803 7.856a9.927 9.927 0 01-.116 1.625h-12.258c.233 2.639 2.128 4.233 4.571 4.233 2.041 0 3.176-.988 3.785-2.235h3.582c-.902 2.777-3.435 5.007-7.367 5.007zm-4.541-9.683h8.878c-.056-2.462-2.007-4.03-4.48-4.03-2.244 0-4.048 1.525-4.398 4.03z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M153.116 9.885a6.864 6.864 0 013.088.633 6.839 6.839 0 012.475 1.946v-2.325h3.349v15.974h-3.349v-2.38a6.906 6.906 0 01-5.611 2.638c-4.165 0-7.514-3.39-7.514-8.32s3.353-8.166 7.562-8.166zm.699 2.87c-2.473 0-4.86 1.853-4.86 5.304 0 3.451 2.387 5.45 4.86 5.45 2.473 0 4.864-1.938 4.864-5.39 0-3.45-2.361-5.372-4.864-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M169.72 26.114h-3.319V10.139h3.319v2.325c.928-1.595 2.534-2.579 4.804-2.579v3.438h-.863c-2.447 0-3.958 1.014-3.958 4.405l.017 8.386z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M188.966 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.044 1.504-4.044 4.435v8.922h-3.319V10.14h3.319v1.826a6.16 6.16 0 014.774-2.08c3.755 0 6.582 2.347 6.582 6.812v9.416h-3.293v-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M196.049 5.905a2.105 2.105 0 011.309-1.931 2.117 2.117 0 012.294.458 2.102 2.102 0 01-1.48 3.588 2.109 2.109 0 01-1.509-.612 2.08 2.08 0 01-.614-1.503zm.431 4.234h3.319v15.974h-3.319V10.14z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M215.558 17.191c0-2.93-1.602-4.435-4.019-4.435s-4.048 1.504-4.048 4.435v8.922h-3.337V10.14h3.319v1.826a6.192 6.192 0 014.796-2.08c3.755 0 6.56 2.338 6.56 6.803v9.425h-3.289l.018-8.922z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M229.538 9.885c2.62 0 4.571 1.216 5.559 2.579v-2.325h3.349V26.37c0 4.35-2.823 7.629-7.829 7.629-4.282 0-7.454-2.119-7.864-5.656h3.289c.496 1.655 2.274 2.785 4.575 2.785 2.559 0 4.48-1.569 4.48-4.758v-2.664a6.903 6.903 0 01-5.559 2.665c-4.222 0-7.571-3.392-7.571-8.321 0-4.93 3.337-8.166 7.571-8.166zm.699 2.87c-2.478 0-4.864 1.853-4.864 5.304 0 3.452 2.386 5.45 4.864 5.45 2.477 0 4.86-1.938 4.86-5.39 0-3.45-2.357-5.372-4.86-5.372v.009z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M242.702 26.316a2.134 2.134 0 01-1.987-1.287 2.114 2.114 0 011.53-2.908 2.138 2.138 0 012.194.896c.235.349.361.76.361 1.18a2.096 2.096 0 01-1.289 1.956 2.11 2.11 0 01-.809.163z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M260.584 21.996h-8.477l-1.455 4.117h-3.453l7.251-20.2h3.846l7.247 20.2h-3.492l-1.467-4.117zM256.38 9.932l-3.341 9.365h6.612l-3.271-9.365z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path d="M268.681 5.961H272v20.152h-3.319V5.961z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218zm.126-1.59c-3.734 0-6.76-3.207-6.76-7.16 0-3.954 3.018-7.16 6.75-7.16 3.734 0 6.76 3.206 6.76 7.16s-3.021 7.16-6.76 7.16h.01zm-.126-6.28c.729 0 1.44-.214 2.046-.617a3.67 3.67 0 001.356-1.646 3.652 3.652 0 00-.798-3.995 3.687 3.687 0 00-4.012-.794 3.679 3.679 0 00-1.653 1.35 3.655 3.655 0 00-.62 2.037c.002.971.39 1.902 1.08 2.59a3.698 3.698 0 002.601 1.076z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73zm-.125-2.918c-6.289 0-11.386-4.925-11.386-11.002C5.257 6.52 10.36 1.59 16.643 1.59c6.284 0 11.386 4.93 11.386 11.007s-5.097 11.002-11.386 11.002zm-.242-4.508c4.77 0 8.633-3.679 8.633-8.218 0-4.538-3.885-8.221-8.633-8.221-4.747 0-8.632 3.679-8.632 8.221 0 4.543 3.885 8.218 8.632 8.218z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M16.643 33.145c-3.292 0-6.51-.972-9.246-2.793a16.588 16.588 0 01-6.13-7.438A16.507 16.507 0 01.32 13.34a16.55 16.55 0 014.555-8.485A16.665 16.665 0 0113.396.318a16.71 16.71 0 019.616.944 16.628 16.628 0 017.47 6.103 16.522 16.522 0 012.804 9.207c0 4.396-1.753 8.61-4.874 11.719a16.68 16.68 0 01-11.769 4.854zm.125-6.628c6.906 0 12.517-5.698 12.517-12.73 0-7.03-5.61-12.725-12.517-12.725-6.906 0-12.517 5.698-12.517 12.725 0 7.027 5.611 12.73 12.517 12.73z" data-sentry-element="path" data-sentry-source-file="DLAILogo.tsx"></path></g></svg></div><nav class="mt-6 md:mt-10"><ul class="flex flex-wrap justify-center space-x-8 gap-x-8"><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/courses/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Courses</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/the-batch/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">The Batch</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/community/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Community</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/careers/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">Careers</div></a></li><li class="mt-4 md:mt-0 !ml-0 flex-1 sm:flex-auto"><a href="/about/"><div class="text-xl text-brand hover:underline whitespace-nowrap ">About</div></a></li></ul></nav><div class="flex mt-12"><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.facebook.com/1027125564106325" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsFacebook" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.instagram.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsInstagram" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C5.829 0 5.556.01 4.703.048 3.85.088 3.269.222 2.76.42a3.9 3.9 0 0 0-1.417.923A3.9 3.9 0 0 0 .42 2.76C.222 3.268.087 3.85.048 4.7.01 5.555 0 5.827 0 8.001c0 2.172.01 2.444.048 3.297.04.852.174 1.433.372 1.942.205.526.478.972.923 1.417.444.445.89.719 1.416.923.51.198 1.09.333 1.942.372C5.555 15.99 5.827 16 8 16s2.444-.01 3.298-.048c.851-.04 1.434-.174 1.943-.372a3.9 3.9 0 0 0 1.416-.923c.445-.445.718-.891.923-1.417.197-.509.332-1.09.372-1.942C15.99 10.445 16 10.173 16 8s-.01-2.445-.048-3.299c-.04-.851-.175-1.433-.372-1.941a3.9 3.9 0 0 0-.923-1.417A3.9 3.9 0 0 0 13.24.42c-.51-.198-1.092-.333-1.943-.372C10.443.01 10.172 0 7.998 0zm-.717 1.442h.718c2.136 0 2.389.007 3.232.046.78.035 1.204.166 1.486.275.373.145.64.319.92.599s.453.546.598.92c.11.281.24.705.275 1.485.039.843.047 1.096.047 3.231s-.008 2.389-.047 3.232c-.035.78-.166 1.203-.275 1.485a2.5 2.5 0 0 1-.599.919c-.28.28-.546.453-.92.598-.28.11-.704.24-1.485.276-.843.038-1.096.047-3.232.047s-2.39-.009-3.233-.047c-.78-.036-1.203-.166-1.485-.276a2.5 2.5 0 0 1-.92-.598 2.5 2.5 0 0 1-.6-.92c-.109-.281-.24-.705-.275-1.485-.038-.843-.046-1.096-.046-3.233s.008-2.388.046-3.231c.036-.78.166-1.204.276-1.486.145-.373.319-.64.599-.92s.546-.453.92-.598c.282-.11.705-.24 1.485-.276.738-.034 1.024-.044 2.515-.045zm4.988 1.328a.96.96 0 1 0 0 1.92.96.96 0 0 0 0-1.92m-4.27 1.122a4.109 4.109 0 1 0 0 8.217 4.109 4.109 0 0 0 0-8.217m0 1.441a2.667 2.667 0 1 1 0 5.334 2.667 2.667 0 0 1 0-5.334"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://twitter.com/deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsTwitter" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334q.002-.211-.006-.422A6.7 6.7 0 0 0 16 3.542a6.7 6.7 0 0 1-1.889.518 3.3 3.3 0 0 0 1.447-1.817 6.5 6.5 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.32 9.32 0 0 1-6.767-3.429 3.29 3.29 0 0 0 1.018 4.382A3.3 3.3 0 0 1 .64 6.575v.045a3.29 3.29 0 0 0 2.632 3.218 3.2 3.2 0 0 1-.865.115 3 3 0 0 1-.614-.057 3.28 3.28 0 0 0 3.067 2.277A6.6 6.6 0 0 1 .78 13.58a6 6 0 0 1-.78-.045A9.34 9.34 0 0 0 5.026 15"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.linkedin.com/company/18246783" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsLinkedin" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854zm4.943 12.248V6.169H2.542v7.225zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248S2.4 3.226 2.4 3.934c0 .694.521 1.248 1.327 1.248zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016l.016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225z"></path></svg></a><a rel="noopener noreferrer" target="_blank" class="mx-3 text-2xl text-white transition-colors lg:text-3xl hover:text-white" href="https://www.youtube.com/c/Deeplearningai" data-sentry-element="SocialLink" data-sentry-source-file="index.tsx" data-sentry-component="SocialLink"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16" data-sentry-element="BsYoutube" data-sentry-source-file="index.tsx" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.01 2.01 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.01 2.01 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31 31 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.01 2.01 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A100 100 0 0 1 7.858 2zM6.4 5.209v4.818l4.157-2.408z"></path></svg></a></div></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"cmsData":{"settings":{"processEnv":{"siteUrl":"https://www.deeplearning.ai","platform":"vercel","darkMode":{"defaultMode":"light","overrideOS":true},"nextImages":{"feature":true,"inline":false,"quality":80,"source":false},"rssFeed":true,"memberSubscriptions":false,"commenting":{"system":null,"commentoUrl":"https://cdn.commento.io","disqusShortname":"short-name-here"},"prism":{"enable":true,"ignoreMissing":true},"toc":{"enable":false,"maxDepth":2},"isr":{"enable":false,"revalidate":10,"maxNumberOfPosts":20,"maxNumberOfPages":20},"algoliaEnv":"production"},"title":"The Batch | DeepLearning.AI","description":"Weekly AI news for engineers, executives, and enthusiasts.","logo":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","icon":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","accent_color":"#F65B66","cover_image":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","facebook":"DeepLearningAIHQ/","twitter":"@DeepLearningAI","locale":"en","timezone":"America/Los_Angeles","codeinjection_head":null,"codeinjection_foot":null,"navigation":[],"secondary_navigation":[],"meta_title":"The Batch | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-1.png","og_title":"The Batch | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter.png","twitter_title":"The Batch | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","members_support_address":"noreply","members_enabled":true,"donations_enabled":false,"allow_self_signup":true,"members_invite_only":false,"members_signup_access":"all","paid_members_enabled":false,"firstpromoter_account":null,"portal_button_style":"icon-and-text","portal_button_signup_text":"Subscribe","portal_button_icon":"icon-3","portal_signup_terms_html":null,"portal_signup_checkbox_required":false,"portal_plans":["free","monthly","yearly"],"portal_default_plan":"yearly","portal_name":false,"portal_button":true,"comments_enabled":"off","recommendations_enabled":false,"outbound_link_tagging":true,"default_email_address":"dl-staging-website@ghost.io","support_email_address":"noreply@dl-staging-website.ghost.io","editor_default_email_recipients":"disabled","labs":{},"lang":"en","url":"https://dl-staging-website.ghost.io","version":"5.121","iconImage":{"url":"https://dl-staging-website.ghost.io/content/images/size/w256h256/2021/05/logo.png","dimensions":{"width":256,"height":256}},"logoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/LogoFiles_DeepLearning_PrimaryLogo.png","dimensions":{"width":2677,"height":601}},"coverImage":{"url":"https://dl-staging-website.ghost.io/content/images/2021/04/v2osk-Ovn1hyBge38-unsplash.jpg","dimensions":{"width":2000,"height":1335}}},"post":{"slug":"issue-263","id":"66c63a632786f7000196beac","uuid":"6f49fc31-ccc0-4a9a-92a4-80419eef1617","title":"AI Agents Generate Novel Research, Google Imagen 3 Raises the Bar, Alibaba’s Open Models for Specialized Tasks, Scaling Laws for Data Quality","html":"\u003cp\u003eDear friends,\u003c/p\u003e\u003cp\u003eI’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications. Two examples are the new Federal Trade Commission (FTC)\u0026nbsp;\u003ca href=\"https://www.ftc.gov/news-events/news/press-releases/2024/08/federal-trade-commission-announces-final-rule-banning-fake-reviews-testimonials?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eban on fake product reviews\u003c/a\u003e\u0026nbsp;and the\u0026nbsp;\u003ca href=\"https://www.durbin.senate.gov/imo/media/doc/defiance_act_of_2024.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eDEFIANCE Act\u003c/a\u003e, which imposes punishments for creating and disseminating non-consensual deepfake porn. Both rules take a sensible approach to regulating AI insofar as they target harmful applications rather than general-purpose AI technology.\u003c/p\u003e\u003cp\u003eAs I\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/blenders-versus-bombs-or-why-californias-proposed-ai-law-is-bad-for-everyone/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003edescribed\u003c/a\u003e\u0026nbsp;previously, the best way to ensure AI safety is to regulate it at the application level rather than the technology level. This is important because the technology is general-purpose and its builders (such as a developer who releases an open-weights foundation model) cannot control how someone else might use it. If, however, someone applies AI in a nefarious way, we should stop that application.\u0026nbsp;\u003c/p\u003e\u003cp\u003eEven before generative AI, fake reviews were a problem on many websites, and many tech companies dedicate considerable resources to combating them. A telltale sign of old-school fake reviews is the use of similar wording in different reviews. AI’s ability to automatically paraphrase or rewrite is making fake reviews harder to detect.\u003c/p\u003e\u003cp\u003eImportantly, the FTC is not going after the makers of foundation models for fake reviews. The provider of an open weights AI model, after all, can’t control what someone else uses it for. Even if one were to try to train a model to put up guardrails against writing reviews, I don’t know how it could distinguish between a real user of a product asking for help writing a legitimate review and a spammer who wanted a fake review. The FTC appropriately aims to ban the application of fake reviews along with other deceptive practices such as buying positive reviews.\u0026nbsp;\u003c/p\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80--1.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed--80--1.jpg 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed--80--1.jpg 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80--1.jpg 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003cp\u003eThe DEFIANCE Act, which passed unanimously in the Senate (and still requires passage in the House of Representatives before the President can sign it into law) imposes civil penalties for the creating and distributing non-consensual, deepfake porn. This disgusting application is\u0026nbsp;\u003ca href=\"https://www.nytimes.com/2024/04/08/technology/deepfake-ai-nudes-westfield-high-school.html?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eharming\u003c/a\u003e\u0026nbsp;many people including underage girls. While many image generation models do have guardrails against generating porn, these guardrails often can be circumvented via jailbreak prompts or fine-tuning (for models with open weights).\u003c/p\u003e\u003cp\u003eAgain, DEFIANCE regulates an application, not the underlying technology. It aims to punish people who engage in the application of creating and distributing non-consensual intimate images, regardless of how they are generated — whether the perpetrator uses a diffusion model, a generative adversarial network, or Microsoft Paint to create an image pixel by pixel.\u003c/p\u003e\u003cp\u003eI hope DEFIANCE passes in the House and gets signed into law. Both rules guard against harmful AI applications without stifling AI technology itself (unlike California’s\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/californias-proposed-ai-safety-law-puts-developers-at-risk-california-sb-1047-is-intended-to-make-ai-safer-but-its-unclear-requirements-put-developers-innovation-and-open-source-in-jeop/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003epoorly designed\u003c/a\u003e\u0026nbsp;SB-1047), and they offer a good model for how the U.S. and other nations can protect citizens against other potentially harmful applications.\u0026nbsp;\u003cbr\u003e\u003cbr\u003eKeep learning!\u003c/p\u003e\u003cp\u003eAndrew\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003ch2 id=\"a-message-from-deeplearningai\"\u003eA MESSAGE FROM\u0026nbsp;DEEPLEARNING.AI\u003c/h2\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003ca href=\"https://www.deeplearning.ai/short-courses/building-ai-applications-with-haystack?ref=dl-staging-website.ghost.io\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1680\" height=\"945\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png 1000w, https://dl-staging-website.ghost.io/content/images/size/w1600/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png 1600w, https://dl-staging-website.ghost.io/content/images/2024/08/The-Batch-ads-and-exclusive-banners---2024-08-20T091645.073.png 1680w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/a\u003e\u003c/figure\u003e\u003cp\u003eBuild flexible, maintainable applications with our new course, “Building AI Applications with Haystack.” Guided by Tuana Çelik, you’ll build projects like a RAG app and a self-reflecting agent using the Haystack framework.\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/short-courses/building-ai-applications-with-haystack/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eJoin for free\u003c/a\u003e\u003c/p\u003e\u003ch1 id=\"news\"\u003eNews\u003c/h1\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T140739.984.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed---2024-08-21T140739.984.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed---2024-08-21T140739.984.png 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T140739.984.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"ai-agents-for-ai-research\"\u003eAI Agents for AI Research\u003c/h1\u003e\u003cp\u003eWhile some observers argue that large language models can’t produce truly original output, new work prompted them to generate novel scientific research.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Researchers proposed\u0026nbsp;\u003ca href=\"https://www.arxiv.org/abs/2408.06292?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eAI Scientist\u003c/a\u003e, an agentic workflow that directs large language models to generate ideas for AI research, produce code to test them, and document the enquiry. You can see examples of its output and download the code to generate your own papers\u0026nbsp;\u003ca href=\"https://github.com/SakanaAI/AI-Scientist?tab=readme-ov-file\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8#read%20me\" rel=\"noopener\"\u003ehere\u003c/a\u003e. The team included Chris Lu, Cong Lu, Robert Tjarko Lange, and colleagues at Tokyo-based startup Sakana AI, University of Oxford, University of British Columbia, Vector Institute, and the Canadian Institute for Advanced Research.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;The authors used Claude Sonnet 3.5, GPT-4o, DeepSeek Coder, and LLama 3.1 405B to generate papers in three categories: diffusion image modeling, transformer-based language modeling, and “grokking,” which the authors define as generalization and speed of learning in deep neural networks.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe authors prompted a given large language model (LLM) to generate “the next creative and impactful idea for research” in one of the three categories. Then they provided an API to search papers and asked the LLM to either determine whether its idea was novel (in which case it moved to the next step) or, if it couldn’t determine an answer, generate a search query to find related works. Then the authors asked again in light of the search results. They repeated this process until the LLM made a decision.\u003c/li\u003e\u003cli\u003eOnce they had a novel idea, they prompted the LLM to generate a list of experiments and run them using the\u0026nbsp;\u003ca href=\"https://github.com/paul-gauthier/aider?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eAider\u003c/a\u003e\u0026nbsp;Python library. Then they prompted it to generate notes about the results and generate figures by altering an existing Python script.\u003c/li\u003e\u003cli\u003eThey prompted the LLM to generate a paper, one section at a time, given the notes, figures, sections generated so far, and tips on how to write a paper based on an existing\u0026nbsp;\u003ca href=\"https://docs.google.com/document/d/16R1E2ExKUCP5SlXWHr-KzbVDx9DBUclra-EbU8IB-iE/edit?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eguide\u003c/a\u003e. Then they prompted it to search for related works and add relevant citations. Finally, they asked it to remove redundancy, reduce verbosity, and finalize the document’s format.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;The team used GPT-4o to evaluate the generated papers according to the\u0026nbsp;\u003ca href=\"https://neurips.cc/Conferences/2022/ReviewerGuidelines?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eguidelines\u003c/a\u003e\u0026nbsp;for papers presented at the Neural Information Processing Systems (NeurIPS) conference. The guidelines include an overall score between 1 (very strongly reject) and 10 (award-quality: flawless and groundbreaking) and a decision to reject or accept the paper.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eOf the four LLMs, Claude Sonnet 3.5 performed best. Its highest-scoring papers achieved 6 (weak accept). With respect to one of Claude’s works, the authors wrote, “The AI Scientist correctly identifies an interesting and well-motivated direction in diffusion modeling research . . . It proposes a comprehensive experimental plan to investigate its idea, and successfully implements it all, achieving good results.\" The authors provide an archive of Claude’s output\u0026nbsp;\u003ca href=\"https://drive.google.com/drive/folders/1Mmpz6M1FK4q8e-SewgZcUzdeD0Q2zC39?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003ehere\u003c/a\u003e.\u0026nbsp;\u003c/li\u003e\u003cli\u003eGPT-4o ranked second. Its highest-scoring paper achieved 5 (borderline accept).\u003c/li\u003e\u003cli\u003eThe generated papers achieved an average score of 4.05 or less (4 is borderline reject) across all models and categories of experiment. The experiments generally involved small networks that were trained and tested on generated data. The authors note that the system often failed to implement its ideas, sometimes fabricated results, and sometimes failed to cite the most relevant papers, among other issues.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;Agentic workflows are a rising theme in AI research from simpler design patterns like\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003ereflection\u003c/a\u003e\u0026nbsp;to complex workflows for\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2405.11804?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003etranslating literature\u003c/a\u003e. These workflows make it possible to break down complex problems into more manageable subtasks. By breaking the task of conducting AI research into various stages of generating ideas, testing them, and writing a paper, an LLM that has access to the right tools can generate novel research papers with actual experimental results.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;Rather than merely synthesizing existing knowledge, this work points a fascinating direction for using AI to generate new knowledge! Right now, an LLM can suggest starting points for human researchers along with experiments that back up its suggestions.\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142127.807.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed---2024-08-21T142127.807.gif 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed---2024-08-21T142127.807.gif 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142127.807.gif 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"google-imagen-3-raises-the-bar\"\u003eGoogle Imagen 3 Raises the Bar\u003c/h1\u003e\u003cp\u003eImage generation continued its rapid march forward with a new version of Google’s flagship text-to-image model.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;Google\u0026nbsp;\u003ca href=\"https://deepmind.google/technologies/imagen-3/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eintroduced\u003c/a\u003e\u0026nbsp;Imagen 3, a proprietary model that improves upon the previous version’s image quality and prompt adherence, with features like inpainting and outpainting to be added in the future. Imagen 3 is available via Google’s\u0026nbsp;\u003ca href=\"https://aitestkitchen.withgoogle.com/tools/image-fx?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eImageFX\u003c/a\u003e\u0026nbsp;web user interface and\u0026nbsp;\u003ca href=\"https://cloud.google.com/generative-ai-studio?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eVertex AI\u003c/a\u003e\u0026nbsp;Platform. It follows closely upon the releases of Black Forest Labs’ Flux.1 family (open to varying degrees), Midjourney v6.1, and Stability AI Stable Diffusion XL 1 (open weights) — all in the last month.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;The accompanying\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2408.07009?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003epaper\u003c/a\u003e\u0026nbsp;does not describe the model’s architecture and training procedures in detail. The authors trained a diffusion model on an unspecified “large” dataset of images, text, and associated annotations that was filtered to remove unsafe, violent, low-quality, generated, and duplicate images as well as personally identifying information. Google’s Gemini large language model generated some image captions used in training to make their language more diverse.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;Imagen 3 mostly outperformed competing models in head-to-head comparisons based on prompts from datasets including\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2404.01291?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eGenAI-Bench\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2205.11487?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eDrawBench\u003c/a\u003e, and\u0026nbsp;\u003ca href=\"https://cdn.openai.com/papers/dall-e-3.pdf?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eDALL-E 3 Eval\u003c/a\u003e. The team compared Imagen 3 to Midjourney v6.0, OpenAI DALL-E 3, Stable Diffusion 3 Large, and Stable Diffusion XL 1.0. More than 3,000 evaluators from 71 countries rated the models’ responses in side-by-side comparisons. The raters evaluated image quality, preference regardless of the prompt, adherence to the prompt, adherence to a highly detailed prompt, and ability to generate the correct numbers of objects specified in a prompt. Their ratings (between 1 and 5) were used to compute Elo ratings.\u003c/p\u003e\u003cul\u003e\u003cli\u003eImagen 3 swept the overall preference tests. On GenAI-Bench and DrawBench, Imagen 3 (1,099 Elo and 1,068 Elo respectively) beat the next-best Stable Diffusion 3 (1,047 Elo and 1,053 Elo respectively). On DALL-E 3 Eval, Imagen 3 (1,079 Elo) beat the next-best MidJourney v6.0 (1,068 Elo).\u003c/li\u003e\u003cli\u003eLikewise, Imagen 3 swept the prompt-image alignment benchmarks. On GenAI-Bench and DrawBench, Imagen 3 (1,083 Elo and 1,064 Elo respectively) outperformed the next-best Stable Diffusion 3 (1,047 Elo for both datasets). On DALL-E 3 Eval, Imagen 3 (1,078) narrowly edged out DALL-E 3 (1,077 Elo) and Stable Diffusion 3 (1,069 Elo).\u003c/li\u003e\u003cli\u003eImagen 3 showed exceptional strength in following detailed prompts in the\u0026nbsp;\u003ca href=\"http://arxiv.org/abs/2404.19753?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eDOCCI\u003c/a\u003e\u0026nbsp;dataset (photographs with detailed descriptions that averaged 136 words). In that category, Imagen 3 (1,193 Elo) outperformed next-best Midjourney v6.0 (1,079 Elo).\u003c/li\u003e\u003cli\u003eAlthough none of the models tested did very well at generating specified numbers of objects from the\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2406.14774?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eGeckoNum\u003c/a\u003e\u0026nbsp;dataset, Imagen 3 (58.6 Elo) outperformed the next-best DALL-E 3 (46.0 Elo).\u003c/li\u003e\u003cli\u003eImagen 3 lost to Midjourney v6.0 across the board in tests of visual appeal regardless of the prompt. It was slightly behind on GenAI-Bench (1,095 Elo versus 1,101 Elo), farther behind on DrawBench (1,063 Elo versus 1,075 Elo), and well behind on DALL-E 3 Eval (1,047 Elo versus 1,095 Elo).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;Each wave of advances makes image generators more useful for a wider variety of purposes. Google’s emphasis on filtering the training data for safety may limit Imagen 3’s utility in some situations (indeed, some users\u0026nbsp;\u003ca href=\"https://www.reddit.com/r/Bard/comments/1eo3ge9/imagen_3_is_available_for_everyone_google_is/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003ecomplained\u003c/a\u003e\u0026nbsp;that Imagen 3 is more restrictive than Imagen 2, while the Grok2 large language model’s use of an unguardrailed version of Flux.1 for image generation has garnered\u0026nbsp;\u003ca href=\"https://www.yahoo.com/tech/grok-2-0-takes-guardrails-175824863.html?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eheadlines\u003c/a\u003e). Nonetheless, precautions are an important ingredient in the evolving text-to-image recipe.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;It’s difficult to compare the benchmarks reported for Imagen 3 and the recently released\u0026nbsp;\u003ca href=\"https://www.deeplearning.ai/the-batch/black-forest-labs-flux-1-outperforms-top-text-to-image-models/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eFlux.1\u003c/a\u003e, which claims similar improvements over earlier models. In any case, Google has yet to publish a benchmark for generating text, a valuable capability for commercial applications. The Flux.1 models, two of which are open to some degree, may prove to be formidable rivals in this area.\u0026nbsp;\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142223.196.gif\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"676\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed---2024-08-21T142223.196.gif 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed---2024-08-21T142223.196.gif 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142223.196.gif 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"open-models-for-math-and-audio\"\u003eOpen Models for Math and Audio\u003c/h1\u003e\u003cp\u003eAlibaba followed up its open-weights Qwen2 large language models with specialized variations.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat’s new:\u003c/strong\u003e\u0026nbsp;\u003ca href=\"https://qwenlm.github.io/blog/qwen2-math/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eQwen2-Math\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://qwenlm.github.io/blog/qwen2-audio/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eQwen2-Audio\u003c/a\u003e\u0026nbsp;are model families devoted to, respectively, solving math problems and generating text directly from audio. Both set new states of the art in a variety of English and Chinese benchmarks, and some versions offer open weights. Notably Qwen2-Math-Instruct-72B, whose 72 billion parameters are fine-tuned according to human preferences, outperformed top models including Claude 3.5 Sonnet, Gemini 1.5-Pro, GPT-4o, and Llama-3.1-405B on some math benchmarks.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eMath mavens:\u003c/strong\u003e\u0026nbsp;Qwen2-Math models include\u0026nbsp;\u003ca href=\"https://github.com/QwenLM/Qwen2-Math?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003epretrained\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://huggingface.co/Qwen/Qwen2-Math-72B-Instruct?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003einstruction-tuned\u003c/a\u003e\u0026nbsp;variations that comprise 1.5 billion, 7 billion, and 72 billion parameters. The\u0026nbsp;\u003ca href=\"https://huggingface.co/Qwen/Qwen2-Math-72B-Instruct/blob/main/LICENSE?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003elicense\u003c/a\u003e\u0026nbsp;for the largest version is free for noncommerical development and commercial developers who have less than 100 million monthly active users.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;Qwen2-Math base models were initialized to Qwen2 weights and further pretrained on a corpus of math articles, books, exams, and data generated by Qwen2. The instruction-tuned versions were fine-tuned on more model-generated data using supervised learning followed by a reinforcement learning algorithm called\u0026nbsp;\u003ca href=\"https://arxiv.org/pdf/2402.03300?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003egroup relative policy optimization\u003c/a\u003e. The team removed examples that significantly overlapped benchmark test sets and prominent math competitions.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;Using few-shot, chain-of-thought prompting, Qwen2-Math-Instruct-72B achieved state-of-the-art performance in English math benchmarks including\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2103.03874?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eMATH\u003c/a\u003e\u0026nbsp;and Chinese math benchmarks including\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2306.16636?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eCMATH\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://arxiv.org/pdf/2304.06364?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eGaoKao Math Cloze, and GaoKao Math QA\u003c/a\u003e. (The 72 billion-parameter Qwen2-Math achieved state-of-the-art scores in\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2110.14168?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eGSM8k\u003c/a\u003e\u0026nbsp;and\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2009.03300?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eMMLU STEM\u003c/a\u003e.) Qwen2-Math-Instruct-72B also outperformed Claude 3 Opus, GPT-4 Turbo, Gemini 1.5 Pro and Gemini Math-Specialized 1.5 Pro in the\u0026nbsp;\u003ca href=\"https://aime24.aimedicine.info/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eAIME 2024\u003c/a\u003e\u0026nbsp;math competition in some settings. The smaller, instruction-tuned versions outperformed other models of the same size by some measures.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eAudio/text to text:\u003c/strong\u003e\u0026nbsp;A revision of the earlier Qwen-Audio,\u0026nbsp;\u003ca href=\"https://github.com/QwenLM/Qwen2-Audio?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eQwen2-Audio\u003c/a\u003e\u0026nbsp;takes text and audio inputs and generates text outputs. It’s designed to (i) provide text chat in response to voice input including voice transcription and translation between eight languages and (ii) discuss audio input including voice, music, and natural sounds. Weights (8.2 billion parameters) are available for base and instruction-tuned versions. You can try it\u0026nbsp;\u003ca href=\"https://huggingface.co/spaces/Qwen/Qwen2-Audio-Instruct-Demo?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;Given a text prompt and audio, a\u0026nbsp;\u003ca href=\"https://github.com/openai/whisper?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eWhisperlarge-v3\u003c/a\u003e\u0026nbsp;audio encoder embeds the audio, and a pretrained Qwen-7B language model uses the text prompt and audio embedding to generate text. The team further pretrained the system to predict the next text token based on a text-audio dataset that included 370,000 hours of recorded speech, 140,000 hours of music, and 10,000 hours of other sounds. They fine-tuned the system for chat in a supervised fashion and for factuality and prompt adherence using\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2305.18290?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eDPO\u003c/a\u003e. You can read the technical report\u0026nbsp;\u003ca href=\"https://arxiv.org/pdf/2407.10759?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;Qwen2-Audio outperformed previous state-of-the-art models in benchmarks that evaluate speech recognition (\u003ca href=\"https://ieeexplore.ieee.org/document/7178964?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eLibrispeech\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/1808.10583?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eAISHELL-2\u003c/a\u003e,\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2205.12446?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eFLEURS-ZH\u003c/a\u003e), speech-to-text translation (\u003ca href=\"https://arxiv.org/abs/2007.10310?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eCoVoST2\u003c/a\u003e), and audio classification (\u003ca href=\"https://arxiv.org/abs/2205.03433?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eVocalsound\u003c/a\u003e) as well as\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2402.07729?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eAIR-Bench\u003c/a\u003e\u0026nbsp;tests for evaluating interpretation of speech, music, sound, and mixed-audio soundscapes.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;Qwen2 delivered extraordinary performance with open weights, putting Alibaba on the map of large language models (LLMs). These specialized additions to the family push forward math performance and audio integration in AI while delivering state-of-the-art models into the hands of more developers.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe’re thinking:\u003c/strong\u003e\u0026nbsp;It’s thrilling to see models with open weights that outperform proprietary models. The white-hot competition between open and closed technology is good for everyone!\u003c/p\u003e\u003chr\u003e\u003cfigure class=\"kg-card kg-image-card\"\u003e\u003cimg src=\"https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142304.320.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"675\" srcset=\"https://dl-staging-website.ghost.io/content/images/size/w600/2024/08/unnamed---2024-08-21T142304.320.png 600w, https://dl-staging-website.ghost.io/content/images/size/w1000/2024/08/unnamed---2024-08-21T142304.320.png 1000w, https://dl-staging-website.ghost.io/content/images/2024/08/unnamed---2024-08-21T142304.320.png 1200w\" sizes=\"(min-width: 720px) 720px\"\u003e\u003c/figure\u003e\u003ch1 id=\"scaling-laws-for-data-quality\"\u003eScaling Laws for Data Quality\u003c/h1\u003e\u003cp\u003eWhen training vision-language models, developers often remove lower-quality examples from the training set. But keeping only the highest-quality examples may not be ideal, researchers found.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhat's new:\u003c/strong\u003e\u0026nbsp;Sachin Goyal, Pratyush Maini, and colleagues at Carnegie Mellon University derived\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2404.07177?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003escaling laws for filtering data\u003c/a\u003e\u0026nbsp;that describe how the utility of examples — in terms of how much they increase performance (or decrease loss) — falls when they are used over and over again in training.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eKey insight:\u003c/strong\u003e\u0026nbsp;When computational resources are limited relative to the amount of data available, some AI developers try to select the highest-quality examples and train on them for multiple iterations. However, the utility of examples declines a little bit every time they’re used. As computational resources rise, it’s better to introduce new examples even if they’re of slightly lower quality.\u0026nbsp;\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow it works:\u003c/strong\u003e\u0026nbsp;The authors used 128 million text-image pairs from\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2304.14108?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eDataComp\u003c/a\u003e\u0026nbsp;to train various\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2103.00020?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eCLIP\u003c/a\u003e\u0026nbsp;models, varying the data quality and number of times a model saw each example during training.\u0026nbsp;\u003c/p\u003e\u003cul\u003e\u003cli\u003eThe authors divided the dataset into subsets, each containing 10 percent of the examples, of graduated quality. They evaluated quality according to\u0026nbsp;\u003ca href=\"https://arxiv.org/abs/2307.03132?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eText Masking and Re-Scoring\u003c/a\u003e\u0026nbsp;(T-MARS) scores from a pretrained\u0026nbsp;\u003ca href=\"https://openai.com/index/clip/?ref=dl-staging-website.ghost.io\u0026utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eCLIP\u003c/a\u003e, measuring the similarity between CLIP embeddings of an image and corresponding text.\u003c/li\u003e\u003cli\u003eThey trained a model on each subset, repeating it up to 10 times. Each time the model was trained on a particular subset, they evaluated the model’s error rate on ImageNet classification and fit a scaling curve to the error rates.\u0026nbsp;\u003c/li\u003e\u003cli\u003eThey calculated scaling curves for combinations of subsets (for example, the highest-quality 30 percent of examples) by taking a weighted average of the scaling curves of the individual subsets.\u0026nbsp;\u003c/li\u003e\u003cli\u003eTo verify the scaling curves, the authors trained nine instances of CLIP using the highest-quality 10 percent, 30 percent, or 40 percent examples while presenting 32 million, 128 million, or 640 million examples (including repeats).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eResults:\u003c/strong\u003e\u0026nbsp;The authors rated each model’s performance according to the average across 18 visual tasks, mostly involving classification accuracy (including ImageNet). The more examples a model saw, the more its performance benefited from training on lower-quality examples in addition to the highest-quality examples. Of the models that saw 32 million examples, the one trained on the highest-quality 10 percent of examples did best. Of the models that saw 128 million examples, the one trained on the highest-quality 30 percent of examples did the best. Of the models that saw 640 million examples, the one trained on the highest-quality 40 percent of examples did the best. These results confirmed theoretical predictions based on the scaling curves.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWhy it matters:\u003c/strong\u003e\u0026nbsp;The practice of pretraining vision-language models on a certain percentage of only the highest-quality examples is not ideal. A better approach is to perform experiments to determine the best percentage given the available compute budget: Train first on a small amount of data and filter for quality according to the scaling curves.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eWe're thinking:\u003c/strong\u003e\u0026nbsp;This work affirms the fundamental principle of\u0026nbsp;\u003ca href=\"https://datacentricai.org/?utm_campaign=The%20Batch\u0026utm_source=hs_email\u0026utm_medium=email\u0026_hsenc=p2ANqtz--fHYp_TdGAB9wL4bp4CJGBmNyeAl0abSFzSTtvqHS4DmyrNppST7tT1XPj-lHyIlYFfAs8\" rel=\"noopener\"\u003eData-centric AI\u003c/a\u003e: Systematically engineering training data is essential for getting optimal performance from a given architecture. However, it shows that using only the highest-quality data works best with smaller compute budgets. With more compute, lower-quality data can improve performance more than repeating the highest-quality examples too many times.\u003c/p\u003e","comment_id":"66c63a632786f7000196beac","feature_image":"https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80-.jpg","featured":false,"visibility":"public","created_at":"2024-08-21T12:05:07.000-07:00","updated_at":"2024-09-05T12:54:01.000-07:00","published_at":"2024-08-21T12:25:32.000-07:00","custom_excerpt":"The Batch AI News and Insights: I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications. ","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"authors":[{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"}],"tags":[{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},{"id":"66c63ef52786f7000196bece","name":"issue-263","slug":"issue-263","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/issue-263/"},{"id":"66da0c5a98bf2a0001c498b0","name":"Aug 21, 2024","slug":"aug-21-2024","description":null,"feature_image":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/aug-21-2024/"}],"primary_author":{"id":"1","name":"DeepLearning.AI","slug":"deeplearning-ai","profile_image":"https://dl-staging-website.ghost.io/content/images/2021/06/Icon_Black.png","cover_image":null,"bio":null,"website":"https://www.deeplearning.ai/","location":null,"facebook":"DeepLearningAIHQ","twitter":"@DeepLearningAI_","meta_title":null,"meta_description":null,"threads":null,"bluesky":null,"mastodon":null,"tiktok":null,"youtube":null,"instagram":null,"linkedin":null,"url":"https://dl-staging-website.ghost.io/author/deeplearning-ai/"},"primary_tag":{"id":"60bfddad274d5b003b10621c","name":"The Batch Newsletter","slug":"the-batch","description":"Weekly AI news and perspective for engineers, executives, and enthusiasts.","feature_image":null,"visibility":"public","og_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-3.png","og_title":"The Batch Newsletter | DeepLearning.AI","og_description":"Weekly AI news for engineers, executives, and enthusiasts.","twitter_image":"https://dl-staging-website.ghost.io/content/images/2021/08/thebatchlogotwitter-2.png","twitter_title":"The Batch Newsletter | DeepLearning.AI","twitter_description":"Weekly AI news for engineers, executives, and enthusiasts.","meta_title":"The Batch Newsletter | DeepLearning.AI","meta_description":"Weekly AI news for engineers, executives, and enthusiasts.","codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"url":"https://dl-staging-website.ghost.io/tag/the-batch/"},"url":"https://dl-staging-website.ghost.io/issue-263/","excerpt":"The Batch AI News and Insights: I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications. ","reading_time":13,"access":true,"comments":false,"og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"AI Agents Generate Novel Research, Google Imagen 3 Raises the Bar, Alibaba’s Open Models for Specialized Tasks, Scaling Laws for Data Quality","meta_description":"The Batch AI News and Insights: I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications...","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null},"seoImage":{"url":"https://dl-staging-website.ghost.io/content/images/2024/08/unnamed--80-.jpg","dimensions":{"width":1200,"height":676}},"banner":{"title":"Data Analytics Professional Certificate","databaseId":36316,"id":"cG9zdDozNjMxNg==","featuredImage":{"node":{"altText":"","mediaItemUrl":"https://home-wordpress.deeplearning.ai/wp-content/uploads/2025/03/Vertical-side-banner-ads-7.png"}},"bannerCustomFields":{"bannerUrl":{"url":"https://bit.ly/3XWMC3m","isUrlExternal":null}}},"announcementBanners":{"nodes":[{"title":"Databricks C1","date":"2025-06-04T07:12:29","databaseId":36631,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/3HlCuvP","courseName":"DSPy: Build and Optimize Agentic Apps","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(126deg, rgba(27, 50, 57, 1) 0%, rgba(97, 135, 149, 1) 100%)","isOpenInNewTab":true}},{"title":"Predibase C2","date":"2025-05-21T07:27:34","databaseId":36555,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/4jfKdc6","courseName":"Reinforcement Fine-Tuning LLMs with GRPO","backgroundColor":"#05256C","backgroundGradientColor":"linear-gradient(118deg, rgba(255, 188, 165, 1) 0%, rgba(255, 98, 43, 1) 53%, rgba(255, 98, 43, 1) 100%)","isOpenInNewTab":true}},{"title":"Anthropic C2","date":"2025-05-14T07:50:27","databaseId":36509,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#000000","courseLink":"https://bit.ly/3YJduVg","courseName":"MCP: Build Rich-Context AI Apps with Anthropic","backgroundColor":"#d35050","backgroundGradientColor":"linear-gradient(311deg, rgba(217, 119, 87, 1) 0%, rgba(241, 210, 199, 1) 100%)","isOpenInNewTab":true}},{"title":"LiveKit C1","date":"2025-05-07T08:03:11","databaseId":36482,"announcementBannerCustomFields":{"text":"✨ New course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4m8aEmO","courseName":"Building AI Voice Agents for Production","backgroundColor":"#000000","backgroundGradientColor":null,"isOpenInNewTab":true}},{"title":"MemGPT C1","date":"2025-04-30T01:43:46","databaseId":35628,"announcementBannerCustomFields":{"text":"✨ Updated course! Enroll in","textColor":"#FFFFFF","courseLink":"https://bit.ly/4eb7j0L","courseName":"LLMs as Operating Systems: Agent Memory","backgroundColor":"#8c1d0a","backgroundGradientColor":null,"isOpenInNewTab":true}}]}}},"__N_SSG":true},"page":"/the-batch/[slug]","query":{"slug":"issue-263"},"buildId":"Qy9Eh4N0MrfE3ddReY9v4","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>